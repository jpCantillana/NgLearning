{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric as tg\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GraphConv, global_add_pool\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([[0, 1, 1, 2],\n",
    "                           [1, 0, 2, 1]], dtype=torch.long)\n",
    "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = tg.utils.to_networkx(data1, to_undirected=True)\n",
    "nx.draw(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([[0, 1],\n",
    "                           [1, 0],\n",
    "                           [1, 2],\n",
    "                           [2, 1]], dtype=torch.long)\n",
    "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index.t().contiguous())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to add graphs with our need for Ng sets for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_graph_list = []\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if i != j:\n",
    "            complete_graph_list.append([i,j])\n",
    "edge_index = torch.tensor(complete_graph_list, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_to_check = [[2,6], [3,6], [5,6], [4,6]]\n",
    "dependent_arcs = []\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if i != j:\n",
    "            if [i,j] in edges_to_check:\n",
    "                dependent_arcs.append(1)\n",
    "            else:\n",
    "                dependent_arcs.append(0)\n",
    "y = torch.tensor(dependent_arcs, dtype=torch.long)\n",
    "x = torch.tensor([[0, 3, 2], [1, 1, 6], [2, 3, 6], [3, 5, 6], [4, 6, 5], [5, 4, 4], [6, 3, 4], [7, 1, 3]], dtype=torch.float)\n",
    "attr = [[i] for i in range(len(edge_index))]\n",
    "loc_list = [[0, 3, 2], [1, 1, 6], [2, 3, 6], [3, 5, 6], [4, 6, 5], [5, 4, 4], [6, 3, 4], [7, 1, 3]]\n",
    "loc_dict = {(i[0],j[0]): sqrt((i[1]-j[1])**2 + (i[2]-j[2])**2) if i != j else next for i in loc_list for j in loc_list}\n",
    "cnt = -1\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if i != j:\n",
    "            cnt += 1\n",
    "            attr[cnt].append(loc_dict[i,j])\n",
    "attr = torch.tensor(attr, dtype=torch.long)\n",
    "pos = []\n",
    "for i in loc_list:\n",
    "    pos.append([i[1], i[2]])\n",
    "pos = torch.tensor(pos)\n",
    "\n",
    "data1 = Data(x=x, y=y.t().contiguous(), edge_index=edge_index.t().contiguous(), pos=pos, edge_attr=attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  4],\n",
      "        [ 1,  4],\n",
      "        [ 2,  4],\n",
      "        [ 3,  4],\n",
      "        [ 4,  2],\n",
      "        [ 5,  2],\n",
      "        [ 6,  2],\n",
      "        [ 7,  4],\n",
      "        [ 8,  2],\n",
      "        [ 9,  4],\n",
      "        [10,  5],\n",
      "        [11,  3],\n",
      "        [12,  2],\n",
      "        [13,  3],\n",
      "        [14,  4],\n",
      "        [15,  2],\n",
      "        [16,  2],\n",
      "        [17,  3],\n",
      "        [18,  2],\n",
      "        [19,  2],\n",
      "        [20,  3],\n",
      "        [21,  4],\n",
      "        [22,  4],\n",
      "        [23,  2],\n",
      "        [24,  1],\n",
      "        [25,  2],\n",
      "        [26,  2],\n",
      "        [27,  5],\n",
      "        [28,  4],\n",
      "        [29,  5],\n",
      "        [30,  3],\n",
      "        [31,  1],\n",
      "        [32,  2],\n",
      "        [33,  3],\n",
      "        [34,  5],\n",
      "        [35,  2],\n",
      "        [36,  3],\n",
      "        [37,  2],\n",
      "        [38,  2],\n",
      "        [39,  2],\n",
      "        [40,  1],\n",
      "        [41,  3],\n",
      "        [42,  2],\n",
      "        [43,  2],\n",
      "        [44,  2],\n",
      "        [45,  2],\n",
      "        [46,  3],\n",
      "        [47,  1],\n",
      "        [48,  2],\n",
      "        [49,  2],\n",
      "        [50,  3],\n",
      "        [51,  3],\n",
      "        [52,  5],\n",
      "        [53,  5],\n",
      "        [54,  3],\n",
      "        [55,  2]])\n"
     ]
    }
   ],
   "source": [
    "print(data1.edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_to_check = [[3,1], [2,1]]\n",
    "dependent_arcs = []\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if i != j:\n",
    "            if [i,j] in edges_to_check:\n",
    "                dependent_arcs.append(1)\n",
    "            else:\n",
    "                dependent_arcs.append(0)\n",
    "y = torch.tensor(dependent_arcs, dtype=torch.long)\n",
    "x = torch.tensor([[0, 4, 4], [1, 2, 4], [2, 2, 5], [3, 2, 2], [4, 4, 1], [5, 6, 6], [6, 7, 3], [7, 3, 7]], dtype=torch.float)\n",
    "attr = [[i] for i in range(len(edge_index))]\n",
    "loc_list = [[0, 4, 4], [1, 2, 4], [2, 2, 5], [3, 2, 2], [4, 4, 1], [5, 6, 6], [6, 7, 3], [7, 3, 7]]\n",
    "loc_dict = {(i[0],j[0]): sqrt((i[1]-j[1])**2 + (i[2]-j[2])**2) if i != j else next for i in loc_list for j in loc_list}\n",
    "cnt = -1\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if i != j:\n",
    "            cnt += 1\n",
    "            attr[cnt].append(loc_dict[i,j])\n",
    "attr = torch.tensor(attr, dtype=torch.long)\n",
    "pos = []\n",
    "for i in loc_list:\n",
    "    pos.append([i[1], i[2]])\n",
    "pos = torch.tensor(pos)\n",
    "\n",
    "data2 = Data(x=x, y=y.t().contiguous(), edge_index=edge_index.t().contiguous(), pos=pos, edge_attr=attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_to_check = [[3,7], [5,7]]\n",
    "dependent_arcs = []\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if i != j:\n",
    "            if [i,j] in edges_to_check:\n",
    "                dependent_arcs.append(1)\n",
    "            else:\n",
    "                dependent_arcs.append(0)\n",
    "y = torch.tensor(dependent_arcs, dtype=torch.long)\n",
    "x = torch.tensor([[0, 6, 2], [1, 1, 3], [2, 3, 7], [3, 6, 6], [4, 1, 7], [5, 6, 4], [6, 4, 3], [7, 7, 5]], dtype=torch.float)\n",
    "attr = [[i] for i in range(len(edge_index))]\n",
    "loc_list = [[0, 6, 2], [1, 1, 3], [2, 3, 7], [3, 6, 6], [4, 1, 7], [5, 6, 4], [6, 4, 3], [7, 7, 5]]\n",
    "loc_dict = {(i[0],j[0]): sqrt((i[1]-j[1])**2 + (i[2]-j[2])**2) if i != j else next for i in loc_list for j in loc_list}\n",
    "cnt = -1\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if i != j:\n",
    "            cnt += 1\n",
    "            attr[cnt].append(loc_dict[i,j])\n",
    "attr = torch.tensor(attr, dtype=torch.long)\n",
    "pos = []\n",
    "for i in loc_list:\n",
    "    pos.append([i[1], i[2]])\n",
    "pos = torch.tensor(pos)\n",
    "\n",
    "data3 = Data(x=x, y=y.t().contiguous(), edge_index=edge_index.t().contiguous(), pos=pos, edge_attr=attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_to_check = [[1,7], [5,7], [2,3], [6,3]]\n",
    "dependent_arcs = []\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if i != j:\n",
    "            if [i,j] in edges_to_check:\n",
    "                dependent_arcs.append(1)\n",
    "            else:\n",
    "                dependent_arcs.append(0)\n",
    "y = torch.tensor(dependent_arcs, dtype=torch.long)\n",
    "x = torch.tensor([[0, 4, 3], [1, 1, 1], [2, 6, 6], [3, 7,5], [4, 2, 5], [5, 3, 1], [6, 7,3], [7, 2,2]], dtype=torch.float)\n",
    "attr = [[i] for i in range(len(edge_index))]\n",
    "loc_list = [[0, 4, 3], [1, 1, 1], [2, 6, 6], [3, 7,5], [4, 2, 5], [5, 3, 1], [6, 7,3], [7, 2,2]]\n",
    "loc_dict = {(i[0],j[0]): sqrt((i[1]-j[1])**2 + (i[2]-j[2])**2) if i != j else next for i in loc_list for j in loc_list}\n",
    "cnt = -1\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if i != j:\n",
    "            cnt += 1\n",
    "            attr[cnt].append(loc_dict[i,j])\n",
    "attr = torch.tensor(attr, dtype=torch.long)\n",
    "pos = []\n",
    "for i in loc_list:\n",
    "    pos.append([i[1], i[2]])\n",
    "pos = torch.tensor(pos)\n",
    "\n",
    "data4 = Data(x=x, y=y.t().contiguous(), edge_index=edge_index.t().contiguous(), pos=pos, edge_attr=attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # self.conv1 = GCNConv(3, 2)\n",
    "        # self.conv2 = SAGEConv(2,8)\n",
    "        num_features = 3\n",
    "        dim = 56\n",
    "        features = 56\n",
    "\n",
    "        self.conv1 = GraphConv(num_features, dim)\n",
    "        self.conv2 = GraphConv(dim, dim)\n",
    "        self.conv3 = GraphConv(dim, dim)\n",
    "        self.conv4 = GraphConv(dim, dim)\n",
    "        self.conv5 = GraphConv(dim, dim)\n",
    "\n",
    "        self.lin1 = Linear(dim, dim)\n",
    "        self.lin2 = Linear(dim, 56)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_weight=None):\n",
    "        # x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # x = self.conv1(x, edge_index)\n",
    "        # x = F.relu(x)\n",
    "        # x = F.sigmoid(self.conv2(x, edge_index))\n",
    "        # x = F.dropout(x, training=self.training)\n",
    "        # x = self.conv2(x, edge_index)\n",
    "\n",
    "        x = self.conv1(x, edge_index, edge_weight).relu()\n",
    "        x = self.conv2(x, edge_index, edge_weight).relu()\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "        # return x\n",
    "        # return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    if epoch == 51:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.5 * param_group['lr']\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in dataloader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        # print(output, data.y)\n",
    "        # print(output)\n",
    "        loss = F.l1_loss(output[0], data.y)\n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return loss_all / 4\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "    return correct / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [data1, data2, data3, data4]\n",
    "dataloader = DataLoader(data_list, batch_size=1)\n",
    "data_iter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 109.2643, Train Acc: 2.0000, Test Acc: 2.0000\n",
      "Epoch: 002, Loss: 85.5794, Train Acc: 3.0000, Test Acc: 3.0000\n",
      "Epoch: 003, Loss: 51.6510, Train Acc: 0.5000, Test Acc: 0.5000\n",
      "Epoch: 004, Loss: 36.3253, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 005, Loss: 39.6960, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 006, Loss: 28.4578, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 007, Loss: 15.0359, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 008, Loss: 15.1526, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 009, Loss: 7.6742, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 010, Loss: 5.1370, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 011, Loss: 4.5597, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 012, Loss: 4.8443, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 013, Loss: 4.0847, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 014, Loss: 4.0836, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 015, Loss: 4.0906, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 016, Loss: 4.0819, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 017, Loss: 4.0902, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 018, Loss: 4.0814, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 019, Loss: 4.0814, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 020, Loss: 4.0814, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 021, Loss: 4.0814, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 022, Loss: 4.0814, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 023, Loss: 4.0813, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 024, Loss: 4.0813, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 025, Loss: 4.0813, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 026, Loss: 4.0813, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 027, Loss: 4.0813, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 028, Loss: 4.0813, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 029, Loss: 4.0812, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 030, Loss: 4.0812, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 031, Loss: 4.0812, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 032, Loss: 4.0812, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 033, Loss: 4.0812, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 034, Loss: 4.0811, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 035, Loss: 4.0811, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 036, Loss: 4.0811, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 037, Loss: 4.0811, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 038, Loss: 4.0811, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 039, Loss: 4.0811, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 040, Loss: 4.0810, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 041, Loss: 4.0810, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 042, Loss: 4.0810, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 043, Loss: 4.0810, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 044, Loss: 4.0810, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 045, Loss: 4.0809, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 046, Loss: 4.0809, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 047, Loss: 4.0809, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 048, Loss: 4.0809, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 049, Loss: 4.0809, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 050, Loss: 4.0809, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 051, Loss: 4.0808, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 052, Loss: 4.0808, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 053, Loss: 4.0808, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 054, Loss: 4.0808, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 055, Loss: 4.0808, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 056, Loss: 4.0808, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 057, Loss: 4.0808, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 058, Loss: 4.0808, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 059, Loss: 4.0808, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 060, Loss: 4.0808, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 061, Loss: 4.0807, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 062, Loss: 4.0807, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 063, Loss: 4.0807, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 064, Loss: 4.0807, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 065, Loss: 4.0807, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 066, Loss: 4.0807, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 067, Loss: 4.0807, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 068, Loss: 4.0807, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 069, Loss: 4.0807, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 070, Loss: 4.0807, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 071, Loss: 4.0807, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 072, Loss: 4.0806, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 073, Loss: 4.0806, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 074, Loss: 4.0806, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 075, Loss: 4.0806, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 076, Loss: 4.0806, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 077, Loss: 4.0806, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 078, Loss: 4.0806, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 079, Loss: 4.0806, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 080, Loss: 4.0806, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 081, Loss: 4.0806, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 082, Loss: 4.0806, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 083, Loss: 4.0805, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 084, Loss: 4.0805, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 085, Loss: 4.0805, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 086, Loss: 4.0805, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 087, Loss: 4.0805, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 088, Loss: 4.0805, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 089, Loss: 4.0805, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 090, Loss: 4.0805, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 091, Loss: 4.0805, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 092, Loss: 4.0805, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 093, Loss: 4.0805, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 094, Loss: 4.0804, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 095, Loss: 4.0804, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 096, Loss: 4.0804, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 097, Loss: 4.0804, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 098, Loss: 4.0804, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 099, Loss: 4.0804, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 100, Loss: 4.0804, Train Acc: 0.0000, Test Acc: 0.0000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train(epoch)\n",
    "    train_acc = test(dataloader)\n",
    "    test_acc = test(dataloader)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "          f'Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.1071, -4.1501, -4.0723, -4.0766, -4.0141, -4.0125, -4.0495, -4.0744,\n",
      "         -3.9675, -3.9538, -4.0993, -3.9662, -4.0108, -4.0182, -3.9789, -4.0058,\n",
      "         -4.0230, -4.0914, -3.9619, -4.0678, -4.0535, -3.9740, -3.9900, -4.0338,\n",
      "         -3.9822, -3.9320, -3.9537, -4.0625, -4.0228, -4.0420, -3.9686, -4.0979,\n",
      "         -4.0211, -3.9790, -4.1766, -4.0589, -4.0102, -3.9617, -4.0303, -4.0415,\n",
      "         -3.9674, -3.9588, -4.0272, -4.0858, -4.0073, -4.0519, -4.0711, -4.0162,\n",
      "         -4.0823, -3.9716, -3.9764, -3.9318, -4.0532, -4.0651, -4.0218, -4.1204]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model(data1.x, data1.edge_index, data1.batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([-3.8952], grad_fn=<MaxBackward0>),\n",
      "indices=tensor([43]))\n"
     ]
    }
   ],
   "source": [
    "print(model(data1.x, data1.edge_index, data1.batch).max(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017054583756193526 0 1\n",
      "0.018807433040471482 0 2\n",
      "0.01864622709283721 0 3\n",
      "0.0185163893971816 0 4\n",
      "0.018901960220738468 0 5\n",
      "0.01913803475351945 0 6\n",
      "0.0169980851558266 0 7\n",
      "0.019324264818562746 1 0\n",
      "0.0172632634093058 1 2\n",
      "0.01623241383072122 1 3\n",
      "0.019217034505268488 1 4\n",
      "0.018716506438001275 1 5\n",
      "0.019432408517537848 1 6\n",
      "0.017196665509857295 1 7\n",
      "0.018392704079795236 2 0\n",
      "0.01813460166125799 2 1\n",
      "0.019511106863446582 2 3\n",
      "0.015423137776570301 2 4\n",
      "0.018696017565700843 2 5\n",
      "0.01652251705407729 2 6\n",
      "0.01994147649637608 2 7\n",
      "0.019009712026023782 3 0\n",
      "0.01645280542877164 3 1\n",
      "0.017392912456847313 3 2\n",
      "0.016292868869727594 3 4\n",
      "0.01966279356453263 3 5\n",
      "0.01931744727108654 3 6\n",
      "0.017623689701140186 3 7\n",
      "0.016355358295294855 4 0\n",
      "0.01608023142455609 4 1\n",
      "0.017299966654668774 4 2\n",
      "0.01933289154131632 4 3\n",
      "0.016939075731398065 4 5\n",
      "0.01789201449024852 4 6\n",
      "0.01835694281012177 4 7\n",
      "0.018236770311277073 5 0\n",
      "0.016057865118093186 5 1\n",
      "0.018025354080497637 5 2\n",
      "0.017245080789824133 5 3\n",
      "0.015916940086974186 5 4\n",
      "0.019076357749319362 5 6\n",
      "0.019734749374627294 5 7\n",
      "0.01676300995663953 6 0\n",
      "0.015939300115182325 6 1\n",
      "0.015374939454773152 6 2\n",
      "0.02008194374401311 6 3\n",
      "0.016627127315132457 6 4\n",
      "0.016959458634823364 6 5\n",
      "0.018718835936375047 6 7\n",
      "0.0200573202752883 7 0\n",
      "0.016296544042591118 7 1\n",
      "0.017140824109182783 7 2\n",
      "0.019088769160430657 7 3\n",
      "0.017660729753981738 7 4\n",
      "0.017942969237189108 7 5\n",
      "0.016979485527736426 7 6\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    "\n",
    "sol1 = model(data1.x, data1.edge_index, data1.batch).tolist()[0]\n",
    "\n",
    "pos = -1\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if i != j:\n",
    "            pos +=1\n",
    "            print(exp(sol1[pos]), i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "batches = {}\n",
    "for batch_idx in range(len(data_list)):\n",
    "    batches[batch_idx] = data_list[batch_idx].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    in_between_loss = 0\n",
    "    within_loss = 0\n",
    "    for batch_idx in range(len(data_list)):\n",
    "        batch = batches[batch_idx]\n",
    "        out = model(batch)\n",
    "        loss = F.mse_loss(out, batches[batch_idx].y)\n",
    "        within_loss += loss.data.item()\n",
    "    in_between_loss = [within_loss/len(data_list)]\n",
    "    loss.data = torch.tensor(in_between_loss, dtype=torch.float)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0027, 0.0695, 0.0690, 0.0038, 0.0034, 0.0640, 0.0821, 0.0026],\n",
      "        [0.0027, 0.0695, 0.0690, 0.0038, 0.0034, 0.0640, 0.0821, 0.0026],\n",
      "        [0.0027, 0.0695, 0.0690, 0.0038, 0.0034, 0.0640, 0.0821, 0.0026],\n",
      "        [0.0027, 0.0695, 0.0690, 0.0038, 0.0034, 0.0640, 0.0821, 0.0026],\n",
      "        [0.0027, 0.0695, 0.0690, 0.0038, 0.0034, 0.0640, 0.0821, 0.0026],\n",
      "        [0.0027, 0.0695, 0.0690, 0.0038, 0.0034, 0.0640, 0.0821, 0.0026],\n",
      "        [0.0027, 0.0695, 0.0690, 0.0038, 0.0034, 0.0640, 0.0821, 0.0026],\n",
      "        [0.0027, 0.0695, 0.0690, 0.0038, 0.0034, 0.0640, 0.0821, 0.0026]],\n",
      "       grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(data1)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred)\n\u001b[1;32m----> 4\u001b[0m correct \u001b[38;5;241m=\u001b[39m ([pred[i][j] \u001b[38;5;241m==\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[j] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m)])\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m      5\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(correct) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mint\u001b[39m(data\u001b[38;5;241m.\u001b[39mtest_mask\u001b[38;5;241m.\u001b[39msum())\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = model(data1)\n",
    "print(pred)\n",
    "correct = ([pred[i][j] == data.y[i][j] for i in range(8) for j in range(8)]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "\n",
    "path = '.'\n",
    "dataset = TUDataset(path, name='Mutagenicity').shuffle()\n",
    "test_dataset = dataset[:len(dataset) // 10]\n",
    "train_dataset = dataset[len(dataset) // 10:]\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: Mutagenicity(4337):\n",
      "======================\n",
      "Number of graphs: 4337\n",
      "Number of features: 14\n",
      "Number of classes: 2\n",
      "\n",
      "Data(edge_index=[2, 24], x=[13, 14], edge_attr=[24, 3], y=[1])\n",
      "===========================================================================================================\n",
      "Number of nodes: 13\n",
      "Number of edges: 24\n",
      "Average node degree: 1.85\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "# dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('===========================================================================================================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        num_features = dataset.num_features\n",
    "        self.dim = dim\n",
    "\n",
    "        self.conv1 = GraphConv(num_features, dim)\n",
    "        self.conv2 = GraphConv(dim, dim)\n",
    "        self.conv3 = GraphConv(dim, dim)\n",
    "        self.conv4 = GraphConv(dim, dim)\n",
    "        self.conv5 = GraphConv(dim, dim)\n",
    "\n",
    "        self.lin1 = Linear(dim, dim)\n",
    "        self.lin2 = Linear(dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_weight=None):\n",
    "        x = self.conv1(x, edge_index, edge_weight).relu()\n",
    "        x = self.conv2(x, edge_index, edge_weight).relu()\n",
    "        x = self.conv3(x, edge_index, edge_weight).relu()\n",
    "        x = self.conv4(x, edge_index, edge_weight).relu()\n",
    "        x = self.conv5(x, edge_index, edge_weight).relu()\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "    \n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    if epoch == 51:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.5 * param_group['lr']\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        print(output)\n",
    "        print(data.y)\n",
    "        exit()\n",
    "        loss = F.nll_loss(output, data.y)\n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-7.9544e-02, -2.5710e+00],\n",
      "        [ 0.0000e+00, -2.4521e+01],\n",
      "        [-3.2041e+00, -4.1441e-02],\n",
      "        [-1.3136e-01, -2.0948e+00],\n",
      "        [-3.5234e-01, -1.2142e+00],\n",
      "        [-2.9585e-01, -1.3622e+00],\n",
      "        [-1.0162e-02, -4.5942e+00],\n",
      "        [-1.0766e+00, -4.1666e-01],\n",
      "        [-4.9121e-01, -9.4645e-01],\n",
      "        [-1.9291e-01, -1.7405e+00],\n",
      "        [-2.3842e-07, -1.5343e+01],\n",
      "        [-6.2136e-04, -7.3840e+00],\n",
      "        [-4.8757e-01, -9.5223e-01],\n",
      "        [-6.1950e-01, -7.7265e-01],\n",
      "        [-3.7913e+00, -2.2825e-02],\n",
      "        [-5.1425e-01, -9.1117e-01],\n",
      "        [-3.0185e-01, -1.3450e+00],\n",
      "        [ 0.0000e+00, -1.8063e+01],\n",
      "        [-1.1951e-01, -2.1835e+00],\n",
      "        [-9.5585e-02, -2.3951e+00],\n",
      "        [-2.2438e+00, -1.1211e-01],\n",
      "        [-3.7700e-01, -1.1581e+00],\n",
      "        [-7.2500e-01, -6.6228e-01],\n",
      "        [-2.9350e+00, -5.4594e-02],\n",
      "        [-2.7798e-01, -1.4160e+00],\n",
      "        [-1.1085e+01, -1.5378e-05],\n",
      "        [-1.8863e-01, -1.7608e+00],\n",
      "        [-7.6554e-01, -6.2565e-01],\n",
      "        [-3.3738e-01, -1.2505e+00],\n",
      "        [-1.0064e+00, -4.5496e-01],\n",
      "        [-1.5749e-01, -1.9261e+00],\n",
      "        [-4.8489e-01, -9.5650e-01],\n",
      "        [-5.8681e-01, -8.1215e-01],\n",
      "        [-1.1889e-02, -4.4381e+00],\n",
      "        [-4.0961e+00, -1.6777e-02],\n",
      "        [-6.6611e-03, -5.0148e+00],\n",
      "        [-1.4611e+00, -2.6395e-01],\n",
      "        [-5.0349e-01, -9.2739e-01],\n",
      "        [-6.9202e-03, -4.9768e+00],\n",
      "        [-5.4231e-01, -8.7085e-01],\n",
      "        [-3.7147e-02, -3.3114e+00],\n",
      "        [-6.1296e-02, -2.8225e+00],\n",
      "        [-4.6378e-01, -9.9129e-01],\n",
      "        [-1.1039e-02, -4.5118e+00],\n",
      "        [-3.7526e+00, -2.3736e-02],\n",
      "        [-3.4239e-01, -1.2381e+00],\n",
      "        [-2.0326e-01, -1.6932e+00],\n",
      "        [-4.8975e-01, -9.4876e-01],\n",
      "        [-1.9882e+00, -1.4727e-01],\n",
      "        [-2.9366e-03, -5.8320e+00],\n",
      "        [-2.4221e-01, -1.5366e+00],\n",
      "        [-4.0934e-01, -1.0909e+00],\n",
      "        [-5.7619e-01, -8.2561e-01],\n",
      "        [-1.1512e+00, -3.8018e-01],\n",
      "        [-9.9203e-01, -4.6334e-01],\n",
      "        [-1.4387e+00, -2.7081e-01],\n",
      "        [-1.3349e+00, -3.0542e-01],\n",
      "        [-4.7328e-01, -9.7540e-01],\n",
      "        [-2.2583e+00, -1.1040e-01],\n",
      "        [-7.5238e-01, -6.3722e-01],\n",
      "        [-6.7272e-02, -2.7325e+00],\n",
      "        [-1.0290e+00, -4.4220e-01],\n",
      "        [-4.8089e-01, -9.6295e-01],\n",
      "        [-1.0252e+00, -4.4429e-01],\n",
      "        [-3.4554e-01, -1.2304e+00],\n",
      "        [-1.9990e-01, -1.7082e+00],\n",
      "        [-1.4366e+00, -2.7146e-01],\n",
      "        [-2.7343e+00, -6.7145e-02],\n",
      "        [-6.5302e-01, -7.3495e-01],\n",
      "        [-5.3319e-01, -8.8365e-01],\n",
      "        [-3.9275e-01, -1.1245e+00],\n",
      "        [-2.1319e+00, -1.2626e-01],\n",
      "        [-2.1998e+00, -1.1746e-01],\n",
      "        [-5.3998e-01, -8.7409e-01],\n",
      "        [-6.3521e-02, -2.7880e+00],\n",
      "        [-1.5518e+00, -2.3808e-01],\n",
      "        [-1.4557e-01, -1.9990e+00],\n",
      "        [-2.4965e-01, -1.5099e+00],\n",
      "        [-5.2599e-01, -8.9397e-01],\n",
      "        [-2.3481e-01, -1.5641e+00],\n",
      "        [-2.3205e-01, -1.5746e+00],\n",
      "        [-2.8009e-03, -5.8792e+00],\n",
      "        [-3.3049e+00, -3.7393e-02],\n",
      "        [-8.4933e-01, -5.5810e-01],\n",
      "        [-9.3309e-01, -4.9978e-01],\n",
      "        [-1.1199e+00, -3.9497e-01],\n",
      "        [-6.5687e-02, -2.7555e+00],\n",
      "        [-1.4456e-02, -4.2439e+00],\n",
      "        [-1.5996e+00, -2.2561e-01],\n",
      "        [-1.6226e-01, -1.8986e+00],\n",
      "        [-1.1358e+00, -3.8738e-01],\n",
      "        [-2.6977e-02, -3.6262e+00],\n",
      "        [-2.0997e-03, -6.1670e+00],\n",
      "        [-6.7898e-02, -2.7235e+00],\n",
      "        [-1.5693e-01, -1.9294e+00],\n",
      "        [-1.7614e+00, -1.8851e-01],\n",
      "        [-9.2801e-01, -5.0309e-01],\n",
      "        [-8.7003e-02, -2.4850e+00],\n",
      "        [-6.3584e-02, -2.7870e+00],\n",
      "        [-8.7442e-01, -5.3974e-01],\n",
      "        [-1.2566e+00, -3.3494e-01],\n",
      "        [-2.3817e-01, -1.5515e+00],\n",
      "        [-4.3562e-04, -7.7391e+00],\n",
      "        [-4.6158e-01, -9.9504e-01],\n",
      "        [-3.0749e-02, -3.4972e+00],\n",
      "        [-4.2408e-01, -1.0624e+00],\n",
      "        [-1.5569e-01, -1.9367e+00],\n",
      "        [-8.8639e-02, -2.4672e+00],\n",
      "        [-2.9350e+00, -5.4592e-02],\n",
      "        [-1.4021e+00, -2.8245e-01],\n",
      "        [-3.0468e-01, -1.3370e+00],\n",
      "        [-3.8851e-01, -1.1334e+00],\n",
      "        [-4.4636e-01, -1.0215e+00],\n",
      "        [-1.6928e+00, -2.0335e-01],\n",
      "        [-1.3150e+00, -3.1264e-01],\n",
      "        [-1.4777e+00, -2.5897e-01],\n",
      "        [-1.7947e+00, -1.8174e-01],\n",
      "        [-1.0467e-02, -4.5648e+00],\n",
      "        [-7.0578e-01, -6.8067e-01],\n",
      "        [-4.6622e-03, -5.3706e+00],\n",
      "        [-3.9557e-01, -1.1187e+00],\n",
      "        [-2.1998e-01, -1.6222e+00],\n",
      "        [-2.3756e+00, -9.7571e-02],\n",
      "        [-6.9182e-01, -6.9447e-01],\n",
      "        [-4.2672e-01, -1.0574e+00],\n",
      "        [-1.7677e-01, -1.8200e+00],\n",
      "        [-2.1119e-03, -6.1612e+00],\n",
      "        [-4.9815e-01, -9.3561e-01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
      "        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
      "        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([[-1.8487e-01, -1.7791e+00],\n",
      "        [-2.2455e+00, -1.1191e-01],\n",
      "        [-5.4126e-02, -2.9434e+00],\n",
      "        [-6.0728e-01, -7.8709e-01],\n",
      "        [-9.9325e-01, -4.6262e-01],\n",
      "        [-8.5806e-01, -5.5162e-01],\n",
      "        [-9.0261e-01, -5.2005e-01],\n",
      "        [-2.5377e-01, -1.4955e+00],\n",
      "        [-6.7831e-01, -7.0821e-01],\n",
      "        [-3.8508e-01, -1.1407e+00],\n",
      "        [-1.2007e+00, -3.5810e-01],\n",
      "        [-9.9027e-02, -2.3615e+00],\n",
      "        [-8.3333e-01, -5.7022e-01],\n",
      "        [-1.1096e+00, -3.9999e-01],\n",
      "        [-4.4135e-01, -1.0305e+00],\n",
      "        [-7.8556e-05, -9.4519e+00],\n",
      "        [-1.2857e+00, -3.2358e-01],\n",
      "        [-1.1388e+00, -3.8598e-01],\n",
      "        [-4.6063e-01, -9.9666e-01],\n",
      "        [-4.1810e+00, -1.5401e-02],\n",
      "        [-7.4397e-01, -6.4478e-01],\n",
      "        [-1.0245e+00, -4.4472e-01],\n",
      "        [-2.0050e+00, -1.4464e-01],\n",
      "        [-3.7946e+00, -2.2748e-02],\n",
      "        [-5.8832e-01, -8.1026e-01],\n",
      "        [-3.1049e+00, -4.5867e-02],\n",
      "        [-2.8504e+00, -5.9559e-02],\n",
      "        [-1.5110e+00, -2.4935e-01],\n",
      "        [-5.9066e-01, -8.0735e-01],\n",
      "        [-2.3187e-01, -1.5753e+00],\n",
      "        [-5.1247e-01, -9.1383e-01],\n",
      "        [-1.3848e-01, -2.0455e+00],\n",
      "        [-6.6857e-01, -7.1834e-01],\n",
      "        [-1.0031e+00, -4.5687e-01],\n",
      "        [-1.7299e+00, -1.9516e-01],\n",
      "        [-8.0516e+00, -3.1860e-04],\n",
      "        [-6.7641e-01, -7.1017e-01],\n",
      "        [-2.1830e+00, -1.1958e-01],\n",
      "        [-3.1473e+00, -4.3920e-02],\n",
      "        [-5.9895e-01, -7.9714e-01],\n",
      "        [-1.0724e+00, -4.1884e-01],\n",
      "        [-6.8170e-01, -7.0473e-01],\n",
      "        [-9.0498e-01, -5.1844e-01],\n",
      "        [-1.4804e-01, -1.9834e+00],\n",
      "        [-1.0170e+00, -4.4893e-01],\n",
      "        [-1.7352e+00, -1.9403e-01],\n",
      "        [-4.6488e-01, -9.8942e-01],\n",
      "        [-7.2087e-01, -6.6617e-01],\n",
      "        [-1.0892e-01, -2.2711e+00],\n",
      "        [-2.0411e+00, -1.3913e-01],\n",
      "        [-1.7499e+00, -1.9092e-01],\n",
      "        [-9.2393e-01, -5.0577e-01],\n",
      "        [-9.8639e-01, -4.6668e-01],\n",
      "        [-7.8659e-01, -6.0770e-01],\n",
      "        [-9.4138e-01, -4.9444e-01],\n",
      "        [-5.5919e-01, -8.4786e-01],\n",
      "        [-7.2920e-01, -6.5835e-01],\n",
      "        [-1.8676e+00, -1.6782e-01],\n",
      "        [-1.2146e-02, -4.4168e+00],\n",
      "        [-2.8327e+00, -6.0658e-02],\n",
      "        [-8.7996e-01, -5.3580e-01],\n",
      "        [-2.8671e-01, -1.3892e+00],\n",
      "        [-7.5011e-01, -6.3926e-01],\n",
      "        [-1.4366e+00, -2.7147e-01],\n",
      "        [-9.1784e-01, -5.0979e-01],\n",
      "        [-7.1088e-01, -6.7572e-01],\n",
      "        [-9.0761e-01, -5.1666e-01],\n",
      "        [-2.2363e+00, -1.1301e-01],\n",
      "        [-1.2375e-03, -6.6954e+00],\n",
      "        [-1.6190e+00, -2.2076e-01],\n",
      "        [-9.1240e+00, -1.0907e-04],\n",
      "        [-3.0467e+00, -4.8679e-02],\n",
      "        [-1.8944e+00, -1.6301e-01],\n",
      "        [-1.9501e-01, -1.7306e+00],\n",
      "        [-3.4460e-01, -1.2327e+00],\n",
      "        [-1.4221e+00, -2.7604e-01],\n",
      "        [-1.5371e+00, -2.4207e-01],\n",
      "        [-6.3340e-01, -7.5669e-01],\n",
      "        [-2.5278e-01, -1.4990e+00],\n",
      "        [-3.0981e+00, -4.6187e-02],\n",
      "        [-3.0698e+00, -4.7544e-02],\n",
      "        [-7.0353e-01, -6.8287e-01],\n",
      "        [-2.6939e+00, -7.0015e-02],\n",
      "        [-1.8020e-01, -1.8024e+00],\n",
      "        [-1.7776e-01, -1.8149e+00],\n",
      "        [-1.7794e+00, -1.8481e-01],\n",
      "        [-2.7376e+00, -6.6913e-02],\n",
      "        [-1.3044e+00, -3.1653e-01],\n",
      "        [-3.4055e-01, -1.2426e+00],\n",
      "        [-1.1369e+00, -3.8684e-01],\n",
      "        [-7.4172e-01, -6.4683e-01],\n",
      "        [-1.7913e+00, -1.8241e-01],\n",
      "        [-1.1562e+00, -3.7785e-01],\n",
      "        [-1.5482e-01, -1.9419e+00],\n",
      "        [-5.0369e-02, -3.0135e+00],\n",
      "        [-4.3218e-01, -1.0472e+00],\n",
      "        [-1.0616e+00, -4.2450e-01],\n",
      "        [-1.1170e+00, -3.9640e-01],\n",
      "        [-8.9779e-02, -2.4550e+00],\n",
      "        [-2.7536e+00, -6.5816e-02],\n",
      "        [-8.2044e-02, -2.5412e+00],\n",
      "        [-1.2595e+00, -3.3380e-01],\n",
      "        [-9.8101e-01, -4.6990e-01],\n",
      "        [-1.5559e+00, -2.3698e-01],\n",
      "        [-1.1976e+00, -3.5943e-01],\n",
      "        [-1.8631e+00, -1.6865e-01],\n",
      "        [-1.3920e+00, -2.8579e-01],\n",
      "        [-2.2230e+00, -1.1461e-01],\n",
      "        [-2.2519e+00, -1.1115e-01],\n",
      "        [-6.0214e-01, -7.9327e-01],\n",
      "        [-9.6752e-01, -4.7808e-01],\n",
      "        [-1.1961e+00, -3.6005e-01],\n",
      "        [-1.1715e+00, -3.7091e-01],\n",
      "        [-1.3853e+00, -2.8802e-01],\n",
      "        [-4.3445e+00, -1.3063e-02],\n",
      "        [-3.1026e-01, -1.3215e+00],\n",
      "        [-1.8748e+00, -1.6650e-01],\n",
      "        [-3.0956e+00, -4.6302e-02],\n",
      "        [-2.3700e+00, -9.8141e-02],\n",
      "        [-1.0202e+00, -4.4713e-01],\n",
      "        [-6.6446e-01, -7.2268e-01],\n",
      "        [-1.3627e-01, -2.0605e+00],\n",
      "        [-6.4080e-01, -7.4839e-01],\n",
      "        [-1.9746e-01, -1.7193e+00],\n",
      "        [-6.9543e-01, -6.9086e-01],\n",
      "        [-7.4119e-01, -6.4731e-01],\n",
      "        [-6.4409e-01, -7.4474e-01],\n",
      "        [-3.8910e-01, -1.1322e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 1])\n",
      "tensor([[-3.5960e+00, -2.7816e-02],\n",
      "        [-8.4777e-01, -5.5926e-01],\n",
      "        [-1.4045e+00, -2.8167e-01],\n",
      "        [-2.2460e+00, -1.1184e-01],\n",
      "        [-9.3654e-01, -4.9755e-01],\n",
      "        [-8.5066e-01, -5.5710e-01],\n",
      "        [-1.5011e+00, -2.5217e-01],\n",
      "        [-1.4769e+00, -2.5921e-01],\n",
      "        [-6.1160e-01, -7.8194e-01],\n",
      "        [-7.6508e-01, -6.2604e-01],\n",
      "        [-2.4498e+00, -9.0268e-02],\n",
      "        [-1.4506e+00, -2.6712e-01],\n",
      "        [-3.9596e+00, -1.9254e-02],\n",
      "        [-1.0681e+00, -4.2109e-01],\n",
      "        [-7.3936e-01, -6.4897e-01],\n",
      "        [-7.2755e-01, -6.5989e-01],\n",
      "        [-3.3108e+00, -3.7170e-02],\n",
      "        [-6.0573e-01, -7.8895e-01],\n",
      "        [-1.9398e+00, -1.5518e-01],\n",
      "        [-9.0231e-01, -5.2026e-01],\n",
      "        [-1.0485e+00, -4.3151e-01],\n",
      "        [-2.2790e+00, -1.0801e-01],\n",
      "        [-7.1152e-01, -6.7510e-01],\n",
      "        [-8.6015e-01, -5.5009e-01],\n",
      "        [-2.0037e+00, -1.4484e-01],\n",
      "        [-1.0477e+00, -4.3192e-01],\n",
      "        [-1.9268e+00, -1.5738e-01],\n",
      "        [-2.2188e-01, -1.6145e+00],\n",
      "        [-1.1159e+00, -3.9693e-01],\n",
      "        [-7.0396e-01, -6.8245e-01],\n",
      "        [-1.0639e+00, -4.2330e-01],\n",
      "        [-2.8707e+00, -5.8330e-02],\n",
      "        [-5.8663e-01, -8.1238e-01],\n",
      "        [-1.7127e+00, -1.9892e-01],\n",
      "        [-2.4951e+00, -8.6086e-02],\n",
      "        [-5.5683e-01, -8.5103e-01],\n",
      "        [-7.4692e-01, -6.4212e-01],\n",
      "        [-7.7967e-01, -6.1352e-01],\n",
      "        [-2.2379e+00, -1.1282e-01],\n",
      "        [-8.8427e-01, -5.3276e-01],\n",
      "        [-2.9853e-01, -1.3544e+00],\n",
      "        [-6.9043e-01, -6.9588e-01],\n",
      "        [-9.0427e-01, -5.1892e-01],\n",
      "        [-1.3977e+00, -2.8390e-01],\n",
      "        [-1.2489e+00, -3.3804e-01],\n",
      "        [-5.7653e-01, -8.2518e-01],\n",
      "        [-5.8854e-01, -8.0999e-01],\n",
      "        [-1.0167e+00, -4.4908e-01],\n",
      "        [-1.4429e+00, -2.6951e-01],\n",
      "        [-4.2208e+00, -1.4796e-02],\n",
      "        [-8.1231e-01, -5.8668e-01],\n",
      "        [-7.4591e+00, -5.7633e-04],\n",
      "        [-6.2536e-01, -7.6586e-01],\n",
      "        [-1.2475e+00, -3.3860e-01],\n",
      "        [-6.4717e-01, -7.4134e-01],\n",
      "        [-4.3161e-01, -1.0483e+00],\n",
      "        [-1.0190e+00, -4.4778e-01],\n",
      "        [-2.3377e-01, -1.5680e+00],\n",
      "        [-7.4506e-01, -6.4380e-01],\n",
      "        [-1.2813e+00, -3.2527e-01],\n",
      "        [-8.8719e-01, -5.3071e-01],\n",
      "        [-1.2926e+00, -3.2098e-01],\n",
      "        [-4.2478e+00, -1.4399e-02],\n",
      "        [-8.6497e-01, -5.4656e-01],\n",
      "        [-9.8198e-01, -4.6932e-01],\n",
      "        [-1.0771e+00, -4.1640e-01],\n",
      "        [-4.6419e-01, -9.9059e-01],\n",
      "        [-1.4311e+00, -2.7319e-01],\n",
      "        [-7.6905e-01, -6.2260e-01],\n",
      "        [-1.1768e+00, -3.6854e-01],\n",
      "        [-2.0599e+00, -1.3636e-01],\n",
      "        [-1.7215e+00, -1.9700e-01],\n",
      "        [-1.7648e+00, -1.8781e-01],\n",
      "        [-1.7190e+00, -1.9753e-01],\n",
      "        [-7.8079e-01, -6.1257e-01],\n",
      "        [-1.3859e+00, -2.8782e-01],\n",
      "        [-1.9807e+00, -1.4846e-01],\n",
      "        [-4.9696e-01, -9.3745e-01],\n",
      "        [-4.4779e-01, -1.0190e+00],\n",
      "        [-1.4238e+00, -2.7550e-01],\n",
      "        [-1.0392e+00, -4.3656e-01],\n",
      "        [-6.2005e-01, -7.7201e-01],\n",
      "        [-1.5177e+00, -2.4746e-01],\n",
      "        [-2.2279e-01, -1.6109e+00],\n",
      "        [-1.2887e+00, -3.2244e-01],\n",
      "        [-2.3879e+00, -9.6315e-02],\n",
      "        [-3.4989e+00, -3.0698e-02],\n",
      "        [-1.2867e+00, -3.2322e-01],\n",
      "        [-4.4185e-01, -1.0296e+00],\n",
      "        [-3.4877e-01, -1.2227e+00],\n",
      "        [-1.1329e+00, -3.8877e-01],\n",
      "        [-6.0015e-01, -7.9569e-01],\n",
      "        [-8.1154e-01, -5.8730e-01],\n",
      "        [-9.0126e+00, -1.2182e-04],\n",
      "        [-6.7337e-01, -7.1333e-01],\n",
      "        [-1.4921e+00, -2.5478e-01],\n",
      "        [-6.9590e-01, -6.9040e-01],\n",
      "        [-1.8068e+00, -1.7934e-01],\n",
      "        [-1.3535e+00, -2.9885e-01],\n",
      "        [-7.3403e-01, -6.5387e-01],\n",
      "        [-8.7080e-01, -5.4235e-01],\n",
      "        [-1.1219e+00, -3.9400e-01],\n",
      "        [-9.5975e-01, -4.8287e-01],\n",
      "        [-5.2605e-01, -8.9388e-01],\n",
      "        [-6.5064e-01, -7.3754e-01],\n",
      "        [-1.9253e+00, -1.5763e-01],\n",
      "        [-6.9784e-01, -6.8847e-01],\n",
      "        [-1.2785e+00, -3.2637e-01],\n",
      "        [-5.1509e-01, -9.0994e-01],\n",
      "        [-1.0470e+00, -4.3233e-01],\n",
      "        [-1.1243e+00, -3.9287e-01],\n",
      "        [-1.2684e+00, -3.3027e-01],\n",
      "        [-1.3384e+00, -3.0417e-01],\n",
      "        [-6.8476e+00, -1.0625e-03],\n",
      "        [-1.5079e+00, -2.5024e-01],\n",
      "        [-8.1382e-01, -5.8548e-01],\n",
      "        [-9.9810e-01, -4.5978e-01],\n",
      "        [-6.0862e-01, -7.8548e-01],\n",
      "        [-4.4185e-01, -1.0296e+00],\n",
      "        [-9.5217e-01, -4.8760e-01],\n",
      "        [-4.9539e-01, -9.3989e-01],\n",
      "        [-7.5243e-01, -6.3718e-01],\n",
      "        [-6.5353e-01, -7.3440e-01],\n",
      "        [-1.2584e+00, -3.3424e-01],\n",
      "        [-2.1448e+00, -1.2453e-01],\n",
      "        [-1.3015e+00, -3.1761e-01],\n",
      "        [-1.3119e+00, -3.1375e-01],\n",
      "        [-2.4412e+00, -9.1081e-02]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
      "        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 0, 1, 0])\n",
      "tensor([[-0.5924, -0.8052],\n",
      "        [-1.2003, -0.3583],\n",
      "        [-0.2377, -1.5531],\n",
      "        [-1.4050, -0.2815],\n",
      "        [-0.7211, -0.6659],\n",
      "        [-1.3874, -0.2873],\n",
      "        [-0.3819, -1.1474],\n",
      "        [-0.9469, -0.4909],\n",
      "        [-1.4487, -0.2677],\n",
      "        [-0.5000, -0.9327],\n",
      "        [-0.8139, -0.5854],\n",
      "        [-1.9566, -0.1524],\n",
      "        [-0.1905, -1.7517],\n",
      "        [-0.9533, -0.4869],\n",
      "        [-0.9125, -0.5133],\n",
      "        [-0.7809, -0.6124],\n",
      "        [-0.7180, -0.6689],\n",
      "        [-1.4922, -0.2547],\n",
      "        [-0.3406, -1.2424],\n",
      "        [-0.2140, -1.6470],\n",
      "        [-0.6988, -0.6875],\n",
      "        [-1.0617, -0.4245],\n",
      "        [-0.4675, -0.9851],\n",
      "        [-0.9344, -0.4989],\n",
      "        [-3.3693, -0.0350],\n",
      "        [-1.1417, -0.3846],\n",
      "        [-0.9849, -0.4676],\n",
      "        [-0.8569, -0.5524],\n",
      "        [-1.5795, -0.2308],\n",
      "        [-0.9155, -0.5113],\n",
      "        [-1.2345, -0.3439],\n",
      "        [-0.2724, -1.4335],\n",
      "        [-0.2719, -1.4352],\n",
      "        [-0.6783, -0.7082],\n",
      "        [-0.1389, -2.0428],\n",
      "        [-1.3800, -0.2898],\n",
      "        [-1.2101, -0.3540],\n",
      "        [-1.1132, -0.3982],\n",
      "        [-0.9189, -0.5091],\n",
      "        [-1.5645, -0.2347],\n",
      "        [-0.6687, -0.7182],\n",
      "        [-0.9656, -0.4793],\n",
      "        [-1.3400, -0.3036],\n",
      "        [-0.9785, -0.4714],\n",
      "        [-0.9984, -0.4596],\n",
      "        [-1.9488, -0.1537],\n",
      "        [-0.9677, -0.4779],\n",
      "        [-0.4565, -1.0038],\n",
      "        [-0.9637, -0.4805],\n",
      "        [-0.5841, -0.8155],\n",
      "        [-0.5771, -0.8245],\n",
      "        [-0.7606, -0.6300],\n",
      "        [-0.4698, -0.9812],\n",
      "        [-1.7350, -0.1941],\n",
      "        [-0.6545, -0.7334],\n",
      "        [-1.0027, -0.4571],\n",
      "        [-1.5159, -0.2480],\n",
      "        [-3.2556, -0.0393],\n",
      "        [-0.7835, -0.6103],\n",
      "        [-1.0573, -0.4268],\n",
      "        [-0.3718, -1.1696],\n",
      "        [-1.0789, -0.4155],\n",
      "        [-0.6734, -0.7133],\n",
      "        [-1.5082, -0.2501],\n",
      "        [-0.8096, -0.5889],\n",
      "        [-0.5659, -0.8389],\n",
      "        [-0.0752, -2.6245],\n",
      "        [-1.6234, -0.2197],\n",
      "        [-1.1078, -0.4009],\n",
      "        [-0.5418, -0.8716],\n",
      "        [-1.2062, -0.3557],\n",
      "        [-0.7260, -0.6613],\n",
      "        [-1.4367, -0.2714],\n",
      "        [-0.5367, -0.8787],\n",
      "        [-0.6173, -0.7752],\n",
      "        [-0.7618, -0.6289],\n",
      "        [-0.8973, -0.5237],\n",
      "        [-0.7502, -0.6392],\n",
      "        [-0.4285, -1.0541],\n",
      "        [-0.6220, -0.7698],\n",
      "        [-1.8573, -0.1697],\n",
      "        [-0.3044, -1.3377],\n",
      "        [-0.9444, -0.4925],\n",
      "        [-0.9704, -0.4763],\n",
      "        [-0.5144, -0.9110],\n",
      "        [-0.5698, -0.8339],\n",
      "        [-3.0965, -0.0463],\n",
      "        [-0.9170, -0.5104],\n",
      "        [-1.7351, -0.1941],\n",
      "        [-0.4224, -1.0656],\n",
      "        [-2.2059, -0.1167],\n",
      "        [-2.8312, -0.0608],\n",
      "        [-1.4470, -0.2682],\n",
      "        [-2.2576, -0.1105],\n",
      "        [-1.0939, -0.4078],\n",
      "        [-3.4799, -0.0313],\n",
      "        [-0.3937, -1.1226],\n",
      "        [-0.7135, -0.6732],\n",
      "        [-3.2204, -0.0408],\n",
      "        [-0.8076, -0.5905],\n",
      "        [-0.4569, -1.0030],\n",
      "        [-0.5628, -0.8431],\n",
      "        [-0.4798, -0.9647],\n",
      "        [-0.8529, -0.5554],\n",
      "        [-0.2507, -1.5062],\n",
      "        [-0.5971, -0.7994],\n",
      "        [-0.8009, -0.5959],\n",
      "        [-1.7785, -0.1850],\n",
      "        [-0.9269, -0.5038],\n",
      "        [-0.1859, -1.7741],\n",
      "        [-0.6580, -0.7296],\n",
      "        [-0.4369, -1.0386],\n",
      "        [-0.6126, -0.7808],\n",
      "        [-0.9501, -0.4889],\n",
      "        [-0.8239, -0.5775],\n",
      "        [-0.5528, -0.8565],\n",
      "        [-0.8177, -0.5824],\n",
      "        [-0.7224, -0.6647],\n",
      "        [-1.6111, -0.2227],\n",
      "        [-0.7402, -0.6482],\n",
      "        [-0.7114, -0.6753],\n",
      "        [-0.3544, -1.2093],\n",
      "        [-1.2948, -0.3201],\n",
      "        [-0.5822, -0.8179],\n",
      "        [-0.4697, -0.9814],\n",
      "        [-0.7747, -0.6178],\n",
      "        [-1.1874, -0.3639],\n",
      "        [-1.1679, -0.3725]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
      "        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
      "        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
      "        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([[-5.1888e-01, -9.0433e-01],\n",
      "        [-1.2163e+00, -3.5145e-01],\n",
      "        [-1.2515e+00, -3.3697e-01],\n",
      "        [-2.6140e+00, -7.6062e-02],\n",
      "        [-6.2480e-01, -7.6651e-01],\n",
      "        [-6.7038e-01, -7.1645e-01],\n",
      "        [-9.9044e-01, -4.6428e-01],\n",
      "        [-1.1335e+00, -3.8846e-01],\n",
      "        [-5.4040e-01, -8.7350e-01],\n",
      "        [-1.5800e+00, -2.3064e-01],\n",
      "        [-9.6134e-01, -4.8188e-01],\n",
      "        [-2.2215e+00, -1.1479e-01],\n",
      "        [-2.3726e-01, -1.5549e+00],\n",
      "        [-5.7013e-01, -8.3345e-01],\n",
      "        [-8.5253e-01, -5.5571e-01],\n",
      "        [-1.7753e+00, -1.8564e-01],\n",
      "        [-2.0868e-01, -1.6695e+00],\n",
      "        [-3.3739e-01, -1.2505e+00],\n",
      "        [-1.0703e+00, -4.1994e-01],\n",
      "        [-7.8838e-01, -6.0620e-01],\n",
      "        [-6.3975e-01, -7.4956e-01],\n",
      "        [-1.6040e-01, -1.9092e+00],\n",
      "        [-7.1539e-01, -6.7138e-01],\n",
      "        [-8.5791e-01, -5.5173e-01],\n",
      "        [-6.1053e-01, -7.8321e-01],\n",
      "        [-1.1174e+00, -3.9622e-01],\n",
      "        [-6.1286e-01, -7.8045e-01],\n",
      "        [-1.4163e-01, -2.0245e+00],\n",
      "        [-7.3707e-01, -6.5107e-01],\n",
      "        [-3.4878e-01, -1.2226e+00],\n",
      "        [-8.6983e-01, -5.4304e-01],\n",
      "        [-6.3328e-01, -7.5682e-01],\n",
      "        [-7.3913e-01, -6.4919e-01],\n",
      "        [-3.1559e-01, -1.3070e+00],\n",
      "        [-5.3242e-01, -8.8476e-01],\n",
      "        [-9.4551e-01, -4.9181e-01],\n",
      "        [-1.7390e-01, -1.8349e+00],\n",
      "        [-6.6860e-01, -7.1831e-01],\n",
      "        [-3.1044e-01, -1.3210e+00],\n",
      "        [-4.0494e-01, -1.0997e+00],\n",
      "        [-1.0170e+00, -4.4893e-01],\n",
      "        [-6.3277e-01, -7.5740e-01],\n",
      "        [-5.7117e-01, -8.3209e-01],\n",
      "        [-3.5836e-01, -1.2000e+00],\n",
      "        [-4.0963e-01, -1.0903e+00],\n",
      "        [-5.6969e-01, -8.3403e-01],\n",
      "        [-1.0875e+00, -4.1105e-01],\n",
      "        [-2.8829e-01, -1.3845e+00],\n",
      "        [-9.4592e-01, -4.9155e-01],\n",
      "        [-9.4558e-01, -4.9177e-01],\n",
      "        [-7.3994e-01, -6.4845e-01],\n",
      "        [-7.0833e-01, -6.7820e-01],\n",
      "        [-7.6096e-02, -2.6136e+00],\n",
      "        [-5.5486e-01, -8.5368e-01],\n",
      "        [-6.1267e-01, -7.8067e-01],\n",
      "        [-1.5778e+00, -2.3121e-01],\n",
      "        [-3.8684e-01, -1.1369e+00],\n",
      "        [-1.0557e+00, -4.2764e-01],\n",
      "        [-1.3753e+00, -2.9136e-01],\n",
      "        [-2.2842e+00, -1.0742e-01],\n",
      "        [-7.3510e-01, -6.5289e-01],\n",
      "        [-2.9810e-01, -1.3557e+00],\n",
      "        [-6.4345e-01, -7.4545e-01],\n",
      "        [-4.3696e-01, -1.0385e+00],\n",
      "        [-5.1074e-01, -9.1641e-01],\n",
      "        [-4.7953e-01, -9.6515e-01],\n",
      "        [-8.8540e-01, -5.3196e-01],\n",
      "        [-3.6896e-01, -1.1759e+00],\n",
      "        [-8.1246e-01, -5.8657e-01],\n",
      "        [-5.8630e-01, -8.1279e-01],\n",
      "        [-1.1673e+00, -3.7280e-01],\n",
      "        [-6.9899e-01, -6.8734e-01],\n",
      "        [-7.0066e-01, -6.8569e-01],\n",
      "        [-1.8702e-03, -6.2826e+00],\n",
      "        [-7.8598e-01, -6.0820e-01],\n",
      "        [-1.4400e+00, -2.7040e-01],\n",
      "        [-9.1895e-01, -5.0906e-01],\n",
      "        [-5.1718e-01, -9.0683e-01],\n",
      "        [-4.3123e-01, -1.0490e+00],\n",
      "        [-9.7649e-01, -4.7262e-01],\n",
      "        [-1.0700e+00, -4.2008e-01],\n",
      "        [-6.8187e-01, -7.0455e-01],\n",
      "        [-9.1840e-01, -5.0942e-01],\n",
      "        [-8.5936e-01, -5.5066e-01],\n",
      "        [-1.0482e+00, -4.3168e-01],\n",
      "        [-3.2422e-01, -1.2841e+00],\n",
      "        [-4.4107e-01, -1.0310e+00],\n",
      "        [-8.5134e-01, -5.5659e-01],\n",
      "        [-3.5895e-01, -1.1987e+00],\n",
      "        [-5.8480e-01, -8.1467e-01],\n",
      "        [-9.0129e-01, -5.2095e-01],\n",
      "        [-9.6192e-01, -4.8152e-01],\n",
      "        [-8.5615e-01, -5.5303e-01],\n",
      "        [-7.7986e-01, -6.1336e-01],\n",
      "        [-7.6367e-01, -6.2727e-01],\n",
      "        [-3.0867e-01, -1.3259e+00],\n",
      "        [-8.5968e-01, -5.5043e-01],\n",
      "        [-1.8263e+00, -1.7555e-01],\n",
      "        [-3.5307e-01, -1.2124e+00],\n",
      "        [-6.7357e-01, -7.1311e-01],\n",
      "        [-9.5975e-01, -4.8287e-01],\n",
      "        [-2.4875e-01, -1.5131e+00],\n",
      "        [-2.7388e-01, -1.4289e+00],\n",
      "        [-1.0390e+00, -4.3664e-01],\n",
      "        [-1.3303e+00, -3.0708e-01],\n",
      "        [-1.1524e+00, -3.7961e-01],\n",
      "        [-1.3508e+00, -2.9979e-01],\n",
      "        [-2.9609e-01, -1.3615e+00],\n",
      "        [-1.8125e-01, -1.7971e+00],\n",
      "        [-9.8498e-01, -4.6752e-01],\n",
      "        [-1.0510e+00, -4.3016e-01],\n",
      "        [-7.8779e-01, -6.0669e-01],\n",
      "        [-6.8677e-01, -6.9956e-01],\n",
      "        [-1.1246e+00, -3.9272e-01],\n",
      "        [-2.0890e+00, -1.3218e-01],\n",
      "        [-2.6239e-01, -1.4663e+00],\n",
      "        [-1.8438e+00, -1.7223e-01],\n",
      "        [-1.8639e+00, -1.6850e-01],\n",
      "        [-5.3989e-01, -8.7422e-01],\n",
      "        [-5.3055e-01, -8.8742e-01],\n",
      "        [-1.2004e+00, -3.5821e-01],\n",
      "        [-2.3086e-01, -1.5792e+00],\n",
      "        [-4.8576e-01, -9.5511e-01],\n",
      "        [-4.8088e-01, -9.6296e-01],\n",
      "        [-3.2042e-01, -1.2941e+00],\n",
      "        [-5.7680e-01, -8.2484e-01],\n",
      "        [-9.7695e-01, -4.7234e-01],\n",
      "        [-7.2793e-01, -6.5953e-01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 1, 1, 0, 1, 0, 1])\n",
      "tensor([[-0.2038, -1.6909],\n",
      "        [-0.0388, -3.2682],\n",
      "        [-0.3951, -1.1196],\n",
      "        [-0.5041, -0.9264],\n",
      "        [-0.7057, -0.6807],\n",
      "        [-0.3377, -1.2496],\n",
      "        [-0.2815, -1.4050],\n",
      "        [-0.3394, -1.2455],\n",
      "        [-1.1801, -0.3671],\n",
      "        [-0.7254, -0.6619],\n",
      "        [-0.3107, -1.3204],\n",
      "        [-0.4231, -1.0643],\n",
      "        [-0.6782, -0.7083],\n",
      "        [-0.7493, -0.6400],\n",
      "        [-0.6752, -0.7115],\n",
      "        [-0.4086, -1.0923],\n",
      "        [-0.4743, -0.9737],\n",
      "        [-0.3019, -1.3448],\n",
      "        [-0.0164, -4.1200],\n",
      "        [-0.6935, -0.6928],\n",
      "        [-0.9474, -0.4906],\n",
      "        [-0.5469, -0.8645],\n",
      "        [-0.2905, -1.3778],\n",
      "        [-0.6073, -0.7871],\n",
      "        [-0.5192, -0.9039],\n",
      "        [-0.3310, -1.2665],\n",
      "        [-0.0665, -2.7432],\n",
      "        [-0.2581, -1.4807],\n",
      "        [-1.2761, -0.3273],\n",
      "        [-0.6038, -0.7913],\n",
      "        [-0.2659, -1.4545],\n",
      "        [-0.6488, -0.7395],\n",
      "        [-0.5151, -0.9099],\n",
      "        [-0.7333, -0.6546],\n",
      "        [-0.9301, -0.5017],\n",
      "        [-0.3997, -1.1102],\n",
      "        [-0.9957, -0.4612],\n",
      "        [-0.1183, -2.1933],\n",
      "        [-0.0538, -2.9484],\n",
      "        [-0.3987, -1.1123],\n",
      "        [-1.1236, -0.3932],\n",
      "        [-0.4494, -1.0162],\n",
      "        [-0.4542, -1.0077],\n",
      "        [-1.1766, -0.3687],\n",
      "        [-0.6260, -0.7651],\n",
      "        [-1.3206, -0.3106],\n",
      "        [-0.9602, -0.4826],\n",
      "        [-0.0791, -2.5765],\n",
      "        [-0.7436, -0.6451],\n",
      "        [-0.2726, -1.4330],\n",
      "        [-0.3692, -1.1754],\n",
      "        [-0.4500, -1.0151],\n",
      "        [-0.4938, -0.9424],\n",
      "        [-0.9592, -0.4832],\n",
      "        [-1.3764, -0.2910],\n",
      "        [-0.4364, -1.0394],\n",
      "        [-0.6362, -0.7536],\n",
      "        [-0.3076, -1.3289],\n",
      "        [-0.8784, -0.5369],\n",
      "        [-0.0951, -2.4002],\n",
      "        [-0.5223, -0.8993],\n",
      "        [-0.5422, -0.8710],\n",
      "        [-0.1294, -2.1086],\n",
      "        [-0.3145, -1.3098],\n",
      "        [-0.5059, -0.9237],\n",
      "        [-0.8325, -0.5708],\n",
      "        [-0.3339, -1.2593],\n",
      "        [-0.3348, -1.2570],\n",
      "        [-0.2319, -1.5751],\n",
      "        [-0.5296, -0.8888],\n",
      "        [-0.5766, -0.8251],\n",
      "        [-0.4082, -1.0931],\n",
      "        [-0.2834, -1.3993],\n",
      "        [-0.3433, -1.2360],\n",
      "        [-0.7599, -0.6306],\n",
      "        [-0.4851, -0.9561],\n",
      "        [-0.5637, -0.8419],\n",
      "        [-0.2573, -1.4836],\n",
      "        [-0.5289, -0.8897],\n",
      "        [-0.1483, -1.9817],\n",
      "        [-0.7313, -0.6564],\n",
      "        [-0.7833, -0.6105],\n",
      "        [-0.4024, -1.1047],\n",
      "        [-0.4161, -1.0776],\n",
      "        [-1.2420, -0.3408],\n",
      "        [-0.1466, -1.9928],\n",
      "        [-0.7993, -0.5972],\n",
      "        [-0.6090, -0.7851],\n",
      "        [-0.3843, -1.1424],\n",
      "        [-0.7942, -0.6014],\n",
      "        [-0.5919, -0.8059],\n",
      "        [-0.7914, -0.6037],\n",
      "        [-0.5846, -0.8149],\n",
      "        [-0.3286, -1.2727],\n",
      "        [-0.4450, -1.0239],\n",
      "        [-0.5198, -0.9030],\n",
      "        [-0.4478, -1.0190],\n",
      "        [-0.5869, -0.8120],\n",
      "        [-0.9250, -0.5051],\n",
      "        [-0.4774, -0.9687],\n",
      "        [-0.7137, -0.6730],\n",
      "        [-0.5139, -0.9117],\n",
      "        [-0.6571, -0.7306],\n",
      "        [-0.7097, -0.6769],\n",
      "        [-1.1504, -0.3806],\n",
      "        [-0.0414, -3.2059],\n",
      "        [-0.1814, -1.7964],\n",
      "        [-0.6305, -0.7599],\n",
      "        [-0.4619, -0.9945],\n",
      "        [-1.0131, -0.4511],\n",
      "        [-0.4685, -0.9833],\n",
      "        [-0.8150, -0.5845],\n",
      "        [-0.6555, -0.7323],\n",
      "        [-0.1086, -2.2740],\n",
      "        [-0.8681, -0.5443],\n",
      "        [-0.0067, -5.0075],\n",
      "        [-0.5567, -0.8511],\n",
      "        [-0.3877, -1.1352],\n",
      "        [-0.5318, -0.8856],\n",
      "        [-0.5864, -0.8127],\n",
      "        [-1.1846, -0.3651],\n",
      "        [-0.5903, -0.8078],\n",
      "        [-0.0635, -2.7880],\n",
      "        [-0.4190, -1.0722],\n",
      "        [-0.7963, -0.5997],\n",
      "        [-0.7361, -0.6520],\n",
      "        [-1.2333, -0.3444],\n",
      "        [-0.5699, -0.8337]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
      "        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,\n",
      "        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
      "        0, 0, 1, 1, 1, 0, 1, 1])\n",
      "tensor([[-0.7034, -0.6830],\n",
      "        [-0.3813, -1.1488],\n",
      "        [-0.7915, -0.6036],\n",
      "        [-0.5418, -0.8715],\n",
      "        [-0.6506, -0.7376],\n",
      "        [-0.4011, -1.1074],\n",
      "        [-0.6423, -0.7468],\n",
      "        [-0.6125, -0.7809],\n",
      "        [-1.7379, -0.1935],\n",
      "        [-0.4702, -0.9804],\n",
      "        [-0.4142, -1.0814],\n",
      "        [-0.3673, -1.1797],\n",
      "        [-0.2678, -1.4483],\n",
      "        [-0.6223, -0.7694],\n",
      "        [-0.3247, -1.2827],\n",
      "        [-0.4422, -1.0289],\n",
      "        [-0.6124, -0.7810],\n",
      "        [-0.4758, -0.9713],\n",
      "        [-0.6887, -0.6976],\n",
      "        [-0.8706, -0.5425],\n",
      "        [-0.3136, -1.3122],\n",
      "        [-0.6217, -0.7701],\n",
      "        [-0.4947, -0.9409],\n",
      "        [-0.2497, -1.5097],\n",
      "        [-0.5119, -0.9146],\n",
      "        [-0.6378, -0.7518],\n",
      "        [-0.5982, -0.7981],\n",
      "        [-0.2045, -1.6878],\n",
      "        [-0.4743, -0.9738],\n",
      "        [-0.6639, -0.7233],\n",
      "        [-0.4149, -1.0799],\n",
      "        [-0.5894, -0.8089],\n",
      "        [-0.8329, -0.5706],\n",
      "        [-0.7270, -0.6604],\n",
      "        [-0.5140, -0.9116],\n",
      "        [-0.1097, -2.2645],\n",
      "        [-0.1009, -2.3441],\n",
      "        [-0.4596, -0.9985],\n",
      "        [-0.6575, -0.7301],\n",
      "        [-0.5709, -0.8325],\n",
      "        [-0.7403, -0.6481],\n",
      "        [-0.4169, -1.0762],\n",
      "        [-0.5807, -0.8199],\n",
      "        [-0.8030, -0.5942],\n",
      "        [-0.2726, -1.4330],\n",
      "        [-0.6077, -0.7865],\n",
      "        [-0.5679, -0.8363],\n",
      "        [-1.4964, -0.2535],\n",
      "        [-0.2779, -1.4164],\n",
      "        [-0.8530, -0.5553],\n",
      "        [-0.6456, -0.7430],\n",
      "        [-0.7173, -0.6696],\n",
      "        [-0.3057, -1.3340],\n",
      "        [-1.1684, -0.3723],\n",
      "        [-0.2928, -1.3710],\n",
      "        [-0.1568, -1.9304],\n",
      "        [-0.5328, -0.8841],\n",
      "        [-0.5320, -0.8854],\n",
      "        [-0.1791, -1.8080],\n",
      "        [-0.6128, -0.7805],\n",
      "        [-0.2574, -1.4831],\n",
      "        [-1.1131, -0.3983],\n",
      "        [-0.7192, -0.6677],\n",
      "        [-0.1859, -1.7739],\n",
      "        [-0.6880, -0.6984],\n",
      "        [-0.3255, -1.2807],\n",
      "        [-0.6777, -0.7088],\n",
      "        [-0.1804, -1.8013],\n",
      "        [-0.3477, -1.2254],\n",
      "        [-0.0913, -2.4384],\n",
      "        [-0.2925, -1.3719],\n",
      "        [-0.5607, -0.8458],\n",
      "        [-0.1203, -2.1776],\n",
      "        [-0.4130, -1.0838],\n",
      "        [-0.3388, -1.2469],\n",
      "        [-0.4873, -0.9527],\n",
      "        [-0.2938, -1.3682],\n",
      "        [-0.4222, -1.0659],\n",
      "        [-0.6542, -0.7337],\n",
      "        [-0.3125, -1.3152],\n",
      "        [-0.1146, -2.2228],\n",
      "        [-0.2755, -1.4238],\n",
      "        [-0.3378, -1.2495],\n",
      "        [-0.2216, -1.6157],\n",
      "        [-0.7123, -0.6743],\n",
      "        [-0.5469, -0.8644],\n",
      "        [-0.1125, -2.2409],\n",
      "        [-0.2189, -1.6265],\n",
      "        [-0.5288, -0.8899],\n",
      "        [-0.5975, -0.7990],\n",
      "        [-0.4812, -0.9624],\n",
      "        [-0.8066, -0.5913],\n",
      "        [-0.7040, -0.6824],\n",
      "        [-0.9046, -0.5187],\n",
      "        [-1.1953, -0.3604],\n",
      "        [-0.2974, -1.3578],\n",
      "        [-0.2354, -1.5620],\n",
      "        [-0.9593, -0.4831],\n",
      "        [-0.1017, -2.3366],\n",
      "        [-0.3547, -1.2086],\n",
      "        [-0.3375, -1.2503],\n",
      "        [-0.3012, -1.3467],\n",
      "        [-0.2163, -1.6372],\n",
      "        [-0.1651, -1.8824],\n",
      "        [-0.0486, -3.0474],\n",
      "        [-0.2124, -1.6538],\n",
      "        [-0.8425, -0.5632],\n",
      "        [-0.0445, -3.1350],\n",
      "        [-0.5087, -0.9194],\n",
      "        [-1.5288, -0.2444],\n",
      "        [-0.6996, -0.6868],\n",
      "        [-0.7707, -0.6212],\n",
      "        [-0.4543, -1.0076],\n",
      "        [-0.4622, -0.9939],\n",
      "        [-0.5042, -0.9263],\n",
      "        [-0.6256, -0.7656],\n",
      "        [-0.7108, -0.6758],\n",
      "        [-0.1340, -2.0764],\n",
      "        [-0.0871, -2.4839],\n",
      "        [-0.4375, -1.0375],\n",
      "        [-0.5954, -0.8015],\n",
      "        [-0.6040, -0.7910],\n",
      "        [-0.7578, -0.6324],\n",
      "        [-0.2358, -1.5603],\n",
      "        [-0.6983, -0.6880],\n",
      "        [-0.5664, -0.8384],\n",
      "        [-0.3151, -1.3083],\n",
      "        [-0.6999, -0.6864]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
      "        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 1, 0, 0])\n",
      "tensor([[-4.3191e-01, -1.0477e+00],\n",
      "        [-3.2087e-01, -1.2929e+00],\n",
      "        [-6.2010e-01, -7.7196e-01],\n",
      "        [-5.6475e-02, -2.9021e+00],\n",
      "        [-2.0589e+00, -1.3651e-01],\n",
      "        [-5.6241e-01, -8.4359e-01],\n",
      "        [-5.8476e-01, -8.1472e-01],\n",
      "        [-4.8083e-01, -9.6304e-01],\n",
      "        [-7.0635e-01, -6.8011e-01],\n",
      "        [-3.8451e-01, -1.1419e+00],\n",
      "        [-3.0076e-01, -1.3481e+00],\n",
      "        [-4.7732e-01, -9.6875e-01],\n",
      "        [-4.3730e-01, -1.0378e+00],\n",
      "        [-1.2807e+00, -3.2550e-01],\n",
      "        [-7.8065e-01, -6.1269e-01],\n",
      "        [-1.5325e-01, -1.9513e+00],\n",
      "        [-3.9596e-01, -1.1179e+00],\n",
      "        [-5.7970e-01, -8.2113e-01],\n",
      "        [-6.2509e-01, -7.6617e-01],\n",
      "        [-8.2866e-01, -5.7383e-01],\n",
      "        [-1.0912e-01, -2.2694e+00],\n",
      "        [-5.9397e-01, -8.0326e-01],\n",
      "        [-4.1500e-01, -1.0798e+00],\n",
      "        [-4.0107e-01, -1.1075e+00],\n",
      "        [-6.8111e-01, -7.0533e-01],\n",
      "        [-1.2009e+00, -3.5798e-01],\n",
      "        [-5.3088e-01, -8.8694e-01],\n",
      "        [-5.3408e-01, -8.8239e-01],\n",
      "        [-5.7266e-01, -8.3017e-01],\n",
      "        [-3.5517e-01, -1.2075e+00],\n",
      "        [-4.6001e-01, -9.9770e-01],\n",
      "        [-7.1303e-01, -6.7365e-01],\n",
      "        [-3.5170e-01, -1.2157e+00],\n",
      "        [-7.0321e-01, -6.8318e-01],\n",
      "        [-5.7199e-01, -8.3103e-01],\n",
      "        [-8.3601e-01, -5.6817e-01],\n",
      "        [-5.8534e-01, -8.1400e-01],\n",
      "        [-5.3786e-01, -8.7706e-01],\n",
      "        [-3.8409e-01, -1.1428e+00],\n",
      "        [-3.8666e-01, -1.1373e+00],\n",
      "        [-1.0530e+00, -4.2905e-01],\n",
      "        [-5.1701e-01, -9.0708e-01],\n",
      "        [-4.5216e-01, -1.0113e+00],\n",
      "        [-7.1938e-01, -6.6758e-01],\n",
      "        [-3.8570e-01, -1.1394e+00],\n",
      "        [-5.3391e-01, -8.8263e-01],\n",
      "        [-6.2827e-01, -7.6253e-01],\n",
      "        [-9.0666e-02, -2.4456e+00],\n",
      "        [-6.9912e-01, -6.8721e-01],\n",
      "        [-6.7577e-01, -7.1083e-01],\n",
      "        [-4.4762e-01, -1.0193e+00],\n",
      "        [-1.9417e-01, -1.7345e+00],\n",
      "        [-8.1807e-01, -5.8211e-01],\n",
      "        [-5.9717e-01, -7.9932e-01],\n",
      "        [-4.1104e-01, -1.0876e+00],\n",
      "        [-3.7334e-01, -1.1661e+00],\n",
      "        [-2.9444e-01, -1.3663e+00],\n",
      "        [-6.2673e-01, -7.6429e-01],\n",
      "        [-6.5958e-01, -7.2788e-01],\n",
      "        [-4.6021e-01, -9.9737e-01],\n",
      "        [-5.0793e-01, -9.2065e-01],\n",
      "        [-2.7523e-01, -1.4246e+00],\n",
      "        [-5.5829e-01, -8.4907e-01],\n",
      "        [-5.0971e-01, -9.1797e-01],\n",
      "        [-1.5049e+00, -2.5109e-01],\n",
      "        [-2.2432e-01, -1.6047e+00],\n",
      "        [-6.3104e-01, -7.5936e-01],\n",
      "        [-4.3311e-01, -1.0455e+00],\n",
      "        [-6.3133e-01, -7.5904e-01],\n",
      "        [-3.4625e-01, -1.2287e+00],\n",
      "        [-4.7460e-01, -9.7322e-01],\n",
      "        [-3.4149e-01, -1.2403e+00],\n",
      "        [-6.6630e-01, -7.2073e-01],\n",
      "        [-7.5578e-01, -6.3420e-01],\n",
      "        [-9.3696e-02, -2.4142e+00],\n",
      "        [-8.2945e-01, -5.7322e-01],\n",
      "        [-3.5576e-01, -1.2061e+00],\n",
      "        [-4.9892e-01, -9.3441e-01],\n",
      "        [-2.3026e-01, -1.5814e+00],\n",
      "        [-5.7051e-01, -8.3295e-01],\n",
      "        [-2.1179e-01, -1.6562e+00],\n",
      "        [-2.1165e-01, -1.6568e+00],\n",
      "        [-6.2261e-01, -7.6904e-01],\n",
      "        [-3.9927e-01, -1.1111e+00],\n",
      "        [-3.1988e-01, -1.2955e+00],\n",
      "        [-2.6933e-01, -1.4435e+00],\n",
      "        [-6.8382e-01, -7.0256e-01],\n",
      "        [-1.3131e+00, -3.1331e-01],\n",
      "        [-7.4784e-01, -6.4129e-01],\n",
      "        [-4.6111e-01, -9.9583e-01],\n",
      "        [-4.3845e-01, -1.0357e+00],\n",
      "        [-9.9237e-02, -2.3595e+00],\n",
      "        [-3.4584e-01, -1.2297e+00],\n",
      "        [-5.3391e-01, -8.8263e-01],\n",
      "        [-1.0251e+00, -4.4437e-01],\n",
      "        [-6.9750e-01, -6.8882e-01],\n",
      "        [-3.9748e-01, -1.1148e+00],\n",
      "        [-8.1415e-01, -5.8522e-01],\n",
      "        [-5.4580e-02, -2.9353e+00],\n",
      "        [-6.7096e-02, -2.7350e+00],\n",
      "        [-3.8238e-01, -1.1464e+00],\n",
      "        [-4.8922e-01, -9.4960e-01],\n",
      "        [-2.6954e-01, -1.4428e+00],\n",
      "        [-5.7976e-01, -8.2105e-01],\n",
      "        [-1.2171e-03, -6.7119e+00],\n",
      "        [-5.5657e-01, -8.5137e-01],\n",
      "        [-1.5701e-01, -1.9289e+00],\n",
      "        [-5.2940e-01, -8.8906e-01],\n",
      "        [-4.5576e-01, -1.0050e+00],\n",
      "        [-3.7434e-01, -1.1639e+00],\n",
      "        [-4.7661e-02, -3.0674e+00],\n",
      "        [-1.1180e-01, -2.2464e+00],\n",
      "        [-4.1964e-01, -1.0708e+00],\n",
      "        [-4.5393e-01, -1.0082e+00],\n",
      "        [-3.6297e-01, -1.1894e+00],\n",
      "        [-4.4824e-01, -1.0182e+00],\n",
      "        [-7.7485e-01, -6.1762e-01],\n",
      "        [-2.0088e-01, -1.7038e+00],\n",
      "        [-5.6734e-01, -8.3709e-01],\n",
      "        [-6.0190e-01, -7.9356e-01],\n",
      "        [-1.9250e-01, -1.7424e+00],\n",
      "        [-4.8844e-01, -9.5084e-01],\n",
      "        [-4.8682e-01, -9.5341e-01],\n",
      "        [-2.2072e-01, -1.6192e+00],\n",
      "        [-3.2946e-01, -1.2705e+00],\n",
      "        [-5.8913e-01, -8.0926e-01],\n",
      "        [-7.7675e-02, -2.5938e+00],\n",
      "        [-6.8933e-01, -6.9698e-01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
      "        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
      "        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
      "        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 0])\n",
      "tensor([[-0.2489, -1.5127],\n",
      "        [-0.3517, -1.2156],\n",
      "        [-0.3271, -1.2766],\n",
      "        [-0.4743, -0.9737],\n",
      "        [-0.6763, -0.7103],\n",
      "        [-0.2776, -1.4170],\n",
      "        [-0.4378, -1.0370],\n",
      "        [-0.3396, -1.2449],\n",
      "        [-0.6999, -0.6864],\n",
      "        [-0.4959, -0.9390],\n",
      "        [-0.6385, -0.7510],\n",
      "        [-0.7221, -0.6650],\n",
      "        [-0.2419, -1.5376],\n",
      "        [-0.5793, -0.8217],\n",
      "        [-0.1474, -1.9873],\n",
      "        [-0.6667, -0.7203],\n",
      "        [-0.8052, -0.5924],\n",
      "        [-1.3316, -0.3066],\n",
      "        [-0.0600, -2.8431],\n",
      "        [-0.5874, -0.8114],\n",
      "        [-0.6256, -0.7656],\n",
      "        [-0.6275, -0.7634],\n",
      "        [-0.3131, -1.3137],\n",
      "        [-0.3745, -1.1636],\n",
      "        [-0.6683, -0.7186],\n",
      "        [-0.5717, -0.8314],\n",
      "        [-0.5838, -0.8160],\n",
      "        [-0.1619, -1.9009],\n",
      "        [-0.7130, -0.6736],\n",
      "        [-0.5207, -0.9016],\n",
      "        [-0.8924, -0.5271],\n",
      "        [-0.4000, -1.1096],\n",
      "        [-0.7277, -0.6598],\n",
      "        [-0.3112, -1.3188],\n",
      "        [-0.6057, -0.7890],\n",
      "        [-0.5762, -0.8257],\n",
      "        [-0.0515, -2.9912],\n",
      "        [-0.5032, -0.9278],\n",
      "        [-0.7575, -0.6327],\n",
      "        [-0.7089, -0.6776],\n",
      "        [-0.1181, -2.1946],\n",
      "        [-0.3329, -1.2617],\n",
      "        [-0.3790, -1.1537],\n",
      "        [-0.1622, -1.8989],\n",
      "        [-0.5393, -0.8751],\n",
      "        [-0.1684, -1.8647],\n",
      "        [-0.2372, -1.5550],\n",
      "        [-0.4495, -1.0159],\n",
      "        [-0.5533, -0.8558],\n",
      "        [-0.7567, -0.6334],\n",
      "        [-0.3618, -1.1922],\n",
      "        [-1.0622, -0.4242],\n",
      "        [-0.0440, -3.1457],\n",
      "        [-0.3147, -1.3094],\n",
      "        [-0.5327, -0.8843],\n",
      "        [-1.2853, -0.3237],\n",
      "        [-0.5480, -0.8630],\n",
      "        [-0.1330, -2.0834],\n",
      "        [-0.5453, -0.8667],\n",
      "        [-0.5801, -0.8206],\n",
      "        [-0.1594, -1.9147],\n",
      "        [-0.3239, -1.2849],\n",
      "        [-0.5761, -0.8257],\n",
      "        [-0.5224, -0.8992],\n",
      "        [-0.2383, -1.5511],\n",
      "        [-0.7645, -0.6266],\n",
      "        [-0.2826, -1.4016],\n",
      "        [-0.3565, -1.2045],\n",
      "        [-0.2966, -1.3601],\n",
      "        [-0.3982, -1.1133],\n",
      "        [-0.4176, -1.0748],\n",
      "        [-0.2246, -1.6037],\n",
      "        [-0.4653, -0.9887],\n",
      "        [-0.7522, -0.6374],\n",
      "        [-0.6144, -0.7786],\n",
      "        [-0.3340, -1.2591],\n",
      "        [-0.8228, -0.5784],\n",
      "        [-0.7030, -0.6834],\n",
      "        [-0.4905, -0.9475],\n",
      "        [-0.8280, -0.5743],\n",
      "        [-0.5762, -0.8256],\n",
      "        [-0.5597, -0.8471],\n",
      "        [-0.1653, -1.8813],\n",
      "        [-0.2056, -1.6828],\n",
      "        [-0.6363, -0.7534],\n",
      "        [-0.2525, -1.4999],\n",
      "        [-0.5695, -0.8342],\n",
      "        [-0.2814, -1.4053],\n",
      "        [-0.6564, -0.7313],\n",
      "        [-0.7102, -0.6764],\n",
      "        [-0.3948, -1.1203],\n",
      "        [-0.1533, -1.9509],\n",
      "        [-0.1182, -2.1940],\n",
      "        [-0.4549, -1.0065],\n",
      "        [-0.4927, -0.9440],\n",
      "        [-0.6399, -0.7494],\n",
      "        [-0.8375, -0.5671],\n",
      "        [-0.3164, -1.3048],\n",
      "        [-0.2835, -1.3990],\n",
      "        [-0.5376, -0.8775],\n",
      "        [-0.1494, -1.9750],\n",
      "        [-0.6429, -0.7460],\n",
      "        [-0.4165, -1.0769],\n",
      "        [-0.8320, -0.5712],\n",
      "        [-0.6240, -0.7675],\n",
      "        [-0.5870, -0.8119],\n",
      "        [-0.2519, -1.5019],\n",
      "        [-0.4944, -0.9415],\n",
      "        [-0.4985, -0.9351],\n",
      "        [-0.3156, -1.3070],\n",
      "        [-0.3679, -1.1783],\n",
      "        [-0.6503, -0.7379],\n",
      "        [-0.7758, -0.6168],\n",
      "        [-0.6813, -0.7051],\n",
      "        [-0.7158, -0.6710],\n",
      "        [-0.3176, -1.3017],\n",
      "        [-0.4892, -0.9497],\n",
      "        [-0.1918, -1.7454],\n",
      "        [-0.5910, -0.8069],\n",
      "        [-0.6670, -0.7200],\n",
      "        [-0.1044, -2.3113],\n",
      "        [-0.7152, -0.6716],\n",
      "        [-0.8816, -0.5347],\n",
      "        [-0.3553, -1.2072],\n",
      "        [-0.4097, -1.0902],\n",
      "        [-0.5209, -0.9013],\n",
      "        [-0.3749, -1.1627],\n",
      "        [-0.5874, -0.8114]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
      "        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
      "        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
      "        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 1])\n",
      "tensor([[-0.5893, -0.8090],\n",
      "        [-0.4763, -0.9704],\n",
      "        [-0.5573, -0.8504],\n",
      "        [-0.2291, -1.5861],\n",
      "        [-0.1725, -1.8425],\n",
      "        [-0.5935, -0.8039],\n",
      "        [-0.5793, -0.8216],\n",
      "        [-0.5826, -0.8175],\n",
      "        [-0.9516, -0.4880],\n",
      "        [-0.6285, -0.7623],\n",
      "        [-0.5778, -0.8235],\n",
      "        [-0.6037, -0.7914],\n",
      "        [-0.4303, -1.0507],\n",
      "        [-0.6997, -0.6866],\n",
      "        [-0.3193, -1.2970],\n",
      "        [-0.4575, -1.0021],\n",
      "        [-0.8723, -0.5413],\n",
      "        [-0.3664, -1.1816],\n",
      "        [-0.6487, -0.7396],\n",
      "        [-0.5078, -0.9209],\n",
      "        [-0.6928, -0.6935],\n",
      "        [-0.4259, -1.0589],\n",
      "        [-0.5710, -0.8323],\n",
      "        [-0.5429, -0.8700],\n",
      "        [-0.5058, -0.9239],\n",
      "        [-0.0076, -4.8785],\n",
      "        [-0.6570, -0.7307],\n",
      "        [-0.7667, -0.6247],\n",
      "        [-0.5622, -0.8439],\n",
      "        [-0.5420, -0.8713],\n",
      "        [-0.3709, -1.1716],\n",
      "        [-0.4434, -1.0268],\n",
      "        [-0.4613, -0.9955],\n",
      "        [-0.5951, -0.8019],\n",
      "        [-0.2817, -1.4043],\n",
      "        [-0.2568, -1.4851],\n",
      "        [-0.3746, -1.1633],\n",
      "        [-0.6765, -0.7101],\n",
      "        [-0.5828, -0.8173],\n",
      "        [-0.5318, -0.8856],\n",
      "        [-0.2226, -1.6117],\n",
      "        [-0.7123, -0.6744],\n",
      "        [-0.5742, -0.8282],\n",
      "        [-0.7042, -0.6822],\n",
      "        [-0.3576, -1.2019],\n",
      "        [-0.7503, -0.6390],\n",
      "        [-0.6169, -0.7757],\n",
      "        [-0.5870, -0.8120],\n",
      "        [-0.6273, -0.7636],\n",
      "        [-0.6542, -0.7336],\n",
      "        [-0.6581, -0.7294],\n",
      "        [-0.2100, -1.6640],\n",
      "        [-0.4993, -0.9338],\n",
      "        [-0.6937, -0.6926],\n",
      "        [-0.6983, -0.6880],\n",
      "        [-0.5677, -0.8366],\n",
      "        [-0.5897, -0.8086],\n",
      "        [-0.7080, -0.6785],\n",
      "        [-0.0830, -2.5305],\n",
      "        [-0.7279, -0.6596],\n",
      "        [-0.2675, -1.4496],\n",
      "        [-0.5921, -0.8055],\n",
      "        [-0.7280, -0.6595],\n",
      "        [-0.2177, -1.6317],\n",
      "        [-0.6414, -0.7477],\n",
      "        [-0.9891, -0.4651],\n",
      "        [-0.7061, -0.6803],\n",
      "        [-0.8985, -0.5228],\n",
      "        [-0.4624, -0.9937],\n",
      "        [-0.1038, -2.3172],\n",
      "        [-0.5631, -0.8426],\n",
      "        [-0.5598, -0.8470],\n",
      "        [-0.3481, -1.2244],\n",
      "        [-0.4646, -0.9899],\n",
      "        [-0.5316, -0.8859],\n",
      "        [-0.1296, -2.1074],\n",
      "        [-0.6500, -0.7383],\n",
      "        [-0.5970, -0.7995],\n",
      "        [-1.1047, -0.4024],\n",
      "        [-0.2462, -1.5221],\n",
      "        [-0.7701, -0.6217],\n",
      "        [-0.2532, -1.4975],\n",
      "        [-0.7262, -0.6612],\n",
      "        [-0.4202, -1.0699],\n",
      "        [-0.4058, -1.0979],\n",
      "        [-0.3172, -1.3025],\n",
      "        [-0.6062, -0.7884],\n",
      "        [-0.6191, -0.7731],\n",
      "        [-0.2657, -1.4553],\n",
      "        [-0.5183, -0.9052],\n",
      "        [-0.4647, -0.9898],\n",
      "        [-0.5859, -0.8133],\n",
      "        [-0.6644, -0.7227],\n",
      "        [-0.7437, -0.6450],\n",
      "        [-0.4651, -0.9891],\n",
      "        [-0.5604, -0.8462],\n",
      "        [-0.6899, -0.6964],\n",
      "        [-0.8038, -0.5935],\n",
      "        [-0.6941, -0.6922],\n",
      "        [-0.5436, -0.8691],\n",
      "        [-1.1337, -0.3884],\n",
      "        [-0.5324, -0.8848],\n",
      "        [-0.3610, -1.1939],\n",
      "        [-0.6006, -0.7952],\n",
      "        [-0.5647, -0.8405],\n",
      "        [-0.4089, -1.0917],\n",
      "        [-0.3267, -1.2777],\n",
      "        [-0.2297, -1.5835],\n",
      "        [-0.5221, -0.8996],\n",
      "        [-0.6227, -0.7690],\n",
      "        [-0.3997, -1.1102],\n",
      "        [-0.4340, -1.0439],\n",
      "        [-0.3634, -1.1885],\n",
      "        [-0.7221, -0.6650],\n",
      "        [-0.7159, -0.6709],\n",
      "        [-0.0883, -2.4704],\n",
      "        [-0.6630, -0.7242],\n",
      "        [-0.6354, -0.7544],\n",
      "        [-0.6960, -0.6903],\n",
      "        [-0.4383, -1.0360],\n",
      "        [-0.8936, -0.5263],\n",
      "        [-0.6007, -0.7950],\n",
      "        [-0.4886, -0.9506],\n",
      "        [-0.5143, -0.9110],\n",
      "        [-0.4543, -1.0075],\n",
      "        [-0.4570, -1.0029],\n",
      "        [-0.4932, -0.9434],\n",
      "        [-0.6871, -0.6993]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
      "        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 1, 1, 0])\n",
      "tensor([[-0.6023, -0.7930],\n",
      "        [-0.5334, -0.8833],\n",
      "        [-0.5363, -0.8793],\n",
      "        [-0.6523, -0.7357],\n",
      "        [-0.6207, -0.7713],\n",
      "        [-0.5303, -0.8877],\n",
      "        [-0.9786, -0.4714],\n",
      "        [-0.7802, -0.6131],\n",
      "        [-0.6620, -0.7253],\n",
      "        [-0.1489, -1.9778],\n",
      "        [-0.6504, -0.7378],\n",
      "        [-0.4377, -1.0371],\n",
      "        [-0.5626, -0.8433],\n",
      "        [-0.4119, -1.0859],\n",
      "        [-0.3718, -1.1695],\n",
      "        [-0.6347, -0.7552],\n",
      "        [-0.6613, -0.7261],\n",
      "        [-0.1066, -2.2911],\n",
      "        [-0.4115, -1.0867],\n",
      "        [-0.6618, -0.7255],\n",
      "        [-0.6986, -0.6878],\n",
      "        [-0.2506, -1.5064],\n",
      "        [-0.8065, -0.5914],\n",
      "        [-0.2841, -1.3970],\n",
      "        [-0.7088, -0.6777],\n",
      "        [-0.6571, -0.7305],\n",
      "        [-0.7692, -0.6225],\n",
      "        [-0.3665, -1.1814],\n",
      "        [-0.5936, -0.8037],\n",
      "        [-0.6317, -0.7586],\n",
      "        [-0.5904, -0.8077],\n",
      "        [-0.5261, -0.8939],\n",
      "        [-0.6475, -0.7410],\n",
      "        [-0.4819, -0.9613],\n",
      "        [-0.6590, -0.7285],\n",
      "        [-0.8585, -0.5513],\n",
      "        [-0.5643, -0.8411],\n",
      "        [-0.3974, -1.1150],\n",
      "        [-0.9047, -0.5186],\n",
      "        [-0.4090, -1.0915],\n",
      "        [-0.5328, -0.8842],\n",
      "        [-0.3457, -1.2301],\n",
      "        [-0.2186, -1.6280],\n",
      "        [-0.7854, -0.6087],\n",
      "        [-0.5032, -0.9278],\n",
      "        [-0.3693, -1.1750],\n",
      "        [-0.8820, -0.5344],\n",
      "        [-0.4005, -1.1087],\n",
      "        [-0.7774, -0.6154],\n",
      "        [-0.4945, -0.9413],\n",
      "        [-0.6070, -0.7874],\n",
      "        [-0.5581, -0.8493],\n",
      "        [-0.6450, -0.7438],\n",
      "        [-0.6777, -0.7088],\n",
      "        [-0.4687, -0.9830],\n",
      "        [-0.5359, -0.8799],\n",
      "        [-0.7872, -0.6072],\n",
      "        [-0.5269, -0.8927],\n",
      "        [-0.4114, -1.0868],\n",
      "        [-0.6844, -0.7019],\n",
      "        [-0.3569, -1.2034],\n",
      "        [-0.6693, -0.7176],\n",
      "        [-0.5119, -0.9146],\n",
      "        [-0.5255, -0.8946],\n",
      "        [-0.7911, -0.6040],\n",
      "        [-0.5610, -0.8454],\n",
      "        [-0.6465, -0.7421],\n",
      "        [-0.4994, -0.9337],\n",
      "        [-0.4952, -0.9402],\n",
      "        [-0.3002, -1.3496],\n",
      "        [-0.5289, -0.8898],\n",
      "        [-0.6734, -0.7133],\n",
      "        [-0.4926, -0.9442],\n",
      "        [-0.9071, -0.5170],\n",
      "        [-0.4373, -1.0378],\n",
      "        [-1.0122, -0.4516],\n",
      "        [-0.3469, -1.2271],\n",
      "        [-0.4812, -0.9624],\n",
      "        [-0.5586, -0.8487],\n",
      "        [-0.5522, -0.8572],\n",
      "        [-0.6719, -0.7149],\n",
      "        [-0.5285, -0.8904],\n",
      "        [-0.2505, -1.5071],\n",
      "        [-0.7026, -0.6838],\n",
      "        [-0.6730, -0.7137],\n",
      "        [-0.7596, -0.6309],\n",
      "        [-0.5560, -0.8521],\n",
      "        [-0.5517, -0.8579],\n",
      "        [-0.6131, -0.7801],\n",
      "        [-0.4997, -0.9333],\n",
      "        [-0.4558, -1.0050],\n",
      "        [-0.5216, -0.9004],\n",
      "        [-0.5408, -0.8730],\n",
      "        [-0.3856, -1.1395],\n",
      "        [-0.5283, -0.8907],\n",
      "        [-0.6956, -0.6907],\n",
      "        [-0.5959, -0.8008],\n",
      "        [-0.6896, -0.6967],\n",
      "        [-0.2417, -1.5386],\n",
      "        [-0.5216, -0.9004],\n",
      "        [-0.7159, -0.6709],\n",
      "        [-0.2161, -1.6380],\n",
      "        [-0.6315, -0.7588],\n",
      "        [-0.3617, -1.1922],\n",
      "        [-0.5334, -0.8834],\n",
      "        [-0.6271, -0.7638],\n",
      "        [-0.6680, -0.7190],\n",
      "        [-0.5670, -0.8375],\n",
      "        [-0.7455, -0.6434],\n",
      "        [-0.7115, -0.6752],\n",
      "        [-0.6378, -0.7517],\n",
      "        [-0.9265, -0.5041],\n",
      "        [-0.6486, -0.7398],\n",
      "        [-0.5913, -0.8066],\n",
      "        [-0.5999, -0.7960],\n",
      "        [-0.2724, -1.4337],\n",
      "        [-1.1357, -0.3874],\n",
      "        [-0.2178, -1.6309],\n",
      "        [-0.3400, -1.2439],\n",
      "        [-0.8284, -0.5740],\n",
      "        [-0.6588, -0.7287],\n",
      "        [-0.6563, -0.7314],\n",
      "        [-0.4955, -0.9397],\n",
      "        [-0.6760, -0.7106],\n",
      "        [-0.6052, -0.7896],\n",
      "        [-0.5446, -0.8677],\n",
      "        [-0.0794, -2.5723],\n",
      "        [-0.2874, -1.3873]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
      "        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0])\n",
      "tensor([[-0.7964, -0.5995],\n",
      "        [-0.6178, -0.7747],\n",
      "        [-0.7235, -0.6637],\n",
      "        [-0.7728, -0.6194],\n",
      "        [-0.9416, -0.4943],\n",
      "        [-0.3957, -1.1184],\n",
      "        [-0.4572, -1.0026],\n",
      "        [-0.7095, -0.6771],\n",
      "        [-0.5544, -0.8542],\n",
      "        [-0.6641, -0.7231],\n",
      "        [-0.4795, -0.9652],\n",
      "        [-0.5720, -0.8310],\n",
      "        [-0.7645, -0.6266],\n",
      "        [-0.6685, -0.7185],\n",
      "        [-0.3106, -1.3206],\n",
      "        [-0.2957, -1.3626],\n",
      "        [-0.6590, -0.7285],\n",
      "        [-0.8907, -0.5283],\n",
      "        [-0.6495, -0.7388],\n",
      "        [-0.8442, -0.5620],\n",
      "        [-0.4458, -1.0225],\n",
      "        [-0.5000, -0.9327],\n",
      "        [-0.6537, -0.7343],\n",
      "        [-0.4989, -0.9345],\n",
      "        [-0.8824, -0.5341],\n",
      "        [-0.8892, -0.5293],\n",
      "        [-0.4695, -0.9817],\n",
      "        [-0.3440, -1.2341],\n",
      "        [-0.7967, -0.5993],\n",
      "        [-0.4645, -0.9900],\n",
      "        [-0.5640, -0.8415],\n",
      "        [-0.5408, -0.8729],\n",
      "        [-0.9019, -0.5205],\n",
      "        [-0.5003, -0.9323],\n",
      "        [-0.7770, -0.6158],\n",
      "        [-0.9613, -0.4819],\n",
      "        [-0.7499, -0.6394],\n",
      "        [-0.4892, -0.9496],\n",
      "        [-0.7913, -0.6037],\n",
      "        [-0.2632, -1.4635],\n",
      "        [-0.6924, -0.6939],\n",
      "        [-0.6676, -0.7194],\n",
      "        [-0.5870, -0.8119],\n",
      "        [-0.7475, -0.6416],\n",
      "        [-0.3389, -1.2468],\n",
      "        [-0.6426, -0.7464],\n",
      "        [-0.4583, -1.0007],\n",
      "        [-0.1878, -1.7647],\n",
      "        [-0.1400, -2.0356],\n",
      "        [-0.7977, -0.5985],\n",
      "        [-0.7513, -0.6382],\n",
      "        [-0.6299, -0.7607],\n",
      "        [-0.5845, -0.8150],\n",
      "        [-0.4670, -0.9858],\n",
      "        [-0.4213, -1.0676],\n",
      "        [-0.4943, -0.9415],\n",
      "        [-0.5200, -0.9027],\n",
      "        [-0.5654, -0.8397],\n",
      "        [-0.5843, -0.8153],\n",
      "        [-1.0679, -0.4212],\n",
      "        [-0.5874, -0.8114],\n",
      "        [-0.6104, -0.7833],\n",
      "        [-0.5309, -0.8868],\n",
      "        [-0.6248, -0.7665],\n",
      "        [-0.7015, -0.6849],\n",
      "        [-0.2728, -1.4323],\n",
      "        [-0.6713, -0.7154],\n",
      "        [-0.6253, -0.7659],\n",
      "        [-0.5807, -0.8199],\n",
      "        [-0.7810, -0.6124],\n",
      "        [-0.6184, -0.7740],\n",
      "        [-0.5753, -0.8268],\n",
      "        [-0.4885, -0.9507],\n",
      "        [-0.3893, -1.1317],\n",
      "        [-0.6146, -0.7784],\n",
      "        [-0.7782, -0.6147],\n",
      "        [-0.5915, -0.8063],\n",
      "        [-0.5065, -0.9228],\n",
      "        [-0.6772, -0.7093],\n",
      "        [-0.6824, -0.7040],\n",
      "        [-0.6680, -0.7190],\n",
      "        [-0.3273, -1.2762],\n",
      "        [-0.1636, -1.8912],\n",
      "        [-0.4630, -0.9925],\n",
      "        [-0.6548, -0.7331],\n",
      "        [-0.0521, -2.9799],\n",
      "        [-0.7179, -0.6690],\n",
      "        [-0.6950, -0.6913],\n",
      "        [-0.6207, -0.7712],\n",
      "        [-0.2432, -1.5331],\n",
      "        [-0.5658, -0.8391],\n",
      "        [-0.3969, -1.1159],\n",
      "        [-0.4011, -1.1074],\n",
      "        [-0.8762, -0.5385],\n",
      "        [-0.9431, -0.4933],\n",
      "        [-0.7736, -0.6187],\n",
      "        [-0.5399, -0.8742],\n",
      "        [-1.1553, -0.3783],\n",
      "        [-0.2267, -1.5955],\n",
      "        [-0.4878, -0.9518],\n",
      "        [-0.5986, -0.7976],\n",
      "        [-0.5449, -0.8672],\n",
      "        [-0.8534, -0.5550],\n",
      "        [-0.6493, -0.7390],\n",
      "        [-0.7112, -0.6754],\n",
      "        [-0.3861, -1.1384],\n",
      "        [-0.5694, -0.8344],\n",
      "        [-0.2743, -1.4274],\n",
      "        [-0.6044, -0.7906],\n",
      "        [-0.7910, -0.6040],\n",
      "        [-0.7404, -0.6480],\n",
      "        [-0.6686, -0.7183],\n",
      "        [-0.7328, -0.6551],\n",
      "        [-0.7348, -0.6531],\n",
      "        [-0.3275, -1.2756],\n",
      "        [-0.7246, -0.6626],\n",
      "        [-0.8005, -0.5962],\n",
      "        [-0.7172, -0.6697],\n",
      "        [-1.1054, -0.4021],\n",
      "        [-0.6817, -0.7047],\n",
      "        [-0.5122, -0.9142],\n",
      "        [-0.8353, -0.5687],\n",
      "        [-0.5468, -0.8646],\n",
      "        [-0.5908, -0.8072],\n",
      "        [-0.6952, -0.6911],\n",
      "        [-0.3018, -1.3450],\n",
      "        [-0.7098, -0.6767],\n",
      "        [-0.6381, -0.7514]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
      "        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
      "        0, 0, 1, 1, 1, 0, 0, 1])\n",
      "tensor([[-0.7006, -0.6858],\n",
      "        [-0.6539, -0.7340],\n",
      "        [-0.5604, -0.8463],\n",
      "        [-0.4320, -1.0476],\n",
      "        [-0.9567, -0.4848],\n",
      "        [-0.6592, -0.7283],\n",
      "        [-0.6662, -0.7209],\n",
      "        [-0.8611, -0.5494],\n",
      "        [-0.6220, -0.7698],\n",
      "        [-0.5897, -0.8086],\n",
      "        [-0.7398, -0.6486],\n",
      "        [-0.6509, -0.7373],\n",
      "        [-0.6772, -0.7093],\n",
      "        [-0.7235, -0.6637],\n",
      "        [-0.5713, -0.8319],\n",
      "        [-1.4302, -0.2735],\n",
      "        [-0.3865, -1.1377],\n",
      "        [-0.6681, -0.7188],\n",
      "        [-0.3946, -1.1207],\n",
      "        [-0.8787, -0.5367],\n",
      "        [-0.1537, -1.9489],\n",
      "        [-0.6753, -0.7114],\n",
      "        [-0.4440, -1.0257],\n",
      "        [-0.6300, -0.7606],\n",
      "        [-0.5793, -0.8217],\n",
      "        [-0.7047, -0.6818],\n",
      "        [-0.7091, -0.6775],\n",
      "        [-0.6173, -0.7753],\n",
      "        [-0.7826, -0.6111],\n",
      "        [-0.7674, -0.6240],\n",
      "        [-0.7465, -0.6425],\n",
      "        [-0.4329, -1.0458],\n",
      "        [-0.7692, -0.6225],\n",
      "        [-0.7503, -0.6391],\n",
      "        [-0.6859, -0.7004],\n",
      "        [-0.6458, -0.7429],\n",
      "        [-0.5335, -0.8832],\n",
      "        [-0.5897, -0.8085],\n",
      "        [-0.6924, -0.6939],\n",
      "        [-0.6917, -0.6946],\n",
      "        [-0.6743, -0.7123],\n",
      "        [-0.4252, -1.0603],\n",
      "        [-0.8740, -0.5400],\n",
      "        [-0.2140, -1.6468],\n",
      "        [-0.3902, -1.1299],\n",
      "        [-0.5189, -0.9043],\n",
      "        [-0.6576, -0.7300],\n",
      "        [-0.7687, -0.6229],\n",
      "        [-0.5856, -0.8136],\n",
      "        [-0.9896, -0.4648],\n",
      "        [-0.6668, -0.7202],\n",
      "        [-0.4793, -0.9655],\n",
      "        [-0.8876, -0.5304],\n",
      "        [-0.8530, -0.5553],\n",
      "        [-0.6838, -0.7026],\n",
      "        [-0.6071, -0.7873],\n",
      "        [-0.2107, -1.6608],\n",
      "        [-0.7515, -0.6380],\n",
      "        [-0.5937, -0.8035],\n",
      "        [-0.7764, -0.6163],\n",
      "        [-0.5748, -0.8274],\n",
      "        [-0.6003, -0.7955],\n",
      "        [-0.4701, -0.9807],\n",
      "        [-0.8485, -0.5587],\n",
      "        [-0.5636, -0.8419],\n",
      "        [-0.6800, -0.7065],\n",
      "        [-1.0715, -0.4193],\n",
      "        [-0.7197, -0.6673],\n",
      "        [-0.3958, -1.1183],\n",
      "        [-0.8017, -0.5952],\n",
      "        [-0.6301, -0.7605],\n",
      "        [-0.7671, -0.6243],\n",
      "        [-0.8312, -0.5718],\n",
      "        [-0.6363, -0.7535],\n",
      "        [-0.3574, -1.2024],\n",
      "        [-0.5573, -0.8503],\n",
      "        [-0.7573, -0.6329],\n",
      "        [-0.5978, -0.7986],\n",
      "        [-0.5732, -0.8294],\n",
      "        [-0.8631, -0.5479],\n",
      "        [-0.6852, -0.7012],\n",
      "        [-1.1778, -0.3681],\n",
      "        [-0.7164, -0.6704],\n",
      "        [-0.7782, -0.6148],\n",
      "        [-0.5461, -0.8657],\n",
      "        [-0.6355, -0.7543],\n",
      "        [-0.8602, -0.5500],\n",
      "        [-0.5593, -0.8477],\n",
      "        [-0.8492, -0.5582],\n",
      "        [-0.6345, -0.7554],\n",
      "        [-0.6299, -0.7606],\n",
      "        [-0.5797, -0.8212],\n",
      "        [-0.8508, -0.5570],\n",
      "        [-0.5733, -0.8293],\n",
      "        [-0.6554, -0.7324],\n",
      "        [-0.6137, -0.7795],\n",
      "        [-0.7465, -0.6425],\n",
      "        [-0.8446, -0.5617],\n",
      "        [-0.4035, -1.1026],\n",
      "        [-1.0073, -0.4545],\n",
      "        [-0.7122, -0.6745],\n",
      "        [-0.7891, -0.6056],\n",
      "        [-0.5424, -0.8708],\n",
      "        [-0.5178, -0.9059],\n",
      "        [-0.9423, -0.4939],\n",
      "        [-0.5135, -0.9123],\n",
      "        [-0.6896, -0.6967],\n",
      "        [-0.8001, -0.5966],\n",
      "        [-0.6316, -0.7587],\n",
      "        [-0.6884, -0.6979],\n",
      "        [-0.6917, -0.6945],\n",
      "        [-0.8117, -0.5872],\n",
      "        [-0.6260, -0.7651],\n",
      "        [-0.7277, -0.6598],\n",
      "        [-0.6944, -0.6919],\n",
      "        [-0.9385, -0.4963],\n",
      "        [-0.6798, -0.7066],\n",
      "        [-0.6862, -0.7001],\n",
      "        [-0.8609, -0.5496],\n",
      "        [-0.4493, -1.0163],\n",
      "        [-1.0085, -0.4538],\n",
      "        [-0.5924, -0.8051],\n",
      "        [-0.5344, -0.8820],\n",
      "        [-0.7757, -0.6169],\n",
      "        [-0.7462, -0.6428],\n",
      "        [-0.5009, -0.9314],\n",
      "        [-0.7433, -0.6453],\n",
      "        [-0.4583, -1.0006]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
      "        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
      "        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
      "        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0])\n",
      "tensor([[-0.7003, -0.6860],\n",
      "        [-0.2748, -1.4259],\n",
      "        [-0.6244, -0.7670],\n",
      "        [-0.9440, -0.4927],\n",
      "        [-0.6941, -0.6922],\n",
      "        [-0.6817, -0.7047],\n",
      "        [-1.3410, -0.3032],\n",
      "        [-1.0317, -0.4407],\n",
      "        [-0.6515, -0.7366],\n",
      "        [-0.7178, -0.6690],\n",
      "        [-0.5931, -0.8043],\n",
      "        [-0.6186, -0.7737],\n",
      "        [-0.5806, -0.8200],\n",
      "        [-0.6668, -0.7202],\n",
      "        [-0.6308, -0.7596],\n",
      "        [-0.7043, -0.6821],\n",
      "        [-0.6225, -0.7691],\n",
      "        [-0.5366, -0.8789],\n",
      "        [-0.6763, -0.7102],\n",
      "        [-0.4936, -0.9428],\n",
      "        [-0.7298, -0.6578],\n",
      "        [-0.6522, -0.7358],\n",
      "        [-0.9032, -0.5196],\n",
      "        [-0.8937, -0.5261],\n",
      "        [-0.8367, -0.5676],\n",
      "        [-0.7711, -0.6209],\n",
      "        [-0.6341, -0.7559],\n",
      "        [-0.5627, -0.8431],\n",
      "        [-0.8651, -0.5465],\n",
      "        [-1.0107, -0.4525],\n",
      "        [-0.7075, -0.6790],\n",
      "        [-0.9671, -0.4783],\n",
      "        [-0.7116, -0.6750],\n",
      "        [-0.6732, -0.7135],\n",
      "        [-0.6971, -0.6892],\n",
      "        [-0.5015, -0.9304],\n",
      "        [-0.6054, -0.7893],\n",
      "        [-0.8647, -0.5468],\n",
      "        [-0.7558, -0.6342],\n",
      "        [-0.6934, -0.6929],\n",
      "        [-0.6029, -0.7924],\n",
      "        [-0.7962, -0.5998],\n",
      "        [-0.2150, -1.6427],\n",
      "        [-0.6592, -0.7283],\n",
      "        [-0.7285, -0.6590],\n",
      "        [-0.6947, -0.6916],\n",
      "        [-0.6509, -0.7372],\n",
      "        [-0.6160, -0.7768],\n",
      "        [-0.8068, -0.5911],\n",
      "        [-0.6872, -0.6992],\n",
      "        [-0.7371, -0.6511],\n",
      "        [-0.7062, -0.6803],\n",
      "        [-0.7111, -0.6756],\n",
      "        [-0.5611, -0.8453],\n",
      "        [-0.7130, -0.6737],\n",
      "        [-0.6050, -0.7899],\n",
      "        [-0.8411, -0.5643],\n",
      "        [-0.6878, -0.6985],\n",
      "        [-0.6741, -0.7125],\n",
      "        [-0.2900, -1.3794],\n",
      "        [-0.2276, -1.5916],\n",
      "        [-0.8115, -0.5873],\n",
      "        [-0.5027, -0.9287],\n",
      "        [-0.3759, -1.1606],\n",
      "        [-0.8645, -0.5469],\n",
      "        [-0.6410, -0.7482],\n",
      "        [-0.6900, -0.6963],\n",
      "        [-0.6372, -0.7524],\n",
      "        [-0.8181, -0.5821],\n",
      "        [-0.6773, -0.7092],\n",
      "        [-0.7730, -0.6192],\n",
      "        [-0.8043, -0.5931],\n",
      "        [-0.3442, -1.2336],\n",
      "        [-0.6807, -0.7058],\n",
      "        [-0.8623, -0.5485],\n",
      "        [-0.5311, -0.8867],\n",
      "        [-0.6808, -0.7057],\n",
      "        [-0.6239, -0.7675],\n",
      "        [-0.7454, -0.6435],\n",
      "        [-0.7111, -0.6755],\n",
      "        [-0.6152, -0.7776],\n",
      "        [-0.6249, -0.7664],\n",
      "        [-0.6433, -0.7456],\n",
      "        [-0.5150, -0.9101],\n",
      "        [-0.7503, -0.6391],\n",
      "        [-0.5819, -0.8184],\n",
      "        [-0.6334, -0.7567],\n",
      "        [-0.4247, -1.0613],\n",
      "        [-0.6651, -0.7220],\n",
      "        [-0.5293, -0.8892],\n",
      "        [-0.5851, -0.8143],\n",
      "        [-0.8671, -0.5450],\n",
      "        [-0.6688, -0.7181],\n",
      "        [-0.7076, -0.6789],\n",
      "        [-1.0549, -0.4281],\n",
      "        [-0.7662, -0.6251],\n",
      "        [-0.5500, -0.8602],\n",
      "        [-0.5851, -0.8143],\n",
      "        [-0.6705, -0.7163],\n",
      "        [-0.5127, -0.9135],\n",
      "        [-0.7288, -0.6587],\n",
      "        [-0.5023, -0.9291],\n",
      "        [-0.5808, -0.8197],\n",
      "        [-0.7443, -0.6445],\n",
      "        [-0.6483, -0.7401],\n",
      "        [-0.6848, -0.7016],\n",
      "        [-0.5741, -0.8284],\n",
      "        [-0.8942, -0.5258],\n",
      "        [-0.4665, -0.9867],\n",
      "        [-0.7621, -0.6286],\n",
      "        [-0.8466, -0.5601],\n",
      "        [-0.6375, -0.7521],\n",
      "        [-0.6892, -0.6971],\n",
      "        [-0.7246, -0.6627],\n",
      "        [-0.7899, -0.6050],\n",
      "        [-0.6686, -0.7183],\n",
      "        [-0.6520, -0.7361],\n",
      "        [-0.8567, -0.5526],\n",
      "        [-0.5817, -0.8185],\n",
      "        [-0.8030, -0.5941],\n",
      "        [-1.0447, -0.4336],\n",
      "        [-0.7232, -0.6640],\n",
      "        [-0.5702, -0.8333],\n",
      "        [-0.3226, -1.2884],\n",
      "        [-0.6589, -0.7286],\n",
      "        [-0.6943, -0.6920],\n",
      "        [-0.5943, -0.8029],\n",
      "        [-0.8028, -0.5943]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
      "        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        0, 1, 0, 0, 1, 1, 1, 1])\n",
      "tensor([[-0.5793, -0.8216],\n",
      "        [-0.6825, -0.7039],\n",
      "        [-0.6801, -0.7064],\n",
      "        [-0.5834, -0.8165],\n",
      "        [-0.6251, -0.7661],\n",
      "        [-0.8662, -0.5456],\n",
      "        [-0.4367, -1.0390],\n",
      "        [-0.9944, -0.4619],\n",
      "        [-0.6874, -0.6989],\n",
      "        [-0.5663, -0.8385],\n",
      "        [-0.9950, -0.4616],\n",
      "        [-0.6342, -0.7558],\n",
      "        [-0.8753, -0.5391],\n",
      "        [-0.5322, -0.8850],\n",
      "        [-0.6375, -0.7521],\n",
      "        [-0.6498, -0.7384],\n",
      "        [-0.6306, -0.7599],\n",
      "        [-0.3527, -1.2133],\n",
      "        [-0.5276, -0.8916],\n",
      "        [-0.9909, -0.4640],\n",
      "        [-0.6474, -0.7411],\n",
      "        [-0.6662, -0.7208],\n",
      "        [-0.6486, -0.7397],\n",
      "        [-0.6097, -0.7842],\n",
      "        [-0.6965, -0.6898],\n",
      "        [-0.6232, -0.7684],\n",
      "        [-0.6033, -0.7919],\n",
      "        [-0.6971, -0.6892],\n",
      "        [-0.8644, -0.5470],\n",
      "        [-0.7332, -0.6547],\n",
      "        [-0.7058, -0.6806],\n",
      "        [-0.6806, -0.7059],\n",
      "        [-0.6686, -0.7183],\n",
      "        [-0.5307, -0.8872],\n",
      "        [-0.7273, -0.6602],\n",
      "        [-1.2161, -0.3515],\n",
      "        [-0.7208, -0.6663],\n",
      "        [-0.3867, -1.1373],\n",
      "        [-0.7896, -0.6052],\n",
      "        [-0.6584, -0.7291],\n",
      "        [-0.8308, -0.5721],\n",
      "        [-0.7156, -0.6712],\n",
      "        [-0.8691, -0.5435],\n",
      "        [-0.8311, -0.5719],\n",
      "        [-0.6365, -0.7532],\n",
      "        [-0.5658, -0.8392],\n",
      "        [-0.6063, -0.7882],\n",
      "        [-0.7418, -0.6468],\n",
      "        [-0.4010, -1.1076],\n",
      "        [-0.7626, -0.6282],\n",
      "        [-0.6079, -0.7863],\n",
      "        [-0.5097, -0.9180],\n",
      "        [-0.6573, -0.7303],\n",
      "        [-0.9078, -0.5165],\n",
      "        [-0.7734, -0.6188],\n",
      "        [-0.6765, -0.7101],\n",
      "        [-0.6495, -0.7388],\n",
      "        [-0.5139, -0.9118],\n",
      "        [-0.6368, -0.7529],\n",
      "        [-0.7522, -0.6373],\n",
      "        [-0.6118, -0.7817],\n",
      "        [-0.8180, -0.5821],\n",
      "        [-0.6997, -0.6866],\n",
      "        [-0.8329, -0.5706],\n",
      "        [-0.8791, -0.5364],\n",
      "        [-0.7811, -0.6123],\n",
      "        [-0.5867, -0.8123],\n",
      "        [-0.8486, -0.5586],\n",
      "        [-0.5893, -0.8091],\n",
      "        [-0.8158, -0.5839],\n",
      "        [-0.5777, -0.8237],\n",
      "        [-0.7959, -0.6000],\n",
      "        [-0.8796, -0.5361],\n",
      "        [-0.8573, -0.5522],\n",
      "        [-0.6280, -0.7629],\n",
      "        [-0.8155, -0.5841],\n",
      "        [-0.6389, -0.7506],\n",
      "        [-0.5704, -0.8331],\n",
      "        [-0.6196, -0.7726],\n",
      "        [-0.3808, -1.1497],\n",
      "        [-0.7560, -0.6340],\n",
      "        [-0.7016, -0.6847],\n",
      "        [-0.6041, -0.7909],\n",
      "        [-0.6382, -0.7513],\n",
      "        [-0.5702, -0.8334],\n",
      "        [-0.5764, -0.8253],\n",
      "        [-0.6493, -0.7391],\n",
      "        [-0.6721, -0.7146],\n",
      "        [-0.6110, -0.7826],\n",
      "        [-0.7440, -0.6448],\n",
      "        [-0.4675, -0.9851],\n",
      "        [-0.3925, -1.1251],\n",
      "        [-0.7799, -0.6133],\n",
      "        [-0.8334, -0.5702],\n",
      "        [-0.4844, -0.9573],\n",
      "        [-0.4268, -1.0572],\n",
      "        [-0.7389, -0.6494],\n",
      "        [-0.6314, -0.7590],\n",
      "        [-0.2825, -1.4019],\n",
      "        [-0.8662, -0.5457],\n",
      "        [-0.7434, -0.6453],\n",
      "        [-0.7194, -0.6676],\n",
      "        [-0.8682, -0.5443],\n",
      "        [-0.7447, -0.6441],\n",
      "        [-0.6515, -0.7366],\n",
      "        [-0.9678, -0.4779],\n",
      "        [-0.6699, -0.7170],\n",
      "        [-0.4826, -0.9602],\n",
      "        [-0.6649, -0.7223],\n",
      "        [-0.8271, -0.5750],\n",
      "        [-0.6265, -0.7646],\n",
      "        [-0.8808, -0.5352],\n",
      "        [-0.6609, -0.7265],\n",
      "        [-0.9970, -0.4604],\n",
      "        [-0.7944, -0.6012],\n",
      "        [-0.4929, -0.9438],\n",
      "        [-0.4074, -1.0947],\n",
      "        [-0.9725, -0.4750],\n",
      "        [-0.6961, -0.6902],\n",
      "        [-0.9245, -0.5054],\n",
      "        [-0.6110, -0.7827],\n",
      "        [-0.5925, -0.8051],\n",
      "        [-0.6552, -0.7326],\n",
      "        [-0.6236, -0.7679],\n",
      "        [-0.8247, -0.5769],\n",
      "        [-0.6057, -0.7890],\n",
      "        [-0.8772, -0.5378],\n",
      "        [-0.7491, -0.6401]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
      "        0, 0, 1, 1, 0, 0, 1, 1])\n",
      "tensor([[-0.9896, -0.4648],\n",
      "        [-1.0165, -0.4492],\n",
      "        [-0.7518, -0.6377],\n",
      "        [-0.6181, -0.7743],\n",
      "        [-0.6718, -0.7150],\n",
      "        [-0.6622, -0.7251],\n",
      "        [-0.6626, -0.7247],\n",
      "        [-0.5895, -0.8088],\n",
      "        [-0.7202, -0.6668],\n",
      "        [-1.0613, -0.4246],\n",
      "        [-0.8447, -0.5616],\n",
      "        [-0.8391, -0.5658],\n",
      "        [-0.5305, -0.8875],\n",
      "        [-0.6832, -0.7032],\n",
      "        [-0.6502, -0.7381],\n",
      "        [-1.3003, -0.3181],\n",
      "        [-0.7216, -0.6655],\n",
      "        [-0.7986, -0.5977],\n",
      "        [-0.6134, -0.7798],\n",
      "        [-0.8047, -0.5928],\n",
      "        [-0.8504, -0.5573],\n",
      "        [-0.5155, -0.9093],\n",
      "        [-0.6893, -0.6970],\n",
      "        [-0.6464, -0.7421],\n",
      "        [-0.6775, -0.7091],\n",
      "        [-0.6498, -0.7385],\n",
      "        [-0.7506, -0.6388],\n",
      "        [-0.6776, -0.7089],\n",
      "        [-0.6512, -0.7369],\n",
      "        [-0.6375, -0.7521],\n",
      "        [-0.7774, -0.6154],\n",
      "        [-0.6234, -0.7681],\n",
      "        [-0.8848, -0.5324],\n",
      "        [-0.6076, -0.7867],\n",
      "        [-0.6739, -0.7128],\n",
      "        [-0.8418, -0.5638],\n",
      "        [-0.9508, -0.4885],\n",
      "        [-0.5997, -0.7962],\n",
      "        [-0.6904, -0.6959],\n",
      "        [-0.6740, -0.7127],\n",
      "        [-0.5101, -0.9174],\n",
      "        [-0.7524, -0.6373],\n",
      "        [-1.0183, -0.4482],\n",
      "        [-0.7142, -0.6725],\n",
      "        [-0.6167, -0.7760],\n",
      "        [-0.7968, -0.5992],\n",
      "        [-0.7060, -0.6805],\n",
      "        [-1.1079, -0.4008],\n",
      "        [-1.2676, -0.3306],\n",
      "        [-0.9183, -0.5095],\n",
      "        [-0.9651, -0.4796],\n",
      "        [-0.6196, -0.7725],\n",
      "        [-1.0062, -0.4551],\n",
      "        [-0.8273, -0.5749],\n",
      "        [-0.7297, -0.6578],\n",
      "        [-0.6674, -0.7196],\n",
      "        [-0.6471, -0.7415],\n",
      "        [-0.7185, -0.6684],\n",
      "        [-0.6947, -0.6916],\n",
      "        [-0.7096, -0.6770],\n",
      "        [-0.7145, -0.6723],\n",
      "        [-0.8434, -0.5625],\n",
      "        [-0.6286, -0.7622],\n",
      "        [-0.8953, -0.5251],\n",
      "        [-0.5114, -0.9154],\n",
      "        [-0.6373, -0.7523],\n",
      "        [-0.7677, -0.6238],\n",
      "        [-0.5802, -0.8205],\n",
      "        [-0.6795, -0.7070],\n",
      "        [-0.6591, -0.7284],\n",
      "        [-0.7974, -0.5988],\n",
      "        [-1.2380, -0.3424],\n",
      "        [-0.5183, -0.9052],\n",
      "        [-0.7164, -0.6704],\n",
      "        [-0.8124, -0.5866],\n",
      "        [-0.7304, -0.6572],\n",
      "        [-0.6088, -0.7852],\n",
      "        [-0.5125, -0.9138],\n",
      "        [-0.4891, -0.9498],\n",
      "        [-0.7378, -0.6504],\n",
      "        [-0.7189, -0.6680],\n",
      "        [-0.7001, -0.6863],\n",
      "        [-0.7106, -0.6760],\n",
      "        [-0.6899, -0.6964],\n",
      "        [-1.1160, -0.3969],\n",
      "        [-0.5902, -0.8079],\n",
      "        [-0.6273, -0.7636],\n",
      "        [-1.4323, -0.2728],\n",
      "        [-0.6967, -0.6896],\n",
      "        [-0.8228, -0.5784],\n",
      "        [-0.7013, -0.6850],\n",
      "        [-1.3223, -0.3100],\n",
      "        [-0.6944, -0.6919],\n",
      "        [-0.6409, -0.7482],\n",
      "        [-0.8901, -0.5287],\n",
      "        [-0.7347, -0.6533],\n",
      "        [-0.6274, -0.7635],\n",
      "        [-0.8238, -0.5776],\n",
      "        [-0.7748, -0.6176],\n",
      "        [-0.7900, -0.6049],\n",
      "        [-0.8662, -0.5456],\n",
      "        [-0.5410, -0.8727],\n",
      "        [-0.6801, -0.7064],\n",
      "        [-0.7249, -0.6624],\n",
      "        [-0.4353, -1.0415],\n",
      "        [-0.6032, -0.7920],\n",
      "        [-0.7706, -0.6213],\n",
      "        [-1.0130, -0.4512],\n",
      "        [-0.6297, -0.7608],\n",
      "        [-0.4408, -1.0315],\n",
      "        [-0.5600, -0.8467],\n",
      "        [-0.7141, -0.6726],\n",
      "        [-0.6151, -0.7779],\n",
      "        [-0.6878, -0.6985],\n",
      "        [-0.6160, -0.7768],\n",
      "        [-0.7213, -0.6658],\n",
      "        [-0.6688, -0.7181],\n",
      "        [-0.6145, -0.7785],\n",
      "        [-0.6559, -0.7318],\n",
      "        [-0.5704, -0.8331],\n",
      "        [-0.7893, -0.6054],\n",
      "        [-0.4526, -1.0106],\n",
      "        [-0.7126, -0.6740],\n",
      "        [-0.6255, -0.7657],\n",
      "        [-0.9012, -0.5210],\n",
      "        [-0.8628, -0.5482],\n",
      "        [-0.6091, -0.7849],\n",
      "        [-0.8302, -0.5726]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,\n",
      "        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
      "        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0, 0, 1])\n",
      "tensor([[-0.6158, -0.7769],\n",
      "        [-0.8886, -0.5298],\n",
      "        [-0.6092, -0.7848],\n",
      "        [-0.4646, -0.9900],\n",
      "        [-0.6220, -0.7697],\n",
      "        [-1.1115, -0.3991],\n",
      "        [-0.7983, -0.5980],\n",
      "        [-0.7358, -0.6522],\n",
      "        [-0.6775, -0.7091],\n",
      "        [-0.7015, -0.6848],\n",
      "        [-0.9235, -0.5061],\n",
      "        [-0.7337, -0.6542],\n",
      "        [-0.5900, -0.8082],\n",
      "        [-0.7754, -0.6172],\n",
      "        [-0.4123, -1.0851],\n",
      "        [-0.9088, -0.5159],\n",
      "        [-0.6816, -0.7048],\n",
      "        [-0.7141, -0.6726],\n",
      "        [-0.7155, -0.6713],\n",
      "        [-0.8907, -0.5283],\n",
      "        [-1.1563, -0.3778],\n",
      "        [-0.7348, -0.6531],\n",
      "        [-0.9728, -0.4748],\n",
      "        [-0.5822, -0.8179],\n",
      "        [-0.6082, -0.7860],\n",
      "        [-0.8035, -0.5937],\n",
      "        [-0.7045, -0.6819],\n",
      "        [-0.6068, -0.7876],\n",
      "        [-0.6585, -0.7290],\n",
      "        [-0.5269, -0.8926],\n",
      "        [-0.5154, -0.9094],\n",
      "        [-0.7553, -0.6347],\n",
      "        [-0.7268, -0.6606],\n",
      "        [-0.5298, -0.8884],\n",
      "        [-0.6204, -0.7716],\n",
      "        [-0.7381, -0.6502],\n",
      "        [-0.6894, -0.6969],\n",
      "        [-0.6012, -0.7944],\n",
      "        [-0.7060, -0.6804],\n",
      "        [-0.6132, -0.7801],\n",
      "        [-0.5684, -0.8357],\n",
      "        [-0.8426, -0.5631],\n",
      "        [-0.6792, -0.7073],\n",
      "        [-0.5285, -0.8903],\n",
      "        [-0.3835, -1.1441],\n",
      "        [-0.6956, -0.6907],\n",
      "        [-0.5382, -0.8766],\n",
      "        [-0.6256, -0.7656],\n",
      "        [-0.6567, -0.7310],\n",
      "        [-0.5321, -0.8852],\n",
      "        [-0.5594, -0.8475],\n",
      "        [-0.5273, -0.8921],\n",
      "        [-0.5269, -0.8926],\n",
      "        [-0.6634, -0.7238],\n",
      "        [-0.9272, -0.5036],\n",
      "        [-0.6975, -0.6889],\n",
      "        [-0.6595, -0.7279],\n",
      "        [-0.2828, -1.4010],\n",
      "        [-0.5496, -0.8608],\n",
      "        [-0.5826, -0.8174],\n",
      "        [-0.5514, -0.8584],\n",
      "        [-0.8165, -0.5834],\n",
      "        [-0.6282, -0.7626],\n",
      "        [-1.0490, -0.4312],\n",
      "        [-0.5076, -0.9212],\n",
      "        [-0.7386, -0.6497],\n",
      "        [-0.7284, -0.6591],\n",
      "        [-0.7218, -0.6653],\n",
      "        [-0.3755, -1.1613],\n",
      "        [-0.7360, -0.6521],\n",
      "        [-0.6908, -0.6955],\n",
      "        [-0.7899, -0.6049],\n",
      "        [-0.8582, -0.5515],\n",
      "        [-0.5586, -0.8486],\n",
      "        [-0.5399, -0.8741],\n",
      "        [-0.6871, -0.6992],\n",
      "        [-0.7892, -0.6055],\n",
      "        [-0.5338, -0.8828],\n",
      "        [-0.5693, -0.8346],\n",
      "        [-0.7335, -0.6543],\n",
      "        [-0.5515, -0.8582],\n",
      "        [-0.7172, -0.6696],\n",
      "        [-0.8430, -0.5629],\n",
      "        [-0.6664, -0.7206],\n",
      "        [-0.7018, -0.6846],\n",
      "        [-0.3032, -1.3411],\n",
      "        [-0.8313, -0.5718],\n",
      "        [-0.8147, -0.5848],\n",
      "        [-0.6842, -0.7021],\n",
      "        [-0.3625, -1.1905],\n",
      "        [-0.6605, -0.7269],\n",
      "        [-1.0506, -0.4303],\n",
      "        [-0.5911, -0.8068],\n",
      "        [-0.7064, -0.6801],\n",
      "        [-0.6975, -0.6888],\n",
      "        [-0.6193, -0.7729],\n",
      "        [-0.6845, -0.7019],\n",
      "        [-0.7939, -0.6016],\n",
      "        [-0.9234, -0.5061],\n",
      "        [-0.7665, -0.6248],\n",
      "        [-0.6846, -0.7017],\n",
      "        [-0.5660, -0.8388],\n",
      "        [-0.4955, -0.9398],\n",
      "        [-0.7670, -0.6244],\n",
      "        [-0.5770, -0.8246],\n",
      "        [-0.7087, -0.6779],\n",
      "        [-0.6116, -0.7820],\n",
      "        [-0.7620, -0.6288],\n",
      "        [-0.7867, -0.6076],\n",
      "        [-0.8550, -0.5539],\n",
      "        [-0.7382, -0.6500],\n",
      "        [-0.4694, -0.9818],\n",
      "        [-0.4963, -0.9385],\n",
      "        [-0.4950, -0.9404],\n",
      "        [-1.0100, -0.4529],\n",
      "        [-0.3858, -1.1392],\n",
      "        [-0.7198, -0.6671],\n",
      "        [-0.8676, -0.5447],\n",
      "        [-0.6740, -0.7127],\n",
      "        [-0.6428, -0.7462],\n",
      "        [-0.6081, -0.7861],\n",
      "        [-0.7569, -0.6332],\n",
      "        [-0.4665, -0.9868],\n",
      "        [-0.3255, -1.2807],\n",
      "        [-0.6671, -0.7199],\n",
      "        [-0.7329, -0.6549],\n",
      "        [-0.7394, -0.6489],\n",
      "        [-0.6733, -0.7134]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
      "        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
      "        1, 1, 0, 0, 1, 0, 1, 0])\n",
      "tensor([[-0.6597, -0.7278],\n",
      "        [-0.7379, -0.6503],\n",
      "        [-0.6892, -0.6971],\n",
      "        [-0.7068, -0.6796],\n",
      "        [-0.6492, -0.7391],\n",
      "        [-0.4914, -0.9462],\n",
      "        [-0.6608, -0.7266],\n",
      "        [-0.5728, -0.8299],\n",
      "        [-0.7984, -0.5979],\n",
      "        [-0.7365, -0.6516],\n",
      "        [-0.7024, -0.6840],\n",
      "        [-0.3887, -1.1330],\n",
      "        [-0.6786, -0.7079],\n",
      "        [-0.5990, -0.7971],\n",
      "        [-0.8981, -0.5231],\n",
      "        [-0.5746, -0.8276],\n",
      "        [-0.6305, -0.7600],\n",
      "        [-0.7071, -0.6794],\n",
      "        [-0.6581, -0.7295],\n",
      "        [-0.7726, -0.6195],\n",
      "        [-0.7359, -0.6521],\n",
      "        [-0.6819, -0.7045],\n",
      "        [-0.6153, -0.7775],\n",
      "        [-1.1330, -0.3887],\n",
      "        [-0.3347, -1.2572],\n",
      "        [-0.7037, -0.6827],\n",
      "        [-0.6941, -0.6922],\n",
      "        [-0.6643, -0.7229],\n",
      "        [-0.4959, -0.9391],\n",
      "        [-0.5655, -0.8395],\n",
      "        [-0.5901, -0.8080],\n",
      "        [-0.5707, -0.8327],\n",
      "        [-0.9446, -0.4924],\n",
      "        [-0.4126, -1.0845],\n",
      "        [-0.6860, -0.7004],\n",
      "        [-0.7149, -0.6719],\n",
      "        [-1.0397, -0.4363],\n",
      "        [-0.9297, -0.5020],\n",
      "        [-0.6071, -0.7874],\n",
      "        [-0.6196, -0.7725],\n",
      "        [-0.8540, -0.5546],\n",
      "        [-0.6782, -0.7084],\n",
      "        [-0.8629, -0.5481],\n",
      "        [-0.7821, -0.6115],\n",
      "        [-0.3555, -1.2066],\n",
      "        [-0.5537, -0.8553],\n",
      "        [-0.4662, -0.9872],\n",
      "        [-0.8738, -0.5402],\n",
      "        [-0.1826, -1.7902],\n",
      "        [-0.4862, -0.9545],\n",
      "        [-0.7630, -0.6279],\n",
      "        [-1.2423, -0.3407],\n",
      "        [-0.5418, -0.8715],\n",
      "        [-0.7289, -0.6586],\n",
      "        [-0.7848, -0.6092],\n",
      "        [-0.6809, -0.7056],\n",
      "        [-0.5692, -0.8347],\n",
      "        [-0.6608, -0.7265],\n",
      "        [-0.7181, -0.6688],\n",
      "        [-0.7491, -0.6402],\n",
      "        [-0.6878, -0.6986],\n",
      "        [-0.6832, -0.7032],\n",
      "        [-0.6008, -0.7949],\n",
      "        [-0.7565, -0.6336],\n",
      "        [-0.7742, -0.6181],\n",
      "        [-0.6045, -0.7904],\n",
      "        [-0.6043, -0.7907],\n",
      "        [-0.7851, -0.6089],\n",
      "        [-0.6896, -0.6967],\n",
      "        [-0.3627, -1.1900],\n",
      "        [-0.7258, -0.6615],\n",
      "        [-0.4841, -0.9578],\n",
      "        [-0.7336, -0.6542],\n",
      "        [-0.3908, -1.1286],\n",
      "        [-0.6499, -0.7384],\n",
      "        [-0.6231, -0.7685],\n",
      "        [-0.6949, -0.6914],\n",
      "        [-0.7041, -0.6823],\n",
      "        [-0.9451, -0.4921],\n",
      "        [-0.9495, -0.4893],\n",
      "        [-0.6812, -0.7053],\n",
      "        [-0.7044, -0.6821],\n",
      "        [-0.9813, -0.4697],\n",
      "        [-0.7142, -0.6725],\n",
      "        [-0.6717, -0.7150],\n",
      "        [-0.5595, -0.8474],\n",
      "        [-0.7498, -0.6396],\n",
      "        [-0.6738, -0.7129],\n",
      "        [-0.7451, -0.6437],\n",
      "        [-0.6849, -0.7014],\n",
      "        [-0.5465, -0.8650],\n",
      "        [-0.7637, -0.6273],\n",
      "        [-0.7085, -0.6780],\n",
      "        [-0.5229, -0.8985],\n",
      "        [-0.6369, -0.7527],\n",
      "        [-0.7302, -0.6574],\n",
      "        [-0.6912, -0.6951],\n",
      "        [-0.6871, -0.6993],\n",
      "        [-0.7715, -0.6205],\n",
      "        [-0.6699, -0.7169],\n",
      "        [-0.6543, -0.7335],\n",
      "        [-0.7080, -0.6785],\n",
      "        [-0.8193, -0.5812],\n",
      "        [-0.7278, -0.6596],\n",
      "        [-0.6340, -0.7561],\n",
      "        [-0.7645, -0.6265],\n",
      "        [-0.7736, -0.6187],\n",
      "        [-0.4901, -0.9482],\n",
      "        [-0.3305, -1.2678],\n",
      "        [-0.6446, -0.7442],\n",
      "        [-0.8102, -0.5884],\n",
      "        [-0.7416, -0.6470],\n",
      "        [-0.1912, -1.7487],\n",
      "        [-0.7163, -0.6705],\n",
      "        [-0.9366, -0.4975],\n",
      "        [-0.5922, -0.8055],\n",
      "        [-0.4446, -1.0246],\n",
      "        [-0.6399, -0.7494],\n",
      "        [-0.6342, -0.7558],\n",
      "        [-0.8380, -0.5666],\n",
      "        [-0.6854, -0.7009],\n",
      "        [-0.5657, -0.8393],\n",
      "        [-0.6652, -0.7219],\n",
      "        [-0.6526, -0.7354],\n",
      "        [-0.8519, -0.5562],\n",
      "        [-0.9609, -0.4822],\n",
      "        [-0.6583, -0.7293],\n",
      "        [-0.7221, -0.6650]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
      "        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1])\n",
      "tensor([[-0.5048, -0.9254],\n",
      "        [-0.3657, -1.1832],\n",
      "        [-0.7780, -0.6150],\n",
      "        [-0.5730, -0.8298],\n",
      "        [-0.6454, -0.7433],\n",
      "        [-0.5378, -0.8771],\n",
      "        [-0.7165, -0.6703],\n",
      "        [-0.7728, -0.6194],\n",
      "        [-0.5161, -0.9084],\n",
      "        [-0.6750, -0.7116],\n",
      "        [-0.9549, -0.4859],\n",
      "        [-0.6215, -0.7703],\n",
      "        [-0.7908, -0.6042],\n",
      "        [-0.6855, -0.7008],\n",
      "        [-0.5629, -0.8430],\n",
      "        [-0.4206, -1.0690],\n",
      "        [-0.7879, -0.6066],\n",
      "        [-1.1000, -0.4048],\n",
      "        [-0.6795, -0.7070],\n",
      "        [-0.7485, -0.6407],\n",
      "        [-0.6725, -0.7142],\n",
      "        [-0.5479, -0.8631],\n",
      "        [-0.5653, -0.8397],\n",
      "        [-0.6672, -0.7197],\n",
      "        [-0.7392, -0.6491],\n",
      "        [-0.6541, -0.7337],\n",
      "        [-0.4248, -1.0611],\n",
      "        [-0.9671, -0.4784],\n",
      "        [-0.5459, -0.8658],\n",
      "        [-0.6457, -0.7430],\n",
      "        [-0.5814, -0.8190],\n",
      "        [-0.7412, -0.6473],\n",
      "        [-0.6528, -0.7352],\n",
      "        [-0.8672, -0.5450],\n",
      "        [-0.6809, -0.7056],\n",
      "        [-0.7610, -0.6296],\n",
      "        [-0.6984, -0.6879],\n",
      "        [-0.7189, -0.6681],\n",
      "        [-0.8504, -0.5573],\n",
      "        [-0.6551, -0.7327],\n",
      "        [-0.9577, -0.4842],\n",
      "        [-0.6775, -0.7090],\n",
      "        [-0.6978, -0.6885],\n",
      "        [-0.5005, -0.9320],\n",
      "        [-0.5173, -0.9067],\n",
      "        [-0.7653, -0.6258],\n",
      "        [-0.6378, -0.7517],\n",
      "        [-0.7079, -0.6786],\n",
      "        [-0.9379, -0.4967],\n",
      "        [-0.7143, -0.6724],\n",
      "        [-0.7345, -0.6535],\n",
      "        [-0.5373, -0.8778],\n",
      "        [-0.8611, -0.5494],\n",
      "        [-0.8229, -0.5783],\n",
      "        [-0.6967, -0.6896],\n",
      "        [-0.7221, -0.6650],\n",
      "        [-0.6859, -0.7004],\n",
      "        [-0.5675, -0.8369],\n",
      "        [-0.6778, -0.7088],\n",
      "        [-0.5389, -0.8757],\n",
      "        [-0.7293, -0.6582],\n",
      "        [-0.6680, -0.7189],\n",
      "        [-0.5112, -0.9157],\n",
      "        [-0.5172, -0.9068],\n",
      "        [-0.6597, -0.7278],\n",
      "        [-0.7636, -0.6273],\n",
      "        [-0.4770, -0.9693],\n",
      "        [-0.6394, -0.7499],\n",
      "        [-0.6237, -0.7678],\n",
      "        [-0.6720, -0.7147],\n",
      "        [-0.8192, -0.5813],\n",
      "        [-0.6589, -0.7286],\n",
      "        [-0.8406, -0.5647],\n",
      "        [-1.0429, -0.4345],\n",
      "        [-0.7291, -0.6585],\n",
      "        [-0.5503, -0.8599],\n",
      "        [-0.8252, -0.5765],\n",
      "        [-0.6320, -0.7583],\n",
      "        [-0.5649, -0.8403],\n",
      "        [-0.7471, -0.6419],\n",
      "        [-0.6492, -0.7391],\n",
      "        [-0.7624, -0.6284],\n",
      "        [-0.7278, -0.6596],\n",
      "        [-0.6926, -0.6937],\n",
      "        [-0.6840, -0.7024],\n",
      "        [-0.7133, -0.6734],\n",
      "        [-0.9745, -0.4738],\n",
      "        [-0.6652, -0.7218],\n",
      "        [-0.7655, -0.6257],\n",
      "        [-0.4863, -0.9543],\n",
      "        [-1.0635, -0.4235],\n",
      "        [-0.4822, -0.9609],\n",
      "        [-0.6564, -0.7313],\n",
      "        [-0.7672, -0.6242],\n",
      "        [-0.8406, -0.5647],\n",
      "        [-0.4219, -1.0666],\n",
      "        [-0.7788, -0.6143],\n",
      "        [-0.7912, -0.6038],\n",
      "        [-0.3266, -1.2780],\n",
      "        [-0.8312, -0.5719],\n",
      "        [-0.8722, -0.5413],\n",
      "        [-0.7874, -0.6070],\n",
      "        [-0.7063, -0.6802],\n",
      "        [-0.7033, -0.6831],\n",
      "        [-0.6814, -0.7050],\n",
      "        [-0.7253, -0.6620],\n",
      "        [-0.7217, -0.6654],\n",
      "        [-0.6953, -0.6910],\n",
      "        [-0.6615, -0.7258],\n",
      "        [-0.7023, -0.6841],\n",
      "        [-0.6430, -0.7459],\n",
      "        [-0.6661, -0.7209],\n",
      "        [-0.5946, -0.8024],\n",
      "        [-0.7140, -0.6727],\n",
      "        [-1.4315, -0.2731],\n",
      "        [-0.6439, -0.7450],\n",
      "        [-0.6730, -0.7137],\n",
      "        [-0.7526, -0.6370],\n",
      "        [-0.6747, -0.7119],\n",
      "        [-0.8775, -0.5375],\n",
      "        [-0.8020, -0.5950],\n",
      "        [-0.7730, -0.6192],\n",
      "        [-0.6713, -0.7155],\n",
      "        [-0.6972, -0.6891],\n",
      "        [-0.8864, -0.5312],\n",
      "        [-0.6304, -0.7601],\n",
      "        [-0.9284, -0.5028],\n",
      "        [-0.6306, -0.7599]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
      "        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
      "        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
      "        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,\n",
      "        1, 0, 1, 0, 1, 1, 1, 1])\n",
      "tensor([[-0.4752, -0.9723],\n",
      "        [-0.5331, -0.8838],\n",
      "        [-0.6550, -0.7328],\n",
      "        [-0.4371, -1.0382],\n",
      "        [-0.7054, -0.6810],\n",
      "        [-0.4549, -1.0066],\n",
      "        [-0.6987, -0.6877],\n",
      "        [-0.7920, -0.6032],\n",
      "        [-0.7303, -0.6573],\n",
      "        [-0.7081, -0.6784],\n",
      "        [-0.9722, -0.4752],\n",
      "        [-0.4343, -1.0433],\n",
      "        [-0.7117, -0.6749],\n",
      "        [-0.4425, -1.0284],\n",
      "        [-0.7305, -0.6572],\n",
      "        [-0.7980, -0.5983],\n",
      "        [-0.6951, -0.6912],\n",
      "        [-0.7597, -0.6308],\n",
      "        [-0.6704, -0.7164],\n",
      "        [-1.2954, -0.3199],\n",
      "        [-0.6114, -0.7822],\n",
      "        [-0.6244, -0.7670],\n",
      "        [-0.8166, -0.5833],\n",
      "        [-0.8301, -0.5727],\n",
      "        [-0.5701, -0.8335],\n",
      "        [-0.6091, -0.7849],\n",
      "        [-0.6681, -0.7189],\n",
      "        [-0.7942, -0.6013],\n",
      "        [-2.1507, -0.1237],\n",
      "        [-0.7310, -0.6567],\n",
      "        [-1.0113, -0.4521],\n",
      "        [-0.4858, -0.9550],\n",
      "        [-0.5966, -0.8000],\n",
      "        [-0.3730, -1.1668],\n",
      "        [-0.6561, -0.7316],\n",
      "        [-0.4256, -1.0596],\n",
      "        [-0.7592, -0.6312],\n",
      "        [-0.7855, -0.6086],\n",
      "        [-0.6823, -0.7041],\n",
      "        [-0.7116, -0.6750],\n",
      "        [-0.6629, -0.7243],\n",
      "        [-0.6386, -0.7508],\n",
      "        [-0.4155, -1.0789],\n",
      "        [-0.6170, -0.7756],\n",
      "        [-0.6111, -0.7825],\n",
      "        [-0.2166, -1.6359],\n",
      "        [-0.6538, -0.7341],\n",
      "        [-0.7786, -0.6145],\n",
      "        [-0.5426, -0.8704],\n",
      "        [-0.8072, -0.5908],\n",
      "        [-0.8772, -0.5377],\n",
      "        [-0.6884, -0.6979],\n",
      "        [-0.5658, -0.8391],\n",
      "        [-0.7574, -0.6327],\n",
      "        [-0.8245, -0.5771],\n",
      "        [-0.9273, -0.5035],\n",
      "        [-0.5947, -0.8024],\n",
      "        [-0.7330, -0.6549],\n",
      "        [-0.7764, -0.6163],\n",
      "        [-0.6812, -0.7053],\n",
      "        [-0.7569, -0.6332],\n",
      "        [-0.7423, -0.6463],\n",
      "        [-0.7185, -0.6684],\n",
      "        [-0.6565, -0.7312],\n",
      "        [-0.7613, -0.6294],\n",
      "        [-0.5611, -0.8453],\n",
      "        [-0.6110, -0.7827],\n",
      "        [-0.5949, -0.8021],\n",
      "        [-0.6517, -0.7363],\n",
      "        [-0.7267, -0.6607],\n",
      "        [-0.6604, -0.7270],\n",
      "        [-0.8997, -0.5221],\n",
      "        [-0.5570, -0.8507],\n",
      "        [-0.6991, -0.6872],\n",
      "        [-0.6283, -0.7625],\n",
      "        [-0.4654, -0.9885],\n",
      "        [-0.5990, -0.7970],\n",
      "        [-0.6161, -0.7766],\n",
      "        [-0.6447, -0.7441],\n",
      "        [-0.6598, -0.7276],\n",
      "        [-0.4663, -0.9870],\n",
      "        [-0.6614, -0.7260],\n",
      "        [-0.4788, -0.9664],\n",
      "        [-0.1042, -2.3130],\n",
      "        [-0.6468, -0.7418],\n",
      "        [-0.6762, -0.7104],\n",
      "        [-0.7241, -0.6631],\n",
      "        [-0.7069, -0.6796],\n",
      "        [-0.4465, -1.0214],\n",
      "        [-0.4725, -0.9767],\n",
      "        [-1.0041, -0.4563],\n",
      "        [-0.7201, -0.6669],\n",
      "        [-0.6198, -0.7723],\n",
      "        [-0.6956, -0.6907],\n",
      "        [-0.5413, -0.8722],\n",
      "        [-0.6874, -0.6990],\n",
      "        [-0.9727, -0.4749],\n",
      "        [-0.8056, -0.5921],\n",
      "        [-0.8163, -0.5835],\n",
      "        [-0.3895, -1.1313],\n",
      "        [-0.6752, -0.7115],\n",
      "        [-0.6358, -0.7540],\n",
      "        [-0.5292, -0.8894],\n",
      "        [-0.9702, -0.4764],\n",
      "        [-0.6547, -0.7331],\n",
      "        [-0.5913, -0.8065],\n",
      "        [-0.5873, -0.8115],\n",
      "        [-0.6212, -0.7707],\n",
      "        [-0.4732, -0.9755],\n",
      "        [-0.6763, -0.7103],\n",
      "        [-0.7984, -0.5979],\n",
      "        [-0.6329, -0.7573],\n",
      "        [-0.7229, -0.6643],\n",
      "        [-0.6964, -0.6900],\n",
      "        [-0.6713, -0.7154],\n",
      "        [-0.6797, -0.7068],\n",
      "        [-0.6269, -0.7641],\n",
      "        [-0.6985, -0.6878],\n",
      "        [-0.4596, -0.9984],\n",
      "        [-0.4530, -1.0099],\n",
      "        [-0.7432, -0.6455],\n",
      "        [-0.6893, -0.6970],\n",
      "        [-0.6738, -0.7129],\n",
      "        [-0.6181, -0.7743],\n",
      "        [-0.6701, -0.7168],\n",
      "        [-0.6177, -0.7748],\n",
      "        [-0.6846, -0.7018],\n",
      "        [-0.7961, -0.5998]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
      "        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 1, 0, 0, 0, 1])\n",
      "tensor([[-0.9873, -0.4661],\n",
      "        [-0.3625, -1.1904],\n",
      "        [-0.7012, -0.6852],\n",
      "        [-0.6828, -0.7036],\n",
      "        [-0.7045, -0.6819],\n",
      "        [-0.3204, -1.2941],\n",
      "        [-0.3372, -1.2508],\n",
      "        [-0.7124, -0.6742],\n",
      "        [-0.6429, -0.7460],\n",
      "        [-0.8407, -0.5646],\n",
      "        [-0.8652, -0.5464],\n",
      "        [-0.7782, -0.6148],\n",
      "        [-0.7474, -0.6416],\n",
      "        [-0.4415, -1.0303],\n",
      "        [-0.6365, -0.7532],\n",
      "        [-0.4740, -0.9742],\n",
      "        [-0.7789, -0.6142],\n",
      "        [-0.4534, -1.0092],\n",
      "        [-0.6798, -0.7067],\n",
      "        [-0.3041, -1.3386],\n",
      "        [-0.8497, -0.5578],\n",
      "        [-0.7960, -0.5999],\n",
      "        [-0.6877, -0.6987],\n",
      "        [-0.7133, -0.6734],\n",
      "        [-0.6185, -0.7738],\n",
      "        [-0.6263, -0.7648],\n",
      "        [-0.6671, -0.7199],\n",
      "        [-0.9551, -0.4857],\n",
      "        [-0.7196, -0.6674],\n",
      "        [-0.5076, -0.9212],\n",
      "        [-0.6981, -0.6883],\n",
      "        [-0.7713, -0.6207],\n",
      "        [-0.4168, -1.0764],\n",
      "        [-0.4298, -1.0516],\n",
      "        [-0.7308, -0.6569],\n",
      "        [-0.8042, -0.5932],\n",
      "        [-0.6870, -0.6993],\n",
      "        [-0.4918, -0.9456],\n",
      "        [-0.6521, -0.7360],\n",
      "        [-1.0412, -0.4355],\n",
      "        [-0.6741, -0.7125],\n",
      "        [-0.6669, -0.7201],\n",
      "        [-0.5607, -0.8459],\n",
      "        [-0.6571, -0.7306],\n",
      "        [-0.6463, -0.7423],\n",
      "        [-0.5841, -0.8156],\n",
      "        [-0.5926, -0.8049],\n",
      "        [-0.5177, -0.9061],\n",
      "        [-0.5251, -0.8953],\n",
      "        [-0.8004, -0.5963],\n",
      "        [-0.5726, -0.8302],\n",
      "        [-0.4380, -1.0365],\n",
      "        [-0.5971, -0.7994],\n",
      "        [-0.4402, -1.0326],\n",
      "        [-0.6789, -0.7076],\n",
      "        [-0.9257, -0.5046],\n",
      "        [-0.7762, -0.6165],\n",
      "        [-0.4429, -1.0277],\n",
      "        [-0.6376, -0.7520],\n",
      "        [-0.8008, -0.5959],\n",
      "        [-0.3105, -1.3208],\n",
      "        [-0.6142, -0.7789],\n",
      "        [-0.5456, -0.8663],\n",
      "        [-0.4888, -0.9503],\n",
      "        [-0.7452, -0.6436],\n",
      "        [-0.7356, -0.6524],\n",
      "        [-0.6235, -0.7680],\n",
      "        [-0.7464, -0.6426],\n",
      "        [-0.5152, -0.9098],\n",
      "        [-0.6611, -0.7262],\n",
      "        [-0.7257, -0.6616],\n",
      "        [-0.4790, -0.9659],\n",
      "        [-0.7264, -0.6609],\n",
      "        [-0.6953, -0.6910],\n",
      "        [-0.8290, -0.5736],\n",
      "        [-0.5743, -0.8281],\n",
      "        [-0.3127, -1.3149],\n",
      "        [-0.7398, -0.6486],\n",
      "        [-0.7012, -0.6851],\n",
      "        [-0.6570, -0.7307],\n",
      "        [-0.5980, -0.7983],\n",
      "        [-0.7960, -0.5999],\n",
      "        [-0.4047, -1.1002],\n",
      "        [-0.6000, -0.7958],\n",
      "        [-0.6991, -0.6872],\n",
      "        [-0.6598, -0.7276],\n",
      "        [-0.7784, -0.6146],\n",
      "        [-0.7460, -0.6430],\n",
      "        [-0.3683, -1.1775],\n",
      "        [-0.5018, -0.9300],\n",
      "        [-0.6716, -0.7152],\n",
      "        [-0.4835, -0.9587],\n",
      "        [-0.5516, -0.8581],\n",
      "        [-0.6655, -0.7216],\n",
      "        [-0.3720, -1.1691],\n",
      "        [-0.2791, -1.4126],\n",
      "        [-0.4500, -1.0151],\n",
      "        [-0.7323, -0.6555],\n",
      "        [-0.5503, -0.8598],\n",
      "        [-0.5804, -0.8202],\n",
      "        [-0.4242, -1.0621],\n",
      "        [-0.6910, -0.6953],\n",
      "        [-0.7118, -0.6749],\n",
      "        [-0.5826, -0.8175],\n",
      "        [-0.7048, -0.6817],\n",
      "        [-0.7423, -0.6463],\n",
      "        [-0.8310, -0.5720],\n",
      "        [-0.7072, -0.6793],\n",
      "        [-0.5321, -0.8853],\n",
      "        [-0.6947, -0.6916],\n",
      "        [-0.6762, -0.7104],\n",
      "        [-0.1881, -1.7632],\n",
      "        [-0.6878, -0.6985],\n",
      "        [-0.6274, -0.7635],\n",
      "        [-0.4641, -0.9907],\n",
      "        [-0.6783, -0.7082],\n",
      "        [-0.8274, -0.5748],\n",
      "        [-0.5133, -0.9125],\n",
      "        [-0.7767, -0.6160],\n",
      "        [-0.3315, -1.2653],\n",
      "        [-0.5151, -0.9099],\n",
      "        [-0.5684, -0.8358],\n",
      "        [-0.6689, -0.7180],\n",
      "        [-0.6540, -0.7338],\n",
      "        [-0.9247, -0.5052],\n",
      "        [-0.7144, -0.6723],\n",
      "        [-0.6250, -0.7663],\n",
      "        [-0.7460, -0.6429]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
      "        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
      "        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 1, 0, 0])\n",
      "tensor([[-0.5871, -0.8117],\n",
      "        [-0.7883, -0.6063],\n",
      "        [-0.4260, -1.0589],\n",
      "        [-0.7156, -0.6712],\n",
      "        [-0.5756, -0.8264],\n",
      "        [-0.2034, -1.6924],\n",
      "        [-0.6320, -0.7583],\n",
      "        [-0.7471, -0.6420],\n",
      "        [-0.6411, -0.7480],\n",
      "        [-0.6460, -0.7426],\n",
      "        [-0.5230, -0.8983],\n",
      "        [-0.6669, -0.7201],\n",
      "        [-0.6655, -0.7216],\n",
      "        [-0.6738, -0.7129],\n",
      "        [-0.5187, -0.9046],\n",
      "        [-0.6148, -0.7782],\n",
      "        [-0.3323, -1.2633],\n",
      "        [-0.7072, -0.6792],\n",
      "        [-0.5723, -0.8307],\n",
      "        [-0.2737, -1.4294],\n",
      "        [-0.7561, -0.6339],\n",
      "        [-0.8819, -0.5344],\n",
      "        [-0.6272, -0.7638],\n",
      "        [-0.3034, -1.3405],\n",
      "        [-0.4204, -1.0694],\n",
      "        [-0.7344, -0.6535],\n",
      "        [-0.5486, -0.8622],\n",
      "        [-0.3889, -1.1326],\n",
      "        [-0.4688, -0.9829],\n",
      "        [-0.7290, -0.6586],\n",
      "        [-0.6699, -0.7170],\n",
      "        [-0.6874, -0.6989],\n",
      "        [-0.4109, -1.0878],\n",
      "        [-0.6950, -0.6913],\n",
      "        [-0.5234, -0.8978],\n",
      "        [-0.5818, -0.8184],\n",
      "        [-0.6879, -0.6984],\n",
      "        [-0.2180, -1.6304],\n",
      "        [-0.5178, -0.9059],\n",
      "        [-0.6471, -0.7414],\n",
      "        [-0.7319, -0.6559],\n",
      "        [-0.6665, -0.7206],\n",
      "        [-0.8768, -0.5380],\n",
      "        [-0.8205, -0.5802],\n",
      "        [-0.4075, -1.0945],\n",
      "        [-0.7137, -0.6730],\n",
      "        [-1.2819, -0.3251],\n",
      "        [-0.7173, -0.6696],\n",
      "        [-0.6788, -0.7077],\n",
      "        [-0.2372, -1.5551],\n",
      "        [-0.5653, -0.8398],\n",
      "        [-0.5377, -0.8774],\n",
      "        [-0.3007, -1.3483],\n",
      "        [-0.6605, -0.7269],\n",
      "        [-0.6216, -0.7702],\n",
      "        [-0.7456, -0.6433],\n",
      "        [-0.4673, -0.9854],\n",
      "        [-0.7449, -0.6439],\n",
      "        [-0.6741, -0.7125],\n",
      "        [-0.7237, -0.6635],\n",
      "        [-0.7532, -0.6365],\n",
      "        [-0.6429, -0.7461],\n",
      "        [-0.5486, -0.8622],\n",
      "        [-0.4720, -0.9776],\n",
      "        [-0.3853, -1.1401],\n",
      "        [-0.6377, -0.7519],\n",
      "        [-0.7801, -0.6132],\n",
      "        [-0.7455, -0.6434],\n",
      "        [-0.1498, -1.9723],\n",
      "        [-0.5160, -0.9085],\n",
      "        [-0.7126, -0.6741],\n",
      "        [-0.3360, -1.2539],\n",
      "        [-0.6275, -0.7635],\n",
      "        [-0.7545, -0.6353],\n",
      "        [-0.7510, -0.6384],\n",
      "        [-0.7977, -0.5985],\n",
      "        [-0.7046, -0.6818],\n",
      "        [-0.4248, -1.0610],\n",
      "        [-0.6664, -0.7207],\n",
      "        [-0.7590, -0.6314],\n",
      "        [-0.6007, -0.7950],\n",
      "        [-0.1019, -2.3340],\n",
      "        [-0.4074, -1.0948],\n",
      "        [-0.6130, -0.7803],\n",
      "        [-0.3387, -1.2473],\n",
      "        [-0.5881, -0.8106],\n",
      "        [-0.2525, -1.5000],\n",
      "        [-0.2905, -1.3778],\n",
      "        [-0.3878, -1.1350],\n",
      "        [-0.4688, -0.9829],\n",
      "        [-0.7886, -0.6060],\n",
      "        [-0.6685, -0.7185],\n",
      "        [-0.5899, -0.8083],\n",
      "        [-0.7040, -0.6825],\n",
      "        [-0.7205, -0.6665],\n",
      "        [-0.9474, -0.4906],\n",
      "        [-0.6000, -0.7959],\n",
      "        [-1.0883, -0.4106],\n",
      "        [-0.6773, -0.7093],\n",
      "        [-0.3346, -1.2575],\n",
      "        [-0.8098, -0.5887],\n",
      "        [-0.6421, -0.7470],\n",
      "        [-0.5596, -0.8473],\n",
      "        [-0.6609, -0.7265],\n",
      "        [-0.5351, -0.8810],\n",
      "        [-0.7064, -0.6800],\n",
      "        [-0.7051, -0.6813],\n",
      "        [-0.2387, -1.5494],\n",
      "        [-0.8873, -0.5307],\n",
      "        [-0.4202, -1.0698],\n",
      "        [-0.4079, -1.0937],\n",
      "        [-0.3810, -1.1493],\n",
      "        [-0.3533, -1.2119],\n",
      "        [-0.6184, -0.7740],\n",
      "        [-0.3065, -1.3318],\n",
      "        [-0.8324, -0.5709],\n",
      "        [-0.6739, -0.7128],\n",
      "        [-0.7428, -0.6458],\n",
      "        [-0.8018, -0.5951],\n",
      "        [-0.3845, -1.1418],\n",
      "        [-0.6507, -0.7375],\n",
      "        [-0.7124, -0.6743],\n",
      "        [-0.6659, -0.7212],\n",
      "        [-0.9388, -0.4961],\n",
      "        [-0.5053, -0.9247],\n",
      "        [-0.4399, -1.0331],\n",
      "        [-0.5210, -0.9012],\n",
      "        [-0.6609, -0.7265]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
      "        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
      "        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 1, 1])\n",
      "tensor([[-0.3098, -1.3227],\n",
      "        [-0.6834, -0.7030],\n",
      "        [-0.7954, -0.6004],\n",
      "        [-0.5736, -0.8289],\n",
      "        [-0.1392, -2.0407],\n",
      "        [-0.6427, -0.7462],\n",
      "        [-0.4749, -0.9727],\n",
      "        [-0.5141, -0.9114],\n",
      "        [-0.3365, -1.2527],\n",
      "        [-0.4189, -1.0723],\n",
      "        [-0.3348, -1.2568],\n",
      "        [-0.6062, -0.7884],\n",
      "        [-0.4570, -1.0029],\n",
      "        [-0.7384, -0.6499],\n",
      "        [-0.5111, -0.9158],\n",
      "        [-0.5650, -0.8402],\n",
      "        [-0.8303, -0.5726],\n",
      "        [-0.6002, -0.7956],\n",
      "        [-0.2405, -1.5430],\n",
      "        [-0.5718, -0.8313],\n",
      "        [-0.5877, -0.8110],\n",
      "        [-0.7286, -0.6589],\n",
      "        [-0.3098, -1.3228],\n",
      "        [-0.3434, -1.2357],\n",
      "        [-0.8855, -0.5319],\n",
      "        [-0.1800, -1.8035],\n",
      "        [-1.0253, -0.4443],\n",
      "        [-0.7266, -0.6608],\n",
      "        [-0.6054, -0.7893],\n",
      "        [-0.4697, -0.9813],\n",
      "        [-0.5522, -0.8573],\n",
      "        [-0.6818, -0.7046],\n",
      "        [-0.5834, -0.8164],\n",
      "        [-0.8854, -0.5319],\n",
      "        [-0.7359, -0.6521],\n",
      "        [-0.6267, -0.7643],\n",
      "        [-0.3474, -1.2259],\n",
      "        [-0.2995, -1.3516],\n",
      "        [-0.4559, -1.0049],\n",
      "        [-0.7636, -0.6273],\n",
      "        [-0.5763, -0.8254],\n",
      "        [-0.5448, -0.8674],\n",
      "        [-0.6991, -0.6872],\n",
      "        [-0.7667, -0.6247],\n",
      "        [-0.3888, -1.1329],\n",
      "        [-0.7740, -0.6184],\n",
      "        [-0.7737, -0.6186],\n",
      "        [-0.7482, -0.6410],\n",
      "        [-0.7202, -0.6668],\n",
      "        [-0.4736, -0.9748],\n",
      "        [-0.8105, -0.5882],\n",
      "        [-0.5791, -0.8219],\n",
      "        [-0.6607, -0.7267],\n",
      "        [-0.2891, -1.3821],\n",
      "        [-0.7519, -0.6377],\n",
      "        [-0.4271, -1.0566],\n",
      "        [-0.6398, -0.7495],\n",
      "        [-0.7576, -0.6326],\n",
      "        [-0.7055, -0.6809],\n",
      "        [-0.5071, -0.9219],\n",
      "        [-0.8138, -0.5855],\n",
      "        [-0.3171, -1.3029],\n",
      "        [-0.3323, -1.2632],\n",
      "        [-0.7061, -0.6803],\n",
      "        [-0.6932, -0.6931],\n",
      "        [-0.5849, -0.8145],\n",
      "        [-0.4215, -1.0674],\n",
      "        [-0.7491, -0.6402],\n",
      "        [-0.5837, -0.8160],\n",
      "        [-0.6607, -0.7267],\n",
      "        [-0.7489, -0.6404],\n",
      "        [-0.6827, -0.7037],\n",
      "        [-0.6658, -0.7213],\n",
      "        [-0.1831, -1.7881],\n",
      "        [-0.5629, -0.8429],\n",
      "        [-0.4486, -1.0176],\n",
      "        [-0.9855, -0.4672],\n",
      "        [-0.6754, -0.7113],\n",
      "        [-0.5673, -0.8371],\n",
      "        [-0.8260, -0.5759],\n",
      "        [-0.3619, -1.1919],\n",
      "        [-0.7522, -0.6373],\n",
      "        [-0.6996, -0.6867],\n",
      "        [-0.7376, -0.6505],\n",
      "        [-0.7875, -0.6069],\n",
      "        [-0.6910, -0.6953],\n",
      "        [-0.2522, -1.5011],\n",
      "        [-0.7655, -0.6256],\n",
      "        [-0.5853, -0.8141],\n",
      "        [-1.1560, -0.3780],\n",
      "        [-0.8982, -0.5231],\n",
      "        [-0.6618, -0.7255],\n",
      "        [-0.5694, -0.8344],\n",
      "        [-0.6607, -0.7267],\n",
      "        [-0.7266, -0.6608],\n",
      "        [-0.6142, -0.7789],\n",
      "        [-0.2463, -1.5217],\n",
      "        [-0.6191, -0.7731],\n",
      "        [-0.6418, -0.7473],\n",
      "        [-0.8489, -0.5584],\n",
      "        [-0.6375, -0.7520],\n",
      "        [-0.5956, -0.8013],\n",
      "        [-0.4823, -0.9606],\n",
      "        [-0.9068, -0.5172],\n",
      "        [-0.6609, -0.7265],\n",
      "        [-0.5962, -0.8005],\n",
      "        [-0.5904, -0.8076],\n",
      "        [-0.1600, -1.9117],\n",
      "        [-0.4385, -1.0356],\n",
      "        [-0.8672, -0.5450],\n",
      "        [-0.4850, -0.9564],\n",
      "        [-0.6545, -0.7333],\n",
      "        [-0.4184, -1.0732],\n",
      "        [-0.9202, -0.5082],\n",
      "        [-0.5762, -0.8256],\n",
      "        [-1.6256, -0.2191],\n",
      "        [-0.6009, -0.7948],\n",
      "        [-0.7372, -0.6509],\n",
      "        [-0.6999, -0.6864],\n",
      "        [-0.6854, -0.7009],\n",
      "        [-0.2573, -1.4833],\n",
      "        [-0.7145, -0.6722],\n",
      "        [-0.6877, -0.6986],\n",
      "        [-0.2027, -1.6957],\n",
      "        [-0.6260, -0.7651],\n",
      "        [-0.7411, -0.6474],\n",
      "        [-0.2533, -1.4973],\n",
      "        [-0.7333, -0.6545]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
      "        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0])\n",
      "tensor([[-0.2077, -1.6738],\n",
      "        [-0.6300, -0.7605],\n",
      "        [-0.1474, -1.9871],\n",
      "        [-0.7213, -0.6658],\n",
      "        [-0.5045, -0.9259],\n",
      "        [-0.2942, -1.3669],\n",
      "        [-0.4254, -1.0599],\n",
      "        [-0.7508, -0.6386],\n",
      "        [-0.6329, -0.7573],\n",
      "        [-0.6582, -0.7293],\n",
      "        [-0.3393, -1.2457],\n",
      "        [-0.6605, -0.7269],\n",
      "        [-0.5211, -0.9011],\n",
      "        [-0.2918, -1.3740],\n",
      "        [-0.2386, -1.5498],\n",
      "        [-0.8453, -0.5611],\n",
      "        [-0.4695, -0.9817],\n",
      "        [-0.3443, -1.2335],\n",
      "        [-0.7048, -0.6816],\n",
      "        [-0.6492, -0.7391],\n",
      "        [-0.5198, -0.9030],\n",
      "        [-0.3283, -1.2734],\n",
      "        [-0.6878, -0.6985],\n",
      "        [-0.5586, -0.8487],\n",
      "        [-0.6574, -0.7303],\n",
      "        [-0.5258, -0.8943],\n",
      "        [-0.2259, -1.5985],\n",
      "        [-0.1144, -2.2244],\n",
      "        [-0.7162, -0.6706],\n",
      "        [-0.9085, -0.5160],\n",
      "        [-0.5839, -0.8158],\n",
      "        [-0.6563, -0.7314],\n",
      "        [-0.3480, -1.2246],\n",
      "        [-0.6524, -0.7356],\n",
      "        [-0.7389, -0.6494],\n",
      "        [-0.3963, -1.1173],\n",
      "        [-0.4462, -1.0217],\n",
      "        [-0.6667, -0.7203],\n",
      "        [-0.6826, -0.7038],\n",
      "        [-0.3422, -1.2385],\n",
      "        [-0.6558, -0.7319],\n",
      "        [-0.6759, -0.7107],\n",
      "        [-0.7130, -0.6737],\n",
      "        [-0.7331, -0.6547],\n",
      "        [-0.4180, -1.0740],\n",
      "        [-0.4279, -1.0552],\n",
      "        [-0.7169, -0.6700],\n",
      "        [-0.3456, -1.2302],\n",
      "        [-0.6063, -0.7882],\n",
      "        [-0.6944, -0.6919],\n",
      "        [-0.4259, -1.0589],\n",
      "        [-0.7263, -0.6610],\n",
      "        [-0.6491, -0.7392],\n",
      "        [-0.4511, -1.0132],\n",
      "        [-0.7538, -0.6359],\n",
      "        [-0.6772, -0.7094],\n",
      "        [-0.6563, -0.7314],\n",
      "        [-0.2582, -1.4804],\n",
      "        [-0.5924, -0.8052],\n",
      "        [-0.3809, -1.1496],\n",
      "        [-0.2885, -1.3840],\n",
      "        [-0.5677, -0.8366],\n",
      "        [-0.7048, -0.6816],\n",
      "        [-0.5275, -0.8917],\n",
      "        [-0.5196, -0.9032],\n",
      "        [-0.6538, -0.7341],\n",
      "        [-0.3024, -1.3435],\n",
      "        [-0.7051, -0.6813],\n",
      "        [-0.6262, -0.7649],\n",
      "        [-0.4166, -1.0767],\n",
      "        [-0.5498, -0.8605],\n",
      "        [-0.3993, -1.1110],\n",
      "        [-1.0889, -0.4104],\n",
      "        [-0.4853, -0.9558],\n",
      "        [-0.6631, -0.7242],\n",
      "        [-0.4513, -1.0129],\n",
      "        [-0.7491, -0.6402],\n",
      "        [-0.5456, -0.8663],\n",
      "        [-0.7310, -0.6567],\n",
      "        [-0.5063, -0.9231],\n",
      "        [-0.7445, -0.6443],\n",
      "        [-0.3514, -1.2165],\n",
      "        [-0.5554, -0.8530],\n",
      "        [-0.6485, -0.7399],\n",
      "        [-0.6528, -0.7352],\n",
      "        [-0.6074, -0.7869],\n",
      "        [-0.3161, -1.3056],\n",
      "        [-1.0192, -0.4477],\n",
      "        [-0.7107, -0.6759],\n",
      "        [-0.6526, -0.7354],\n",
      "        [-0.7367, -0.6514],\n",
      "        [-0.5831, -0.8168],\n",
      "        [-0.8272, -0.5750],\n",
      "        [-0.5012, -0.9308],\n",
      "        [-0.5259, -0.8941],\n",
      "        [-0.6638, -0.7234],\n",
      "        [-0.6854, -0.7009],\n",
      "        [-0.7850, -0.6090],\n",
      "        [-0.7491, -0.6401],\n",
      "        [-0.4265, -1.0578],\n",
      "        [-0.3762, -1.1597],\n",
      "        [-0.7668, -0.6245],\n",
      "        [-0.2492, -1.5114],\n",
      "        [-0.6326, -0.7576],\n",
      "        [-0.7975, -0.5987],\n",
      "        [-0.8462, -0.5604],\n",
      "        [-0.6399, -0.7494],\n",
      "        [-0.3481, -1.2242],\n",
      "        [-0.8776, -0.5375],\n",
      "        [-0.4971, -0.9372],\n",
      "        [-0.6829, -0.7036],\n",
      "        [-0.7291, -0.6584],\n",
      "        [-0.6946, -0.6917],\n",
      "        [-0.6857, -0.7007],\n",
      "        [-0.6557, -0.7321],\n",
      "        [-0.5206, -0.9019],\n",
      "        [-0.6321, -0.7582],\n",
      "        [-0.7248, -0.6625],\n",
      "        [-0.7675, -0.6239],\n",
      "        [-0.5112, -0.9158],\n",
      "        [-0.7510, -0.6385],\n",
      "        [-0.6291, -0.7616],\n",
      "        [-0.6952, -0.6911],\n",
      "        [-0.1031, -2.3229],\n",
      "        [-0.2452, -1.5257],\n",
      "        [-0.2280, -1.5901],\n",
      "        [-0.6237, -0.7678],\n",
      "        [-0.6981, -0.6882]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0, 1, 1])\n",
      "tensor([[-0.6794, -0.7070],\n",
      "        [-0.6785, -0.7080],\n",
      "        [-0.6645, -0.7227],\n",
      "        [-0.6258, -0.7654],\n",
      "        [-0.6963, -0.6900],\n",
      "        [-0.5015, -0.9305],\n",
      "        [-0.7102, -0.6764],\n",
      "        [-0.7098, -0.6768],\n",
      "        [-0.7345, -0.6535],\n",
      "        [-0.1156, -2.2150],\n",
      "        [-0.4905, -0.9476],\n",
      "        [-0.6746, -0.7120],\n",
      "        [-0.7455, -0.6434],\n",
      "        [-0.0748, -2.6296],\n",
      "        [-0.1251, -2.1406],\n",
      "        [-0.4533, -1.0093],\n",
      "        [-1.1313, -0.3895],\n",
      "        [-0.4131, -1.0835],\n",
      "        [-0.5953, -0.8016],\n",
      "        [-0.2974, -1.3576],\n",
      "        [-0.8003, -0.5964],\n",
      "        [-0.2690, -1.4445],\n",
      "        [-0.6507, -0.7375],\n",
      "        [-0.7274, -0.6600],\n",
      "        [-0.6022, -0.7932],\n",
      "        [-0.6214, -0.7705],\n",
      "        [-0.6127, -0.7806],\n",
      "        [-0.7967, -0.5994],\n",
      "        [-0.5081, -0.9204],\n",
      "        [-0.5253, -0.8949],\n",
      "        [-0.6477, -0.7408],\n",
      "        [-0.7561, -0.6339],\n",
      "        [-0.6749, -0.7118],\n",
      "        [-0.3830, -1.1451],\n",
      "        [-0.2303, -1.5814],\n",
      "        [-0.6591, -0.7284],\n",
      "        [-0.4841, -0.9578],\n",
      "        [-0.5658, -0.8392],\n",
      "        [-0.4413, -1.0307],\n",
      "        [-0.7425, -0.6461],\n",
      "        [-0.5555, -0.8528],\n",
      "        [-0.7122, -0.6744],\n",
      "        [-0.6201, -0.7719],\n",
      "        [-0.8305, -0.5724],\n",
      "        [-0.3291, -1.2715],\n",
      "        [-0.3622, -1.1912],\n",
      "        [-0.1831, -1.7876],\n",
      "        [-0.6381, -0.7514],\n",
      "        [-0.3074, -1.3293],\n",
      "        [-0.4893, -0.9495],\n",
      "        [-0.6172, -0.7754],\n",
      "        [-0.6069, -0.7875],\n",
      "        [-0.6610, -0.7264],\n",
      "        [-0.8477, -0.5593],\n",
      "        [-0.7026, -0.6837],\n",
      "        [-0.8178, -0.5823],\n",
      "        [-0.4545, -1.0072],\n",
      "        [-0.7092, -0.6773],\n",
      "        [-0.4971, -0.9372],\n",
      "        [-0.6975, -0.6888],\n",
      "        [-0.4071, -1.0954],\n",
      "        [-0.5830, -0.8169],\n",
      "        [-0.4873, -0.9526],\n",
      "        [-0.7005, -0.6859],\n",
      "        [-0.5918, -0.8059],\n",
      "        [-0.3738, -1.1652],\n",
      "        [-0.6271, -0.7638],\n",
      "        [-0.6752, -0.7114],\n",
      "        [-0.6287, -0.7620],\n",
      "        [-0.5084, -0.9199],\n",
      "        [-0.5684, -0.8357],\n",
      "        [-0.5908, -0.8071],\n",
      "        [-0.6835, -0.7029],\n",
      "        [-0.7126, -0.6741],\n",
      "        [-0.2408, -1.5416],\n",
      "        [-0.4980, -0.9359],\n",
      "        [-0.5325, -0.8846],\n",
      "        [-0.3781, -1.1558],\n",
      "        [-0.7228, -0.6643],\n",
      "        [-0.4979, -0.9360],\n",
      "        [-0.5813, -0.8192],\n",
      "        [-0.4965, -0.9382],\n",
      "        [-0.2269, -1.5944],\n",
      "        [-0.3114, -1.3182],\n",
      "        [-0.4601, -0.9975],\n",
      "        [-0.9439, -0.4928],\n",
      "        [-0.3557, -1.2062],\n",
      "        [-0.4905, -0.9476],\n",
      "        [-0.5254, -0.8948],\n",
      "        [-0.5325, -0.8847],\n",
      "        [-0.0710, -2.6799],\n",
      "        [-0.6162, -0.7765],\n",
      "        [-0.7461, -0.6429],\n",
      "        [-0.7800, -0.6132],\n",
      "        [-0.5922, -0.8054],\n",
      "        [-0.6359, -0.7539],\n",
      "        [-0.6496, -0.7387],\n",
      "        [-0.7229, -0.6642],\n",
      "        [-0.3961, -1.1177],\n",
      "        [-0.6925, -0.6938],\n",
      "        [-1.0022, -0.4574],\n",
      "        [-0.5923, -0.8053],\n",
      "        [-0.7025, -0.6838],\n",
      "        [-0.5640, -0.8415],\n",
      "        [-0.7592, -0.6311],\n",
      "        [-0.5718, -0.8313],\n",
      "        [-0.4497, -1.0156],\n",
      "        [-0.6689, -0.7180],\n",
      "        [-0.2003, -1.7065],\n",
      "        [-0.4920, -0.9452],\n",
      "        [-0.5940, -0.8032],\n",
      "        [-0.0952, -2.3985],\n",
      "        [-0.6177, -0.7747],\n",
      "        [-0.8691, -0.5436],\n",
      "        [-0.8560, -0.5531],\n",
      "        [-0.7141, -0.6726],\n",
      "        [-0.8542, -0.5545],\n",
      "        [-1.0123, -0.4516],\n",
      "        [-0.4571, -1.0027],\n",
      "        [-0.5882, -0.8104],\n",
      "        [-2.0936, -0.1315],\n",
      "        [-0.6942, -0.6921],\n",
      "        [-0.6366, -0.7531],\n",
      "        [-0.5737, -0.8288],\n",
      "        [-0.5761, -0.8258],\n",
      "        [-0.9479, -0.4903],\n",
      "        [-0.2777, -1.4168],\n",
      "        [-0.1808, -1.7997]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
      "        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
      "        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
      "        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,\n",
      "        1, 0, 1, 0, 0, 1, 0, 0])\n",
      "tensor([[-0.6072, -0.7871],\n",
      "        [-0.5710, -0.8324],\n",
      "        [-0.7036, -0.6828],\n",
      "        [-0.6942, -0.6921],\n",
      "        [-0.4530, -1.0098],\n",
      "        [-0.6366, -0.7531],\n",
      "        [-0.6951, -0.6912],\n",
      "        [-0.5462, -0.8655],\n",
      "        [-0.7755, -0.6171],\n",
      "        [-0.8723, -0.5413],\n",
      "        [-0.6268, -0.7642],\n",
      "        [-0.2809, -1.4070],\n",
      "        [-0.6271, -0.7639],\n",
      "        [-0.5755, -0.8264],\n",
      "        [-0.7121, -0.6746],\n",
      "        [-0.3870, -1.1366],\n",
      "        [-0.4185, -1.0731],\n",
      "        [-0.5140, -0.9116],\n",
      "        [-0.5371, -0.8782],\n",
      "        [-0.8294, -0.5733],\n",
      "        [-0.1816, -1.7954],\n",
      "        [-0.5746, -0.8277],\n",
      "        [-0.6614, -0.7259],\n",
      "        [-0.8826, -0.5339],\n",
      "        [-0.7659, -0.6253],\n",
      "        [-0.7404, -0.6480],\n",
      "        [-0.4991, -0.9342],\n",
      "        [-0.2963, -1.3607],\n",
      "        [-0.6600, -0.7274],\n",
      "        [-0.7223, -0.6648],\n",
      "        [-0.6803, -0.7062],\n",
      "        [-0.6980, -0.6883],\n",
      "        [-0.5550, -0.8535],\n",
      "        [-0.7682, -0.6233],\n",
      "        [-0.5634, -0.8422],\n",
      "        [-0.7080, -0.6785],\n",
      "        [-0.6751, -0.7115],\n",
      "        [-0.3420, -1.2390],\n",
      "        [-0.6707, -0.7161],\n",
      "        [-0.6885, -0.6978],\n",
      "        [-0.5409, -0.8728],\n",
      "        [-0.5165, -0.9079],\n",
      "        [-0.5253, -0.8949],\n",
      "        [-0.1864, -1.7717],\n",
      "        [-0.8687, -0.5438],\n",
      "        [-0.3378, -1.2495],\n",
      "        [-0.8153, -0.5843],\n",
      "        [-0.4707, -0.9797],\n",
      "        [-0.8625, -0.5484],\n",
      "        [-0.6271, -0.7638],\n",
      "        [-0.3149, -1.3089],\n",
      "        [-0.6644, -0.7227],\n",
      "        [-0.5113, -0.9155],\n",
      "        [-0.0736, -2.6451],\n",
      "        [-0.6788, -0.7078],\n",
      "        [-0.8207, -0.5801],\n",
      "        [-0.7531, -0.6366],\n",
      "        [-0.8027, -0.5944],\n",
      "        [-0.7431, -0.6456],\n",
      "        [-0.6616, -0.7258],\n",
      "        [-0.6774, -0.7091],\n",
      "        [-0.6147, -0.7783],\n",
      "        [-0.5144, -0.9110],\n",
      "        [-0.6775, -0.7090],\n",
      "        [-0.7134, -0.6733],\n",
      "        [-0.1874, -1.7666],\n",
      "        [-0.8084, -0.5898],\n",
      "        [-0.7924, -0.6029],\n",
      "        [-0.6204, -0.7717],\n",
      "        [-0.7997, -0.5969],\n",
      "        [-0.4982, -0.9356],\n",
      "        [-0.4001, -1.1094],\n",
      "        [-0.6096, -0.7843],\n",
      "        [-0.2623, -1.4665],\n",
      "        [-0.2408, -1.5416],\n",
      "        [-0.5448, -0.8673],\n",
      "        [-0.6563, -0.7314],\n",
      "        [-0.4044, -1.1007],\n",
      "        [-0.7339, -0.6540],\n",
      "        [-0.5003, -0.9323],\n",
      "        [-0.5245, -0.8962],\n",
      "        [-0.6201, -0.7719],\n",
      "        [-0.7071, -0.6794],\n",
      "        [-0.5586, -0.8487],\n",
      "        [-0.4713, -0.9786],\n",
      "        [-0.5218, -0.9001],\n",
      "        [-0.5803, -0.8204],\n",
      "        [-0.4254, -1.0598],\n",
      "        [-0.8196, -0.5809],\n",
      "        [-0.6495, -0.7388],\n",
      "        [-0.7859, -0.6083],\n",
      "        [-0.6191, -0.7732],\n",
      "        [-0.1493, -1.9758],\n",
      "        [-0.6566, -0.7311],\n",
      "        [-0.6342, -0.7558],\n",
      "        [-0.0979, -2.3719],\n",
      "        [-0.4099, -1.0898],\n",
      "        [-0.6478, -0.7407],\n",
      "        [-0.8171, -0.5828],\n",
      "        [-0.6016, -0.7939],\n",
      "        [-0.3677, -1.1787],\n",
      "        [-0.4530, -1.0098],\n",
      "        [-0.4774, -0.9686],\n",
      "        [-0.5999, -0.7960],\n",
      "        [-0.5938, -0.8035],\n",
      "        [-0.4625, -0.9934],\n",
      "        [-0.7460, -0.6430],\n",
      "        [-0.7099, -0.6766],\n",
      "        [-0.2905, -1.3780],\n",
      "        [-0.6839, -0.7025],\n",
      "        [-0.6444, -0.7444],\n",
      "        [-0.5094, -0.9184],\n",
      "        [-0.8023, -0.5947],\n",
      "        [-0.6380, -0.7515],\n",
      "        [-0.7111, -0.6755],\n",
      "        [-0.1004, -2.3483],\n",
      "        [-0.8428, -0.5630],\n",
      "        [-0.5533, -0.8558],\n",
      "        [-0.6688, -0.7181],\n",
      "        [-0.8418, -0.5638],\n",
      "        [-0.4302, -1.0509],\n",
      "        [-0.5725, -0.8303],\n",
      "        [-0.6298, -0.7608],\n",
      "        [-0.4786, -0.9666],\n",
      "        [-0.5684, -0.8357],\n",
      "        [-0.3782, -1.1555],\n",
      "        [-0.4956, -0.9396],\n",
      "        [-0.8563, -0.5529]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
      "        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
      "        1, 1, 1, 0, 1, 1, 0, 1])\n",
      "tensor([[-0.7756, -0.6169],\n",
      "        [-0.6308, -0.7597],\n",
      "        [-0.7316, -0.6562],\n",
      "        [-0.7459, -0.6431],\n",
      "        [-0.6753, -0.7113],\n",
      "        [-0.6428, -0.7461],\n",
      "        [-0.6131, -0.7801],\n",
      "        [-0.3153, -1.3076],\n",
      "        [-0.5061, -0.9235],\n",
      "        [-0.6518, -0.7363],\n",
      "        [-0.5314, -0.8862],\n",
      "        [-0.5796, -0.8212],\n",
      "        [-0.6777, -0.7088],\n",
      "        [-0.7538, -0.6359],\n",
      "        [-0.6018, -0.7937],\n",
      "        [-0.7966, -0.5994],\n",
      "        [-0.2783, -1.4149],\n",
      "        [-0.4256, -1.0596],\n",
      "        [-0.4192, -1.0718],\n",
      "        [-0.6048, -0.7901],\n",
      "        [-0.6113, -0.7823],\n",
      "        [-0.8265, -0.5755],\n",
      "        [-0.6651, -0.7220],\n",
      "        [-0.4473, -1.0199],\n",
      "        [-0.5107, -0.9165],\n",
      "        [-0.6694, -0.7174],\n",
      "        [-0.8544, -0.5543],\n",
      "        [-0.7609, -0.6297],\n",
      "        [-0.6666, -0.7204],\n",
      "        [-0.7115, -0.6751],\n",
      "        [-0.6396, -0.7498],\n",
      "        [-0.3040, -1.3389],\n",
      "        [-0.7907, -0.6042],\n",
      "        [-0.6498, -0.7385],\n",
      "        [-0.7882, -0.6063],\n",
      "        [-0.6833, -0.7031],\n",
      "        [-0.4163, -1.0773],\n",
      "        [-0.6095, -0.7844],\n",
      "        [-0.7478, -0.6413],\n",
      "        [-0.6382, -0.7513],\n",
      "        [-0.6795, -0.7070],\n",
      "        [-0.5163, -0.9081],\n",
      "        [-0.6572, -0.7304],\n",
      "        [-0.5394, -0.8748],\n",
      "        [-0.6547, -0.7331],\n",
      "        [-0.2870, -1.3884],\n",
      "        [-0.2735, -1.4300],\n",
      "        [-0.6284, -0.7623],\n",
      "        [-0.7514, -0.6381],\n",
      "        [-0.5395, -0.8748],\n",
      "        [-0.7151, -0.6716],\n",
      "        [-0.5311, -0.8867],\n",
      "        [-0.7057, -0.6808],\n",
      "        [-0.2995, -1.3517],\n",
      "        [-0.1170, -2.2034],\n",
      "        [-0.1534, -1.9507],\n",
      "        [-0.5160, -0.9085],\n",
      "        [-0.8345, -0.5693],\n",
      "        [-0.6054, -0.7893],\n",
      "        [-0.3724, -1.1681],\n",
      "        [-0.3876, -1.1354],\n",
      "        [-0.5975, -0.7989],\n",
      "        [-0.6608, -0.7266],\n",
      "        [-0.7491, -0.6402],\n",
      "        [-0.6791, -0.7074],\n",
      "        [-0.7111, -0.6755],\n",
      "        [-0.6961, -0.6902],\n",
      "        [-0.5902, -0.8079],\n",
      "        [-0.0552, -2.9234],\n",
      "        [-0.6887, -0.6976],\n",
      "        [-0.0989, -2.3626],\n",
      "        [-0.6167, -0.7760],\n",
      "        [-0.7448, -0.6440],\n",
      "        [-0.9138, -0.5125],\n",
      "        [-0.8440, -0.5621],\n",
      "        [-0.6114, -0.7822],\n",
      "        [-0.8102, -0.5884],\n",
      "        [-0.6594, -0.7281],\n",
      "        [-0.7389, -0.6494],\n",
      "        [-0.3599, -1.1965],\n",
      "        [-0.5512, -0.8586],\n",
      "        [-0.6490, -0.7393],\n",
      "        [-0.6475, -0.7410],\n",
      "        [-0.2632, -1.4635],\n",
      "        [-0.8460, -0.5606],\n",
      "        [-0.7176, -0.6693],\n",
      "        [-0.4637, -0.9914],\n",
      "        [-0.5458, -0.8660],\n",
      "        [-0.7578, -0.6324],\n",
      "        [-0.9327, -0.5001],\n",
      "        [-0.5366, -0.8789],\n",
      "        [-0.6573, -0.7304],\n",
      "        [-0.6096, -0.7843],\n",
      "        [-0.6264, -0.7647],\n",
      "        [-0.6417, -0.7474],\n",
      "        [-0.5634, -0.8423],\n",
      "        [-0.7409, -0.6476],\n",
      "        [-1.0014, -0.4579],\n",
      "        [-0.4911, -0.9466],\n",
      "        [-0.6847, -0.7017],\n",
      "        [-0.8101, -0.5885],\n",
      "        [-0.8397, -0.5654],\n",
      "        [-0.5938, -0.8035],\n",
      "        [-0.2961, -1.3616],\n",
      "        [-0.3595, -1.1973],\n",
      "        [-0.6388, -0.7506],\n",
      "        [-0.5304, -0.8876],\n",
      "        [-0.6736, -0.7131],\n",
      "        [-0.5936, -0.8037],\n",
      "        [-0.4556, -1.0053],\n",
      "        [-1.0476, -0.4320],\n",
      "        [-0.3738, -1.1651],\n",
      "        [-0.2366, -1.5575],\n",
      "        [-0.6754, -0.7112],\n",
      "        [-0.4828, -0.9599],\n",
      "        [-0.7356, -0.6524],\n",
      "        [-0.6153, -0.7776],\n",
      "        [-0.3983, -1.1131],\n",
      "        [-0.5308, -0.8870],\n",
      "        [-0.1408, -2.0302],\n",
      "        [-0.4249, -1.0609],\n",
      "        [-0.6987, -0.6876],\n",
      "        [-0.6400, -0.7493],\n",
      "        [-0.2816, -1.4048],\n",
      "        [-0.2992, -1.3526],\n",
      "        [-0.6432, -0.7457],\n",
      "        [-0.7726, -0.6195],\n",
      "        [-0.2020, -1.6990]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "        1, 1, 0, 0, 1, 0, 0, 0])\n",
      "tensor([[-0.4623, -0.9938],\n",
      "        [-0.6476, -0.7409],\n",
      "        [-0.4926, -0.9442],\n",
      "        [-0.7941, -0.6014],\n",
      "        [-0.7349, -0.6531],\n",
      "        [-0.4655, -0.9884],\n",
      "        [-0.5558, -0.8524],\n",
      "        [-0.3956, -1.1186],\n",
      "        [-0.5771, -0.8244],\n",
      "        [-0.1008, -2.3448],\n",
      "        [-0.3529, -1.2127],\n",
      "        [-0.4834, -0.9589],\n",
      "        [-0.5390, -0.8755],\n",
      "        [-0.8113, -0.5875],\n",
      "        [-0.5374, -0.8777],\n",
      "        [-0.6824, -0.7040],\n",
      "        [-0.6544, -0.7335],\n",
      "        [-0.6170, -0.7756],\n",
      "        [-0.2316, -1.5761],\n",
      "        [-0.5973, -0.7992],\n",
      "        [-0.5718, -0.8313],\n",
      "        [-0.2102, -1.6630],\n",
      "        [-0.6982, -0.6881],\n",
      "        [-0.5806, -0.8200],\n",
      "        [-0.2093, -1.6670],\n",
      "        [-0.2706, -1.4393],\n",
      "        [-0.6470, -0.7415],\n",
      "        [-0.1962, -1.7252],\n",
      "        [-0.9716, -0.4756],\n",
      "        [-0.2323, -1.5735],\n",
      "        [-0.6523, -0.7357],\n",
      "        [-0.7639, -0.6271],\n",
      "        [-0.0281, -3.5875],\n",
      "        [-0.7290, -0.6585],\n",
      "        [-0.2127, -1.6524],\n",
      "        [-0.9367, -0.4974],\n",
      "        [-0.7288, -0.6587],\n",
      "        [-0.7259, -0.6614],\n",
      "        [-0.7554, -0.6345],\n",
      "        [-0.5820, -0.8182],\n",
      "        [-0.0920, -2.4311],\n",
      "        [-0.4881, -0.9514],\n",
      "        [-0.8514, -0.5566],\n",
      "        [-0.6765, -0.7101],\n",
      "        [-0.3009, -1.3477],\n",
      "        [-0.4923, -0.9447],\n",
      "        [-0.6828, -0.7036],\n",
      "        [-0.5447, -0.8675],\n",
      "        [-0.7027, -0.6837],\n",
      "        [-0.5923, -0.8053],\n",
      "        [-0.5826, -0.8174],\n",
      "        [-0.9727, -0.4749],\n",
      "        [-0.8799, -0.5359],\n",
      "        [-0.2406, -1.5424],\n",
      "        [-0.6572, -0.7304],\n",
      "        [-0.2446, -1.5280],\n",
      "        [-0.4944, -0.9414],\n",
      "        [-0.7583, -0.6320],\n",
      "        [-0.4315, -1.0485],\n",
      "        [-0.6521, -0.7359],\n",
      "        [-0.5158, -0.9089],\n",
      "        [-0.6765, -0.7101],\n",
      "        [-0.5011, -0.9310],\n",
      "        [-0.5148, -0.9104],\n",
      "        [-0.6369, -0.7528],\n",
      "        [-0.2312, -1.5780],\n",
      "        [-0.4850, -0.9563],\n",
      "        [-0.5631, -0.8427],\n",
      "        [-0.5050, -0.9250],\n",
      "        [-0.1589, -1.9177],\n",
      "        [-0.6707, -0.7162],\n",
      "        [-0.7031, -0.6833],\n",
      "        [-0.2097, -1.6650],\n",
      "        [-0.5530, -0.8562],\n",
      "        [-0.6965, -0.6898],\n",
      "        [-0.7198, -0.6672],\n",
      "        [-0.4729, -0.9760],\n",
      "        [-0.6854, -0.7009],\n",
      "        [-0.6678, -0.7191],\n",
      "        [-0.7309, -0.6567],\n",
      "        [-0.6468, -0.7417],\n",
      "        [-0.6453, -0.7434],\n",
      "        [-0.5845, -0.8151],\n",
      "        [-0.7210, -0.6660],\n",
      "        [-0.8206, -0.5801],\n",
      "        [-0.5367, -0.8787],\n",
      "        [-0.4975, -0.9367],\n",
      "        [-0.1829, -1.7889],\n",
      "        [-0.3503, -1.2189],\n",
      "        [-0.3800, -1.1516],\n",
      "        [-0.7850, -0.6090],\n",
      "        [-0.2725, -1.4334],\n",
      "        [-0.6903, -0.6960],\n",
      "        [-0.6517, -0.7364],\n",
      "        [-0.3884, -1.1337],\n",
      "        [-0.2043, -1.6887],\n",
      "        [-0.8671, -0.5450],\n",
      "        [-0.7176, -0.6693],\n",
      "        [-0.6865, -0.6999],\n",
      "        [-0.5178, -0.9059],\n",
      "        [-0.7161, -0.6707],\n",
      "        [-0.5307, -0.8873],\n",
      "        [-0.2053, -1.6844],\n",
      "        [-0.7811, -0.6123],\n",
      "        [-0.6257, -0.7655],\n",
      "        [-0.3464, -1.2283],\n",
      "        [-0.5923, -0.8053],\n",
      "        [-0.8188, -0.5815],\n",
      "        [-0.7045, -0.6820],\n",
      "        [-0.1280, -2.1193],\n",
      "        [-0.7301, -0.6575],\n",
      "        [-0.5177, -0.9061],\n",
      "        [-0.6002, -0.7956],\n",
      "        [-0.6297, -0.7609],\n",
      "        [-0.3914, -1.1274],\n",
      "        [-0.5100, -0.9175],\n",
      "        [-0.7035, -0.6829],\n",
      "        [-0.6800, -0.7065],\n",
      "        [-0.6232, -0.7684],\n",
      "        [-0.2435, -1.5319],\n",
      "        [-0.8639, -0.5474],\n",
      "        [-0.2066, -1.6785],\n",
      "        [-0.3174, -1.3021],\n",
      "        [-0.4239, -1.0627],\n",
      "        [-0.5906, -0.8074],\n",
      "        [-0.7158, -0.6710],\n",
      "        [-0.9050, -0.5184],\n",
      "        [-0.3177, -1.3013]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
      "        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1, 1, 0])\n",
      "tensor([[-0.7036, -0.6828],\n",
      "        [-0.6743, -0.7123],\n",
      "        [-0.6598, -0.7277],\n",
      "        [-0.1563, -1.9332],\n",
      "        [-0.6865, -0.6999],\n",
      "        [-0.6995, -0.6869],\n",
      "        [-0.3152, -1.3081],\n",
      "        [-0.2242, -1.6054],\n",
      "        [-0.6173, -0.7752],\n",
      "        [-0.2294, -1.5850],\n",
      "        [-0.5531, -0.8561],\n",
      "        [-0.6140, -0.7791],\n",
      "        [-0.5046, -0.9257],\n",
      "        [-0.5319, -0.8855],\n",
      "        [-0.4601, -0.9976],\n",
      "        [-0.1268, -2.1279],\n",
      "        [-0.1842, -1.7824],\n",
      "        [-0.6868, -0.6996],\n",
      "        [-0.2989, -1.3534],\n",
      "        [-0.0264, -3.6490],\n",
      "        [-0.1015, -2.3380],\n",
      "        [-0.2898, -1.3799],\n",
      "        [-0.3502, -1.2191],\n",
      "        [-0.7624, -0.6283],\n",
      "        [-0.7493, -0.6400],\n",
      "        [-0.8271, -0.5751],\n",
      "        [-0.3335, -1.2603],\n",
      "        [-0.5003, -0.9323],\n",
      "        [-0.6498, -0.7385],\n",
      "        [-0.6273, -0.7637],\n",
      "        [-0.5974, -0.7990],\n",
      "        [-0.6918, -0.6945],\n",
      "        [-0.2913, -1.3756],\n",
      "        [-0.7372, -0.6509],\n",
      "        [-0.5493, -0.8612],\n",
      "        [-0.4953, -0.9400],\n",
      "        [-2.0695, -0.1350],\n",
      "        [-0.2490, -1.5122],\n",
      "        [-0.5975, -0.7989],\n",
      "        [-0.5001, -0.9326],\n",
      "        [-0.3427, -1.2373],\n",
      "        [-0.4193, -1.0716],\n",
      "        [-0.6130, -0.7803],\n",
      "        [-0.7028, -0.6836],\n",
      "        [-0.4855, -0.9556],\n",
      "        [-0.6016, -0.7939],\n",
      "        [-0.6530, -0.7349],\n",
      "        [-0.3931, -1.1238],\n",
      "        [-0.6636, -0.7236],\n",
      "        [-0.5263, -0.8935],\n",
      "        [-0.7708, -0.6211],\n",
      "        [-0.6078, -0.7865],\n",
      "        [-0.5780, -0.8233],\n",
      "        [-0.7465, -0.6425],\n",
      "        [-0.7190, -0.6680],\n",
      "        [-0.3198, -1.2957],\n",
      "        [-0.1629, -1.8951],\n",
      "        [-0.5010, -0.9312],\n",
      "        [-0.2609, -1.4714],\n",
      "        [-0.4956, -0.9396],\n",
      "        [-0.3004, -1.3491],\n",
      "        [-0.5102, -0.9172],\n",
      "        [-0.5038, -0.9269],\n",
      "        [-0.7414, -0.6471],\n",
      "        [-0.4813, -0.9622],\n",
      "        [-0.5554, -0.8530],\n",
      "        [-0.3400, -1.2440],\n",
      "        [-0.6207, -0.7712],\n",
      "        [-0.6461, -0.7426],\n",
      "        [-0.2307, -1.5797],\n",
      "        [-1.0302, -0.4415],\n",
      "        [-0.4786, -0.9667],\n",
      "        [-0.6978, -0.6885],\n",
      "        [-0.6401, -0.7492],\n",
      "        [-0.1297, -2.1064],\n",
      "        [-0.8021, -0.5949],\n",
      "        [-0.7976, -0.5986],\n",
      "        [-0.5842, -0.8154],\n",
      "        [-0.6102, -0.7836],\n",
      "        [-0.4852, -0.9559],\n",
      "        [-1.0003, -0.4585],\n",
      "        [-0.5110, -0.9160],\n",
      "        [-0.6567, -0.7310],\n",
      "        [-0.3306, -1.2677],\n",
      "        [-0.9177, -0.5099],\n",
      "        [-0.5634, -0.8422],\n",
      "        [-0.2272, -1.5933],\n",
      "        [-0.7167, -0.6702],\n",
      "        [-0.3473, -1.2261],\n",
      "        [-0.6707, -0.7161],\n",
      "        [-0.7415, -0.6470],\n",
      "        [-0.7211, -0.6660],\n",
      "        [-0.2984, -1.3549],\n",
      "        [-0.8033, -0.5939],\n",
      "        [-0.4153, -1.0793],\n",
      "        [-0.4847, -0.9568],\n",
      "        [-0.5589, -0.8482],\n",
      "        [-0.6933, -0.6930],\n",
      "        [-0.6971, -0.6892],\n",
      "        [-0.3138, -1.3117],\n",
      "        [-0.5117, -0.9150],\n",
      "        [-0.6388, -0.7506],\n",
      "        [-0.7205, -0.6665],\n",
      "        [-0.5634, -0.8423],\n",
      "        [-0.7132, -0.6735],\n",
      "        [-0.6726, -0.7141],\n",
      "        [-0.6632, -0.7240],\n",
      "        [-1.6526, -0.2126],\n",
      "        [-0.7497, -0.6397],\n",
      "        [-0.1594, -1.9152],\n",
      "        [-0.6499, -0.7384],\n",
      "        [-0.5894, -0.8089],\n",
      "        [-0.3873, -1.1359],\n",
      "        [-0.5569, -0.8509],\n",
      "        [-0.6208, -0.7711],\n",
      "        [-0.3376, -1.2500],\n",
      "        [-0.3041, -1.3385],\n",
      "        [-0.4974, -0.9367],\n",
      "        [-0.5624, -0.8437],\n",
      "        [-0.7764, -0.6163],\n",
      "        [-0.5511, -0.8588],\n",
      "        [-0.7474, -0.6416],\n",
      "        [-0.4879, -0.9517],\n",
      "        [-0.6569, -0.7307],\n",
      "        [-0.6658, -0.7213],\n",
      "        [-0.7404, -0.6480],\n",
      "        [-0.6614, -0.7260],\n",
      "        [-0.6868, -0.6995]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0, 1, 1])\n",
      "tensor([[-0.5757, -0.8263],\n",
      "        [-0.6791, -0.7074],\n",
      "        [-0.6622, -0.7251],\n",
      "        [-0.4976, -0.9364],\n",
      "        [-0.3494, -1.2211],\n",
      "        [-0.6773, -0.7093],\n",
      "        [-0.6816, -0.7049],\n",
      "        [-0.6968, -0.6895],\n",
      "        [-0.6169, -0.7757],\n",
      "        [-0.6755, -0.7111],\n",
      "        [-0.7382, -0.6501],\n",
      "        [-0.3032, -1.3411],\n",
      "        [-0.6544, -0.7335],\n",
      "        [-0.5607, -0.8459],\n",
      "        [-0.7334, -0.6545],\n",
      "        [-0.4446, -1.0247],\n",
      "        [-0.5405, -0.8734],\n",
      "        [-0.1381, -2.0477],\n",
      "        [-0.3628, -1.1898],\n",
      "        [-0.4036, -1.1024],\n",
      "        [-0.6879, -0.6984],\n",
      "        [-0.5910, -0.8069],\n",
      "        [-0.6887, -0.6976],\n",
      "        [-0.7515, -0.6380],\n",
      "        [-0.7446, -0.6442],\n",
      "        [-0.5820, -0.8182],\n",
      "        [-0.7526, -0.6370],\n",
      "        [-0.2396, -1.5461],\n",
      "        [-0.3388, -1.2470],\n",
      "        [-0.8556, -0.5534],\n",
      "        [-0.8469, -0.5599],\n",
      "        [-0.6616, -0.7257],\n",
      "        [-0.6006, -0.7951],\n",
      "        [-1.0947, -0.4074],\n",
      "        [-0.2612, -1.4701],\n",
      "        [-0.7338, -0.6541],\n",
      "        [-0.6308, -0.7596],\n",
      "        [-0.4905, -0.9475],\n",
      "        [-0.7583, -0.6320],\n",
      "        [-0.6954, -0.6909],\n",
      "        [-0.7841, -0.6097],\n",
      "        [-0.4339, -1.0441],\n",
      "        [-0.2741, -1.4282],\n",
      "        [-0.5312, -0.8866],\n",
      "        [-0.6800, -0.7065],\n",
      "        [-0.6138, -0.7794],\n",
      "        [-0.5391, -0.8754],\n",
      "        [-0.5800, -0.8208],\n",
      "        [-0.6341, -0.7559],\n",
      "        [-0.3678, -1.1784],\n",
      "        [-0.6301, -0.7604],\n",
      "        [-0.8081, -0.5901],\n",
      "        [-0.8159, -0.5838],\n",
      "        [-0.6828, -0.7036],\n",
      "        [-0.7661, -0.6251],\n",
      "        [-0.6853, -0.7010],\n",
      "        [-0.3432, -1.2362],\n",
      "        [-0.4527, -1.0104],\n",
      "        [-0.3027, -1.3424],\n",
      "        [-0.5871, -0.8118],\n",
      "        [-0.6051, -0.7897],\n",
      "        [-0.6960, -0.6903],\n",
      "        [-0.1486, -1.9801],\n",
      "        [-0.8066, -0.5913],\n",
      "        [-0.5460, -0.8657],\n",
      "        [-0.6397, -0.7496],\n",
      "        [-0.6225, -0.7692],\n",
      "        [-0.6298, -0.7608],\n",
      "        [-0.3946, -1.1207],\n",
      "        [-0.7419, -0.6467],\n",
      "        [-0.6567, -0.7310],\n",
      "        [-0.7443, -0.6445],\n",
      "        [-0.5810, -0.8195],\n",
      "        [-0.7043, -0.6821],\n",
      "        [-0.4426, -1.0282],\n",
      "        [-0.9751, -0.4735],\n",
      "        [-0.3982, -1.1134],\n",
      "        [-0.7012, -0.6852],\n",
      "        [-0.9420, -0.4940],\n",
      "        [-0.6225, -0.7691],\n",
      "        [-0.6596, -0.7279],\n",
      "        [-0.6646, -0.7225],\n",
      "        [-0.7043, -0.6821],\n",
      "        [-0.8237, -0.5777],\n",
      "        [-0.1358, -2.0638],\n",
      "        [-0.3859, -1.1390],\n",
      "        [-0.7228, -0.6643],\n",
      "        [-0.0983, -2.3683],\n",
      "        [-0.7855, -0.6086],\n",
      "        [-0.7105, -0.6761],\n",
      "        [-0.3691, -1.1757],\n",
      "        [-0.6023, -0.7930],\n",
      "        [-0.8669, -0.5452],\n",
      "        [-0.6633, -0.7239],\n",
      "        [-0.6237, -0.7678],\n",
      "        [-0.6788, -0.7077],\n",
      "        [-0.5243, -0.8964],\n",
      "        [-0.1336, -2.0787],\n",
      "        [-0.6111, -0.7825],\n",
      "        [-0.7206, -0.6665],\n",
      "        [-0.7629, -0.6279],\n",
      "        [-0.4655, -0.9884],\n",
      "        [-0.6273, -0.7636],\n",
      "        [-0.1030, -2.3237],\n",
      "        [-0.1963, -1.7248],\n",
      "        [-0.3452, -1.2314],\n",
      "        [-0.6458, -0.7428],\n",
      "        [-0.2979, -1.3562],\n",
      "        [-0.4257, -1.0592],\n",
      "        [-0.4677, -0.9846],\n",
      "        [-0.7091, -0.6775],\n",
      "        [-0.8205, -0.5802],\n",
      "        [-0.7429, -0.6457],\n",
      "        [-0.3793, -1.1532],\n",
      "        [-0.3656, -1.1836],\n",
      "        [-0.5648, -0.8404],\n",
      "        [-0.5500, -0.8602],\n",
      "        [-0.7333, -0.6545],\n",
      "        [-0.2050, -1.6854],\n",
      "        [-0.8363, -0.5680],\n",
      "        [-0.4061, -1.0974],\n",
      "        [-0.7171, -0.6697],\n",
      "        [-0.6711, -0.7156],\n",
      "        [-0.6303, -0.7603],\n",
      "        [-0.8002, -0.5965],\n",
      "        [-0.4433, -1.0270],\n",
      "        [-0.5995, -0.7965],\n",
      "        [-0.7467, -0.6423]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 1, 0, 0, 0, 1])\n",
      "tensor([[-0.7614, -0.6293],\n",
      "        [-0.4483, -1.0182],\n",
      "        [-0.5913, -0.8066],\n",
      "        [-0.3287, -1.2725],\n",
      "        [-0.2638, -1.4614],\n",
      "        [-0.6520, -0.7361],\n",
      "        [-0.6297, -0.7609],\n",
      "        [-0.6582, -0.7294],\n",
      "        [-0.3212, -1.2921],\n",
      "        [-0.3405, -1.2428],\n",
      "        [-0.7340, -0.6539],\n",
      "        [-0.6342, -0.7558],\n",
      "        [-0.4796, -0.9650],\n",
      "        [-0.3308, -1.2672],\n",
      "        [-0.7416, -0.6469],\n",
      "        [-0.6666, -0.7204],\n",
      "        [-0.6528, -0.7352],\n",
      "        [-0.4214, -1.0675],\n",
      "        [-0.4304, -1.0505],\n",
      "        [-1.0764, -0.4168],\n",
      "        [-0.5255, -0.8947],\n",
      "        [-0.5882, -0.8104],\n",
      "        [-0.8352, -0.5688],\n",
      "        [-0.8571, -0.5524],\n",
      "        [-0.3518, -1.2155],\n",
      "        [-0.4240, -1.0625],\n",
      "        [-0.5002, -0.9325],\n",
      "        [-0.6275, -0.7634],\n",
      "        [-0.7876, -0.6069],\n",
      "        [-0.7311, -0.6566],\n",
      "        [-0.6553, -0.7325],\n",
      "        [-0.6233, -0.7682],\n",
      "        [-0.5026, -0.9288],\n",
      "        [-0.5992, -0.7969],\n",
      "        [-0.6016, -0.7940],\n",
      "        [-0.5980, -0.7983],\n",
      "        [-0.6139, -0.7792],\n",
      "        [-0.8907, -0.5282],\n",
      "        [-0.2914, -1.3752],\n",
      "        [-0.7390, -0.6493],\n",
      "        [-0.4850, -0.9563],\n",
      "        [-0.7492, -0.6400],\n",
      "        [-0.5928, -0.8047],\n",
      "        [-0.5311, -0.8867],\n",
      "        [-0.6486, -0.7397],\n",
      "        [-0.8997, -0.5220],\n",
      "        [-0.1911, -1.7488],\n",
      "        [-1.0121, -0.4517],\n",
      "        [-0.7313, -0.6564],\n",
      "        [-0.6025, -0.7928],\n",
      "        [-0.7732, -0.6190],\n",
      "        [-1.1786, -0.3678],\n",
      "        [-0.6561, -0.7316],\n",
      "        [-0.6238, -0.7676],\n",
      "        [-0.5780, -0.8233],\n",
      "        [-0.2341, -1.5667],\n",
      "        [-0.9045, -0.5188],\n",
      "        [-0.3866, -1.1375],\n",
      "        [-0.7047, -0.6817],\n",
      "        [-0.3864, -1.1379],\n",
      "        [-0.2312, -1.5778],\n",
      "        [-0.7064, -0.6800],\n",
      "        [-0.5096, -0.9182],\n",
      "        [-0.4574, -1.0022]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
      "        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0])\n",
      "Epoch: 001, Loss: 0.7444, Train Acc: 0.6329, Test Acc: 0.6443\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(dim=32).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1, 2):\n",
    "    loss = train(epoch)\n",
    "    train_acc = test(train_loader)\n",
    "    exit()\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "          f'Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "play with actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_files_list = [\"./export/\"+f for f in os.listdir(\"./export\") ]\n",
    "instance_dict = {}\n",
    "for dir_str in data_files_list:\n",
    "    with open(dir_str, 'r') as text_file:\n",
    "        cnt = 0\n",
    "        instance = \"\"\n",
    "        for line in text_file:\n",
    "            if cnt < 9:\n",
    "                if cnt == 0:\n",
    "                    instance = line.split()[0]\n",
    "                    instance_dict[instance] = []\n",
    "                cnt += 1\n",
    "                continue\n",
    "            split_line = line.split()\n",
    "            instance_dict[instance].append([int(i) for i in split_line])\n",
    "        text_file.close()\n",
    "\n",
    "ng_dict = {}\n",
    "cnt = -1\n",
    "with open(\"ng_outs.csv\", 'r') as text_file:\n",
    "    for line in text_file:\n",
    "        if cnt < 2:\n",
    "            cnt += 1\n",
    "            continue\n",
    "        raw_line = line.strip()\n",
    "        split_line_list = raw_line.split(sep=\";\")\n",
    "        instance = split_line_list[3]\n",
    "        if instance not in ng_dict:\n",
    "            ng_dict[instance] = [[0 for i in range(101)]]\n",
    "        ng_dict[instance].append([0] + [int(i) for i in split_line_list[5:-1]])\n",
    "        if len(split_line_list[5:-1]) != 100:\n",
    "            print(\"case found for instance \"+instance)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print((ng_dict['J000000'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import torch\n",
    "import torch_geometric as tg\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GraphConv, global_add_pool\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_graph_list = []\n",
    "for i in range(101):\n",
    "    for j in range(101):\n",
    "        if i != j:\n",
    "            complete_graph_list.append([i,j])\n",
    "edge_index = torch.tensor(complete_graph_list, dtype=torch.long).t().contiguous()\n",
    "n_edges = len(complete_graph_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for instance_name in ng_dict:\n",
    "    for i in range(101):\n",
    "        for j in range(101):\n",
    "            if i == j:\n",
    "                ng_dict[instance_name][i][j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for instance_name in ng_dict:\n",
    "    y = torch.tensor(ng_dict[instance_name], dtype=torch.double)\n",
    "    x = torch.tensor(instance_dict[instance_name], dtype=torch.double)\n",
    "    attr = [[i] for i in range(n_edges)]\n",
    "    loc_dict = {(i[0],j[0]): sqrt((i[1]-j[1])**2 + (i[2]-j[2])**2) for i in instance_dict[instance_name] for j in instance_dict[instance_name]}\n",
    "    cnt = -1\n",
    "    for i in range(101):\n",
    "        for j in range(101):\n",
    "            if i != j:\n",
    "                cnt += 1\n",
    "                attr[cnt].append(loc_dict[i,j])\n",
    "    attr = torch.tensor(attr, dtype=torch.double)\n",
    "    pos = []\n",
    "    for i in instance_dict[instance_name]:\n",
    "        pos.append([i[1], i[2]])\n",
    "    pos = torch.tensor(pos, dtype=torch.double)\n",
    "    data_list.append(Data(x=x, y=y, edge_index=edge_index, pos=pos, edge_attr=attr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataloader = DataLoader(data_list[428:458], batch_size=1)\n",
    "data_iter = iter(dataloader)\n",
    "data_test = DataLoader(data_list[458:468], batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to produce edges as an adjacency matrix\n",
    "complete_adj_matrix_list = [[0 for i in range(101)] for i in range(101)]\n",
    "for edge in complete_graph_list:\n",
    "    i, j = edge\n",
    "    complete_adj_matrix_list[i][j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Instances:\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "    \n",
    "    def to_torch_geometric(self, start=0, end=-1, batch_size=1):\n",
    "        return DataLoader(data_list[start:end], batch_size=batch_size)\n",
    "    \n",
    "    def to_conv_nets(self, start=0, end=-1, batch_size=1):\n",
    "        final_data = []\n",
    "        nodes = []\n",
    "        nodes_coor = []\n",
    "        nodes_timew = []\n",
    "        x_edges = []\n",
    "        x_edges_values = []\n",
    "        y_edges = []\n",
    "        cnt = 0\n",
    "        current_batch = 0\n",
    "        for graph in self.data_list[start:end]:\n",
    "            if cnt >= batch_size:\n",
    "                cnt = 0\n",
    "                current_batch += 1\n",
    "                nodes = torch.tensor(nodes, dtype=torch.long)\n",
    "                nodes_coor = torch.tensor(nodes_coor, dtype=torch.float)\n",
    "                nodes_timew = torch.tensor(nodes_timew, dtype=torch.long)\n",
    "                x_edges = torch.tensor(x_edges, dtype=torch.long)\n",
    "                x_edges_values = torch.tensor(x_edges_values, dtype=torch.float)\n",
    "                y_edges = torch.tensor(y_edges, dtype=torch.long)\n",
    "                final_data.append((x_edges, x_edges_values, nodes, nodes_coor, nodes_timew, y_edges))\n",
    "                nodes = []\n",
    "                nodes_coor = []\n",
    "                nodes_timew = []\n",
    "                x_edges = []\n",
    "                x_edges_values = []\n",
    "                y_edges = []\n",
    "            nodes.append([i for i in range(101)]) \n",
    "            nodes_coor.append(graph.pos.tolist())\n",
    "            tw = []\n",
    "            x_raw = graph.x.tolist()\n",
    "            for i in range(101):\n",
    "                tw.append([x_raw[i][4], x_raw[i][5]])\n",
    "            nodes_timew.append(tw)\n",
    "            x_edges.append(complete_adj_matrix_list)\n",
    "            dist_matrix = [[0 for _ in range(101)] for _ in range(101)]\n",
    "            dist_list = [i for _, i in graph.edge_attr.tolist()]\n",
    "            pos_dist = 0\n",
    "            for i in range(101):\n",
    "                for j in range(101):\n",
    "                    if i != j:\n",
    "                        dist_matrix[i][j] = dist_list[pos_dist]\n",
    "                        pos_dist += 1\n",
    "            x_edges_values.append(dist_matrix)\n",
    "            y_edges.append(graph.y.tolist()) #TODO: remove the transpose and also the contiguous when generating y\n",
    "            cnt += 1\n",
    "        return final_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = Instances(data_list)\n",
    "dataloader = data_source.to_torch_geometric(start=428, end=458, batch_size=1)\n",
    "datatorch = data_source.to_conv_nets(start=428, end=458, batch_size=1)\n",
    "torchtest = data_source.to_conv_nets(start=458, end=468, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "source": [
    "_, _, _, _, _, y_edges = torchtest[0]\n",
    "print(len(y_edges[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        num_features = 7\n",
    "        dim = 101*101\n",
    "\n",
    "        self.conv1 = GraphConv(num_features, dim)\n",
    "        self.conv2 = GraphConv(dim, dim)\n",
    "\n",
    "        self.lin1 = Linear(dim, dim)\n",
    "\n",
    "        self.double()\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_weight=None):\n",
    "        # x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # x = self.conv1(x, edge_index)\n",
    "        # x = F.relu(x)\n",
    "        # x = F.sigmoid(self.conv2(x, edge_index))\n",
    "        # x = F.dropout(x, training=self.training)\n",
    "        # x = self.conv2(x, edge_index)\n",
    "\n",
    "        x = self.conv1(x, edge_index, edge_weight).relu()\n",
    "        x = self.conv2(x, edge_index, edge_weight).relu()\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # x = self.lin2(x)\n",
    "        x = torch.reshape(x, (101, 101))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    if epoch == 51:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.5 * param_group['lr']\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in dataloader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # print(data.x, data.edge_index, data.batch)\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        # print(output, data.y)\n",
    "        # print(output)\n",
    "        loss = F.l1_loss(output, data.y)\n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "        test_size = 10\n",
    "    return loss_all / test_size\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "        test_size = 10\n",
    "    return correct / test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 12316329.8885, Train Acc: 27914.9000, Test Acc: 9321.4000\n",
      "Epoch: 002, Loss: 15050.8765, Train Acc: 27375.4000, Test Acc: 9180.6000\n",
      "Epoch: 003, Loss: 6520.1387, Train Acc: 29867.4000, Test Acc: 9922.2000\n",
      "Epoch: 004, Loss: 3516.2888, Train Acc: 29887.2000, Test Acc: 9941.8000\n",
      "Epoch: 005, Loss: 3118.8949, Train Acc: 29746.9000, Test Acc: 9921.7000\n",
      "Epoch: 006, Loss: 2790.8632, Train Acc: 29253.6000, Test Acc: 9770.8000\n",
      "Epoch: 007, Loss: 6193.2795, Train Acc: 28825.8000, Test Acc: 9601.0000\n",
      "Epoch: 008, Loss: 10322.0821, Train Acc: 29437.1000, Test Acc: 9821.8000\n",
      "Epoch: 009, Loss: 8848.0151, Train Acc: 29436.2000, Test Acc: 9821.8000\n",
      "Epoch: 010, Loss: 3819.2164, Train Acc: 29756.7000, Test Acc: 9922.1000\n",
      "Epoch: 011, Loss: 2152.1094, Train Acc: 30037.2000, Test Acc: 10022.2000\n",
      "Epoch: 012, Loss: 4402.4668, Train Acc: 29436.8000, Test Acc: 9821.6000\n",
      "Epoch: 013, Loss: 4791.5913, Train Acc: 30337.9000, Test Acc: 10122.4000\n",
      "Epoch: 014, Loss: 5397.9118, Train Acc: 30037.9000, Test Acc: 10022.3000\n",
      "Epoch: 015, Loss: 1904.8505, Train Acc: 29847.7000, Test Acc: 10022.2000\n",
      "Epoch: 016, Loss: 2320.9467, Train Acc: 30337.9000, Test Acc: 10122.4000\n",
      "Epoch: 017, Loss: 1745.1811, Train Acc: 30097.4000, Test Acc: 10022.4000\n",
      "Epoch: 018, Loss: 1567.5528, Train Acc: 30037.4000, Test Acc: 10022.4000\n",
      "Epoch: 019, Loss: 4198.0330, Train Acc: 30037.8000, Test Acc: 10022.2000\n",
      "Epoch: 020, Loss: 3712.2741, Train Acc: 29436.9000, Test Acc: 9822.0000\n",
      "Epoch: 021, Loss: 1016.5476, Train Acc: 29797.5000, Test Acc: 9922.2000\n",
      "Epoch: 022, Loss: 3924.2838, Train Acc: 30037.5000, Test Acc: 10022.0000\n",
      "Epoch: 023, Loss: 3081.6943, Train Acc: 30037.4000, Test Acc: 10022.0000\n",
      "Epoch: 024, Loss: 1626.2530, Train Acc: 30327.9000, Test Acc: 10122.4000\n",
      "Epoch: 025, Loss: 3968.9558, Train Acc: 30318.0000, Test Acc: 10122.4000\n",
      "Epoch: 026, Loss: 690.5695, Train Acc: 30337.9000, Test Acc: 10122.4000\n",
      "Epoch: 027, Loss: 1707.2010, Train Acc: 29767.7000, Test Acc: 9922.2000\n",
      "Epoch: 028, Loss: 3747.1343, Train Acc: 30337.9000, Test Acc: 10122.4000\n",
      "Epoch: 029, Loss: 2478.9858, Train Acc: 30337.9000, Test Acc: 10122.4000\n",
      "Epoch: 030, Loss: 1096.4461, Train Acc: 30337.9000, Test Acc: 10122.4000\n",
      "Epoch: 031, Loss: 210.4835, Train Acc: 30337.9000, Test Acc: 10122.4000\n",
      "Epoch: 032, Loss: 558.1692, Train Acc: 30337.9000, Test Acc: 10122.4000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1, 33):\n",
    "    loss = train(epoch)\n",
    "    train_acc = test(dataloader)\n",
    "    test_acc = test(data_test)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "          f'Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): GraphConv(7, 10201)\n",
      "  (conv2): GraphConv(10201, 10201)\n",
      "  (conv3): GraphConv(10201, 10201)\n",
      "  (conv4): GraphConv(10201, 10201)\n",
      "  (conv5): GraphConv(10201, 10201)\n",
      "  (lin1): Linear(in_features=10201, out_features=10201, bias=True)\n",
      "  (lin2): Linear(in_features=10201, out_features=10201, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0] 1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] 1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "73.0\n",
      "65.0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "data_show = data_list[430]\n",
    "pred = model(data_show.x, data_show.edge_index, data_show.batch).tolist()\n",
    "y = data_show.y\n",
    "diff = 0\n",
    "for j in range(len(pred)):\n",
    "    a_list = [1 if i*100 > 1 else 0 for i in pred[j]]\n",
    "    diff += sum([(y[j][i].tolist() - a_list[i])**2 for i in range(len(a_list))])\n",
    "    print([1 if i*10 > 0.1 else 0 for i in pred[j]], sum(a_list))\n",
    "print(diff)\n",
    "print(sum([y[j][i].tolist() for i in range(101) for j in range(101)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_scatter import scatter\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormNode(nn.Module):\n",
    "    \"\"\"Batch normalization for node features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(BatchNormNode, self).__init__()\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim, track_running_stats=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features (batch_size, num_nodes, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            x_bn: Node features after batch normalization (batch_size, num_nodes, hidden_dim)\n",
    "        \"\"\"\n",
    "        # The batch norm normalizes the hidden dim over batch and node dimensions\n",
    "        if x.dim() == 2:\n",
    "            # If we have sparse version we have only one batch dimension\n",
    "            # simply perform batch norm over this (so this normalizes over batch and node dimension)\n",
    "            return self.batch_norm(x)\n",
    "        x_trans = x.transpose(1, 2).contiguous()  # Reshape input: (batch_size, hidden_dim, num_nodes)\n",
    "        x_trans_bn = self.batch_norm(x_trans)\n",
    "        x_bn = x_trans_bn.transpose(1, 2).contiguous()  # Reshape to original shape\n",
    "        # x_bn2 = self.batch_norm(x.view(-1, x.size(-1))).view_as(x)\n",
    "        # assert torch.allclose(x_bn, x_bn2, atol=1e-5)\n",
    "        return x_bn\n",
    "\n",
    "\n",
    "class BatchNormEdge(nn.Module):\n",
    "    \"\"\"Batch normalization for edge features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(BatchNormEdge, self).__init__()\n",
    "        self.batch_norm = nn.BatchNorm2d(hidden_dim, track_running_stats=False)\n",
    "\n",
    "    def forward(self, e):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            e: Edge features (batch_size, num_nodes, num_nodes, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            e_bn: Edge features after batch normalization (batch_size, num_nodes, num_nodes, hidden_dim)\n",
    "        \"\"\"\n",
    "        # The batch norm normalizes the hidden dim over batch and edge dimensions\n",
    "        if e.dim() == 2:\n",
    "            # If we have sparse version we have only one batch dimension\n",
    "            # simply perform batch norm over this (so this normalizes over batch and node dimension)\n",
    "            # We can use the BatchNorm2d module by inserting dummy dimensions\n",
    "            return self.batch_norm(e[:, :, None, None]).view_as(e)\n",
    "        e_trans = e.transpose(1, 3).contiguous()  # Reshape input: (batch_size, hidden_dim, num_nodes, num_nodes)\n",
    "        e_trans_bn = self.batch_norm(e_trans)\n",
    "        e_bn = e_trans_bn.transpose(1, 3).contiguous()  # Reshape to original\n",
    "        return e_bn\n",
    "\n",
    "class NodeFeatures(nn.Module):\n",
    "    \"\"\"Convnet features for nodes.\n",
    "    \n",
    "    Using `sum` aggregation:\n",
    "        x_i = U*x_i +  sum_j [ gate_ij * (V*x_j) ]\n",
    "    \n",
    "    Using `mean` aggregation:\n",
    "        x_i = U*x_i + ( sum_j [ gate_ij * (V*x_j) ] / sum_j [ gate_ij] )\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim, aggregation=\"mean\"):\n",
    "        super(NodeFeatures, self).__init__() # We must always sum, since mean means 'weighted mean' so sum weighted messages\n",
    "        self.aggregation = aggregation\n",
    "        self.U = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "        self.V = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "\n",
    "    def forward(self, x, edge_gate, edge_index=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features (batch_size, num_nodes, hidden_dim)\n",
    "            edge_gate: Edge gate values (batch_size, num_nodes, num_nodes, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            x_new: Convolved node features (batch_size, num_nodes, hidden_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        Ux = self.U(x)  # B x V x H\n",
    "        Vx = self.V(x)  # B x V x H\n",
    "\n",
    "        if edge_index is not None:\n",
    "            # Sparse version\n",
    "            return self.propagate(edge_index, Ux=Ux, Vx=Vx, edge_gate=edge_gate)\n",
    "\n",
    "        from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "        # The rest is a relatively cheap operation that uses a lot of memory\n",
    "        # No it does not use a lot of memory\n",
    "        use_checkpoint = False\n",
    "        if use_checkpoint:\n",
    "            x_new = checkpoint(self._inner, edge_gate, Ux, Vx)\n",
    "        else:\n",
    "\n",
    "            x_new = self._inner(edge_gate, Ux, Vx)\n",
    "            # print(\"Dense x\", edge_gate.size(), Ux.size(), Vx.size())\n",
    "            # print(x_new.flatten()[-10:])\n",
    "        return x_new\n",
    "\n",
    "    def _inner(self, edge_gate, Ux, Vx):\n",
    "        use_einsum = False\n",
    "        use_matmul = False\n",
    "\n",
    "        if use_einsum:  # Seems to use more memory\n",
    "            x_add = torch.einsum('bijd,bjd->bid', edge_gate, Vx)\n",
    "        elif use_matmul:  # Seems to use same memory as einsum, not much faster\n",
    "            x_add = torch.matmul(\n",
    "                edge_gate.unsqueeze(1).transpose(1, 4).squeeze(-1),\n",
    "                Vx.unsqueeze(1).transpose(1, 3)\n",
    "            ).transpose(1, 3).squeeze(1)\n",
    "        else:\n",
    "            Vx = Vx.unsqueeze(1)  # extend Vx from \"B x V x H\" to \"B x 1 x V x H\"\n",
    "            gateVx = edge_gate * Vx  # B x V x V x H\n",
    "            x_add = torch.sum(gateVx, dim=-2)\n",
    "        if self.aggregation==\"mean\":\n",
    "            x_new = Ux + x_add / (1e-20 + torch.sum(edge_gate, dim=-2))  # B x V x H\n",
    "        elif self.aggregation==\"sum\":\n",
    "            x_new = Ux + x_add  # B x V x H\n",
    "        return x_new\n",
    "\n",
    "    def message(self, edge_gate, Vx_j):\n",
    "        return edge_gate * Vx_j\n",
    "\n",
    "    def update(self, agg, Ux, edge_gate, edge_index):\n",
    "        src, tgt = edge_index\n",
    "        # Aggregate here exactly as in _inner. Normalizing here is more efficient than normalizing the messages.\n",
    "        if self.aggregation == \"mean\":\n",
    "            gate_sum = scatter(edge_gate, tgt, dim=0, dim_size=Ux.size(0), reduce='sum')\n",
    "            return Ux + agg / (1e-20 + gate_sum)\n",
    "        assert self.aggregation == \"sum\"\n",
    "        return Ux + agg  # Skip connection\n",
    "\n",
    "\n",
    "class EdgeFeatures(nn.Module):\n",
    "    \"\"\"Convnet features for edges.\n",
    "\n",
    "    e_ij = U*e_ij + V*(x_i + x_j)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, directed=False):\n",
    "        super(EdgeFeatures, self).__init__()\n",
    "        self.U = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "        self.V = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "        self.W = nn.Linear(hidden_dim, hidden_dim, True) if directed else None\n",
    "        \n",
    "    def forward(self, x, e, edge_index=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features (batch_size, num_nodes, hidden_dim)\n",
    "            e: Edge features (batch_size, num_nodes, num_nodes, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            e_new: Convolved edge features (batch_size, num_nodes, num_nodes, hidden_dim)\n",
    "        \"\"\"\n",
    "        Ue = self.U(e)\n",
    "        Vx = self.V(x)\n",
    "        Wx = Vx if self.W is None else self.W(x)  # If self.W is none, graph is undirected\n",
    "        if edge_index is not None:\n",
    "            # Sparse version\n",
    "            src, dst = edge_index\n",
    "            Wx = Wx[dst]  # = to\n",
    "            Vx = Vx[src]  # = from\n",
    "        else:\n",
    "            Wx = Wx.unsqueeze(1)  # Extend Wx from \"B x V x H\" to \"B x 1 x V x H\" = to\n",
    "            Vx = Vx.unsqueeze(2)  # extend Vx from \"B x V x H\" to \"B x V x 1 x H\" = from\n",
    "\n",
    "        e_new = Ue + Vx + Wx\n",
    "        return e_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualGatedGCNLayer(nn.Module):\n",
    "    \"\"\"Convnet layer with gating and residual connection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, aggregation=\"sum\", directed=False):\n",
    "        super(ResidualGatedGCNLayer, self).__init__()\n",
    "        self.node_feat = NodeFeatures(hidden_dim, aggregation)\n",
    "        self.edge_feat = EdgeFeatures(hidden_dim, directed)\n",
    "        self.bn_node = BatchNormNode(hidden_dim)\n",
    "        self.bn_edge = BatchNormEdge(hidden_dim)\n",
    "\n",
    "    def forward(self, x, e, edge_index=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features (batch_size, num_nodes, hidden_dim)\n",
    "            e: Edge features (batch_size, num_nodes, num_nodes, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            x_new: Convolved node features (batch_size, num_nodes, hidden_dim)\n",
    "            e_new: Convolved edge features (batch_size, num_nodes, num_nodes, hidden_dim)\n",
    "        \"\"\"\n",
    "        e_in = e\n",
    "        x_in = x\n",
    "        # Edge convolution\n",
    "        e_tmp = self.edge_feat(x_in, e_in, edge_index)  # B x V x V x H\n",
    "        # Compute edge gates\n",
    "        edge_gate = F.sigmoid(e_tmp)\n",
    "        # Node convolution\n",
    "        x_tmp = self.node_feat(x_in, edge_gate, edge_index)\n",
    "        # Batch normalization\n",
    "        e_tmp = self.bn_edge(e_tmp)\n",
    "        x_tmp = self.bn_node(x_tmp)\n",
    "        # ReLU Activation\n",
    "        e = F.relu(e_tmp)\n",
    "        x = F.relu(x_tmp)\n",
    "        # Residual connection\n",
    "        x_new = x_in + x\n",
    "        e_new = e_in + e\n",
    "        return x_new, e_new\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Multi-layer Perceptron for output prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, output_dim, L=2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.L = L\n",
    "        U = []\n",
    "        for layer in range(self.L - 1):\n",
    "            U.append(nn.Linear(hidden_dim, hidden_dim, True))\n",
    "        self.U = nn.ModuleList(U)\n",
    "        self.V = nn.Linear(hidden_dim, output_dim, True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input features (batch_size, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            y: Output predictions (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        Ux = x\n",
    "        for U_i in self.U:\n",
    "            Ux = U_i(Ux)  # B x H\n",
    "            Ux = F.relu(Ux)  # B x H\n",
    "        y = self.V(Ux)  # B x O\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualGatedGCNModelVRP(nn.Module):\n",
    "    \"\"\"Residual Gated GCN Model for outputting predictions as edge adjacency matrices.\n",
    "\n",
    "    References:\n",
    "        Paper: https://arxiv.org/pdf/1711.07553v2.pdf\n",
    "        Code: https://github.com/xbresson/spatial_graph_convnets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ResidualGatedGCNModelVRP, self).__init__()\n",
    "        self.dtypeFloat = torch.FloatTensor\n",
    "        self.dtypeLong = torch.LongTensor\n",
    "        # Define net parameters\n",
    "        # self.num_nodes = config.num_nodes\n",
    "        # self.node_dim = config.node_dim\n",
    "        # self.voc_nodes_in = config['voc_nodes_in']\n",
    "        # self.voc_nodes_out = config['num_nodes']  # config['voc_nodes_out']\n",
    "        # self.voc_edges_in = config['voc_edges_in']\n",
    "        # self.voc_edges_out = config['voc_edges_out']\n",
    "        # self.hidden_dim = config['hidden_dim']\n",
    "        # self.num_layers = config['num_layers']\n",
    "        # self.mlp_layers = config['mlp_layers']\n",
    "        # self.aggregation = config['aggregation']\n",
    "        # self.num_segments_checkpoint = config.get('num_segments_checkpoint', 0)\n",
    "        self.num_nodes = 100\n",
    "        self.node_dim = 2\n",
    "        self.voc_nodes_in = 200\n",
    "        self.voc_nodes_out = 2\n",
    "        self.voc_edges_in = 3\n",
    "        self.voc_edges_out = 2\n",
    "        self.hidden_dim = 6\n",
    "        self.num_layers = 30\n",
    "        self.mlp_layers = 3\n",
    "        self.aggregation = \"mean\"\n",
    "        self.num_segments_checkpoint = 5\n",
    "\n",
    "        # Node and edge embedding layers/lookups\n",
    "        self.nodes_coord_embedding = nn.Linear(self.node_dim, self.hidden_dim // 2, bias=False)\n",
    "        # self.nodes_coord_embedding2 = nn.Linear(self.node_dim, self.hidden_dim, bias=False)\n",
    "        self.edges_values_embedding = nn.Linear(1, self.hidden_dim // 2, bias=False)\n",
    "        # self.edges_values_embedding2 = nn.Linear(1, self.hidden_dim, bias=False)\n",
    "        self.edges_embedding = nn.Embedding(self.voc_edges_in, self.hidden_dim // 2)\n",
    "        # self.edges_embedding2 = nn.Embedding(self.voc_edges_in, self.hidden_dim)\n",
    "        self.nodes_embedding = nn.Embedding(self.voc_nodes_in, self.hidden_dim // 2)\n",
    "        # self.nodes_embedding2 = nn.Embedding(self.voc_nodes_in, self.hidden_dim)\n",
    "        # Define GCN Layers\n",
    "        gcn_layers = []\n",
    "        for layer in range(self.num_layers):\n",
    "            gcn_layers.append(ResidualGatedGCNLayer(self.hidden_dim, self.aggregation))\n",
    "        self.gcn_layers = nn.ModuleList(gcn_layers)\n",
    "        # Define MLP classifiers\n",
    "        self.mlp_edges = MLP(self.hidden_dim, self.voc_edges_out, self.mlp_layers)\n",
    "        # self.mlp_nodes = MLP(self.hidden_dim, self.voc_nodes_out, self.mlp_layers)\n",
    "\n",
    "    def forward(self, x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges=None, edge_cw=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_edges: Input edge adjacency matrix (batch_size, num_nodes, num_nodes)\n",
    "            x_edges_values: Input edge distance matrix (batch_size, num_nodes, num_nodes)\n",
    "            x_nodes: Input nodes (batch_size, num_nodes)\n",
    "            x_nodes_coord: Input node coordinates (batch_size, num_nodes, node_dim)\n",
    "            y_edges: Targets for edges (batch_size, num_nodes, num_nodes)\n",
    "            edge_cw: Class weights for edges loss\n",
    "            # y_nodes: Targets for nodes (batch_size, num_nodes, num_nodes)\n",
    "            # node_cw: Class weights for nodes loss\n",
    "\n",
    "        Returns:\n",
    "            y_pred_edges: Predictions for edges (batch_size, num_nodes, num_nodes)\n",
    "            # y_pred_nodes: Predictions for nodes (batch_size, num_nodes)\n",
    "            loss: Value of loss function\n",
    "        \"\"\"\n",
    "        # Node and edge embedding\n",
    "        ## Todo: fix this but gives bugs for now\n",
    "        x_vals = self.nodes_coord_embedding(x_nodes_coord)  # B x V x H\n",
    "        x_tags = self.nodes_embedding(x_nodes)\n",
    "        x = torch.cat((x_vals, x_tags), -1)\n",
    "        # x = self.nodes_embedding2(x_nodes)\n",
    "        e_vals = self.edges_values_embedding(x_edges_values.unsqueeze(3))  # B x V x V x H\n",
    "        e_tags = self.edges_embedding(x_edges)  # B x V x V x H\n",
    "        e = torch.cat((e_vals, e_tags), -1)\n",
    "        #e = self.edges_values_embedding2(x_edges_values.unsqueeze(3))\n",
    "        # GCN layers\n",
    "        if self.num_segments_checkpoint != 0:\n",
    "            layer_functions = [lambda args: layer(*args) for layer in self.gcn_layers]\n",
    "            x, e = torch.utils.checkpoint.checkpoint_sequential(layer_functions, self.num_segments_checkpoint, (x, e))\n",
    "        else:\n",
    "            for layer in range(self.num_layers):\n",
    "                # B x V x H, B x V x V x H\n",
    "                x, e = self.gcn_layers[layer](x, e)\n",
    "        # MLP classifier\n",
    "        y_pred_edges = self.mlp_edges(e)  # B x V x V x voc_edges_out\n",
    "        # y_pred_nodes = self.mlp_nodes(x)  # B x V x voc_nodes_out\n",
    "\n",
    "        #loss = loss_edges(y_pred_edges, y_edges, edge_cw)\n",
    "        # Edge loss\n",
    "        y = F.log_softmax(y_pred_edges, dim=3)  # B x V x V x voc_edges\n",
    "        # For some reason we must make things contiguous to prevent errors during backward\n",
    "        y_perm = y.permute(0, 3, 1, 2).contiguous()  # B x voc_edges x V x V\n",
    "        # y_perm = y.permute(0, 3, 1, 2).contiguous()\n",
    "        # if edge_cw == None:\n",
    "        #     edge_cw = [1 for i in range(101)]\n",
    "        if type(edge_cw) != torch.Tensor:\n",
    "            edge_labels = y_edges.cpu().numpy().flatten()\n",
    "            edge_cw = compute_class_weight(\"balanced\", classes=np.unique(edge_labels), y=edge_labels).tolist()\n",
    "        # print(edge_cw)\n",
    "        if y_edges is not None:\n",
    "            # Compute loss\n",
    "            # print(\"forwarding1.1\")\n",
    "            if edge_cw != None:\n",
    "                edge_cw = torch.Tensor(edge_cw)  # Convert to tensors\n",
    "                # print(\"forwarding1.1.1\")\n",
    "                edge_cw = edge_cw.type(self.dtypeFloat)\n",
    "            # print(\"forwarding1.2\")\n",
    "            loss = nn.NLLLoss(edge_cw)\n",
    "            # print(\"forwarding1.3\")\n",
    "            # print(y_perm)\n",
    "            # print(y_edges)\n",
    "            loss = loss(y_perm, y_edges)\n",
    "            # print(\"forwarding1.4\")\n",
    "        else:\n",
    "            # print(\"forwarding2.1\")\n",
    "            loss = None\n",
    "            # print(\"forwarding2.2\")\n",
    "        \n",
    "        # return y_pred_edges.permute(0, 3, 1, 2)[0]\n",
    "        return y_pred_edges, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    if epoch == 51:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.5 * param_group['lr']\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in datatorch:\n",
    "        # data = data.to(device)\n",
    "        x_edges, x_edges_values, x_nodes, x_nodes_coord, _,  y_edges = data\n",
    "        optimizer.zero_grad()\n",
    "        # print(data.x, data.edge_index, data.batch)\n",
    "        output, loss = model(x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges)\n",
    "        # print(output, data.y)\n",
    "        # print(output)\n",
    "        # loss = F.l1_loss(output, y_edges)\n",
    "        loss.backward()\n",
    "        loss_all += loss.item()\n",
    "        optimizer.step()\n",
    "        test_size = 30\n",
    "    return loss_all / test_size\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        # data = data.to(device)\n",
    "        x_edges, x_edges_values, x_nodes, x_nodes_coord, _,  y_edges = data\n",
    "        output, _ = model(x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges)\n",
    "        pred = torch.tensor([[(1 if output[0][i][j][1] > output[0][i][j][0] else 0) for j in range(101)] for i in range(101)], dtype=torch.double)\n",
    "        correct += pred.eq(y_edges).sum().item()\n",
    "        test_size = 30\n",
    "    return correct / test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.7323, Train Acc: 7865.8000, Test Acc: 2589.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Loss: 0.8166, Train Acc: 4848.8667, Test Acc: 1755.8333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Loss: 0.6549, Train Acc: 5570.5667, Test Acc: 1994.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Loss: 0.5753, Train Acc: 5918.0667, Test Acc: 2131.7667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Loss: 0.5313, Train Acc: 6600.3667, Test Acc: 2280.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Loss: 0.5039, Train Acc: 6781.1333, Test Acc: 2307.8667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Loss: 0.4806, Train Acc: 6833.9000, Test Acc: 2323.2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Loss: 0.4626, Train Acc: 6935.3333, Test Acc: 2351.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Loss: 0.4516, Train Acc: 6998.2000, Test Acc: 2365.9333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 0.4452, Train Acc: 7087.5333, Test Acc: 2376.9333\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = ResidualGatedGCNModelVRP().to(device)\n",
    "model = ResidualGatedGCNModelVRP().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    loss = train(epoch)\n",
    "    train_acc = test(datatorch)\n",
    "    test_acc = test(torchtest)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "          f'Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 5\n",
      "[0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0] 46\n",
      "[0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0] 25\n",
      "[0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0] 50\n",
      "[0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0] 33\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0] 19\n",
      "[0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0] 33\n",
      "[0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0] 42\n",
      "[1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0] 18\n",
      "[0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0] 41\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1] 15\n",
      "[0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0] 37\n",
      "[0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0] 36\n",
      "[0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0] 51\n",
      "[0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1] 43\n",
      "[0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0] 43\n",
      "[0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0] 34\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0] 23\n",
      "[0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1] 38\n",
      "[0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0] 39\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1] 25\n",
      "[0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0] 37\n",
      "[0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0] 52\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0] 35\n",
      "[0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0] 40\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1] 24\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1] 24\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0] 20\n",
      "[0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0] 45\n",
      "[0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1] 43\n",
      "[0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0] 40\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0] 17\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 13\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0] 19\n",
      "[0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0] 44\n",
      "[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1] 31\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0] 18\n",
      "[0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1] 28\n",
      "[0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1] 31\n",
      "[0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0] 35\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0] 17\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] 23\n",
      "[0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0] 36\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1] 22\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 22\n",
      "[0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0] 61\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] 13\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0] 13\n",
      "[0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0] 48\n",
      "[0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1] 31\n",
      "[0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1] 29\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1] 20\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1] 27\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] 19\n",
      "[1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0] 21\n",
      "[0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0] 36\n",
      "[0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0] 34\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1] 19\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0] 24\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0] 22\n",
      "[0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0] 39\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1] 18\n",
      "[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1] 34\n",
      "[0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0] 35\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1] 23\n",
      "[0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0] 41\n",
      "[0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0] 41\n",
      "[0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0] 31\n",
      "[0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0] 37\n",
      "[0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0] 35\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1] 22\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0] 19\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 16\n",
      "[1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0] 28\n",
      "[0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0] 41\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0] 23\n",
      "[0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0] 32\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1] 21\n",
      "[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1] 28\n",
      "[1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0] 29\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1] 27\n",
      "[0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0] 45\n",
      "[0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0] 28\n",
      "[0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0] 42\n",
      "[0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0] 35\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1] 22\n",
      "[0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0] 34\n",
      "[0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0] 45\n",
      "[0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0] 47\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0] 28\n",
      "[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0] 27\n",
      "[0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0] 44\n",
      "[0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1] 42\n",
      "[0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1] 41\n",
      "[0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0] 32\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1] 21\n",
      "[0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0] 40\n",
      "[0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0] 35\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0] 30\n",
      "[0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0] 37\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1] 29\n",
      "3073\n",
      "98\n",
      "[-3.5438714027404785, -1.692360758781433]\n",
      "[-0.841260552406311, -1.9211593866348267]\n",
      "[1.6713130474090576, -2.085721492767334]\n",
      "[0.41111892461776733, -2.1890928745269775]\n",
      "[0.8456291556358337, -2.218912124633789]\n",
      "[2.0084879398345947, -1.9557719230651855]\n",
      "[0.7562243342399597, -2.0557920932769775]\n",
      "[0.3398172855377197, -1.9733374118804932]\n",
      "[-2.621534585952759, -1.763419508934021]\n",
      "[0.8004229664802551, -2.149193048477173]\n",
      "[3.074394941329956, -2.8693618774414062]\n",
      "[0.8563351035118103, -2.192922592163086]\n",
      "[-0.4432384967803955, -1.741307020187378]\n",
      "[0.09775912761688232, -2.1212615966796875]\n",
      "[1.2954344749450684, -2.141472578048706]\n",
      "[0.5547384023666382, -2.1385371685028076]\n",
      "[-1.761308193206787, -1.783448576927185]\n",
      "[0.27926862239837646, -2.2070634365081787]\n",
      "[2.0780179500579834, -2.0153660774230957]\n",
      "[-0.5804957151412964, -1.8906617164611816]\n",
      "[2.925008773803711, -2.810853958129883]\n",
      "[-1.0206365585327148, -1.8999637365341187]\n",
      "[0.6517855525016785, -2.15826153755188]\n",
      "[-0.3818909525871277, -2.0357460975646973]\n",
      "[-0.7208558917045593, -2.0242972373962402]\n",
      "[3.2619681358337402, -3.1461524963378906]\n",
      "[1.8477745056152344, -1.8180243968963623]\n",
      "[-1.1603566408157349, -2.075570583343506]\n",
      "[-1.4075450897216797, -1.8132822513580322]\n",
      "[2.097325563430786, -2.0319149494171143]\n",
      "[-0.9785577058792114, -1.975494623184204]\n",
      "[1.7575950622558594, -2.152662754058838]\n",
      "[3.0131759643554688, -2.8168909549713135]\n",
      "[0.5298446416854858, -2.1767022609710693]\n",
      "[1.0338335037231445, -2.1894965171813965]\n",
      "[1.9173779487609863, -1.8776816129684448]\n",
      "[1.9125337600708008, -1.8735295534133911]\n",
      "[1.9752280712127686, -2.045487403869629]\n",
      "[1.985879898071289, -2.0327138900756836]\n",
      "[0.3520593047142029, -2.018129587173462]\n",
      "[-0.952606737613678, -2.0996761322021484]\n",
      "[1.3187599182128906, -2.084587812423706]\n",
      "[-1.175852656364441, -1.7459379434585571]\n",
      "[1.7989017963409424, -1.7761356830596924]\n",
      "[1.9939022064208984, -1.9978196620941162]\n",
      "[0.22164690494537354, -2.023925304412842]\n",
      "[2.4221177101135254, -2.3102943897247314]\n",
      "[1.1308202743530273, -2.186147689819336]\n",
      "[0.2164584994316101, -2.1472818851470947]\n",
      "[1.9502737522125244, -2.0817317962646484]\n",
      "[2.142735242843628, -2.070835590362549]\n",
      "[1.9224586486816406, -1.8820362091064453]\n",
      "[1.8040308952331543, -1.7805317640304565]\n",
      "[3.2248311042785645, -2.998905658721924]\n",
      "[-1.9798495769500732, -1.7556509971618652]\n",
      "[0.5971062183380127, -1.9735832214355469]\n",
      "[0.7968253493309021, -2.057797431945801]\n",
      "[2.069460868835449, -2.0080320835113525]\n",
      "[-0.9515932202339172, -2.060011148452759]\n",
      "[0.7117111086845398, -2.2006847858428955]\n",
      "[-0.7224010229110718, -2.042849063873291]\n",
      "[3.4765520095825195, -3.694445848464966]\n",
      "[2.0228044986724854, -1.9680428504943848]\n",
      "[-1.0267492532730103, -2.065760374069214]\n",
      "[3.0050222873687744, -2.823334217071533]\n",
      "[-0.5169826149940491, -2.0166780948638916]\n",
      "[0.9572479128837585, -2.128810167312622]\n",
      "[1.5684678554534912, -2.1472902297973633]\n",
      "[-1.1155414581298828, -1.9137721061706543]\n",
      "[0.38161784410476685, -1.9992308616638184]\n",
      "[3.3582849502563477, -3.2629234790802]\n",
      "[0.0036149322986602783, -2.1952779293060303]\n",
      "[2.186417818069458, -2.108275890350342]\n",
      "[-2.0124149322509766, -1.7956234216690063]\n",
      "[-0.5374224781990051, -2.0083184242248535]\n",
      "[-0.04638221859931946, -2.140921115875244]\n",
      "[1.2915127277374268, -2.1475303173065186]\n",
      "[2.0231244564056396, -1.9683170318603516]\n",
      "[1.9979450702667236, -1.9467356204986572]\n",
      "[-2.344243049621582, -1.6893142461776733]\n",
      "[1.749521017074585, -1.7338111400604248]\n",
      "[0.248543381690979, -2.1929571628570557]\n",
      "[1.7167019844055176, -2.059548854827881]\n",
      "[-0.9449130296707153, -1.9331254959106445]\n",
      "[0.7228196263313293, -2.08111834526062]\n",
      "[1.8356609344482422, -1.807641863822937]\n",
      "[1.5225176811218262, -2.164321184158325]\n",
      "[0.5594829320907593, -2.0489068031311035]\n",
      "[-0.45335209369659424, -1.9391474723815918]\n",
      "[0.9426910281181335, -2.1986281871795654]\n",
      "[2.202885150909424, -2.122390031814575]\n",
      "[-0.35267022252082825, -2.0063397884368896]\n",
      "[2.0301673412323, -2.043614149093628]\n",
      "[2.052403450012207, -1.9934120178222656]\n",
      "[-1.5178301334381104, -1.9214779138565063]\n",
      "[2.187248945236206, -2.1089882850646973]\n",
      "[-0.9542647004127502, -1.7227599620819092]\n",
      "[1.6226651668548584, -2.1799986362457275]\n",
      "[-1.3887200355529785, -1.9597866535186768]\n",
      "[-0.7759395837783813, -1.9665918350219727]\n",
      "[1.818389654159546, -1.7928386926651]\n",
      "[-0.8412610292434692, -1.9211595058441162]\n",
      "[-2.6309025287628174, -1.1848610639572144]\n",
      "[-0.5698233842849731, -1.0916355848312378]\n",
      "[-1.9205539226531982, -1.042426586151123]\n",
      "[-1.0905671119689941, -1.2752548456192017]\n",
      "[0.7456285357475281, -0.8733741044998169]\n",
      "[-1.534010887145996, -0.9275083541870117]\n",
      "[-1.9650356769561768, -0.8147201538085938]\n",
      "[-1.6975159645080566, -1.55207097530365]\n",
      "[-1.3719701766967773, -1.0889549255371094]\n",
      "[1.943434476852417, -1.958590030670166]\n",
      "[-1.409320592880249, -1.0528371334075928]\n",
      "[-2.1706702709198, -0.9290279150009155]\n",
      "[-1.9355731010437012, -1.0677119493484497]\n",
      "[-0.8908054828643799, -1.1354764699935913]\n",
      "[-1.2766598463058472, -1.1840529441833496]\n",
      "[-2.4590110778808594, -1.0893104076385498]\n",
      "[-0.990105390548706, -1.4224185943603516]\n",
      "[0.039057403802871704, -0.9779620170593262]\n",
      "[-2.3505871295928955, -1.0290398597717285]\n",
      "[1.09425687789917, -1.387766718864441]\n",
      "[-2.461840867996216, -1.0908832550048828]\n",
      "[-1.5081605911254883, -1.0905874967575073]\n",
      "[-1.5552916526794434, -1.228204369544983]\n",
      "[-2.479583501815796, -1.1007461547851562]\n",
      "[1.5428831577301025, -1.8915038108825684]\n",
      "[0.33192455768585205, -0.5979886054992676]\n",
      "[-1.4881012439727783, -1.5040172338485718]\n",
      "[-2.5881340503692627, -1.161086916923523]\n",
      "[-0.08755508065223694, -0.9556132555007935]\n",
      "[-2.5701212882995605, -1.1510740518569946]\n",
      "[0.23856431245803833, -1.308194875717163]\n",
      "[1.6285545825958252, -1.7315707206726074]\n",
      "[-0.5466822385787964, -1.453844428062439]\n",
      "[-1.2777504920959473, -1.0662331581115723]\n",
      "[0.28943580389022827, -0.6699801683425903]\n",
      "[0.5424341559410095, -0.7187693119049072]\n",
      "[-0.18118947744369507, -1.0943210124969482]\n",
      "[-0.2250625193119049, -1.067809820175171]\n",
      "[-1.9240319728851318, -0.8873951435089111]\n",
      "[-1.1916916370391846, -1.556963324546814]\n",
      "[-0.8933852314949036, -0.995236873626709]\n",
      "[-2.3786120414733887, -1.0446181297302246]\n",
      "[0.6712539792060852, -0.8096276521682739]\n",
      "[-0.2037338763475418, -0.9700020551681519]\n",
      "[-2.067272186279297, -0.8715511560440063]\n",
      "[0.8438470959663391, -1.0956040620803833]\n",
      "[-0.5448901057243347, -1.2493138313293457]\n",
      "[-2.0944676399230957, -0.9717268943786621]\n",
      "[-0.1501908004283905, -1.12697434425354]\n",
      "[0.4270898103713989, -0.9496208429336548]\n",
      "[0.74542635679245, -0.8732008934020996]\n",
      "[0.427034854888916, -0.606682538986206]\n",
      "[1.5357739925384521, -1.8012362718582153]\n",
      "[-1.6517341136932373, -1.3577417135238647]\n",
      "[-1.8658583164215088, -0.7946219444274902]\n",
      "[-1.64060640335083, -0.8873974084854126]\n",
      "[0.7099440693855286, -0.8427890539169312]\n",
      "[-1.510026216506958, -1.4304993152618408]\n",
      "[-0.596781313419342, -1.4259889125823975]\n",
      "[-2.6007463932037354, -1.168097972869873]\n",
      "[1.5258276462554932, -2.3797104358673096]\n",
      "[0.8120619654655457, -1.011052131652832]\n",
      "[-2.5967235565185547, -1.16586172580719]\n",
      "[1.282606840133667, -1.5701504945755005]\n",
      "[-2.4408695697784424, -1.0792258977890015]\n",
      "[-1.3139835596084595, -0.9762865304946899]\n",
      "[-0.5402276515960693, -1.1883705854415894]\n",
      "[-2.3870677947998047, -1.0493186712265015]\n",
      "[-1.9412741661071777, -0.8588559627532959]\n",
      "[1.6194534301757812, -1.9740760326385498]\n",
      "[-1.0319880247116089, -1.4700409173965454]\n",
      "[0.35818392038345337, -1.1496772766113281]\n",
      "[-2.252664566040039, -1.2260934114456177]\n",
      "[-2.4981398582458496, -1.1110610961914062]\n",
      "[-1.0892295837402344, -1.4021679162979126]\n",
      "[-0.9485204219818115, -1.1081098318099976]\n",
      "[0.640468418598175, -0.7832412719726562]\n",
      "[0.2654632329940796, -0.7170044183731079]\n",
      "[-2.0338218212127686, -1.2855885028839111]\n",
      "[0.6839125752449036, -0.8204773664474487]\n",
      "[-1.7588987350463867, -1.1728641986846924]\n",
      "[-0.5240360498428345, -1.0701261758804321]\n",
      "[-2.5842154026031494, -1.158908724784851]\n",
      "[-1.5862276554107666, -0.9221149682998657]\n",
      "[0.998816192150116, -1.0903815031051636]\n",
      "[-0.48844051361083984, -1.2284109592437744]\n",
      "[-1.7502434253692627, -0.8764584064483643]\n",
      "[-2.4077677726745605, -1.060825228691101]\n",
      "[-0.7312371730804443, -1.3320986032485962]\n",
      "[0.7162325978279114, -0.8481788635253906]\n",
      "[-2.4120805263519287, -1.0632226467132568]\n",
      "[-0.28397229313850403, -1.0531549453735352]\n",
      "[-0.24012604355812073, -0.8752436637878418]\n",
      "[-2.5334970951080322, -1.1307154893875122]\n",
      "[1.4391474723815918, -1.4677897691726685]\n",
      "[-2.3967692852020264, -1.0547114610671997]\n",
      "[-0.37620824575424194, -1.2316844463348389]\n",
      "[-2.2311882972717285, -1.2299755811691284]\n",
      "[-2.4561007022857666, -1.0876924991607666]\n",
      "[0.5826878547668457, -0.7337175607681274]\n",
      "[1.6713130474090576, -2.085721492767334]\n",
      "[-0.5698235034942627, -1.0916355848312378]\n",
      "[-1.4169354438781738, -0.4251255691051483]\n",
      "[-1.0983924865722656, -0.9012510776519775]\n",
      "[-1.1922547817230225, -0.741855263710022]\n",
      "[0.030380845069885254, -0.7349779605865479]\n",
      "[0.26257938146591187, -1.0570261478424072]\n",
      "[-0.3818276524543762, -0.8707044124603271]\n",
      "[0.8688066601753235, -1.8021515607833862]\n",
      "[-1.3746225833892822, -0.685517430305481]\n",
      "[0.2102411389350891, -1.421151041984558]\n",
      "[-0.8461567759513855, -0.897114634513855]\n",
      "[0.8445698618888855, -1.2580342292785645]\n",
      "[-1.2309638261795044, -0.8320945501327515]\n",
      "[-1.635477066040039, -0.5690091848373413]\n",
      "[-1.404252290725708, -0.7149666547775269]\n",
      "[0.14010906219482422, -1.4604169130325317]\n",
      "[-0.12515974044799805, -0.9940260648727417]\n",
      "[-1.2369859218597412, -0.30626702308654785]\n",
      "[-0.032101988792419434, -1.189290165901184]\n",
      "[-0.5336258411407471, -1.1454468965530396]\n",
      "[-0.05145224928855896, -1.2804193496704102]\n",
      "[-1.5197439193725586, -0.7113871574401855]\n",
      "[-0.28026896715164185, -1.008218765258789]\n",
      "[-0.49125075340270996, -1.120184302330017]\n",
      "[-0.35148411989212036, -1.489855170249939]\n",
      "[-0.35027819871902466, -0.5024358034133911]\n",
      "[0.3572748899459839, -1.4323532581329346]\n",
      "[-0.3488938510417938, -1.2227352857589722]\n",
      "[-1.2717838287353516, -0.3315674662590027]\n",
      "[-0.4201571047306061, -1.1991795301437378]\n",
      "[0.22391372919082642, -0.8339383602142334]\n",
      "[0.29011160135269165, -1.431857705116272]\n",
      "[0.23835045099258423, -1.0133074522018433]\n",
      "[-1.201966404914856, -0.744192361831665]\n",
      "[-0.3943895101547241, -0.41664546728134155]\n",
      "[0.03155401349067688, -0.7038450241088867]\n",
      "[-1.3238694667816162, -0.35142284631729126]\n",
      "[-1.4150128364562988, -0.36498093605041504]\n",
      "[0.349859356880188, -1.135424256324768]\n",
      "[0.5282023549079895, -1.4131327867507935]\n",
      "[0.6511455178260803, -1.1150652170181274]\n",
      "[0.441650927066803, -1.364508032798767]\n",
      "[-0.2054135799407959, -0.6990795135498047]\n",
      "[0.18533313274383545, -0.7588684558868408]\n",
      "[-1.5040440559387207, -0.683975338935852]\n",
      "[1.3077518939971924, -1.46587336063385]\n",
      "[1.2332816123962402, -1.4871628284454346]\n",
      "[-0.8338155150413513, -0.9817602634429932]\n",
      "[-1.3220577239990234, -0.41434845328330994]\n",
      "[-0.5800092220306396, -0.35926300287246704]\n",
      "[0.07277557253837585, -0.6165310144424438]\n",
      "[-0.36457112431526184, -0.4903753697872162]\n",
      "[-0.04766958951950073, -1.4807114601135254]\n",
      "[0.8959901928901672, -1.723509669303894]\n",
      "[-0.0018130838871002197, -0.9361382722854614]\n",
      "[-0.07939574122428894, -0.9305473566055298]\n",
      "[0.1358891725540161, -0.5104422569274902]\n",
      "[0.32494646310806274, -1.3434348106384277]\n",
      "[0.041719019412994385, -0.931830883026123]\n",
      "[-0.4508298635482788, -1.184369683265686]\n",
      "[-0.37710827589035034, -2.002215623855591]\n",
      "[-0.7836315035820007, -0.5337674617767334]\n",
      "[-0.2963271737098694, -1.2992218732833862]\n",
      "[-0.4988963007926941, -1.2050801515579224]\n",
      "[-0.4593047499656677, -1.1322722434997559]\n",
      "[-0.7414669990539551, -0.8110231161117554]\n",
      "[-1.3762285709381104, -0.54646897315979]\n",
      "[0.05589500069618225, -1.3869221210479736]\n",
      "[0.4003397226333618, -1.129820466041565]\n",
      "[-0.2262064814567566, -1.5987342596054077]\n",
      "[0.09716737270355225, -1.114906668663025]\n",
      "[1.035963773727417, -1.1692441701889038]\n",
      "[0.3586772084236145, -1.5638059377670288]\n",
      "[-0.5270562767982483, -1.1000339984893799]\n",
      "[0.021384447813034058, -1.0635498762130737]\n",
      "[-1.3535003662109375, -0.6011821031570435]\n",
      "[-0.024831146001815796, -0.491750031709671]\n",
      "[-0.3092416822910309, -0.39565205574035645]\n",
      "[0.4254268407821655, -1.5586590766906738]\n",
      "[-0.23573020100593567, -0.6462761163711548]\n",
      "[-1.131084680557251, -0.8570321798324585]\n",
      "[-1.427964210510254, -0.4190760850906372]\n",
      "[-0.3305743634700775, -1.2044134140014648]\n",
      "[-0.5370546579360962, -0.8618866205215454]\n",
      "[0.0001475811004638672, -0.7998778820037842]\n",
      "[-1.299325704574585, -0.5724289417266846]\n",
      "[-0.8001290559768677, -0.8045456409454346]\n",
      "[-0.784756064414978, -0.9387162923812866]\n",
      "[-0.7536290287971497, -0.7684624195098877]\n",
      "[-0.10555911064147949, -0.7265598773956299]\n",
      "[-0.7033867835998535, -0.9836817979812622]\n",
      "[-1.4375452995300293, -0.37998947501182556]\n",
      "[-1.04458749294281, -0.2805391848087311]\n",
      "[0.04526925086975098, -1.4450559616088867]\n",
      "[-0.11690264940261841, -0.930146336555481]\n",
      "[0.3816719055175781, -1.2421538829803467]\n",
      "[-1.3858366012573242, -0.5571914911270142]\n",
      "[0.13167405128479004, -1.3976224660873413]\n",
      "[-0.37210312485694885, -1.1398143768310547]\n",
      "[-0.40620771050453186, -0.5182327032089233]\n",
      "[0.41111892461776733, -2.1890928745269775]\n",
      "[-1.9205536842346191, -1.042426586151123]\n",
      "[-1.0983927249908447, -0.901250958442688]\n",
      "[-2.1310830116271973, -0.9070221185684204]\n",
      "[-1.3333375453948975, -1.0366590023040771]\n",
      "[0.20394623279571533, -0.7181469202041626]\n",
      "[-0.8413691520690918, -1.003906011581421]\n",
      "[-1.8678231239318848, -0.7696973085403442]\n",
      "[-0.4239332973957062, -1.8565510511398315]\n",
      "[-1.6930105686187744, -0.870163083076477]\n",
      "[1.2229740619659424, -1.6112613677978516]\n",
      "[-1.555199384689331, -0.9228081703186035]\n",
      "[-0.8207421898841858, -1.0683465003967285]\n",
      "[-1.9583368301391602, -0.9568576812744141]\n",
      "[-1.5331764221191406, -0.9317575693130493]\n",
      "[-1.466850757598877, -0.9892410039901733]\n",
      "[-1.1945955753326416, -1.412905216217041]\n",
      "[-0.8017363548278809, -1.2718992233276367]\n",
      "[-1.2461421489715576, -0.622989296913147]\n",
      "[-1.3708291053771973, -1.1105239391326904]\n",
      "[0.3317999243736267, -1.2662789821624756]\n",
      "[-1.3908040523529053, -1.2442357540130615]\n",
      "[-1.6737399101257324, -0.9198741912841797]\n",
      "[-1.1728925704956055, -1.2090991735458374]\n",
      "[-1.7679071426391602, -1.1524237394332886]\n",
      "[0.7110632061958313, -1.670576810836792]\n",
      "[-0.36719241738319397, -0.4196520149707794]\n",
      "[-0.7250235080718994, -1.5960283279418945]\n",
      "[-1.6869370937347412, -1.17523992061615]\n",
      "[-1.3352173566818237, -0.6221427917480469]\n",
      "[-1.7283835411071777, -1.1937367916107178]\n",
      "[0.06693992018699646, -1.0728545188903809]\n",
      "[1.0995807647705078, -1.5015177726745605]\n",
      "[-0.3565507233142853, -1.2842499017715454]\n",
      "[-1.466594934463501, -0.8893798589706421]\n",
      "[-0.601238489151001, -0.40841469168663025]\n",
      "[-0.20177561044692993, -0.5131255388259888]\n",
      "[-1.1242599487304688, -0.7633202075958252]\n",
      "[-1.2065067291259766, -0.7660518884658813]\n",
      "[-1.0432238578796387, -1.039961576461792]\n",
      "[-0.4975062608718872, -1.6086690425872803]\n",
      "[-0.19673052430152893, -1.079595923423767]\n",
      "[-1.007361650466919, -1.2635068893432617]\n",
      "[0.15794450044631958, -0.6716187000274658]\n",
      "[-0.7459276914596558, -0.7969970703125]\n",
      "[-2.1899960041046143, -0.9397705793380737]\n",
      "[1.2194406986236572, -1.2794790267944336]\n",
      "[0.5641411542892456, -1.4701541662216187]\n",
      "[-2.1402831077575684, -0.9121363162994385]\n",
      "[-1.2158854007720947, -0.7830972671508789]\n",
      "[-0.5120211839675903, -0.6319525241851807]\n",
      "[0.4476807713508606, -0.6180028915405273]\n",
      "[-0.2927247881889343, -0.442570298910141]\n",
      "[0.8419191241264343, -1.5904431343078613]\n",
      "[-0.3924294710159302, -1.7035698890686035]\n",
      "[-1.301608681678772, -0.8397473096847534]\n",
      "[-1.229783535003662, -0.9014877080917358]\n",
      "[0.372408926486969, -0.5534873008728027]\n",
      "[-0.7665879726409912, -1.5141441822052002]\n",
      "[-0.49916815757751465, -1.2186222076416016]\n",
      "[-1.7863597869873047, -1.1731162071228027]\n",
      "[0.659641683101654, -2.1837477684020996]\n",
      "[0.1940355896949768, -0.6922348737716675]\n",
      "[-1.600193738937378, -1.3331457376480103]\n",
      "[0.4926493763923645, -1.3598634004592896]\n",
      "[-1.8026857376098633, -1.083700180053711]\n",
      "[-1.5531210899353027, -0.8372161388397217]\n",
      "[-1.3154804706573486, -0.896109938621521]\n",
      "[-1.2735278606414795, -1.3268909454345703]\n",
      "[-1.1509016752243042, -0.9907007217407227]\n",
      "[0.7988926768302917, -1.7653639316558838]\n",
      "[-0.6879470348358154, -1.3657701015472412]\n",
      "[0.28857511281967163, -1.0446820259094238]\n",
      "[-0.9565774202346802, -1.5629867315292358]\n",
      "[-1.8818085193634033, -1.0562679767608643]\n",
      "[-0.7607566118240356, -1.3141587972640991]\n",
      "[-1.300933599472046, -0.9029015302658081]\n",
      "[0.2948709726333618, -0.5026419162750244]\n",
      "[-0.6095914840698242, -0.4513624608516693]\n",
      "[-0.838280439376831, -1.5790119171142578]\n",
      "[0.23588788509368896, -0.5701017379760742]\n",
      "[-1.8360514640808105, -1.0226733684539795]\n",
      "[-1.0993695259094238, -0.8757684230804443]\n",
      "[-1.6758134365081787, -1.1640362739562988]\n",
      "[-1.5448672771453857, -0.8601894378662109]\n",
      "[0.6842545866966248, -0.8609262704849243]\n",
      "[-1.3029543161392212, -0.8993161916732788]\n",
      "[-1.852304458618164, -0.7812674045562744]\n",
      "[-2.1138012409210205, -0.8974156379699707]\n",
      "[-0.8534350395202637, -1.078378677368164]\n",
      "[0.566813588142395, -0.7678244113922119]\n",
      "[-2.059893846511841, -0.9455292224884033]\n",
      "[-1.323296308517456, -0.7466398477554321]\n",
      "[-1.240992784500122, -0.5441920757293701]\n",
      "[-1.2750532627105713, -1.4413585662841797]\n",
      "[0.7995923161506653, -1.1095308065414429]\n",
      "[-1.137331247329712, -1.1118050813674927]\n",
      "[-1.3166162967681885, -0.8903547525405884]\n",
      "[-1.1338520050048828, -1.465367078781128]\n",
      "[-1.6609199047088623, -1.1487230062484741]\n",
      "[0.02769094705581665, -0.49457719922065735]\n",
      "[0.8456291556358337, -2.218912124633789]\n",
      "[-1.0905674695968628, -1.275254726409912]\n",
      "[-1.1922547817230225, -0.741855263710022]\n",
      "[-1.3333377838134766, -1.0366590023040771]\n",
      "[-2.0016143321990967, -0.8072125911712646]\n",
      "[0.9798608422279358, -1.0741348266601562]\n",
      "[0.40261757373809814, -1.395882248878479]\n",
      "[-0.6123595237731934, -1.133957028388977]\n",
      "[0.05700215697288513, -1.9295144081115723]\n",
      "[-1.8114683628082275, -0.7805198431015015]\n",
      "[0.9620537161827087, -1.387939453125]\n",
      "[-0.7626656889915466, -1.1751452684402466]\n",
      "[1.099531650543213, -1.6114420890808105]\n",
      "[-1.9054834842681885, -0.8826642036437988]\n",
      "[-2.0505788326263428, -0.6568269729614258]\n",
      "[-2.1781153678894043, -0.8355915546417236]\n",
      "[-0.48875391483306885, -1.6195040941238403]\n",
      "[-1.7076363563537598, -0.892164945602417]\n",
      "[-1.3776264190673828, -0.44241759181022644]\n",
      "[-0.260081946849823, -1.4704850912094116]\n",
      "[0.7947779297828674, -1.4907552003860474]\n",
      "[-0.5739040374755859, -1.468127727508545]\n",
      "[-2.087761402130127, -0.7628408670425415]\n",
      "[-1.7527439594268799, -0.9353417158126831]\n",
      "[-1.2987604141235352, -1.2195947170257568]\n",
      "[0.7450453639030457, -1.6297320127487183]\n",
      "[0.24532461166381836, -0.4759388864040375]\n",
      "[-1.0166113376617432, -1.389405608177185]\n",
      "[-1.0185692310333252, -1.3638794422149658]\n",
      "[-1.2650961875915527, -0.4899084270000458]\n",
      "[-1.1676959991455078, -1.3192987442016602]\n",
      "[-1.0533281564712524, -0.687423825263977]\n",
      "[1.476792335510254, -1.740685224533081]\n",
      "[-1.3468303680419922, -0.890105128288269]\n",
      "[-1.1105413436889648, -1.0140056610107422]\n",
      "[0.08621194958686829, -0.46517816185951233]\n",
      "[0.62117999792099, -0.7667092084884644]\n",
      "[-1.5011827945709229, -0.5188069343566895]\n",
      "[-1.5344643592834473, -0.5426712036132812]\n",
      "[0.20157229900360107, -1.4498612880706787]\n",
      "[-0.9180876612663269, -1.3511683940887451]\n",
      "[1.314554214477539, -1.4801167249679565]\n",
      "[-0.05921241641044617, -1.569087028503418]\n",
      "[0.6762024760246277, -0.8214820623397827]\n",
      "[0.23277312517166138, -1.0194754600524902]\n",
      "[-1.7802319526672363, -0.8248610496520996]\n",
      "[1.8865478038787842, -1.851257085800171]\n",
      "[1.9582862854003906, -1.912744164466858]\n",
      "[-0.9046885967254639, -1.213761568069458]\n",
      "[-1.7501189708709717, -0.5047332048416138]\n",
      "[-1.135169506072998, -0.4107438623905182]\n",
      "[0.22403442859649658, -0.4650033414363861]\n",
      "[0.2065562605857849, -0.47551724314689636]\n",
      "[1.2288930416107178, -1.7226265668869019]\n",
      "[0.39493393898010254, -1.9277033805847168]\n",
      "[-0.03222295641899109, -1.2419359683990479]\n",
      "[0.0020523369312286377, -1.2844839096069336]\n",
      "[7.25090503692627e-05, -0.41817331314086914]\n",
      "[-1.007161259651184, -1.3133409023284912]\n",
      "[-1.5343210697174072, -0.8127362728118896]\n",
      "[-1.05841064453125, -1.3418914079666138]\n",
      "[0.7125470042228699, -2.1805381774902344]\n",
      "[0.3297317624092102, -0.6767240762710571]\n",
      "[-1.094578742980957, -1.4011973142623901]\n",
      "[0.7525443434715271, -1.438964605331421]\n",
      "[-0.757554292678833, -1.3799800872802734]\n",
      "[-0.5503522753715515, -1.1636492013931274]\n",
      "[-1.9074249267578125, -0.6015257835388184]\n",
      "[-0.32591092586517334, -1.617257833480835]\n",
      "[0.09358912706375122, -1.3913724422454834]\n",
      "[0.9411582350730896, -1.7799127101898193]\n",
      "[-1.4597008228302002, -1.0196096897125244]\n",
      "[1.4425058364868164, -1.47164785861969]\n",
      "[-0.39414215087890625, -1.6953907012939453]\n",
      "[-0.8999199867248535, -1.3233274221420288]\n",
      "[-1.5346989631652832, -0.9680308103561401]\n",
      "[-1.328449010848999, -0.841161847114563]\n",
      "[-0.11593510210514069, -0.3832811713218689]\n",
      "[0.15233314037322998, -0.5132356882095337]\n",
      "[-0.5621880292892456, -1.623738408088684]\n",
      "[0.3739131689071655, -0.5572715997695923]\n",
      "[-1.9379558563232422, -0.8752346038818359]\n",
      "[-1.1992669105529785, -0.706484317779541]\n",
      "[-0.8970086574554443, -1.376408576965332]\n",
      "[-0.31649845838546753, -1.2215651273727417]\n",
      "[0.5166677236557007, -0.6771316528320312]\n",
      "[-1.9936718940734863, -0.5900920629501343]\n",
      "[-0.6124579310417175, -1.1427353620529175]\n",
      "[-1.0923324823379517, -1.1844477653503418]\n",
      "[-1.9041156768798828, -0.7487426996231079]\n",
      "[1.104985237121582, -1.1998708248138428]\n",
      "[-1.0954641103744507, -1.1998234987258911]\n",
      "[-1.644622802734375, -0.527517557144165]\n",
      "[-0.5516533851623535, -0.5407662391662598]\n",
      "[-0.6948310136795044, -1.5755829811096191]\n",
      "[0.4991610646247864, -0.8716393709182739]\n",
      "[0.36779069900512695, -1.574936032295227]\n",
      "[-2.072352886199951, -0.5716221332550049]\n",
      "[-0.8253015279769897, -1.4678212404251099]\n",
      "[-1.1469206809997559, -1.2529726028442383]\n",
      "[0.04671579599380493, -0.43787840008735657]\n",
      "[2.0084872245788574, -1.9557714462280273]\n",
      "[0.7456281781196594, -0.8733738660812378]\n",
      "[0.03038078546524048, -0.7349779605865479]\n",
      "[0.20394641160964966, -0.7181469202041626]\n",
      "[0.9798606038093567, -1.0741347074508667]\n",
      "[-1.7210040092468262, -0.32151901721954346]\n",
      "[-0.2894382178783417, -0.42077744007110596]\n",
      "[0.22658908367156982, -0.6141177415847778]\n",
      "[1.4533216953277588, -1.4799385070800781]\n",
      "[0.6994883418083191, -0.9240303039550781]\n",
      "[-0.47503286600112915, -1.6047954559326172]\n",
      "[-0.23660153150558472, -0.489145427942276]\n",
      "[-0.027142316102981567, -0.557750940322876]\n",
      "[0.8098675608634949, -1.0074819326400757]\n",
      "[0.5070242285728455, -0.8006559610366821]\n",
      "[0.8495513796806335, -0.9624464511871338]\n",
      "[1.0541586875915527, -1.1378154754638672]\n",
      "[0.9118779301643372, -1.0158666372299194]\n",
      "[0.1858392357826233, -0.8174828290939331]\n",
      "[0.6884453892707825, -0.8243625164031982]\n",
      "[-2.346376657485962, -0.6536271572113037]\n",
      "[1.1645193099975586, -1.2324059009552002]\n",
      "[0.3252696394920349, -0.661629319190979]\n",
      "[0.9399651885032654, -1.0399402379989624]\n",
      "[0.8417444825172424, -0.9557552337646484]\n",
      "[-1.4670729637145996, -1.3849989175796509]\n",
      "[-1.4118731021881104, -0.47201189398765564]\n",
      "[1.1232752799987793, -1.1970555782318115]\n",
      "[0.6795126795768738, -0.8167061805725098]\n",
      "[-0.020801842212677002, -0.762362003326416]\n",
      "[0.8731004595756531, -0.9826304912567139]\n",
      "[1.1379923820495605, -1.2096694707870483]\n",
      "[-1.623666524887085, -0.914886474609375]\n",
      "[1.1810147762298584, -1.2465441226959229]\n",
      "[-0.04149016737937927, -0.5935770273208618]\n",
      "[-1.2463839054107666, -0.49604877829551697]\n",
      "[-1.8169605731964111, -0.28978031873703003]\n",
      "[0.41778355836868286, -0.8556557893753052]\n",
      "[0.2577514052391052, -0.8149755001068115]\n",
      "[-0.06158360838890076, -0.5153520107269287]\n",
      "[1.1746649742126465, -1.2411015033721924]\n",
      "[-0.5088248252868652, -0.4328993856906891]\n",
      "[1.0289058685302734, -1.1161713600158691]\n",
      "[-1.515934705734253, -0.5526865720748901]\n",
      "[-1.169271469116211, -0.2184610664844513]\n",
      "[0.3008548617362976, -0.6413406133651733]\n",
      "[-0.42596668004989624, -0.8193109035491943]\n",
      "[-0.018891125917434692, -0.8168582916259766]\n",
      "[0.14203989505767822, -0.6384235620498657]\n",
      "[0.6464878916740417, -0.8905031681060791]\n",
      "[0.6095921993255615, -1.000888705253601]\n",
      "[0.01408451795578003, -1.1837137937545776]\n",
      "[-1.2473965883255005, -0.5592037439346313]\n",
      "[-1.767249584197998, -1.0588984489440918]\n",
      "[1.628354787826538, -1.6299597024917603]\n",
      "[-0.259591668844223, -0.3936103284358978]\n",
      "[-0.33931300044059753, -0.3718275725841522]\n",
      "[0.33229905366897583, -1.1926666498184204]\n",
      "[1.1997358798980713, -1.262589931488037]\n",
      "[1.1111445426940918, -1.1866581439971924]\n",
      "[0.9467901587486267, -1.0696213245391846]\n",
      "[-1.6336333751678467, -1.8110960721969604]\n",
      "[-1.1928811073303223, -0.7327473163604736]\n",
      "[0.859962522983551, -0.9713698625564575]\n",
      "[-1.8881416320800781, -0.9413034915924072]\n",
      "[0.7875552773475647, -0.933525800704956]\n",
      "[-0.5606145858764648, -0.305193692445755]\n",
      "[0.7377334237098694, -0.9492295980453491]\n",
      "[1.0056192874908447, -1.096212387084961]\n",
      "[0.08852577209472656, -0.5546135902404785]\n",
      "[-1.5571656227111816, -1.3881783485412598]\n",
      "[1.0164988040924072, -1.1055372953414917]\n",
      "[-0.6003868579864502, -0.6216816902160645]\n",
      "[1.1943669319152832, -1.2579883337020874]\n",
      "[0.759465754032135, -0.9501205682754517]\n",
      "[1.0246548652648926, -1.1125279664993286]\n",
      "[0.21023374795913696, -0.7709310054779053]\n",
      "[0.12372726202011108, -1.1196396350860596]\n",
      "[-1.1007274389266968, -0.5890628099441528]\n",
      "[1.1779820919036865, -1.243944764137268]\n",
      "[-0.8126524686813354, -0.8916943073272705]\n",
      "[0.8388710618019104, -0.9658492803573608]\n",
      "[-0.0030730068683624268, -0.7159204483032227]\n",
      "[1.0225465297698975, -1.1107207536697388]\n",
      "[-0.17578333616256714, -0.4491441547870636]\n",
      "[-0.36298656463623047, -1.169193148612976]\n",
      "[0.9468250870704651, -1.0521036386489868]\n",
      "[0.02062264084815979, -0.4858211576938629]\n",
      "[0.5369289517402649, -0.6944975852966309]\n",
      "[1.0604205131530762, -1.143182635307312]\n",
      "[-1.7638111114501953, -0.368927925825119]\n",
      "[0.6905545592308044, -0.9118263721466064]\n",
      "[0.2509257197380066, -0.694193959236145]\n",
      "[-0.7025864124298096, -0.5914390087127686]\n",
      "[1.084780216217041, -1.164061427116394]\n",
      "[-0.4870772361755371, -1.2514113187789917]\n",
      "[0.41881662607192993, -0.6781946420669556]\n",
      "[0.8629763722419739, -0.973953127861023]\n",
      "[1.116520643234253, -1.1912660598754883]\n",
      "[1.0869617462158203, -1.1659311056137085]\n",
      "[-0.6558530330657959, -0.886216402053833]\n",
      "[0.7562243342399597, -2.0557920932769775]\n",
      "[-1.534010648727417, -0.9275084733963013]\n",
      "[0.26257938146591187, -1.0570260286331177]\n",
      "[-0.8413692712783813, -1.0039058923721313]\n",
      "[0.4026174545288086, -1.3958821296691895]\n",
      "[-0.2894382178783417, -0.42077744007110596]\n",
      "[-2.587859630584717, -1.160934567451477]\n",
      "[-1.7772564888000488, -0.71033775806427]\n",
      "[0.0929916501045227, -1.7447329759597778]\n",
      "[0.03962966799736023, -1.1757067441940308]\n",
      "[1.4778437614440918, -2.0629475116729736]\n",
      "[-1.7414023876190186, -0.6904071569442749]\n",
      "[-2.4201364517211914, -1.067700743675232]\n",
      "[-0.18497295677661896, -1.2512531280517578]\n",
      "[0.2846400737762451, -1.1656501293182373]\n",
      "[0.3687431216239929, -1.3502538204193115]\n",
      "[-1.3212181329727173, -1.1422947645187378]\n",
      "[0.95371013879776, -1.6897170543670654]\n",
      "[1.0396645069122314, -1.2139573097229004]\n",
      "[-2.2498862743377686, -0.97306227684021]\n",
      "[-0.5064661502838135, -0.984139084815979]\n",
      "[-1.2513185739517212, -1.0475480556488037]\n",
      "[-0.1887129247188568, -1.1461306810379028]\n",
      "[0.5476202368736267, -1.5332087278366089]\n",
      "[-0.4144730567932129, -1.2941398620605469]\n",
      "[0.4737958312034607, -1.8171794414520264]\n",
      "[0.48883479833602905, -0.8255918025970459]\n",
      "[0.7519561648368835, -1.8391802310943604]\n",
      "[-1.4150276184082031, -1.028387427330017]\n",
      "[0.46706122159957886, -1.035760760307312]\n",
      "[-0.7470875978469849, -1.2333076000213623]\n",
      "[1.645141363143921, -1.6443474292755127]\n",
      "[0.14451736211776733, -1.1980503797531128]\n",
      "[1.399294137954712, -1.7273529767990112]\n",
      "[-0.9087932705879211, -0.8543510437011719]\n",
      "[0.5768716931343079, -0.8062927722930908]\n",
      "[0.12813794612884521, -0.6073566675186157]\n",
      "[0.5120590329170227, -1.160641074180603]\n",
      "[0.8110840916633606, -1.1593453884124756]\n",
      "[-2.5625109672546387, -1.146843671798706]\n",
      "[1.0283207893371582, -1.8858914375305176]\n",
      "[-2.172844648361206, -0.9302365779876709]\n",
      "[-2.0317938327789307, -0.8530358076095581]\n",
      "[0.5030685663223267, -0.9402651786804199]\n",
      "[-1.4085993766784668, -0.5054092407226562]\n",
      "[-1.1111562252044678, -0.8480784893035889]\n",
      "[-1.3606228828430176, -0.47874006628990173]\n",
      "[-2.0107624530792236, -0.8940609693527222]\n",
      "[-1.7724535465240479, -0.7449363470077515]\n",
      "[0.6285186409950256, -1.2026017904281616]\n",
      "[1.34488844871521, -1.387000322341919]\n",
      "[1.5385351181030273, -1.6271463632583618]\n",
      "[0.7200552821159363, -0.9928377866744995]\n",
      "[0.11354488134384155, -1.4373369216918945]\n",
      "[-1.166218638420105, -1.2472755908966064]\n",
      "[-2.2432384490966797, -0.9693669080734253]\n",
      "[-2.3137624263763428, -1.0085697174072266]\n",
      "[1.598076343536377, -1.6040079593658447]\n",
      "[0.7068530917167664, -1.7550480365753174]\n",
      "[1.242884874343872, -1.665899634361267]\n",
      "[-0.8167387247085571, -1.1988334655761719]\n",
      "[0.2934454083442688, -2.235166549682617]\n",
      "[0.5078339576721191, -1.0297229290008545]\n",
      "[-0.4116761088371277, -1.4262555837631226]\n",
      "[0.051666438579559326, -1.3548305034637451]\n",
      "[-1.7453505992889404, -0.8545186519622803]\n",
      "[-1.924424171447754, -0.7921450138092041]\n",
      "[0.7403339743614197, -1.2999529838562012]\n",
      "[-1.9117352962493896, -0.904055118560791]\n",
      "[-2.3688716888427734, -1.0392037630081177]\n",
      "[0.39527857303619385, -1.8244855403900146]\n",
      "[1.0125513076782227, -1.758819341659546]\n",
      "[-1.579313039779663, -0.6003051996231079]\n",
      "[-0.6021645069122314, -1.4139549732208252]\n",
      "[-1.5047366619110107, -0.9205915927886963]\n",
      "[0.9647960066795349, -1.6966694593429565]\n",
      "[0.13497281074523926, -1.1197212934494019]\n",
      "[1.5326018333435059, -1.5731475353240967]\n",
      "[0.47133785486221313, -0.9920737743377686]\n",
      "[-0.2232000231742859, -1.532766342163086]\n",
      "[1.084625005722046, -1.3874379396438599]\n",
      "[-0.060306400060653687, -1.3456193208694458]\n",
      "[0.26938915252685547, -1.0213968753814697]\n",
      "[-1.2832348346710205, -1.0427217483520508]\n",
      "[-2.0338492393493652, -0.8529720306396484]\n",
      "[1.4134137630462646, -1.6819987297058105]\n",
      "[0.8398148417472839, -1.374572515487671]\n",
      "[-1.921323537826538, -0.7904214859008789]\n",
      "[-1.9060587882995605, -0.7819361686706543]\n",
      "[0.9037197232246399, -1.5183827877044678]\n",
      "[-0.5800604224205017, -0.2810000777244568]\n",
      "[-1.3814902305603027, -0.9082067012786865]\n",
      "[-0.16132956743240356, -1.0317739248275757]\n",
      "[0.5477856993675232, -0.8760733604431152]\n",
      "[-0.6385959386825562, -1.3685160875320435]\n",
      "[1.318131923675537, -1.6783195734024048]\n",
      "[-2.541841983795166, -1.1353541612625122]\n",
      "[0.4299984574317932, -1.3256579637527466]\n",
      "[0.009100139141082764, -1.5580013990402222]\n",
      "[-0.5924354791641235, -1.2204086780548096]\n",
      "[1.0830063819885254, -1.3265268802642822]\n",
      "[0.3398172855377197, -1.9733374118804932]\n",
      "[-1.9650356769561768, -0.8147201538085938]\n",
      "[-0.3818276524543762, -0.8707044124603271]\n",
      "[-1.8678231239318848, -0.7696973085403442]\n",
      "[-0.612359344959259, -1.1339571475982666]\n",
      "[0.22658926248550415, -0.6141176223754883]\n",
      "[-1.7772564888000488, -0.71033775806427]\n",
      "[-2.135648012161255, -0.9095597267150879]\n",
      "[-0.45479005575180054, -1.6363636255264282]\n",
      "[-1.0971013307571411, -0.888480544090271]\n",
      "[1.3229234218597412, -1.9167520999908447]\n",
      "[-1.8186240196228027, -0.7333329916000366]\n",
      "[-1.5858986377716064, -0.9566712379455566]\n",
      "[-1.2752642631530762, -0.982284665107727]\n",
      "[-0.9134973287582397, -0.9042747020721436]\n",
      "[-0.688306987285614, -1.0789364576339722]\n",
      "[-1.4897100925445557, -1.1088711023330688]\n",
      "[-0.14054326713085175, -1.3878880739212036]\n",
      "[-0.2658202350139618, -0.7986758947372437]\n",
      "[-2.16929292678833, -0.9282623529434204]\n",
      "[-0.19124335050582886, -1.1066029071807861]\n",
      "[-1.5616846084594727, -0.9682794809341431]\n",
      "[-1.1997942924499512, -0.9009852409362793]\n",
      "[-0.5508502721786499, -1.2431684732437134]\n",
      "[-1.4411005973815918, -1.0504876375198364]\n",
      "[0.46851295232772827, -1.7601006031036377]\n",
      "[0.3838896155357361, -0.7836503982543945]\n",
      "[-0.2717496156692505, -1.574771523475647]\n",
      "[-1.7767302989959717, -0.929711103439331]\n",
      "[-0.6405582427978516, -0.7507355213165283]\n",
      "[-1.525655746459961, -1.0383731126785278]\n",
      "[0.7735405564308167, -1.2550994157791138]\n",
      "[0.5528280735015869, -1.3784688711166382]\n",
      "[0.3026937246322632, -1.4158862829208374]\n",
      "[-1.5383079051971436, -0.6965725421905518]\n",
      "[0.2793896794319153, -0.7588506937026978]\n",
      "[0.4102301001548767, -0.7510474920272827]\n",
      "[-0.6500177383422852, -0.8553493022918701]\n",
      "[-0.5885475873947144, -0.8706644773483276]\n",
      "[-2.036679983139038, -0.8545455932617188]\n",
      "[-0.00858980417251587, -1.614015817642212]\n",
      "[-0.9163426756858826, -0.7959682941436768]\n",
      "[-2.0057153701782227, -0.837333083152771]\n",
      "[0.5616286396980286, -0.9426287412643433]\n",
      "[-0.7608574628829956, -0.6299195289611816]\n",
      "[-1.7474915981292725, -0.6937921047210693]\n",
      "[0.2919185161590576, -0.8801311254501343]\n",
      "[-0.3664191961288452, -1.1242629289627075]\n",
      "[-2.0818703174591064, -0.8796659708023071]\n",
      "[-0.4733791947364807, -0.9113262891769409]\n",
      "[0.6214753985404968, -0.8639073371887207]\n",
      "[1.2271032333374023, -1.2889660596847534]\n",
      "[0.48930782079696655, -0.9005107879638672]\n",
      "[0.3818128705024719, -1.525874376296997]\n",
      "[-1.000747561454773, -1.323702096939087]\n",
      "[-1.9419975280761719, -0.8019137382507324]\n",
      "[-1.9100849628448486, -0.7841740846633911]\n",
      "[1.1298854351043701, -1.2027209997177124]\n",
      "[-0.3160823881626129, -1.491808533668518]\n",
      "[0.1726934313774109, -1.3618704080581665]\n",
      "[-1.6412353515625, -0.9989569187164307]\n",
      "[0.3717910647392273, -2.220691204071045]\n",
      "[0.2794201970100403, -0.8638449907302856]\n",
      "[-1.3801839351654053, -1.197258710861206]\n",
      "[0.13149690628051758, -1.3437155485153198]\n",
      "[-2.005448579788208, -0.8371846675872803]\n",
      "[-1.8722293376922607, -0.7631310224533081]\n",
      "[-0.47165510058403015, -1.0002992153167725]\n",
      "[-1.9111502170562744, -0.928631067276001]\n",
      "[-1.9900166988372803, -0.8286064863204956]\n",
      "[0.4900374412536621, -1.8177309036254883]\n",
      "[-0.07699650526046753, -1.4634225368499756]\n",
      "[-0.32066938281059265, -0.815065860748291]\n",
      "[-0.976493775844574, -1.331171989440918]\n",
      "[-1.9479482173919678, -0.805221438407898]\n",
      "[-0.13209211826324463, -1.3995440006256104]\n",
      "[-0.7238631248474121, -0.8920248746871948]\n",
      "[1.1069974899291992, -1.1831039190292358]\n",
      "[0.1299533247947693, -0.6986194849014282]\n",
      "[-0.7006193399429321, -1.406104326248169]\n",
      "[0.9238550066947937, -1.225543737411499]\n",
      "[-1.1418771743774414, -1.074243187904358]\n",
      "[-0.428226113319397, -0.838735818862915]\n",
      "[-1.6920366287231445, -0.9295594692230225]\n",
      "[-1.9752836227416992, -0.8204166889190674]\n",
      "[1.1793038845062256, -1.446423888206482]\n",
      "[-0.23347824811935425, -1.056937575340271]\n",
      "[-2.080700635910034, -0.8790156841278076]\n",
      "[-2.1005656719207764, -0.890058159828186]\n",
      "[-0.14144907891750336, -1.2236403226852417]\n",
      "[-0.09072846174240112, -0.48046019673347473]\n",
      "[-1.9134612083435059, -0.7860509157180786]\n",
      "[-1.093510627746582, -0.7776734828948975]\n",
      "[-0.30233824253082275, -0.6964448690414429]\n",
      "[-1.2331225872039795, -1.2310757637023926]\n",
      "[1.0888786315917969, -1.4764899015426636]\n",
      "[-2.2246670722961426, -0.9590436220169067]\n",
      "[-0.34360817074775696, -1.0573335886001587]\n",
      "[-0.9103983044624329, -1.3399792909622192]\n",
      "[-1.4012999534606934, -1.0163358449935913]\n",
      "[0.8593377470970154, -1.1334487199783325]\n",
      "[-2.6215338706970215, -1.7634196281433105]\n",
      "[-1.6975162029266357, -1.55207097530365]\n",
      "[0.8688066601753235, -1.8021516799926758]\n",
      "[-0.42393383383750916, -1.8565510511398315]\n",
      "[0.05700230598449707, -1.9295144081115723]\n",
      "[1.453321933746338, -1.4799387454986572]\n",
      "[0.09299150109291077, -1.7447328567504883]\n",
      "[-0.454789400100708, -1.6363637447357178]\n",
      "[-3.2745344638824463, -1.5426421165466309]\n",
      "[-0.01866486668586731, -1.8413684368133545]\n",
      "[2.616464614868164, -2.4768693447113037]\n",
      "[0.05699077248573303, -1.8668919801712036]\n",
      "[-1.0914525985717773, -1.4206862449645996]\n",
      "[-0.7226927280426025, -1.7929273843765259]\n",
      "[0.4680063724517822, -1.8396022319793701]\n",
      "[-0.23004481196403503, -1.841052770614624]\n",
      "[-2.6020660400390625, -1.4028784036636353]\n",
      "[-0.3937184810638428, -1.9365942478179932]\n",
      "[1.436492919921875, -1.6464300155639648]\n",
      "[-1.320432186126709, -1.5482749938964844]\n",
      "[2.3379905223846436, -2.299135446548462]\n",
      "[-1.849388837814331, -1.540609359741211]\n",
      "[-0.1806274950504303, -1.8433666229248047]\n",
      "[-1.0765968561172485, -1.7374038696289062]\n",
      "[-1.5658626556396484, -1.6732800006866455]\n",
      "[2.6686408519744873, -2.646681785583496]\n",
      "[1.3335731029510498, -1.3773019313812256]\n",
      "[-1.6994497776031494, -1.8117603063583374]\n",
      "[-2.2865798473358154, -1.4270870685577393]\n",
      "[1.4696784019470215, -1.6428041458129883]\n",
      "[-1.8370418548583984, -1.611574649810791]\n",
      "[1.0830531120300293, -1.8617043495178223]\n",
      "[2.555593729019165, -2.424696922302246]\n",
      "[-0.07546627521514893, -1.9213975667953491]\n",
      "[0.2163938283920288, -1.8713620901107788]\n",
      "[1.4488978385925293, -1.4761468172073364]\n",
      "[1.3579602241516113, -1.3982040882110596]\n",
      "[1.1889729499816895, -1.753997564315796]\n",
      "[1.1897430419921875, -1.7440752983093262]\n",
      "[-0.33343014121055603, -1.696600317955017]\n",
      "[-1.4196441173553467, -1.8585208654403687]\n",
      "[0.5846295952796936, -1.7548843622207642]\n",
      "[-1.880211591720581, -1.407715916633606]\n",
      "[1.259411096572876, -1.3137376308441162]\n",
      "[1.1694362163543701, -1.6954516172409058]\n",
      "[-0.6242145299911499, -1.6783124208450317]\n",
      "[1.9613299369812012, -1.9153528213500977]\n",
      "[0.4577350616455078, -1.8666222095489502]\n",
      "[-0.602150559425354, -1.8072537183761597]\n",
      "[1.1681456565856934, -1.7883203029632568]\n",
      "[1.6358535289764404, -1.636386752128601]\n",
      "[1.4189369678497314, -1.4504674673080444]\n",
      "[1.301788091659546, -1.350058913230896]\n",
      "[2.6596994400024414, -2.54682993888855]\n",
      "[-2.4466986656188965, -1.4894825220108032]\n",
      "[-0.1102151870727539, -1.6505205631256104]\n",
      "[0.05867201089859009, -1.7328922748565674]\n",
      "[1.568005084991455, -1.5782338380813599]\n",
      "[-1.5583860874176025, -1.7823405265808105]\n",
      "[0.051456302404403687, -1.933406114578247]\n",
      "[-1.5702812671661377, -1.6873743534088135]\n",
      "[2.8585212230682373, -3.136483669281006]\n",
      "[1.4769985675811768, -1.5002319812774658]\n",
      "[-1.8794090747833252, -1.7089388370513916]\n",
      "[2.423760175704956, -2.3460702896118164]\n",
      "[-1.3337221145629883, -1.6648451089859009]\n",
      "[0.1753239631652832, -1.8008856773376465]\n",
      "[0.770138680934906, -1.854132056236267]\n",
      "[-1.8787486553192139, -1.5628976821899414]\n",
      "[-0.31121379137039185, -1.6775386333465576]\n",
      "[2.760301351547241, -2.7517268657684326]\n",
      "[-0.6277464032173157, -1.9347891807556152]\n",
      "[1.623633623123169, -1.8007556200027466]\n",
      "[-2.8873281478881836, -1.4097609519958496]\n",
      "[-1.3707797527313232, -1.6535383462905884]\n",
      "[-0.6815525889396667, -1.8761584758758545]\n",
      "[0.48159515857696533, -1.8578312397003174]\n",
      "[1.5131127834320068, -1.5311856269836426]\n",
      "[1.5281755924224854, -1.5440958738327026]\n",
      "[-3.005239248275757, -1.392946720123291]\n",
      "[1.2784652709960938, -1.3300690650939941]\n",
      "[-0.565503716468811, -1.881186604499817]\n",
      "[0.914914071559906, -1.7762831449508667]\n",
      "[-1.793130874633789, -1.5682687759399414]\n",
      "[-0.04746690392494202, -1.7519326210021973]\n",
      "[1.4571492671966553, -1.4832192659378052]\n",
      "[0.7392644286155701, -1.8719873428344727]\n",
      "[-0.2332444041967392, -1.7131710052490234]\n",
      "[-1.2941210269927979, -1.5770901441574097]\n",
      "[0.2060829997062683, -1.915747880935669]\n",
      "[1.7180683612823486, -1.7068531513214111]\n",
      "[-1.1922082901000977, -1.653930902481079]\n",
      "[1.2245721817016602, -1.7442866563796997]\n",
      "[1.5535578727722168, -1.565851092338562]\n",
      "[-2.383721351623535, -1.5472115278244019]\n",
      "[1.7929418087005615, -1.7710272073745728]\n",
      "[-1.5949428081512451, -1.4038691520690918]\n",
      "[0.8402475714683533, -1.879590392112732]\n",
      "[-2.1990909576416016, -1.6142189502716064]\n",
      "[-1.6192188262939453, -1.6136873960494995]\n",
      "[1.305755853652954, -1.3534597158432007]\n",
      "[0.800422728061676, -2.149193048477173]\n",
      "[-1.3719701766967773, -1.088955044746399]\n",
      "[-1.3746223449707031, -0.685517430305481]\n",
      "[-1.6930105686187744, -0.870163083076477]\n",
      "[-1.8114681243896484, -0.7805200815200806]\n",
      "[0.6994884610176086, -0.9240304231643677]\n",
      "[0.039629727602005005, -1.1757068634033203]\n",
      "[-1.0971013307571411, -0.888480544090271]\n",
      "[-0.01866486668586731, -1.8413684368133545]\n",
      "[-1.9308302402496338, -0.7957059144973755]\n",
      "[0.8678515553474426, -1.5054082870483398]\n",
      "[-1.1326971054077148, -0.9345697164535522]\n",
      "[0.3526052236557007, -1.3315702676773071]\n",
      "[-1.9872562885284424, -0.827072024345398]\n",
      "[-2.001697063446045, -0.6657299995422363]\n",
      "[-1.9463911056518555, -0.7486135959625244]\n",
      "[-0.6797301769256592, -1.4519838094711304]\n",
      "[-1.179670810699463, -1.0249251127243042]\n",
      "[-1.6007728576660156, -0.416360467672348]\n",
      "[-0.6668547987937927, -1.2217496633529663]\n",
      "[0.23252207040786743, -1.3000798225402832]\n",
      "[-0.834037184715271, -1.290204644203186]\n",
      "[-2.0403292179107666, -0.7718759775161743]\n",
      "[-1.366546392440796, -1.0042901039123535]\n",
      "[-1.4444165229797363, -1.1140925884246826]\n",
      "[0.3884766697883606, -1.6355767250061035]\n",
      "[0.057678401470184326, -0.5700912475585938]\n",
      "[-0.7431827187538147, -1.4413551092147827]\n",
      "[-1.2069809436798096, -1.2015985250473022]\n",
      "[-1.6111340522766113, -0.4441002905368805]\n",
      "[-1.3266243934631348, -1.1853227615356445]\n",
      "[-0.4952412545681, -0.8347822427749634]\n",
      "[1.0516557693481445, -1.5891990661621094]\n",
      "[-0.7821210622787476, -1.0299152135849]\n",
      "[-1.379030704498291, -0.8218436241149902]\n",
      "[-0.3017372488975525, -0.43440917134284973]\n",
      "[0.46504276990890503, -0.8405263423919678]\n",
      "[-1.5434482097625732, -0.5253815650939941]\n",
      "[-1.5876030921936035, -0.5452255010604858]\n",
      "[-0.1849713921546936, -1.1989283561706543]\n",
      "[-0.584101140499115, -1.432215690612793]\n",
      "[0.5981805324554443, -1.2506802082061768]\n",
      "[-0.37434709072113037, -1.3487017154693604]\n",
      "[0.4830161929130554, -0.859541654586792]\n",
      "[0.13647902011871338, -0.8975986242294312]\n",
      "[-1.9763424396514893, -0.8210052251815796]\n",
      "[1.5496809482574463, -1.5625282526016235]\n",
      "[1.278972864151001, -1.614175796508789]\n",
      "[-1.4287142753601074, -0.9853527545928955]\n",
      "[-1.6553864479064941, -0.5471251010894775]\n",
      "[-1.080780267715454, -0.43269088864326477]\n",
      "[0.24566036462783813, -0.498383492231369]\n",
      "[-0.029922157526016235, -0.51958167552948]\n",
      "[0.7242185473442078, -1.6420258283615112]\n",
      "[0.16728585958480835, -1.7518341541290283]\n",
      "[-0.3670634627342224, -1.0265815258026123]\n",
      "[-0.37480083107948303, -1.0509291887283325]\n",
      "[0.030434459447860718, -0.4373038113117218]\n",
      "[-0.7692314982414246, -1.3633577823638916]\n",
      "[-0.9579060077667236, -0.9558095932006836]\n",
      "[-1.3025699853897095, -1.188855528831482]\n",
      "[0.34468621015548706, -2.138720750808716]\n",
      "[-0.014994680881500244, -0.683037519454956]\n",
      "[-1.2341605424880981, -1.3054872751235962]\n",
      "[0.27994388341903687, -1.3651264905929565]\n",
      "[-1.1372023820877075, -1.1663159132003784]\n",
      "[-0.9062222242355347, -0.9147709608078003]\n",
      "[-1.7790157794952393, -0.653650164604187]\n",
      "[-0.6427881717681885, -1.406907320022583]\n",
      "[-0.2185303419828415, -1.1675201654434204]\n",
      "[0.5376243591308594, -1.7561566829681396]\n",
      "[-0.9873383641242981, -1.1383756399154663]\n",
      "[1.0787134170532227, -1.2022658586502075]\n",
      "[-0.5138440132141113, -1.5686886310577393]\n",
      "[-1.2548487186431885, -1.1236445903778076]\n",
      "[-1.061556339263916, -1.086403489112854]\n",
      "[-1.5556552410125732, -0.7059866189956665]\n",
      "[-0.13239990174770355, -0.4039738178253174]\n",
      "[-0.28690946102142334, -0.46448156237602234]\n",
      "[-0.5357264280319214, -1.540355920791626]\n",
      "[0.2614670395851135, -0.7079424858093262]\n",
      "[-2.0048229694366455, -0.8368369340896606]\n",
      "[-1.3809924125671387, -0.6577969789505005]\n",
      "[-1.1471545696258545, -1.2021487951278687]\n",
      "[-0.7846935391426086, -0.9772783517837524]\n",
      "[0.46360546350479126, -0.8446550369262695]\n",
      "[-1.7751843929290771, -0.654613733291626]\n",
      "[-1.1230627298355103, -0.8874346017837524]\n",
      "[-1.479858636856079, -0.961790919303894]\n",
      "[-1.4223995208740234, -0.7984822988510132]\n",
      "[0.6042025685310364, -0.8424235582351685]\n",
      "[-1.457515001296997, -1.0030186176300049]\n",
      "[-1.6896107196807861, -0.5316751003265381]\n",
      "[-1.298475742340088, -0.42377638816833496]\n",
      "[-0.835243821144104, -1.4454127550125122]\n",
      "[0.48439329862594604, -0.985345721244812]\n",
      "[-0.2674887776374817, -1.2812869548797607]\n",
      "[-1.844970464706421, -0.6428508758544922]\n",
      "[-0.8497002124786377, -1.4107930660247803]\n",
      "[-1.2944862842559814, -1.1272553205490112]\n",
      "[-0.02207714319229126, -0.5188809633255005]\n",
      "[3.074394941329956, -2.8693618774414062]\n",
      "[1.943434476852417, -1.958590030670166]\n",
      "[0.2102411389350891, -1.421151041984558]\n",
      "[1.2229745388031006, -1.6112614870071411]\n",
      "[0.9620537161827087, -1.387939453125]\n",
      "[-0.47503286600112915, -1.6047954559326172]\n",
      "[1.4778437614440918, -2.0629475116729736]\n",
      "[1.3229234218597412, -1.9167520999908447]\n",
      "[2.616464614868164, -2.4768693447113037]\n",
      "[0.8678515553474426, -1.5054082870483398]\n",
      "[-3.552783489227295, -0.8117283582687378]\n",
      "[1.076615571975708, -1.704026699066162]\n",
      "[1.9134266376495361, -2.3599963188171387]\n",
      "[1.383643388748169, -1.6579105854034424]\n",
      "[0.2171773910522461, -1.1011172533035278]\n",
      "[1.0175883769989014, -1.3693219423294067]\n",
      "[2.3327765464782715, -2.233720064163208]\n",
      "[1.5143749713897705, -1.532267451286316]\n",
      "[-0.35364872217178345, -1.149409532546997]\n",
      "[1.9836125373840332, -2.203842878341675]\n",
      "[-2.1456544399261475, -1.5403648614883423]\n",
      "[2.0984413623809814, -2.217099905014038]\n",
      "[0.7886070609092712, -1.267621397972107]\n",
      "[1.7316968441009521, -1.718534231185913]\n",
      "[1.9011085033416748, -1.9189119338989258]\n",
      "[-3.208282947540283, -1.255591630935669]\n",
      "[-0.873237669467926, -1.3334623575210571]\n",
      "[2.0195677280426025, -1.9652683734893799]\n",
      "[2.0731089115142822, -2.0111587047576904]\n",
      "[-0.49719899892807007, -1.1145535707473755]\n",
      "[2.0219314098358154, -1.967294454574585]\n",
      "[0.6056720018386841, -1.0733157396316528]\n",
      "[-1.2743704319000244, -1.7790074348449707]\n",
      "[1.5768711566925049, -1.5858328342437744]\n",
      "[0.7444748282432556, -1.5533167123794556]\n",
      "[-0.7619615197181702, -1.325254201889038]\n",
      "[-0.5796573758125305, -1.5086190700531006]\n",
      "[-0.011475324630737305, -1.2272814512252808]\n",
      "[-0.14669573307037354, -1.2014716863632202]\n",
      "[1.6618142127990723, -2.118042230606079]\n",
      "[2.0443334579467773, -1.9864952564239502]\n",
      "[1.246596097946167, -2.1765048503875732]\n",
      "[2.2622785568237305, -2.405137538909912]\n",
      "[-1.1095566749572754, -1.367197871208191]\n",
      "[0.3896142840385437, -1.7609843015670776]\n",
      "[1.1528716087341309, -1.5493892431259155]\n",
      "[1.105682373046875, -2.4799916744232178]\n",
      "[1.878108263015747, -2.6601178646087646]\n",
      "[1.451174020767212, -1.7896286249160767]\n",
      "[-0.02654421329498291, -1.092598795890808]\n",
      "[-0.486366868019104, -1.0861337184906006]\n",
      "[-1.7880713939666748, -0.967204213142395]\n",
      "[-1.097919225692749, -1.2310466766357422]\n",
      "[-1.895400047302246, -1.6988801956176758]\n",
      "[2.829754114151001, -2.659679889678955]\n",
      "[1.244384765625, -1.955488681793213]\n",
      "[1.298454999923706, -1.9424529075622559]\n",
      "[-1.5165677070617676, -0.9550371170043945]\n",
      "[2.1421360969543457, -2.070322036743164]\n",
      "[1.3709666728973389, -1.4740558862686157]\n",
      "[1.9285147190093994, -1.9817354679107666]\n",
      "[-3.553264617919922, -1.6158761978149414]\n",
      "[-1.5044581890106201, -1.262553334236145]\n",
      "[1.9915142059326172, -1.9412238597869873]\n",
      "[-2.3512625694274902, -1.4303621053695679]\n",
      "[1.867236614227295, -2.0312230587005615]\n",
      "[1.0176236629486084, -1.7488341331481934]\n",
      "[0.21406173706054688, -1.177706003189087]\n",
      "[2.2604739665985107, -2.2402806282043457]\n",
      "[1.5772998332977295, -2.0910933017730713]\n",
      "[-2.936671018600464, -1.4278604984283447]\n",
      "[1.7034144401550293, -1.6942932605743408]\n",
      "[1.0099804401397705, -2.2008800506591797]\n",
      "[2.4126455783843994, -2.302175998687744]\n",
      "[1.8426072597503662, -1.9923624992370605]\n",
      "[1.6915991306304932, -1.6841663122177124]\n",
      "[0.48201704025268555, -1.5305055379867554]\n",
      "[-1.4175796508789062, -1.0169084072113037]\n",
      "[-0.7095212936401367, -1.327690839767456]\n",
      "[2.394265651702881, -2.2864224910736084]\n",
      "[-1.7071497440338135, -1.091719388961792]\n",
      "[1.3471965789794922, -1.6094931364059448]\n",
      "[0.17619210481643677, -1.4015849828720093]\n",
      "[2.0245237350463867, -2.0712239742279053]\n",
      "[1.2213621139526367, -1.8846391439437866]\n",
      "[-2.2343268394470215, -0.9322352409362793]\n",
      "[0.3537406325340271, -1.1772207021713257]\n",
      "[1.2249462604522705, -1.8217856884002686]\n",
      "[1.726156234741211, -1.8905164003372192]\n",
      "[0.954799473285675, -1.3085297346115112]\n",
      "[-0.741707444190979, -1.6116364002227783]\n",
      "[1.703169345855713, -1.9237207174301147]\n",
      "[-0.2674488425254822, -1.0242283344268799]\n",
      "[-0.39617860317230225, -1.3048982620239258]\n",
      "[2.232696533203125, -2.1479413509368896]\n",
      "[-2.4540750980377197, -0.9783498048782349]\n",
      "[2.130337953567505, -2.359448194503784]\n",
      "[0.19677186012268066, -0.981189489364624]\n",
      "[2.229896306991577, -2.145541191101074]\n",
      "[1.912670373916626, -2.0361850261688232]\n",
      "[-1.5726869106292725, -1.0822253227233887]\n",
      "[0.8563351035118103, -2.192922592163086]\n",
      "[-1.409320592880249, -1.0528371334075928]\n",
      "[-0.846156656742096, -0.8971147537231445]\n",
      "[-1.555199384689331, -0.9228081703186035]\n",
      "[-0.7626656889915466, -1.1751452684402466]\n",
      "[-0.23660188913345337, -0.48914554715156555]\n",
      "[-1.7414023876190186, -0.6904071569442749]\n",
      "[-1.8186240196228027, -0.7333329916000366]\n",
      "[0.05699032545089722, -1.866891860961914]\n",
      "[-1.1326971054077148, -0.9345697164535522]\n",
      "[1.076615571975708, -1.704026699066162]\n",
      "[-1.9304935932159424, -0.7955188751220703]\n",
      "[-1.303489327430725, -0.925020694732666]\n",
      "[-1.1118369102478027, -1.0931575298309326]\n",
      "[-1.0882205963134766, -1.028460144996643]\n",
      "[-0.7697325348854065, -1.1261588335037231]\n",
      "[-0.7735111713409424, -1.3915627002716064]\n",
      "[0.010959595441818237, -1.4309241771697998]\n",
      "[-0.9668216109275818, -0.6720049381256104]\n",
      "[-1.5710914134979248, -0.9218775033950806]\n",
      "[-0.3168652653694153, -1.0747265815734863]\n",
      "[-0.9729137420654297, -1.2263928651809692]\n",
      "[-1.138617753982544, -1.0000596046447754]\n",
      "[-0.3269968628883362, -1.3521534204483032]\n",
      "[-1.0547795295715332, -1.2404756546020508]\n",
      "[0.30243605375289917, -1.6548874378204346]\n",
      "[-0.42722976207733154, -0.523161768913269]\n",
      "[0.07307958602905273, -1.7200031280517578]\n",
      "[-1.1607043743133545, -1.1900476217269897]\n",
      "[-1.1279933452606201, -0.6557552814483643]\n",
      "[-1.0895905494689941, -1.2526766061782837]\n",
      "[0.6784202456474304, -1.233758568763733]\n",
      "[0.42968565225601196, -1.3346171379089355]\n",
      "[0.45025742053985596, -1.4432722330093384]\n",
      "[-1.4254145622253418, -0.7767709493637085]\n",
      "[-0.9037384986877441, -0.4080725312232971]\n",
      "[-0.38860028982162476, -0.5776649713516235]\n",
      "[-0.7613650560379028, -0.8427658081054688]\n",
      "[-0.8898639678955078, -0.8312933444976807]\n",
      "[-1.863431453704834, -0.7582404613494873]\n",
      "[0.3129732012748718, -1.7409167289733887]\n",
      "[-1.2122339010238647, -0.734367847442627]\n",
      "[-1.101208209991455, -1.1167991161346436]\n",
      "[0.0189608633518219, -0.7417144775390625]\n",
      "[-1.1419942378997803, -0.5006752014160156]\n",
      "[-1.680253028869629, -0.7732011079788208]\n",
      "[0.2259628176689148, -0.8910337686538696]\n",
      "[-0.42677539587020874, -1.1499240398406982]\n",
      "[-1.9985408782958984, -0.8333449363708496]\n",
      "[-0.8160594701766968, -0.8762309551239014]\n",
      "[0.013889998197555542, -0.7369540929794312]\n",
      "[0.7572261691093445, -0.8882502317428589]\n",
      "[-0.2803371548652649, -0.5552778244018555]\n",
      "[0.2377549409866333, -1.4563777446746826]\n",
      "[-0.13531890511512756, -1.629696249961853]\n",
      "[-1.8762848377227783, -0.7653853893280029]\n",
      "[-1.8650221824645996, -0.759124755859375]\n",
      "[0.5795629620552063, -0.7310391664505005]\n",
      "[0.02686139941215515, -1.6350363492965698]\n",
      "[0.29339104890823364, -1.3739442825317383]\n",
      "[-1.195633888244629, -1.2168675661087036]\n",
      "[0.20868664979934692, -2.1262099742889404]\n",
      "[0.04250037670135498, -0.7757225036621094]\n",
      "[-0.9467296600341797, -1.4024615287780762]\n",
      "[-0.023838400840759277, -1.2688477039337158]\n",
      "[-1.4979474544525146, -1.027418851852417]\n",
      "[-1.871422529220581, -0.7626825571060181]\n",
      "[-0.8374633193016052, -1.0237088203430176]\n",
      "[-1.0316898822784424, -1.2430018186569214]\n",
      "[-1.8324317932128906, -0.7410084009170532]\n",
      "[0.31860119104385376, -1.7104665040969849]\n",
      "[0.1350986361503601, -1.5255322456359863]\n",
      "[-0.5681536197662354, -0.7645246982574463]\n",
      "[-0.46821850538253784, -1.5707712173461914]\n",
      "[-1.4665143489837646, -1.038291573524475]\n",
      "[0.06435036659240723, -1.4739584922790527]\n",
      "[-0.92946457862854, -0.914283275604248]\n",
      "[0.5921220183372498, -0.7418035268783569]\n",
      "[-1.0892505645751953, -0.3887694776058197]\n",
      "[-0.2249736785888672, -1.6289986371994019]\n",
      "[0.4035213589668274, -0.8640974760055542]\n",
      "[-1.0084331035614014, -1.1653966903686523]\n",
      "[-0.862511157989502, -0.8675155639648438]\n",
      "[-1.1748998165130615, -1.1727492809295654]\n",
      "[-1.889814853668213, -0.7729064226150513]\n",
      "[0.7908992171287537, -1.1304713487625122]\n",
      "[-0.7831440567970276, -1.0417044162750244]\n",
      "[-1.9194176197052002, -0.7893620729446411]\n",
      "[-1.7283124923706055, -0.851895809173584]\n",
      "[-0.2082042396068573, -1.2223775386810303]\n",
      "[-0.2299106866121292, -0.47869595885276794]\n",
      "[-1.5275230407714844, -0.9664381742477417]\n",
      "[-1.0643129348754883, -0.7911380529403687]\n",
      "[-1.1795552968978882, -0.5306822061538696]\n",
      "[-0.7415003776550293, -1.4647722244262695]\n",
      "[0.831360399723053, -1.2639431953430176]\n",
      "[-1.8392894268035889, -0.800180196762085]\n",
      "[-0.8809978365898132, -1.0203937292099]\n",
      "[-0.46982550621032715, -1.5390411615371704]\n",
      "[-0.986782968044281, -1.2200706005096436]\n",
      "[0.36434364318847656, -0.8083795309066772]\n",
      "[-0.44323840737342834, -1.7413065433502197]\n",
      "[-2.1706702709198, -0.9290279150009155]\n",
      "[0.8445698618888855, -1.2580342292785645]\n",
      "[-0.8207421898841858, -1.0683465003967285]\n",
      "[1.099531650543213, -1.6114420890808105]\n",
      "[-0.02714213728904724, -0.557750940322876]\n",
      "[-2.4201364517211914, -1.067700743675232]\n",
      "[-1.5858986377716064, -0.9566712379455566]\n",
      "[-1.091451644897461, -1.4206864833831787]\n",
      "[0.3526052236557007, -1.3315702676773071]\n",
      "[1.913426399230957, -2.3599963188171387]\n",
      "[-1.3034889698028564, -0.925020694732666]\n",
      "[-3.2398486137390137, -1.5233609676361084]\n",
      "[-0.11467771232128143, -1.3157310485839844]\n",
      "[0.9504424929618835, -1.413296103477478]\n",
      "[0.7453630566596985, -1.498225450515747]\n",
      "[-2.3883144855499268, -1.0500116348266602]\n",
      "[1.358386516571045, -1.8186589479446411]\n",
      "[1.568666696548462, -1.578800916671753]\n",
      "[-2.668715476989746, -1.2058805227279663]\n",
      "[-0.16517937183380127, -1.095262885093689]\n",
      "[-1.8639826774597168, -1.0158618688583374]\n",
      "[-0.014510303735733032, -1.2484580278396606]\n",
      "[0.5327929854393005, -1.5600641965866089]\n",
      "[-0.9322301149368286, -1.1783546209335327]\n",
      "[0.9053035378456116, -2.0459561347961426]\n",
      "[0.8388918042182922, -0.9533101320266724]\n",
      "[0.709919273853302, -1.8240890502929688]\n",
      "[-2.2561047077178955, -0.976518988609314]\n",
      "[1.5446550846099854, -1.5582205057144165]\n",
      "[-1.4800281524658203, -1.0453721284866333]\n",
      "[2.207972764968872, -2.1267507076263428]\n",
      "[0.43658220767974854, -1.2766079902648926]\n",
      "[1.92771315574646, -1.9118951559066772]\n",
      "[-0.43021684885025024, -1.0597714185714722]\n",
      "[0.8545543551445007, -0.9667345285415649]\n",
      "[0.4586983919143677, -0.7398720979690552]\n",
      "[1.689596176147461, -1.6824496984481812]\n",
      "[1.511246681213379, -1.5295861959457397]\n",
      "[-2.666616678237915, -1.2047137022018433]\n",
      "[1.1230509281158447, -1.9152323007583618]\n",
      "[-2.1420609951019287, -0.9131245613098145]\n",
      "[-2.564357280731201, -1.1478700637817383]\n",
      "[0.8784374594688416, -1.0559707880020142]\n",
      "[-0.9196105003356934, -0.708135724067688]\n",
      "[-1.0410047769546509, -0.9289735555648804]\n",
      "[-1.6283149719238281, -0.6275442838668823]\n",
      "[-2.449636936187744, -1.084099531173706]\n",
      "[-1.8195691108703613, -0.8614503145217896]\n",
      "[1.7794325351715088, -1.7594484090805054]\n",
      "[1.8508591651916504, -1.820668339729309]\n",
      "[1.9740650653839111, -2.056197166442871]\n",
      "[1.05723237991333, -1.1449369192123413]\n",
      "[0.460827112197876, -1.5502853393554688]\n",
      "[-2.3982672691345215, -1.1332529783248901]\n",
      "[-2.164306879043579, -0.9254906177520752]\n",
      "[-2.207170009613037, -0.949317216873169]\n",
      "[2.117109537124634, -2.058134078979492]\n",
      "[0.7344380021095276, -1.7679823637008667]\n",
      "[1.8528542518615723, -1.8736211061477661]\n",
      "[-1.4505751132965088, -1.0423189401626587]\n",
      "[0.7114980816841125, -2.4569902420043945]\n",
      "[0.8900807499885559, -1.1740416288375854]\n",
      "[-0.9769586324691772, -1.269640326499939]\n",
      "[0.4512721300125122, -1.5158700942993164]\n",
      "[-2.1956961154937744, -0.942939281463623]\n",
      "[-1.6647930145263672, -0.908106803894043]\n",
      "[1.588172197341919, -1.5955190658569336]\n",
      "[-2.600949287414551, -1.168210744857788]\n",
      "[-2.4433000087738037, -1.0805768966674805]\n",
      "[0.8151969313621521, -2.0476911067962646]\n",
      "[1.352309226989746, -1.8698982000350952]\n",
      "[-1.5182175636291504, -0.6463497877120972]\n",
      "[-1.7161865234375, -1.138490080833435]\n",
      "[-2.0101144313812256, -0.906195878982544]\n",
      "[1.2949035167694092, -1.8029950857162476]\n",
      "[0.6059941649436951, -1.305078387260437]\n",
      "[1.921910047531128, -1.9164214134216309]\n",
      "[1.1824951171875, -1.2579009532928467]\n",
      "[-1.2431232929229736, -1.2579374313354492]\n",
      "[1.4350883960723877, -1.5927785634994507]\n",
      "[0.23069149255752563, -1.4428648948669434]\n",
      "[0.8431209921836853, -1.2254172563552856]\n",
      "[-1.9563255310058594, -0.9789838790893555]\n",
      "[-1.8716247081756592, -0.920822262763977]\n",
      "[1.7950806617736816, -1.9657080173492432]\n",
      "[1.7282838821411133, -1.7156089544296265]\n",
      "[-1.7780542373657227, -0.9178587198257446]\n",
      "[-2.2328784465789795, -0.9636080265045166]\n",
      "[1.6704771518707275, -1.7602252960205078]\n",
      "[-0.27140313386917114, -0.4140860438346863]\n",
      "[-1.6484777927398682, -0.9350680112838745]\n",
      "[1.6055774688720703, -1.6104371547698975]\n",
      "[1.0029685497283936, -1.0939403772354126]\n",
      "[-1.6155657768249512, -1.1126844882965088]\n",
      "[1.6970586776733398, -1.9581701755523682]\n",
      "[-3.1611104011535645, -1.479592204093933]\n",
      "[1.7892496585845947, -1.7678626775741577]\n",
      "[-0.5367850661277771, -1.4125847816467285]\n",
      "[-0.9936357736587524, -1.1433286666870117]\n",
      "[1.420130729675293, -1.5148264169692993]\n",
      "[0.09775906801223755, -2.1212620735168457]\n",
      "[-1.9355723857879639, -1.0677120685577393]\n",
      "[-1.2309640645980835, -0.832094669342041]\n",
      "[-1.9583368301391602, -0.9568576812744141]\n",
      "[-1.9054834842681885, -0.8826642036437988]\n",
      "[0.8098676800727844, -1.0074819326400757]\n",
      "[-0.18497267365455627, -1.2512530088424683]\n",
      "[-1.2752642631530762, -0.9822847843170166]\n",
      "[-0.7226916551589966, -1.792927622795105]\n",
      "[-1.9872560501098633, -0.8270719051361084]\n",
      "[1.383643627166748, -1.657910704612732]\n",
      "[-1.1118369102478027, -1.0931575298309326]\n",
      "[-0.11467771232128143, -1.3157310485839844]\n",
      "[-2.3920600414276123, -1.0520936250686646]\n",
      "[-1.8914084434509277, -0.7975231409072876]\n",
      "[-2.049623727798462, -0.8617407083511353]\n",
      "[-1.3224053382873535, -1.4064396619796753]\n",
      "[-1.6135201454162598, -1.0738166570663452]\n",
      "[-0.9775885939598083, -0.6147031784057617]\n",
      "[-1.0592880249023438, -1.2475382089614868]\n",
      "[0.8420976996421814, -1.4543075561523438]\n",
      "[-1.4186780452728271, -1.2647671699523926]\n",
      "[-2.1025140285491943, -0.8911412954330444]\n",
      "[-1.9924912452697754, -0.989211916923523]\n",
      "[-2.148979425430298, -1.0605332851409912]\n",
      "[1.05949068069458, -1.764170527458191]\n",
      "[0.19462156295776367, -0.500757098197937]\n",
      "[-1.4713008403778076, -1.3970469236373901]\n",
      "[-1.8582696914672852, -1.1533465385437012]\n",
      "[-1.0194194316864014, -0.6326161623001099]\n",
      "[-2.005857229232788, -1.1346594095230103]\n",
      "[-0.561151921749115, -0.9176403284072876]\n",
      "[1.5048635005950928, -1.7570734024047852]\n",
      "[-1.1634461879730225, -1.0885546207427979]\n",
      "[-1.371497392654419, -0.9620620012283325]\n",
      "[0.022330641746520996, -0.5078932046890259]\n",
      "[0.579465925693512, -0.7309560775756836]\n",
      "[-1.0399284362792969, -0.7108241319656372]\n",
      "[-1.2300806045532227, -0.7021993398666382]\n",
      "[-0.46126818656921387, -1.2671048641204834]\n",
      "[-1.2804198265075684, -1.403685450553894]\n",
      "[0.6413847804069519, -1.3504633903503418]\n",
      "[-0.8904135823249817, -1.3379607200622559]\n",
      "[0.657500684261322, -0.8548169136047363]\n",
      "[-0.03867802023887634, -0.9859329462051392]\n",
      "[-2.1440632343292236, -0.9142376184463501]\n",
      "[1.6076645851135254, -1.6122260093688965]\n",
      "[1.2924985885620117, -1.702540397644043]\n",
      "[-1.5802857875823975, -1.0898003578186035]\n",
      "[-1.1412267684936523, -0.7183932065963745]\n",
      "[-0.5365013480186462, -0.5786285400390625]\n",
      "[0.4486055374145508, -0.6187955141067505]\n",
      "[0.21780502796173096, -0.5145988464355469]\n",
      "[1.3283474445343018, -1.7589102983474731]\n",
      "[-0.42675694823265076, -1.7253108024597168]\n",
      "[-0.6312475204467773, -1.0890847444534302]\n",
      "[-0.5702387094497681, -1.1462230682373047]\n",
      "[0.3496384024620056, -0.5393052101135254]\n",
      "[-1.5045511722564697, -1.3189786672592163]\n",
      "[-1.275158166885376, -1.030811071395874]\n",
      "[-1.9129414558410645, -1.1588505506515503]\n",
      "[1.012573003768921, -2.2875254154205322]\n",
      "[0.5467981696128845, -0.8245060443878174]\n",
      "[-1.9391660690307617, -1.2529205083847046]\n",
      "[0.922823965549469, -1.5019302368164062]\n",
      "[-1.5718107223510742, -1.1840425729751587]\n",
      "[-0.8700513243675232, -1.0737658739089966]\n",
      "[-1.510502576828003, -0.7839380502700806]\n",
      "[-1.1614859104156494, -1.3969688415527344]\n",
      "[-0.5735663771629333, -1.2113420963287354]\n",
      "[1.1928117275238037, -1.884034276008606]\n",
      "[-1.5252349376678467, -1.1612578630447388]\n",
      "[1.1091477870941162, -1.258891224861145]\n",
      "[-1.2028825283050537, -1.5160624980926514]\n",
      "[-1.7281973361968994, -1.13083815574646]\n",
      "[-1.5969882011413574, -1.1082571744918823]\n",
      "[-1.3490962982177734, -0.874325156211853]\n",
      "[0.2329559326171875, -0.4884563386440277]\n",
      "[0.10271656513214111, -0.5567069053649902]\n",
      "[-1.2838995456695557, -1.4711871147155762]\n",
      "[0.4923252463340759, -0.6562676429748535]\n",
      "[-2.355910539627075, -1.031998872756958]\n",
      "[-1.2321782112121582, -0.8047505617141724]\n",
      "[-1.7445909976959229, -1.171906590461731]\n",
      "[-0.8813145756721497, -1.0958327054977417]\n",
      "[0.7199093699455261, -0.8513302803039551]\n",
      "[-1.3297111988067627, -0.8056042194366455]\n",
      "[-1.1923459768295288, -1.0146021842956543]\n",
      "[-1.8938074111938477, -0.9843711853027344]\n",
      "[-1.5186290740966797, -0.9181035757064819]\n",
      "[0.9556418061256409, -1.0533767938613892]\n",
      "[-1.9165523052215576, -1.0154179334640503]\n",
      "[-1.3695213794708252, -0.6864490509033203]\n",
      "[-0.6691615581512451, -0.6248307228088379]\n",
      "[-1.518852710723877, -1.3939659595489502]\n",
      "[0.9468989968299866, -1.1315172910690308]\n",
      "[-0.595999002456665, -1.305071473121643]\n",
      "[-1.2952803373336792, -0.8033251762390137]\n",
      "[-1.5968422889709473, -1.349869966506958]\n",
      "[-1.985551118850708, -1.074813723564148]\n",
      "[0.3337450623512268, -0.5302393436431885]\n",
      "[1.2954344749450684, -2.141472578048706]\n",
      "[-0.8908054828643799, -1.1354764699935913]\n",
      "[-1.6354773044586182, -0.5690091848373413]\n",
      "[-1.5331764221191406, -0.9317575693130493]\n",
      "[-2.050579071044922, -0.6568268537521362]\n",
      "[0.5070242285728455, -0.8006559610366821]\n",
      "[0.28464001417160034, -1.1656501293182373]\n",
      "[-0.9134973287582397, -0.9042747020721436]\n",
      "[0.46800607442855835, -1.839601993560791]\n",
      "[-2.001697301864624, -0.6657301187515259]\n",
      "[0.2171773910522461, -1.1011172533035278]\n",
      "[-1.0882205963134766, -1.0284600257873535]\n",
      "[0.9504424929618835, -1.413296103477478]\n",
      "[-1.8914084434509277, -0.7975231409072876]\n",
      "[-2.2012288570404053, -0.5612980127334595]\n",
      "[-2.2700085639953613, -0.6257119178771973]\n",
      "[-0.19950833916664124, -1.509179949760437]\n",
      "[-1.2856662273406982, -0.8235105276107788]\n",
      "[-1.591416358947754, -0.4043474793434143]\n",
      "[-0.20221751928329468, -1.3010387420654297]\n",
      "[0.15670067071914673, -1.1830835342407227]\n",
      "[-0.34464579820632935, -1.3389251232147217]\n",
      "[-2.2362494468688965, -0.665006160736084]\n",
      "[-1.210191249847412, -0.8778406381607056]\n",
      "[-0.9760576486587524, -1.105079174041748]\n",
      "[-0.018687427043914795, -1.3323185443878174]\n",
      "[-0.26049306988716125, -0.4119320809841156]\n",
      "[-0.47121214866638184, -1.3299293518066406]\n",
      "[-0.7356009483337402, -1.2488219738006592]\n",
      "[-1.5966637134552002, -0.43326470255851746]\n",
      "[-0.8585200309753418, -1.2010499238967896]\n",
      "[-1.4537763595581055, -0.6507004499435425]\n",
      "[0.9812350869178772, -1.4317822456359863]\n",
      "[-0.9128006100654602, -0.8309518098831177]\n",
      "[-1.4422385692596436, -0.8642038106918335]\n",
      "[-0.5848102569580078, -0.39521583914756775]\n",
      "[0.16232389211654663, -0.5612932443618774]\n",
      "[-1.7411527633666992, -0.42668434977531433]\n",
      "[-1.7668812274932861, -0.4595845639705658]\n",
      "[0.0897769033908844, -1.2207963466644287]\n",
      "[-0.3705475926399231, -1.29337477684021]\n",
      "[0.8877533078193665, -1.2588919401168823]\n",
      "[0.12135913968086243, -1.4389817714691162]\n",
      "[0.012631088495254517, -0.5445427894592285]\n",
      "[-0.29694604873657227, -0.7953082323074341]\n",
      "[-2.011276960372925, -0.7098362445831299]\n",
      "[1.7159478664398193, -1.7050355672836304]\n",
      "[1.5928542613983154, -1.6235387325286865]\n",
      "[-1.2012388706207275, -1.067833662033081]\n",
      "[-1.8508257865905762, -0.4627368748188019]\n",
      "[-1.5206329822540283, -0.3685459494590759]\n",
      "[-0.5591176748275757, -0.37647759914398193]\n",
      "[-0.40141698718070984, -0.409392774105072]\n",
      "[0.5778763890266418, -1.4551305770874023]\n",
      "[0.6820884346961975, -1.8183449506759644]\n",
      "[-0.1202971339225769, -1.018770694732666]\n",
      "[-0.4106226861476898, -1.031988501548767]\n",
      "[-0.7153076529502869, -0.3572278320789337]\n",
      "[-0.477277547121048, -1.2514539957046509]\n",
      "[-1.3817765712738037, -0.745377779006958]\n",
      "[-0.8177491426467896, -1.2042664289474487]\n",
      "[0.014826476573944092, -1.8765288591384888]\n",
      "[-0.535926342010498, -0.3702102601528168]\n",
      "[-0.760631799697876, -1.2885085344314575]\n",
      "[0.01088261604309082, -1.1416985988616943]\n",
      "[-0.6633086204528809, -1.209902286529541]\n",
      "[-1.0133616924285889, -0.9639146327972412]\n",
      "[-2.0919811725616455, -0.5301418304443359]\n",
      "[-0.15400098264217377, -1.4787309169769287]\n",
      "[0.03343468904495239, -1.1771330833435059]\n",
      "[0.2011878490447998, -1.483579397201538]\n",
      "[-0.9375348091125488, -0.9580538272857666]\n",
      "[1.1163978576660156, -1.2501327991485596]\n",
      "[-0.03686821460723877, -1.592726469039917]\n",
      "[-0.774653434753418, -1.162389874458313]\n",
      "[-1.0133919715881348, -0.9067209959030151]\n",
      "[-1.6867618560791016, -0.6817255020141602]\n",
      "[-0.8561756610870361, -0.3200133740901947]\n",
      "[-0.5835290551185608, -0.44230887293815613]\n",
      "[-0.11684495210647583, -1.5444966554641724]\n",
      "[-0.3544052839279175, -0.429514080286026]\n",
      "[-1.820589303970337, -0.8117896318435669]\n",
      "[-1.6485278606414795, -0.5302153825759888]\n",
      "[-0.6663637161254883, -1.2433099746704102]\n",
      "[-0.9509177207946777, -0.9744479656219482]\n",
      "[-0.22699031233787537, -0.47467300295829773]\n",
      "[-2.1118690967559814, -0.5220227241516113]\n",
      "[-1.1995224952697754, -0.8991378545761108]\n",
      "[-1.0072296857833862, -1.0133028030395508]\n",
      "[-2.1783597469329834, -0.5940382480621338]\n",
      "[0.6552413105964661, -0.8407888412475586]\n",
      "[-0.9758006930351257, -1.038084626197815]\n",
      "[-1.7771492004394531, -0.4803904592990875]\n",
      "[-1.3560471534729004, -0.386673241853714]\n",
      "[-0.36157938838005066, -1.4636234045028687]\n",
      "[-0.31305235624313354, -0.5578820705413818]\n",
      "[0.48075413703918457, -1.418073058128357]\n",
      "[-2.1238386631011963, -0.5360363721847534]\n",
      "[-0.40629759430885315, -1.3782432079315186]\n",
      "[-0.830280601978302, -1.1366726160049438]\n",
      "[-0.6029664278030396, -0.35496294498443604]\n",
      "[0.5547384023666382, -2.1385371685028076]\n",
      "[-1.2766598463058472, -1.1840529441833496]\n",
      "[-1.404252290725708, -0.7149666547775269]\n",
      "[-1.4668505191802979, -0.9892410039901733]\n",
      "[-2.1781153678894043, -0.8355914354324341]\n",
      "[0.8495514988899231, -0.9624465703964233]\n",
      "[0.36874306201934814, -1.350253939628601]\n",
      "[-0.6883070468902588, -1.078936219215393]\n",
      "[-0.23004469275474548, -1.8410526514053345]\n",
      "[-1.9463913440704346, -0.7486135959625244]\n",
      "[1.0175883769989014, -1.3693220615386963]\n",
      "[-0.7697327136993408, -1.1261588335037231]\n",
      "[0.7453630566596985, -1.498225450515747]\n",
      "[-2.049623727798462, -0.8617408275604248]\n",
      "[-2.2700085639953613, -0.6257119178771973]\n",
      "[-2.3262269496917725, -0.9512389898300171]\n",
      "[-0.7224776744842529, -1.513710856437683]\n",
      "[-2.0757246017456055, -0.8762496709823608]\n",
      "[-1.3830695152282715, -0.46520909667015076]\n",
      "[-0.38927045464515686, -1.3912216424942017]\n",
      "[1.0200715065002441, -1.4284812211990356]\n",
      "[-0.7718778848648071, -1.37740159034729]\n",
      "[-2.214190721511841, -0.7891961336135864]\n",
      "[-2.106558322906494, -0.8933894634246826]\n",
      "[-1.5516624450683594, -1.1316766738891602]\n",
      "[0.941895067691803, -1.588804006576538]\n",
      "[0.27101755142211914, -0.5259716510772705]\n",
      "[-1.463139533996582, -1.2748801708221436]\n",
      "[-1.2498270273208618, -1.2584729194641113]\n",
      "[-1.2733557224273682, -0.5124416351318359]\n",
      "[-1.403566598892212, -1.220293402671814]\n",
      "[-1.365628719329834, -0.6531322002410889]\n",
      "[1.6333136558532715, -1.7626827955245972]\n",
      "[-1.8219540119171143, -0.7730304002761841]\n",
      "[-1.3511860370635986, -0.9402385950088501]\n",
      "[0.10558927059173584, -0.5201761722564697]\n",
      "[0.5625545978546143, -0.7164613008499146]\n",
      "[-1.5522449016571045, -0.516618013381958]\n",
      "[-1.6617207527160645, -0.534311056137085]\n",
      "[0.1431446671485901, -1.392788290977478]\n",
      "[-1.3911991119384766, -1.2364526987075806]\n",
      "[1.189969539642334, -1.4199069738388062]\n",
      "[-0.22682665288448334, -1.480573296546936]\n",
      "[0.5323867797851562, -0.6906044483184814]\n",
      "[0.2236977219581604, -0.9789615869522095]\n",
      "[-1.8482677936553955, -0.7738206386566162]\n",
      "[1.830967903137207, -1.803619384765625]\n",
      "[1.8537178039550781, -1.8231184482574463]\n",
      "[-0.9929629564285278, -1.167285442352295]\n",
      "[-1.9753620624542236, -0.4774017632007599]\n",
      "[-1.1549278497695923, -0.4498918354511261]\n",
      "[0.172807514667511, -0.48281142115592957]\n",
      "[0.22121703624725342, -0.5315995216369629]\n",
      "[1.4056925773620605, -1.7275917530059814]\n",
      "[0.19173574447631836, -1.840930461883545]\n",
      "[-0.09798082709312439, -1.1833198070526123]\n",
      "[-0.035688549280166626, -1.2360140085220337]\n",
      "[-0.057623982429504395, -0.46638110280036926]\n",
      "[-1.4461731910705566, -1.2065247297286987]\n",
      "[-1.9029607772827148, -0.7802139520645142]\n",
      "[-1.264014720916748, -1.2554435729980469]\n",
      "[0.9116572737693787, -2.126437187194824]\n",
      "[0.32222050428390503, -0.6485549211502075]\n",
      "[-1.3656973838806152, -1.3165466785430908]\n",
      "[0.9713122248649597, -1.4039592742919922]\n",
      "[-0.9032773375511169, -1.3016847372055054]\n",
      "[-0.41124171018600464, -1.1227258443832397]\n",
      "[-2.1296989917755127, -0.5766911506652832]\n",
      "[-0.4991001486778259, -1.529072642326355]\n",
      "[0.031081467866897583, -1.332834243774414]\n",
      "[1.1492083072662354, -1.7410122156143188]\n",
      "[-1.9499309062957764, -0.8995580673217773]\n",
      "[1.3982350826263428, -1.432723879814148]\n",
      "[-0.659690260887146, -1.5933653116226196]\n",
      "[-1.058955192565918, -1.2445746660232544]\n",
      "[-2.0244550704956055, -0.8479009866714478]\n",
      "[-1.5576562881469727, -0.7808375358581543]\n",
      "[-0.15843117237091064, -0.43346479535102844]\n",
      "[0.17459845542907715, -0.5691790580749512]\n",
      "[-0.8888407945632935, -1.5000256299972534]\n",
      "[0.3487030863761902, -0.5482052564620972]\n",
      "[-2.0772666931152344, -0.8771069049835205]\n",
      "[-1.411557674407959, -0.6793777942657471]\n",
      "[-1.0994415283203125, -1.2815724611282349]\n",
      "[-0.3593040108680725, -1.1718558073043823]\n",
      "[0.47859370708465576, -0.644498348236084]\n",
      "[-2.153245687484741, -0.6379092931747437]\n",
      "[-0.6617761850357056, -1.0913180112838745]\n",
      "[-1.2297239303588867, -1.1062675714492798]\n",
      "[-2.193166971206665, -0.8433475494384766]\n",
      "[1.0856037139892578, -1.1647671461105347]\n",
      "[-1.248328685760498, -1.1250404119491577]\n",
      "[-1.773207426071167, -0.5206341743469238]\n",
      "[-0.5931532979011536, -0.5539687871932983]\n",
      "[-0.950076162815094, -1.4737558364868164]\n",
      "[0.5284318923950195, -0.8474797010421753]\n",
      "[0.26347941160202026, -1.5008758306503296]\n",
      "[-2.239159107208252, -0.6241623163223267]\n",
      "[-1.1490416526794434, -1.373894453048706]\n",
      "[-1.3867683410644531, -1.155756950378418]\n",
      "[0.045582354068756104, -0.47230860590934753]\n",
      "[-1.7613074779510498, -1.7834484577178955]\n",
      "[-2.4590110778808594, -1.0893104076385498]\n",
      "[0.14010876417160034, -1.4604169130325317]\n",
      "[-1.1945953369140625, -1.412905216217041]\n",
      "[-0.48875391483306885, -1.6195040941238403]\n",
      "[1.0541582107543945, -1.137815237045288]\n",
      "[-1.3212181329727173, -1.1422947645187378]\n",
      "[-1.4897103309631348, -1.1088709831237793]\n",
      "[-2.6020660400390625, -1.4028784036636353]\n",
      "[-0.6797305345535278, -1.4519836902618408]\n",
      "[2.3327765464782715, -2.233720064163208]\n",
      "[-0.7735111713409424, -1.3915627002716064]\n",
      "[-2.3883144855499268, -1.0500116348266602]\n",
      "[-1.3224050998687744, -1.4064396619796753]\n",
      "[-0.19950826466083527, -1.5091798305511475]\n",
      "[-0.722477912902832, -1.513710856437683]\n",
      "[-2.920463800430298, -1.3458218574523926]\n",
      "[-0.6515069603919983, -1.695236086845398]\n",
      "[0.7427697777748108, -1.3497920036315918]\n",
      "[-2.472200870513916, -1.096642255783081]\n",
      "[1.6633193492889404, -1.69408118724823]\n",
      "[-2.554425001144409, -1.1423487663269043]\n",
      "[-0.8287829160690308, -1.4528312683105469]\n",
      "[-1.3053793907165527, -1.464766263961792]\n",
      "[-2.182230234146118, -1.2565619945526123]\n",
      "[2.1010921001434326, -2.2290332317352295]\n",
      "[0.8786711096763611, -0.9874049425125122]\n",
      "[-1.6021747589111328, -1.6291025876998901]\n",
      "[-2.7017974853515625, -1.224269986152649]\n",
      "[0.7250441908836365, -1.3321375846862793]\n",
      "[-2.503012180328369, -1.1629900932312012]\n",
      "[0.7161791920661926, -1.6249028444290161]\n",
      "[2.0479094982147217, -1.9895603656768799]\n",
      "[-0.2555052936077118, -1.7156896591186523]\n",
      "[-0.5676729679107666, -1.431140661239624]\n",
      "[0.9769375920295715, -1.071629285812378]\n",
      "[0.9394922852516174, -1.0395349264144897]\n",
      "[0.5024937391281128, -1.4616050720214844]\n",
      "[0.48460787534713745, -1.4393326044082642]\n",
      "[-1.7753896713256836, -1.0803163051605225]\n",
      "[-1.2842011451721191, -1.694856882095337]\n",
      "[-0.7575960159301758, -1.1837059259414673]\n",
      "[-2.7575488090515137, -1.2552610635757446]\n",
      "[0.8617460131645203, -0.9728986024856567]\n",
      "[0.2579907774925232, -1.2645479440689087]\n",
      "[-1.357313632965088, -1.2316981554031372]\n",
      "[1.102754831314087, -1.2778314352035522]\n",
      "[-0.8126989603042603, -1.30694580078125]\n",
      "[-1.4420037269592285, -1.3155415058135986]\n",
      "[0.5197625756263733, -1.498578429222107]\n",
      "[1.0971901416778564, -1.314620852470398]\n",
      "[1.0537774562835693, -1.1374889612197876]\n",
      "[0.8501289486885071, -0.9629415273666382]\n",
      "[2.0547633171081543, -2.101088523864746]\n",
      "[-2.677032470703125, -1.2105036973953247]\n",
      "[-1.5233652591705322, -1.0513050556182861]\n",
      "[-1.3034329414367676, -1.1429613828659058]\n",
      "[1.1278131008148193, -1.2009449005126953]\n",
      "[-1.561115026473999, -1.5742558240890503]\n",
      "[-0.22433054447174072, -1.7097324132919312]\n",
      "[-2.3180999755859375, -1.218664288520813]\n",
      "[2.2608582973480225, -2.6543960571289062]\n",
      "[1.2101671695709229, -1.2715305089950562]\n",
      "[-2.4799952507019043, -1.290603756904602]\n",
      "[1.8315918445587158, -1.9040541648864746]\n",
      "[-2.1934452056884766, -1.1458953619003296]\n",
      "[-0.8311564922332764, -1.282180666923523]\n",
      "[0.13282954692840576, -1.553163766860962]\n",
      "[-2.6475565433502197, -1.194118618965149]\n",
      "[-1.7411251068115234, -1.0686331987380981]\n",
      "[2.1732754707336426, -2.311720371246338]\n",
      "[-0.794642984867096, -1.7117425203323364]\n",
      "[0.6146832704544067, -1.377232551574707]\n",
      "[-2.8509833812713623, -1.3071993589401245]\n",
      "[-2.2095580101013184, -1.1473442316055298]\n",
      "[-0.8496010303497314, -1.6435160636901855]\n",
      "[-0.23628012835979462, -1.474759817123413]\n",
      "[1.0679411888122559, -1.1496286392211914]\n",
      "[1.061201810836792, -1.1438523530960083]\n",
      "[-2.684708833694458, -1.214770793914795]\n",
      "[0.9422357678413391, -1.0418864488601685]\n",
      "[-1.150245189666748, -1.5118285417556763]\n",
      "[0.18482649326324463, -1.438607931137085]\n",
      "[-2.5254669189453125, -1.1262516975402832]\n",
      "[-1.1532971858978271, -1.2101624011993408]\n",
      "[1.2411634922027588, -1.2980974912643433]\n",
      "[0.14987081289291382, -1.5845823287963867]\n",
      "[-1.2590699195861816, -1.1852238178253174]\n",
      "[-2.1406803131103516, -1.0593173503875732]\n",
      "[-0.2223234921693802, -1.652923345565796]\n",
      "[1.0337226390838623, -1.1202999353408813]\n",
      "[-1.9942138195037842, -1.168054223060608]\n",
      "[0.4944542646408081, -1.4337644577026367]\n",
      "[0.7969431281089783, -1.235094666481018]\n",
      "[-2.795797824859619, -1.2765228748321533]\n",
      "[1.6522455215454102, -1.6504364013671875]\n",
      "[-2.6642353534698486, -1.203390121459961]\n",
      "[0.2509770393371582, -1.5946053266525269]\n",
      "[-2.454367160797119, -1.3055135011672974]\n",
      "[-2.252568006515503, -1.185073971748352]\n",
      "[0.9057334065437317, -1.010600209236145]\n",
      "[0.27926844358444214, -2.2070634365081787]\n",
      "[-0.9901052713394165, -1.4224185943603516]\n",
      "[-0.1251598298549652, -0.9940260648727417]\n",
      "[-0.8017363548278809, -1.2718992233276367]\n",
      "[-1.7076361179351807, -0.892164945602417]\n",
      "[0.9118780493736267, -1.015866756439209]\n",
      "[0.95371013879776, -1.6897169351577759]\n",
      "[-0.14054326713085175, -1.3878880739212036]\n",
      "[-0.3937184810638428, -1.9365942478179932]\n",
      "[-1.179670810699463, -1.0249251127243042]\n",
      "[1.5143749713897705, -1.532267451286316]\n",
      "[0.010959714651107788, -1.4309242963790894]\n",
      "[1.358386754989624, -1.8186589479446411]\n",
      "[-1.6135201454162598, -1.0738167762756348]\n",
      "[-1.2856667041778564, -0.8235105276107788]\n",
      "[-2.0757246017456055, -0.8762496709823608]\n",
      "[-0.6515069603919983, -1.695236086845398]\n",
      "[-2.553081512451172, -1.1416020393371582]\n",
      "[-0.15326698124408722, -0.7139744758605957]\n",
      "[-0.030978232622146606, -1.661291480064392]\n",
      "[1.7390360832214355, -1.8033112287521362]\n",
      "[-0.5675737857818604, -1.5903416872024536]\n",
      "[-1.5415644645690918, -0.9486755132675171]\n",
      "[-2.4067223072052, -1.0602442026138306]\n",
      "[-1.3494398593902588, -1.324022650718689]\n",
      "[1.6136784553527832, -1.8113943338394165]\n",
      "[0.6750109791755676, -0.8128478527069092]\n",
      "[-2.047576665878296, -1.2470234632492065]\n",
      "[-1.1135852336883545, -1.4564423561096191]\n",
      "[0.03822946548461914, -0.7637689113616943]\n",
      "[-1.2363148927688599, -1.416364312171936]\n",
      "[-1.6581861972808838, -0.6538329124450684]\n",
      "[2.1043853759765625, -2.037965774536133]\n",
      "[-2.311561107635498, -1.0073460340499878]\n",
      "[-0.3351338505744934, -1.2449102401733398]\n",
      "[0.6632410883903503, -0.8027597665786743]\n",
      "[0.8172513842582703, -0.9347621202468872]\n",
      "[-0.309403657913208, -0.7963920831680298]\n",
      "[-0.28773343563079834, -0.8063958883285522]\n",
      "[0.6577968001365662, -1.7048932313919067]\n",
      "[-2.1157782077789307, -1.1669033765792847]\n",
      "[1.599410057067871, -1.6733455657958984]\n",
      "[-0.013011783361434937, -1.7101123332977295]\n",
      "[0.763275682926178, -0.8884994983673096]\n",
      "[1.0692870616912842, -1.2427135705947876]\n",
      "[-1.2591193914413452, -1.0813699960708618]\n",
      "[2.0701987743377686, -2.00866436958313]\n",
      "[2.174253463745117, -2.0978498458862305]\n",
      "[-0.4274612069129944, -1.4588578939437866]\n",
      "[-0.6713900566101074, -0.7401151657104492]\n",
      "[-0.4062437117099762, -0.5805026292800903]\n",
      "[0.5688353776931763, -0.7218445539474487]\n",
      "[0.6543067097663879, -0.7951021194458008]\n",
      "[1.9991176128387451, -2.013015031814575]\n",
      "[0.23943614959716797, -2.0119166374206543]\n",
      "[0.4718390703201294, -1.5075528621673584]\n",
      "[0.554199755191803, -1.5627714395523071]\n",
      "[0.3827190399169922, -0.5926727056503296]\n",
      "[-1.9270575046539307, -1.1987873315811157]\n",
      "[-2.2894184589385986, -0.9950374364852905]\n",
      "[-1.0002014636993408, -1.4745709896087646]\n",
      "[1.7195978164672852, -2.3034491539001465]\n",
      "[0.7133708596229553, -0.8457260131835938]\n",
      "[-1.2379021644592285, -1.4821583032608032]\n",
      "[1.6102955341339111, -1.6869913339614868]\n",
      "[-0.5292301177978516, -1.562008261680603]\n",
      "[0.23014885187149048, -1.449752688407898]\n",
      "[-1.0584914684295654, -0.8028830289840698]\n",
      "[-0.2615157961845398, -1.7588157653808594]\n",
      "[0.5465145111083984, -1.642633080482483]\n",
      "[1.812981367111206, -1.9898650646209717]\n",
      "[-2.464895009994507, -1.092581033706665]\n",
      "[1.6793529987335205, -1.6736702919006348]\n",
      "[-0.6988891363143921, -1.7326353788375854]\n",
      "[-0.6980804204940796, -1.49906325340271]\n",
      "[-2.472552537918091, -1.0968376398086548]\n",
      "[-0.4644548296928406, -1.0867180824279785]\n",
      "[0.3433956503868103, -0.5662456750869751]\n",
      "[0.707851231098175, -0.840995192527771]\n",
      "[-1.1371148824691772, -1.5826131105422974]\n",
      "[0.6624776721000671, -0.8021055459976196]\n",
      "[-1.5945618152618408, -1.0781970024108887]\n",
      "[-0.07271724939346313, -0.9654639959335327]\n",
      "[-0.8674553632736206, -1.5031700134277344]\n",
      "[0.24159115552902222, -1.495349645614624]\n",
      "[0.7182286381721497, -0.8498896360397339]\n",
      "[-1.3085404634475708, -0.7610394954681396]\n",
      "[-0.06620520353317261, -1.4142308235168457]\n",
      "[-0.8187083005905151, -1.3815758228302002]\n",
      "[-2.07081937789917, -0.8735229969024658]\n",
      "[1.5034873485565186, -1.5229355096817017]\n",
      "[-0.8465529084205627, -1.3886663913726807]\n",
      "[-0.38490933179855347, -0.7849123477935791]\n",
      "[0.6315889954566956, -0.8330937623977661]\n",
      "[-0.9044367671012878, -1.6353875398635864]\n",
      "[0.9146211743354797, -1.018217921257019]\n",
      "[0.6151928901672363, -1.7835859060287476]\n",
      "[-1.3526315689086914, -0.7210369110107422]\n",
      "[-1.2409427165985107, -1.482267141342163]\n",
      "[-1.207078456878662, -1.3535603284835815]\n",
      "[0.5483958125114441, -0.704325795173645]\n",
      "[2.0780179500579834, -2.0153660774230957]\n",
      "[0.03905743360519409, -0.9779620170593262]\n",
      "[-1.2369859218597412, -0.30626702308654785]\n",
      "[-1.2461421489715576, -0.622989296913147]\n",
      "[-1.3776264190673828, -0.4424176514148712]\n",
      "[0.1858392357826233, -0.8174828290939331]\n",
      "[1.0396647453308105, -1.2139573097229004]\n",
      "[-0.2658202350139618, -0.7986758947372437]\n",
      "[1.436492919921875, -1.6464300155639648]\n",
      "[-1.6007728576660156, -0.416360467672348]\n",
      "[-0.35364872217178345, -1.149409532546997]\n",
      "[-0.9668216109275818, -0.6720049381256104]\n",
      "[1.5686671733856201, -1.578801155090332]\n",
      "[-0.9775887727737427, -0.6147031784057617]\n",
      "[-1.591416358947754, -0.4043474793434143]\n",
      "[-1.3830695152282715, -0.46520909667015076]\n",
      "[0.7427697777748108, -1.3497920036315918]\n",
      "[-0.15326692163944244, -0.7139744758605957]\n",
      "[-1.0460259914398193, -0.1985006034374237]\n",
      "[0.6592743992805481, -1.1587321758270264]\n",
      "[-0.5266929268836975, -1.2181439399719238]\n",
      "[0.5853315591812134, -1.160738468170166]\n",
      "[-1.7103691101074219, -0.4405989348888397]\n",
      "[-0.12445303797721863, -0.7488782405853271]\n",
      "[0.04523149132728577, -0.9312120676040649]\n",
      "[-0.7017464637756348, -1.3236308097839355]\n",
      "[-0.3701152503490448, -0.47489461302757263]\n",
      "[0.6350824236869812, -1.1788651943206787]\n",
      "[0.22178775072097778, -1.1041499376296997]\n",
      "[-1.1191551685333252, -0.268912672996521]\n",
      "[0.13864201307296753, -1.0365204811096191]\n",
      "[-0.18100324273109436, -0.6014819145202637]\n",
      "[0.29808902740478516, -1.4295521974563599]\n",
      "[0.11759614944458008, -0.7491614818572998]\n",
      "[-1.2732019424438477, -0.5302196741104126]\n",
      "[-0.4023815989494324, -0.39570146799087524]\n",
      "[0.110918790102005, -0.7174279689788818]\n",
      "[-1.1717486381530762, -0.25098395347595215]\n",
      "[-1.2171108722686768, -0.2776278853416443]\n",
      "[0.8627389073371887, -1.1337987184524536]\n",
      "[0.7172144055366516, -1.1529332399368286]\n",
      "[1.3628506660461426, -1.4023957252502441]\n",
      "[1.047562837600708, -1.3010390996932983]\n",
      "[-0.2832232415676117, -0.6461631059646606]\n",
      "[0.5528832674026489, -0.8128467798233032]\n",
      "[-1.6381828784942627, -0.5040481090545654]\n",
      "[1.6218914985656738, -1.770565152168274]\n",
      "[1.8633341789245605, -1.8313605785369873]\n",
      "[-0.8480441570281982, -0.7874515056610107]\n",
      "[-1.2584041357040405, -0.2876015305519104]\n",
      "[-0.972452700138092, -0.21873006224632263]\n",
      "[-0.540191650390625, -0.3907076418399811]\n",
      "[-0.5057368278503418, -0.41437914967536926]\n",
      "[-0.12052863836288452, -1.445691466331482]\n",
      "[1.5431339740753174, -1.612141489982605]\n",
      "[0.6867467761039734, -0.9847177267074585]\n",
      "[0.3051455616950989, -0.9480479955673218]\n",
      "[-0.39194196462631226, -0.3636842370033264]\n",
      "[0.6066799759864807, -1.1044718027114868]\n",
      "[-0.1030682921409607, -0.664084792137146]\n",
      "[0.15951603651046753, -1.0259654521942139]\n",
      "[-0.6891812682151794, -1.8726847171783447]\n",
      "[-1.105987548828125, -0.38337698578834534]\n",
      "[0.26888298988342285, -1.114102840423584]\n",
      "[-0.6737951040267944, -1.1336336135864258]\n",
      "[0.20627468824386597, -1.0264843702316284]\n",
      "[-0.731690526008606, -0.7184371948242188]\n",
      "[-1.4541621208190918, -0.3475751280784607]\n",
      "[0.7201836705207825, -1.304569125175476]\n",
      "[0.863281786441803, -1.124693751335144]\n",
      "[-0.5000154972076416, -1.4703527688980103]\n",
      "[0.1378524899482727, -0.8327585458755493]\n",
      "[1.2898974418640137, -1.3398674726486206]\n",
      "[0.9409219622612, -1.4176132678985596]\n",
      "[0.08333107829093933, -0.979395866394043]\n",
      "[0.060896217823028564, -0.7902929782867432]\n",
      "[-1.4117822647094727, -0.38696181774139404]\n",
      "[-0.5172834992408752, -0.322741836309433]\n",
      "[-0.23887526988983154, -0.39695149660110474]\n",
      "[0.9068862795829773, -1.398179292678833]\n",
      "[-0.673092782497406, -0.45129087567329407]\n",
      "[-0.9558061361312866, -0.6165437698364258]\n",
      "[-1.1914993524551392, -0.28997692465782166]\n",
      "[0.27964097261428833, -1.0793437957763672]\n",
      "[-0.28295132517814636, -0.8266335725784302]\n",
      "[-0.6417056322097778, -0.5195579528808594]\n",
      "[-1.4700050354003906, -0.34423062205314636]\n",
      "[-0.6546033620834351, -0.7449992895126343]\n",
      "[-0.4453354477882385, -0.8560811281204224]\n",
      "[-0.9449976086616516, -0.4903477728366852]\n",
      "[0.10467952489852905, -0.9255071878433228]\n",
      "[-0.3056551218032837, -0.8577477931976318]\n",
      "[-1.2541310787200928, -0.29941725730895996]\n",
      "[-0.47328102588653564, -0.24985110759735107]\n",
      "[0.6263734102249146, -1.2940142154693604]\n",
      "[-0.7689594030380249, -0.6319141387939453]\n",
      "[1.3358204364776611, -1.379228115081787]\n",
      "[-1.501852035522461, -0.3591998219490051]\n",
      "[0.643819272518158, -1.2047549486160278]\n",
      "[0.17021387815475464, -0.9695736169815063]\n",
      "[-0.8002331256866455, -0.3295840919017792]\n",
      "[-0.5804953575134277, -1.8906619548797607]\n",
      "[-2.3505871295928955, -1.0290398597717285]\n",
      "[-0.03210219740867615, -1.189290165901184]\n",
      "[-1.3708291053771973, -1.1105239391326904]\n",
      "[-0.2600817084312439, -1.4704853296279907]\n",
      "[0.6884452700614929, -0.8243622779846191]\n",
      "[-2.2498860359191895, -0.97306227684021]\n",
      "[-2.16929292678833, -0.9282623529434204]\n",
      "[-1.320432186126709, -1.5482749938964844]\n",
      "[-0.666854977607727, -1.2217495441436768]\n",
      "[1.9836125373840332, -2.203842878341675]\n",
      "[-1.5710914134979248, -0.9218772649765015]\n",
      "[-2.668715476989746, -1.2058805227279663]\n",
      "[-1.0592882633209229, -1.2475382089614868]\n",
      "[-0.20221728086471558, -1.3010388612747192]\n",
      "[-0.38927045464515686, -1.3912216424942017]\n",
      "[-2.472200870513916, -1.096642255783081]\n",
      "[-0.030977696180343628, -1.6612913608551025]\n",
      "[0.6592745184898376, -1.1587321758270264]\n",
      "[-3.0237653255462646, -1.403244972229004]\n",
      "[0.6351611018180847, -1.3149522542953491]\n",
      "[-2.3411126136779785, -1.0237730741500854]\n",
      "[-0.7522512674331665, -1.2444278001785278]\n",
      "[-0.5733645558357239, -1.4557946920394897]\n",
      "[-1.646284818649292, -1.1724644899368286]\n",
      "[1.3746757507324219, -2.0178511142730713]\n",
      "[0.8294082283973694, -1.0627851486206055]\n",
      "[-0.5731553435325623, -1.703721284866333]\n",
      "[-2.4117062091827393, -1.0630145072937012]\n",
      "[0.551239550113678, -1.1212834119796753]\n",
      "[-1.8864896297454834, -1.0982787609100342]\n",
      "[1.1157565116882324, -1.5749050378799438]\n",
      "[1.3035905361175537, -1.5697705745697021]\n",
      "[0.4112936854362488, -1.703782558441162]\n",
      "[-0.9353463053703308, -1.0731956958770752]\n",
      "[0.8533515334129333, -1.063405156135559]\n",
      "[0.8612648844718933, -0.972486138343811]\n",
      "[0.39450907707214355, -1.2654471397399902]\n",
      "[0.4010184407234192, -1.246836543083191]\n",
      "[-2.5476598739624023, -1.1385881900787354]\n",
      "[-0.2700907289981842, -1.7659276723861694]\n",
      "[-1.467897891998291, -0.8851968050003052]\n",
      "[-2.827721118927002, -1.294268250465393]\n",
      "[1.0897183418273926, -1.2178775072097778]\n",
      "[-0.4662931561470032, -0.8556292057037354]\n",
      "[-1.4866023063659668, -0.957361102104187]\n",
      "[-0.02658778429031372, -0.8665590286254883]\n",
      "[-1.214979887008667, -1.1682173013687134]\n",
      "[-2.0073118209838867, -0.8943006992340088]\n",
      "[0.5101045370101929, -1.3264974355697632]\n",
      "[1.1269960403442383, -1.2002445459365845]\n",
      "[1.2694568634033203, -1.3223479986190796]\n",
      "[0.8729068636894226, -1.0699529647827148]\n",
      "[1.2289490699768066, -1.7547972202301025]\n",
      "[-2.380913496017456, -1.0917730331420898]\n",
      "[-2.2700917720794678, -0.9842941761016846]\n",
      "[-2.211366653442383, -0.9516500234603882]\n",
      "[1.0405659675598145, -1.1261652708053589]\n",
      "[-0.5917418003082275, -1.630106806755066]\n",
      "[0.3422399163246155, -1.6732131242752075]\n",
      "[-1.9467649459838867, -1.076019287109375]\n",
      "[1.2753865718841553, -2.4865269660949707]\n",
      "[1.0668706893920898, -1.2127482891082764]\n",
      "[-1.7606213092803955, -1.262571096420288]\n",
      "[1.0309572219848633, -1.6156247854232788]\n",
      "[-2.4019486904144287, -1.0575906038284302]\n",
      "[-1.932459831237793, -0.7966119050979614]\n",
      "[0.17515122890472412, -1.385245442390442]\n",
      "[-2.7196273803710938, -1.23418128490448]\n",
      "[-2.4923715591430664, -1.1078547239303589]\n",
      "[1.3843541145324707, -2.0712692737579346]\n",
      "[-0.06779792904853821, -1.705366611480713]\n",
      "[-0.23342902958393097, -0.9711477756500244]\n",
      "[-1.9044272899627686, -1.22314453125]\n",
      "[-2.2899558544158936, -0.9953361749649048]\n",
      "[-0.1181761622428894, -1.6388304233551025]\n",
      "[-0.3649841248989105, -1.2021434307098389]\n",
      "[1.0668079853057861, -1.1486574411392212]\n",
      "[0.7372190356254578, -0.9163656234741211]\n",
      "[-1.4982032775878906, -1.3509851694107056]\n",
      "[1.3515267372131348, -1.3926899433135986]\n",
      "[-0.8955353498458862, -1.356066107749939]\n",
      "[-0.02536788582801819, -1.15665602684021]\n",
      "[-2.313614845275879, -1.0084877014160156]\n",
      "[-2.0932867527008057, -0.886012077331543]\n",
      "[1.65189790725708, -1.6501383781433105]\n",
      "[0.2731092572212219, -1.439961314201355]\n",
      "[-2.1018340587615967, -0.8907632827758789]\n",
      "[-2.380338668823242, -1.0455780029296875]\n",
      "[0.1588364839553833, -1.5633212327957153]\n",
      "[0.4542163610458374, -0.6236045360565186]\n",
      "[-2.1442861557006836, -0.9143614768981934]\n",
      "[0.31240642070770264, -1.2184282541275024]\n",
      "[0.5887633562088013, -1.0083757638931274]\n",
      "[-1.9093079566955566, -1.1852567195892334]\n",
      "[1.704310655593872, -1.775625228881836]\n",
      "[-3.09102463722229, -1.440632939338684]\n",
      "[0.373019814491272, -1.4556396007537842]\n",
      "[-1.405527114868164, -1.3810335397720337]\n",
      "[-1.683398723602295, -1.1077607870101929]\n",
      "[1.263737678527832, -1.3174458742141724]\n",
      "[2.925008773803711, -2.810853958129883]\n",
      "[1.09425687789917, -1.387766718864441]\n",
      "[-0.5336253643035889, -1.145446538925171]\n",
      "[0.3317999243736267, -1.2662789821624756]\n",
      "[0.7947779297828674, -1.4907552003860474]\n",
      "[-2.346376657485962, -0.6536270380020142]\n",
      "[-0.5064661502838135, -0.984139084815979]\n",
      "[-0.19124335050582886, -1.1066029071807861]\n",
      "[2.337989568710327, -2.2991349697113037]\n",
      "[0.23252207040786743, -1.3000798225402832]\n",
      "[-2.1456544399261475, -1.5403648614883423]\n",
      "[-0.3168652057647705, -1.0747270584106445]\n",
      "[-0.16517937183380127, -1.095262885093689]\n",
      "[0.842097818851471, -1.4543070793151855]\n",
      "[0.15670067071914673, -1.1830835342407227]\n",
      "[1.0200719833374023, -1.4284809827804565]\n",
      "[1.6633195877075195, -1.694081425666809]\n",
      "[1.7390360832214355, -1.8033112287521362]\n",
      "[-0.5266920924186707, -1.2181435823440552]\n",
      "[0.6351611018180847, -1.3149522542953491]\n",
      "[-3.670471668243408, -0.9083089828491211]\n",
      "[1.1455132961273193, -1.600049614906311]\n",
      "[0.5394977331161499, -1.2183308601379395]\n",
      "[1.6772093772888184, -1.7389252185821533]\n",
      "[1.2800936698913574, -1.5621997117996216]\n",
      "[-3.3366427421569824, -1.1768839359283447]\n",
      "[-2.100476026535034, -0.8029447793960571]\n",
      "[2.225782632827759, -2.1420156955718994]\n",
      "[1.4550046920776367, -1.488376498222351]\n",
      "[-0.7052100300788879, -1.225278615951538]\n",
      "[1.382357120513916, -1.5940423011779785]\n",
      "[1.628983736038208, -1.75591242313385]\n",
      "[-3.5711257457733154, -0.7799901962280273]\n",
      "[1.907501220703125, -1.9255423545837402]\n",
      "[-0.31822264194488525, -1.0860093832015991]\n",
      "[-1.898979902267456, -0.840901255607605]\n",
      "[-2.3186609745025635, -0.6974896192550659]\n",
      "[-0.23202741146087646, -1.3497035503387451]\n",
      "[-0.40343910455703735, -1.2452666759490967]\n",
      "[-0.17151916027069092, -1.102346420288086]\n",
      "[2.31037974357605, -2.2145237922668457]\n",
      "[-0.928896963596344, -0.9402405023574829]\n",
      "[1.0357773303985596, -1.4859063625335693]\n",
      "[-2.450312614440918, -0.7710789442062378]\n",
      "[-1.392888069152832, -0.8035120964050293]\n",
      "[0.38517695665359497, -1.1356207132339478]\n",
      "[-1.3991031646728516, -1.0774670839309692]\n",
      "[-0.4094018340110779, -1.365698218345642]\n",
      "[0.2783685326576233, -1.2495851516723633]\n",
      "[0.04222416877746582, -1.3445097208023071]\n",
      "[-0.21748805046081543, -1.3755345344543457]\n",
      "[-1.1471266746520996, -1.4151471853256226]\n",
      "[-2.113003969192505, -0.8208131790161133]\n",
      "[-3.8594765663146973, -0.8472574949264526]\n",
      "[1.8581383228302002, -1.9463539123535156]\n",
      "[-0.5690685510635376, -0.9332057237625122]\n",
      "[-0.53236985206604, -0.9449253082275391]\n",
      "[-0.7615354657173157, -1.451780080795288]\n",
      "[2.006418466567993, -2.108370780944824]\n",
      "[1.6757125854492188, -1.7952014207839966]\n",
      "[1.1742174625396729, -1.5804204940795898]\n",
      "[-3.790316581726074, -1.4588416814804077]\n",
      "[-2.1100077629089355, -0.9704644680023193]\n",
      "[1.5344383716583252, -1.7155686616897583]\n",
      "[-3.6219322681427, -0.792218804359436]\n",
      "[0.8035939335823059, -1.4323830604553223]\n",
      "[-0.5637714862823486, -0.9322777986526489]\n",
      "[0.18804854154586792, -1.3358368873596191]\n",
      "[1.2705132961273193, -1.5361220836639404]\n",
      "[-0.23802149295806885, -1.1124480962753296]\n",
      "[-3.6686136722564697, -1.0681157112121582]\n",
      "[1.8919384479522705, -1.9378957748413086]\n",
      "[-1.2974934577941895, -0.9998877048492432]\n",
      "[1.898080587387085, -1.9318346977233887]\n",
      "[0.8463509678840637, -1.4229708909988403]\n",
      "[1.8579282760620117, -1.8903530836105347]\n",
      "[-0.3224559426307678, -1.196081519126892]\n",
      "[-0.9142822623252869, -1.3780635595321655]\n",
      "[-1.8565173149108887, -0.9686037302017212]\n",
      "[2.125908851623535, -2.0564136505126953]\n",
      "[-1.9994890689849854, -1.0376379489898682]\n",
      "[0.8899055123329163, -1.505314588546753]\n",
      "[-0.5678858160972595, -1.1229908466339111]\n",
      "[1.180837869644165, -1.5293138027191162]\n",
      "[-0.43118369579315186, -1.019212245941162]\n",
      "[-1.6753473281860352, -1.28325355052948]\n",
      "[0.453535258769989, -1.4308911561965942]\n",
      "[-0.2866363525390625, -1.034610629081726]\n",
      "[0.6875320076942444, -1.2382460832595825]\n",
      "[1.2514374256134033, -1.5909323692321777]\n",
      "[-2.7041282653808594, -0.6199526786804199]\n",
      "[0.6986517310142517, -1.3679989576339722]\n",
      "[-0.31401151418685913, -1.2527157068252563]\n",
      "[-1.2687511444091797, -0.9979891777038574]\n",
      "[1.684800386428833, -1.818362832069397]\n",
      "[-1.8009231090545654, -1.3392364978790283]\n",
      "[0.505662739276886, -1.2126456499099731]\n",
      "[0.6243417859077454, -1.4468151330947876]\n",
      "[1.7695214748382568, -1.9311885833740234]\n",
      "[1.1768608093261719, -1.6137173175811768]\n",
      "[-1.7075726985931396, -1.0848591327667236]\n",
      "[-1.0206365585327148, -1.8999637365341187]\n",
      "[-2.461840867996216, -1.0908832550048828]\n",
      "[-0.05145221948623657, -1.2804195880889893]\n",
      "[-1.3908040523529053, -1.2442357540130615]\n",
      "[-0.5739040374755859, -1.468127727508545]\n",
      "[1.1645195484161377, -1.2324060201644897]\n",
      "[-1.2513185739517212, -1.0475480556488037]\n",
      "[-1.5616846084594727, -0.9682794809341431]\n",
      "[-1.8493890762329102, -1.5406090021133423]\n",
      "[-0.834037184715271, -1.290204644203186]\n",
      "[2.0984413623809814, -2.217099905014038]\n",
      "[-0.9729140400886536, -1.2263927459716797]\n",
      "[-1.863983392715454, -1.0158617496490479]\n",
      "[-1.4186780452728271, -1.2647671699523926]\n",
      "[-0.34464597702026367, -1.3389250040054321]\n",
      "[-0.7718780040740967, -1.37740159034729]\n",
      "[-2.554424524307251, -1.1423486471176147]\n",
      "[-0.567574143409729, -1.5903418064117432]\n",
      "[0.5853318572044373, -1.160738468170166]\n",
      "[-2.3411126136779785, -1.0237730741500854]\n",
      "[1.1455128192901611, -1.6000500917434692]\n",
      "[-2.5525920391082764, -1.1413298845291138]\n",
      "[-0.9643886089324951, -1.2963471412658691]\n",
      "[-1.1740367412567139, -1.3852897882461548]\n",
      "[-2.2102413177490234, -1.1430706977844238]\n",
      "[1.6755845546722412, -2.114426851272583]\n",
      "[0.928053081035614, -1.0297304391860962]\n",
      "[-1.2695364952087402, -1.6094945669174194]\n",
      "[-2.6094541549682617, -1.172938346862793]\n",
      "[0.5480945110321045, -1.1398836374282837]\n",
      "[-2.4718668460845947, -1.096456527709961]\n",
      "[0.7294067740440369, -1.475719690322876]\n",
      "[1.7191803455352783, -1.930126667022705]\n",
      "[-0.1375247836112976, -1.611730933189392]\n",
      "[-0.7733218669891357, -1.259054183959961]\n",
      "[0.8429394364356995, -0.9567794799804688]\n",
      "[1.0516979694366455, -1.1357066631317139]\n",
      "[0.3602892756462097, -1.2664856910705566]\n",
      "[0.3177341818809509, -1.2538114786148071]\n",
      "[-1.6911718845367432, -0.9931963682174683]\n",
      "[-0.9594483375549316, -1.6662132740020752]\n",
      "[-0.2996275722980499, -1.2119905948638916]\n",
      "[-2.4592678546905518, -1.089453101158142]\n",
      "[1.235713243484497, -1.2970452308654785]\n",
      "[0.44472944736480713, -1.200545310974121]\n",
      "[-1.5282566547393799, -1.0732693672180176]\n",
      "[1.2710812091827393, -1.3237401247024536]\n",
      "[-0.03179338574409485, -1.441022276878357]\n",
      "[-1.644880771636963, -1.1514935493469238]\n",
      "[0.40248507261276245, -1.3105641603469849]\n",
      "[0.963190495967865, -1.1298071146011353]\n",
      "[0.9939867854118347, -1.0862421989440918]\n",
      "[0.8486021161079407, -0.9616328477859497]\n",
      "[1.63932466506958, -2.003216505050659]\n",
      "[-1.8791930675506592, -1.3192495107650757]\n",
      "[-1.5048258304595947, -0.942068338394165]\n",
      "[-1.3100121021270752, -1.0243985652923584]\n",
      "[0.9329069256782532, -1.0338906049728394]\n",
      "[-1.274923324584961, -1.5388681888580322]\n",
      "[-0.15399524569511414, -1.5889283418655396]\n",
      "[-2.444977283477783, -1.0815093517303467]\n",
      "[1.6191120147705078, -2.614534616470337]\n",
      "[1.2385926246643066, -1.3207648992538452]\n",
      "[-2.421189069747925, -1.2061959505081177]\n",
      "[1.4017009735107422, -1.7805501222610474]\n",
      "[-2.3445522785186768, -1.0256850719451904]\n",
      "[-0.8979446887969971, -1.1429861783981323]\n",
      "[0.0034293830394744873, -1.380066990852356]\n",
      "[-2.4620509147644043, -1.0910000801086426]\n",
      "[-1.6865711212158203, -0.971322774887085]\n",
      "[1.7428529262542725, -2.1906213760375977]\n",
      "[-0.6553003787994385, -1.6228547096252441]\n",
      "[0.8962242007255554, -1.3390631675720215]\n",
      "[-2.3889176845550537, -1.220214605331421]\n",
      "[-2.3590610027313232, -1.0337501764297485]\n",
      "[-0.7091318964958191, -1.5576032400131226]\n",
      "[-0.42626917362213135, -1.301085114479065]\n",
      "[0.8796785473823547, -0.9882684946060181]\n",
      "[0.8127643465995789, -0.9309163093566895]\n",
      "[-2.061913013458252, -1.314754843711853]\n",
      "[1.2210688591003418, -1.280874490737915]\n",
      "[-1.244713306427002, -1.3669006824493408]\n",
      "[-0.006514608860015869, -1.257954478263855]\n",
      "[-2.519090175628662, -1.1227070093154907]\n",
      "[-1.2003201246261597, -1.0781358480453491]\n",
      "[1.520998239517212, -1.5379441976547241]\n",
      "[0.04856196045875549, -1.416089653968811]\n",
      "[-1.3281420469284058, -1.0457375049591064]\n",
      "[-2.255966901779175, -0.9764423370361328]\n",
      "[-0.23966553807258606, -1.5127594470977783]\n",
      "[0.9419334530830383, -1.041627287864685]\n",
      "[-2.1912641525268555, -1.0065903663635254]\n",
      "[0.34053951501846313, -1.238609790802002]\n",
      "[0.5935071706771851, -1.0381507873535156]\n",
      "[-2.5798468589782715, -1.156480312347412]\n",
      "[1.7543609142303467, -1.737959623336792]\n",
      "[-2.4208054542541504, -1.0680726766586304]\n",
      "[0.17183667421340942, -1.4182075262069702]\n",
      "[-2.116978168487549, -1.3032846450805664]\n",
      "[-2.274867057800293, -1.0757701396942139]\n",
      "[1.061549186706543, -1.144149899482727]\n",
      "[0.6517857909202576, -2.15826153755188]\n",
      "[-1.5081608295440674, -1.0905872583389282]\n",
      "[-1.5197441577911377, -0.711387038230896]\n",
      "[-1.6737396717071533, -0.9198741912841797]\n",
      "[-2.087761402130127, -0.762840986251831]\n",
      "[0.3252696394920349, -0.661629319190979]\n",
      "[-0.1887129247188568, -1.1461306810379028]\n",
      "[-1.1997942924499512, -0.9009852409362793]\n",
      "[-0.1806274950504303, -1.8433666229248047]\n",
      "[-2.0403292179107666, -0.7718759775161743]\n",
      "[0.7886071801185608, -1.2676215171813965]\n",
      "[-1.138617753982544, -1.0000596046447754]\n",
      "[-0.014510899782180786, -1.248457908630371]\n",
      "[-2.1025140285491943, -0.8911412954330444]\n",
      "[-2.2362494468688965, -0.665006160736084]\n",
      "[-2.214190721511841, -0.7891961336135864]\n",
      "[-0.8287829756736755, -1.4528310298919678]\n",
      "[-1.5415644645690918, -0.9486755132675171]\n",
      "[-1.7103691101074219, -0.4405989348888397]\n",
      "[-0.752251386642456, -1.2444279193878174]\n",
      "[0.5394977331161499, -1.2183308601379395]\n",
      "[-0.9643884897232056, -1.2963472604751587]\n",
      "[-2.2136878967285156, -0.8153946399688721]\n",
      "[-1.696246862411499, -0.9449946880340576]\n",
      "[-1.6290647983551025, -1.0969501733779907]\n",
      "[0.5548188090324402, -1.454390525817871]\n",
      "[-0.3404655456542969, -0.43799206614494324]\n",
      "[-1.040939450263977, -1.3883543014526367]\n",
      "[-1.3674359321594238, -1.1965656280517578]\n",
      "[-1.711723804473877, -0.4714840352535248]\n",
      "[-1.4999241828918457, -1.1738054752349854]\n",
      "[-1.1331849098205566, -0.752301812171936]\n",
      "[1.2582123279571533, -1.5143738985061646]\n",
      "[-1.148031234741211, -0.9504544734954834]\n",
      "[-1.4737966060638428, -0.881313681602478]\n",
      "[-0.5335588455200195, -0.42827990651130676]\n",
      "[-0.06807255744934082, -0.5108492374420166]\n",
      "[-1.7666723728179932, -0.5133757591247559]\n",
      "[-1.8031806945800781, -0.5334459543228149]\n",
      "[-0.3746263384819031, -1.1957160234451294]\n",
      "[-0.8990728259086609, -1.3721407651901245]\n",
      "[0.36723238229751587, -1.1706868410110474]\n",
      "[-0.472532719373703, -1.3697106838226318]\n",
      "[-0.03090953826904297, -0.5036555528640747]\n",
      "[-0.8068097233772278, -0.7647632360458374]\n",
      "[-2.043227434158325, -0.8581851720809937]\n",
      "[1.515568733215332, -1.5332906246185303]\n",
      "[1.1856105327606201, -1.578416109085083]\n",
      "[-1.499596357345581, -1.004523515701294]\n",
      "[-1.919825553894043, -0.5219603776931763]\n",
      "[-1.4127554893493652, -0.4274461567401886]\n",
      "[-0.08323335647583008, -0.4550040066242218]\n",
      "[-0.3590799868106842, -0.4486362040042877]\n",
      "[0.9997215867042542, -1.513407588005066]\n",
      "[0.06243187189102173, -1.7737683057785034]\n",
      "[-0.6650196313858032, -0.9770983457565308]\n",
      "[-0.606799304485321, -1.0316736698150635]\n",
      "[-0.227609783411026, -0.443699449300766]\n",
      "[-1.0593762397766113, -1.3123425245285034]\n",
      "[-1.3288466930389404, -0.8752083778381348]\n",
      "[-1.4526703357696533, -1.1837654113769531]\n",
      "[0.56207275390625, -1.9943370819091797]\n",
      "[-0.19723370671272278, -0.45396479964256287]\n",
      "[-1.41817307472229, -1.2893444299697876]\n",
      "[0.4964028596878052, -1.2247413396835327]\n",
      "[-1.2429256439208984, -1.1771876811981201]\n",
      "[-1.0076899528503418, -0.9264131784439087]\n",
      "[-2.039368152618408, -0.6330565214157104]\n",
      "[-0.7511613965034485, -1.4220337867736816]\n",
      "[-0.48174190521240234, -1.1418824195861816]\n",
      "[0.7360491156578064, -1.5909284353256226]\n",
      "[-1.3333072662353516, -1.066264271736145]\n",
      "[0.7974284291267395, -1.11797297000885]\n",
      "[-0.678292989730835, -1.565461277961731]\n",
      "[-1.3702325820922852, -1.1310789585113525]\n",
      "[-1.407106876373291, -1.0145734548568726]\n",
      "[-1.6771891117095947, -0.7538223266601562]\n",
      "[-0.37477514147758484, -0.4048406779766083]\n",
      "[-0.48569223284721375, -0.477693110704422]\n",
      "[-0.7474006414413452, -1.5206013917922974]\n",
      "[-0.16529467701911926, -0.4786301553249359]\n",
      "[-2.037130832672119, -0.8547961711883545]\n",
      "[-1.5295658111572266, -0.6795313358306885]\n",
      "[-1.2909431457519531, -1.2012946605682373]\n",
      "[-0.9285191297531128, -0.9775689840316772]\n",
      "[0.18407899141311646, -0.5619734525680542]\n",
      "[-2.0549168586730957, -0.6286147832870483]\n",
      "[-1.21946120262146, -0.9017447233200073]\n",
      "[-1.5854017734527588, -0.9734126329421997]\n",
      "[-2.0828030109405518, -0.7168669700622559]\n",
      "[0.7283243536949158, -0.8585426807403564]\n",
      "[-1.573815107345581, -1.009347915649414]\n",
      "[-1.9036586284637451, -0.5205469131469727]\n",
      "[-1.4811406135559082, -0.42718634009361267]\n",
      "[-1.0038312673568726, -1.4381258487701416]\n",
      "[0.25735217332839966, -0.7610300779342651]\n",
      "[-0.27292484045028687, -1.3091431856155396]\n",
      "[-2.1253228187561035, -0.6159138679504395]\n",
      "[-1.0584566593170166, -1.3894200325012207]\n",
      "[-1.473557710647583, -1.113681674003601]\n",
      "[-0.39984315633773804, -0.42019784450531006]\n",
      "[-0.38189107179641724, -2.035745859146118]\n",
      "[-1.5552911758422852, -1.2282044887542725]\n",
      "[-0.28026896715164185, -1.008218765258789]\n",
      "[-1.1728925704956055, -1.2090991735458374]\n",
      "[-1.752744436264038, -0.9353415966033936]\n",
      "[0.9399651885032654, -1.0399402379989624]\n",
      "[0.5476202368736267, -1.5332087278366089]\n",
      "[-0.5508502721786499, -1.2431682348251343]\n",
      "[-1.0765966176986694, -1.7374039888381958]\n",
      "[-1.3665461540222168, -1.0042901039123535]\n",
      "[1.7316968441009521, -1.718534231185913]\n",
      "[-0.32699671387672424, -1.3521531820297241]\n",
      "[0.5327928066253662, -1.5600643157958984]\n",
      "[-1.9924914836883545, -0.9892117977142334]\n",
      "[-1.210191011428833, -0.8778407573699951]\n",
      "[-2.106558084487915, -0.8933893442153931]\n",
      "[-1.305379033088684, -1.4647663831710815]\n",
      "[-2.406722068786621, -1.060244083404541]\n",
      "[-0.12445300817489624, -0.7488783597946167]\n",
      "[-0.5733647346496582, -1.4557946920394897]\n",
      "[1.6772093772888184, -1.7389252185821533]\n",
      "[-1.1740367412567139, -1.3852897882461548]\n",
      "[-1.69624662399292, -0.9449945688247681]\n",
      "[-2.6845972537994385, -1.2147088050842285]\n",
      "[-1.9463214874267578, -1.15382719039917]\n",
      "[1.7150869369506836, -1.9234848022460938]\n",
      "[0.6304757595062256, -0.7746766805648804]\n",
      "[-2.5741426944732666, -1.1533094644546509]\n",
      "[-1.7428979873657227, -1.237499713897705]\n",
      "[-0.014341652393341064, -0.7854406833648682]\n",
      "[-1.845109462738037, -1.2220183610916138]\n",
      "[-1.2742843627929688, -0.765205979347229]\n",
      "[2.1345596313476562, -2.063828229904175]\n",
      "[-2.1815850734710693, -0.9350951910018921]\n",
      "[-0.5940923690795898, -1.205293893814087]\n",
      "[0.6218824982643127, -0.7673113346099854]\n",
      "[0.7799221873283386, -0.9027673006057739]\n",
      "[-0.3144060969352722, -0.8443901538848877]\n",
      "[-0.3041183650493622, -0.8464869260787964]\n",
      "[0.21646928787231445, -1.5327236652374268]\n",
      "[-2.5208888053894043, -1.1237068176269531]\n",
      "[1.2552969455718994, -1.5594942569732666]\n",
      "[-0.6185413002967834, -1.4830554723739624]\n",
      "[0.7509438395500183, -0.8779299259185791]\n",
      "[0.8626348376274109, -1.1708675622940063]\n",
      "[-1.5851452350616455, -0.9816617965698242]\n",
      "[1.9301190376281738, -1.8886018991470337]\n",
      "[1.7818455696105957, -1.8818261623382568]\n",
      "[-0.8444084525108337, -1.3408699035644531]\n",
      "[-0.5856993794441223, -0.8080238103866577]\n",
      "[-0.22838957607746124, -0.6562144756317139]\n",
      "[0.6360146999359131, -0.7794240713119507]\n",
      "[0.6286017894744873, -0.7730704545974731]\n",
      "[1.9928226470947266, -2.054267644882202]\n",
      "[-0.4107266664505005, -1.7891182899475098]\n",
      "[0.0846230685710907, -1.3575704097747803]\n",
      "[0.1754523515701294, -1.4207462072372437]\n",
      "[0.5162608623504639, -0.6767829656600952]\n",
      "[-2.504502773284912, -1.1145981550216675]\n",
      "[-2.127192258834839, -0.9048594236373901]\n",
      "[-1.5743284225463867, -1.2967220544815063]\n",
      "[1.8121206760406494, -2.3861639499664307]\n",
      "[0.7781369090080261, -0.9012371301651001]\n",
      "[-1.8778042793273926, -1.3046561479568481]\n",
      "[1.6272056102752686, -1.7420827150344849]\n",
      "[-1.059561014175415, -1.3819681406021118]\n",
      "[-0.08891382813453674, -1.3439947366714478]\n",
      "[-0.9997557401657104, -0.869246244430542]\n",
      "[-0.8609961271286011, -1.5435893535614014]\n",
      "[0.10759130120277405, -1.4739127159118652]\n",
      "[1.8774559497833252, -2.0752503871917725]\n",
      "[-2.520596742630005, -1.123544454574585]\n",
      "[1.5328786373138428, -1.5481268167495728]\n",
      "[-1.3743433952331543, -1.5124003887176514]\n",
      "[-1.230952501296997, -1.3219842910766602]\n",
      "[-2.5147767066955566, -1.1203092336654663]\n",
      "[-0.6453452110290527, -1.0821127891540527]\n",
      "[0.462979793548584, -0.6311156749725342]\n",
      "[0.6730393767356873, -0.8111579418182373]\n",
      "[-1.8176851272583008, -1.357450246810913]\n",
      "[0.6773388981819153, -0.8148430585861206]\n",
      "[-1.9466888904571533, -1.0312716960906982]\n",
      "[-0.2271391898393631, -0.9794446229934692]\n",
      "[-1.4606761932373047, -1.302169919013977]\n",
      "[-0.12302407622337341, -1.3658899068832397]\n",
      "[0.7806809544563293, -0.9034175872802734]\n",
      "[-1.1780341863632202, -0.8454005718231201]\n",
      "[-0.431613564491272, -1.284679889678955]\n",
      "[-1.3177740573883057, -1.2053037881851196]\n",
      "[-1.9814634323120117, -0.8238518238067627]\n",
      "[1.4144630432128906, -1.446632742881775]\n",
      "[-1.3485682010650635, -1.2241973876953125]\n",
      "[-0.33511462807655334, -0.8255581855773926]\n",
      "[0.5109224319458008, -0.8231770992279053]\n",
      "[-1.5638327598571777, -1.4240251779556274]\n",
      "[1.0489249229431152, -1.1333297491073608]\n",
      "[0.07109153270721436, -1.557233452796936]\n",
      "[-1.1502985954284668, -0.8224906921386719]\n",
      "[-1.9308810234069824, -1.2880029678344727]\n",
      "[-1.805391788482666, -1.1657615900039673]\n",
      "[0.5672006011009216, -0.7204434871673584]\n",
      "[-0.7208558917045593, -2.0242972373962402]\n",
      "[-2.479583740234375, -1.1007462739944458]\n",
      "[-0.49125075340270996, -1.120184302330017]\n",
      "[-1.7679071426391602, -1.1524237394332886]\n",
      "[-1.2987605333328247, -1.2195945978164673]\n",
      "[0.8417441248893738, -0.9557548761367798]\n",
      "[-0.4144732654094696, -1.2941398620605469]\n",
      "[-1.4411005973815918, -1.0504876375198364]\n",
      "[-1.5658626556396484, -1.6732800006866455]\n",
      "[-1.4444165229797363, -1.1140925884246826]\n",
      "[1.9011085033416748, -1.9189119338989258]\n",
      "[-1.0547795295715332, -1.2404755353927612]\n",
      "[-0.9322301149368286, -1.1783546209335327]\n",
      "[-2.148979425430298, -1.0605332851409912]\n",
      "[-0.9760576486587524, -1.105079174041748]\n",
      "[-1.5516624450683594, -1.1316766738891602]\n",
      "[-2.182230234146118, -1.2565619945526123]\n",
      "[-1.3494396209716797, -1.324022650718689]\n",
      "[0.045231372117996216, -0.9312121868133545]\n",
      "[-1.646284818649292, -1.1724644899368286]\n",
      "[1.2800946235656738, -1.5621999502182007]\n",
      "[-2.2102413177490234, -1.1430706977844238]\n",
      "[-1.6290647983551025, -1.0969501733779907]\n",
      "[-1.9463214874267578, -1.15382719039917]\n",
      "[-2.6747608184814453, -1.2092409133911133]\n",
      "[1.5746855735778809, -1.9436397552490234]\n",
      "[0.48683375120162964, -0.65156090259552]\n",
      "[-1.8253984451293945, -1.4477983713150024]\n",
      "[-2.555001974105835, -1.142669677734375]\n",
      "[0.0019854307174682617, -0.9272706508636475]\n",
      "[-2.6433398723602295, -1.1917747259140015]\n",
      "[-0.051852911710739136, -1.1868507862091064]\n",
      "[1.7857351303100586, -1.8901575803756714]\n",
      "[-0.9020242691040039, -1.3370680809020996]\n",
      "[-1.0846753120422363, -1.186294674873352]\n",
      "[0.5300276875495911, -0.6885825395584106]\n",
      "[0.6165951490402222, -0.7627794742584229]\n",
      "[-0.17346937954425812, -1.0430845022201538]\n",
      "[-0.20962262153625488, -1.0296061038970947]\n",
      "[-0.784022331237793, -1.2738208770751953]\n",
      "[-1.5435469150543213, -1.4874231815338135]\n",
      "[0.18306928873062134, -1.3336162567138672]\n",
      "[-1.6486289501190186, -1.2108831405639648]\n",
      "[0.730775773525238, -0.8606438636779785]\n",
      "[0.20535677671432495, -1.1191898584365845]\n",
      "[-2.0525918006896973, -0.9445565938949585]\n",
      "[1.5357708930969238, -1.5506058931350708]\n",
      "[0.6284448504447937, -1.6245070695877075]\n",
      "[-1.6990067958831787, -1.1810109615325928]\n",
      "[-0.21931509673595428, -1.0579338073730469]\n",
      "[0.31373029947280884, -0.8800468444824219]\n",
      "[0.7050378918647766, -0.8385839462280273]\n",
      "[0.4853159785270691, -0.6502599716186523]\n",
      "[1.6552927494049072, -1.9254155158996582]\n",
      "[-1.264482021331787, -1.5861097574234009]\n",
      "[-0.8236626386642456, -1.1419289112091064]\n",
      "[-0.7236508131027222, -1.2131404876708984]\n",
      "[0.6754037737846375, -0.8131844997406006]\n",
      "[-1.8574895858764648, -1.37019681930542]\n",
      "[-0.9193150997161865, -1.3026124238967896]\n",
      "[-2.5550014972686768, -1.1426693201065063]\n",
      "[1.5780043601989746, -2.4470739364624023]\n",
      "[0.9037116169929504, -1.0432101488113403]\n",
      "[-2.696892261505127, -1.221543312072754]\n",
      "[1.3611221313476562, -1.6629034280776978]\n",
      "[-2.1144931316375732, -1.1371413469314575]\n",
      "[-0.8524738550186157, -1.2018895149230957]\n",
      "[-0.6393719911575317, -1.125200867652893]\n",
      "[-1.9076924324035645, -1.2777531147003174]\n",
      "[-0.8836731314659119, -1.2247270345687866]\n",
      "[1.6786837577819824, -2.048978805541992]\n",
      "[-1.4106082916259766, -1.3729217052459717]\n",
      "[1.022826910018921, -1.323290467262268]\n",
      "[-2.0679969787597656, -1.3773425817489624]\n",
      "[-2.281951904296875, -1.0838383436203003]\n",
      "[-1.474766731262207, -1.3120472431182861]\n",
      "[-0.887119710445404, -1.1629005670547485]\n",
      "[0.6106961965560913, -0.7577235698699951]\n",
      "[0.5862014293670654, -0.7367290258407593]\n",
      "[-2.07084059715271, -1.3606406450271606]\n",
      "[0.6787338852882385, -0.8160387277603149]\n",
      "[-1.9731450080871582, -1.1526120901107788]\n",
      "[-0.43897271156311035, -1.0928912162780762]\n",
      "[-2.4716930389404297, -1.0963599681854248]\n",
      "[-0.9734793901443481, -1.186010479927063]\n",
      "[0.9425932765007019, -1.042192816734314]\n",
      "[-0.6344714760780334, -1.1467872858047485]\n",
      "[-1.2573280334472656, -1.1146533489227295]\n",
      "[-2.2925302982330322, -0.9967671632766724]\n",
      "[-1.0034856796264648, -1.2310763597488403]\n",
      "[1.0796003341674805, -1.1596217155456543]\n",
      "[-2.3288753032684326, -1.0169706344604492]\n",
      "[-0.17322947084903717, -1.010459542274475]\n",
      "[0.010412871837615967, -0.8872140645980835]\n",
      "[-2.38859224319458, -1.2546203136444092]\n",
      "[1.3753306865692139, -1.4130924940109253]\n",
      "[-1.3458212614059448, -1.190325140953064]\n",
      "[-0.5198709964752197, -1.1433441638946533]\n",
      "[-2.378225326538086, -1.255643367767334]\n",
      "[-2.5927114486694336, -1.1636314392089844]\n",
      "[0.5634539723396301, -0.7172321081161499]\n",
      "[3.2619681358337402, -3.1461522579193115]\n",
      "[1.5428831577301025, -1.8915038108825684]\n",
      "[-0.3514842987060547, -1.4898556470870972]\n",
      "[0.7110629677772522, -1.670577049255371]\n",
      "[0.7450453639030457, -1.6297320127487183]\n",
      "[-1.4670724868774414, -1.3849989175796509]\n",
      "[0.4737958312034607, -1.8171794414520264]\n",
      "[0.46851295232772827, -1.7601006031036377]\n",
      "[2.6686408519744873, -2.646681785583496]\n",
      "[0.3884766697883606, -1.6355767250061035]\n",
      "[-3.208282947540283, -1.255591630935669]\n",
      "[0.30243635177612305, -1.6548874378204346]\n",
      "[0.9053035378456116, -2.0459561347961426]\n",
      "[1.05949068069458, -1.764170527458191]\n",
      "[-0.018687307834625244, -1.332318663597107]\n",
      "[0.941895067691803, -1.588804006576538]\n",
      "[2.1010918617248535, -2.2290329933166504]\n",
      "[1.6136786937713623, -1.811394214630127]\n",
      "[-0.70174640417099, -1.3236305713653564]\n",
      "[1.3746757507324219, -2.0178511142730713]\n",
      "[-3.3366427421569824, -1.176883578300476]\n",
      "[1.6755845546722412, -2.114426851272583]\n",
      "[0.5548188090324402, -1.454390525817871]\n",
      "[1.7150869369506836, -1.9234848022460938]\n",
      "[1.5746855735778809, -1.9436392784118652]\n",
      "[-4.347477912902832, -0.9411921501159668]\n",
      "[-1.6995751857757568, -1.232763648033142]\n",
      "[2.3079299926757812, -2.2412776947021484]\n",
      "[1.8474655151367188, -1.9953725337982178]\n",
      "[-0.8912516236305237, -1.2961899042129517]\n",
      "[1.6959373950958252, -2.0004544258117676]\n",
      "[1.005894422531128, -1.4742951393127441]\n",
      "[-2.449007272720337, -1.469929575920105]\n",
      "[1.7167975902557373, -1.8979885578155518]\n",
      "[0.08323144912719727, -1.5639840364456177]\n",
      "[-1.5490365028381348, -1.2490986585617065]\n",
      "[-1.533156394958496, -1.340573787689209]\n",
      "[-0.36288297176361084, -1.4364657402038574]\n",
      "[-0.5199809670448303, -1.380652666091919]\n",
      "[0.7453976273536682, -1.9074105024337769]\n",
      "[2.3570644855499268, -2.2835097312927246]\n",
      "[0.175520122051239, -1.866851568222046]\n",
      "[1.7555785179138184, -2.1973822116851807]\n",
      "[-2.0031909942626953, -1.2294045686721802]\n",
      "[-0.5483367443084717, -1.5625029802322388]\n",
      "[0.692852795124054, -1.574236512184143]\n",
      "[-0.06805187463760376, -2.126826286315918]\n",
      "[0.7962432503700256, -2.3671152591705322]\n",
      "[0.8412087559700012, -1.7585618495941162]\n",
      "[-0.2361963987350464, -1.361485242843628]\n",
      "[-0.6228576898574829, -1.3528070449829102]\n",
      "[-1.8517537117004395, -1.2526317834854126]\n",
      "[-1.885218858718872, -1.1551120281219482]\n",
      "[-3.0758535861968994, -1.3845361471176147]\n",
      "[2.4061269760131836, -2.5959346294403076]\n",
      "[0.29003089666366577, -1.7222261428833008]\n",
      "[0.3472411632537842, -1.7273696660995483]\n",
      "[-1.4763784408569336, -1.286940574645996]\n",
      "[2.0920820236206055, -2.28079891204834]\n",
      "[1.4847514629364014, -1.7773826122283936]\n",
      "[1.567723035812378, -1.9831898212432861]\n",
      "[-4.3268938064575195, -1.4758647680282593]\n",
      "[-2.133143186569214, -1.2209346294403076]\n",
      "[1.8057372570037842, -2.0421016216278076]\n",
      "[-3.438100814819336, -1.1561930179595947]\n",
      "[1.3704745769500732, -1.9421963691711426]\n",
      "[0.17048943042755127, -1.6149755716323853]\n",
      "[-0.017043352127075195, -1.4312347173690796]\n",
      "[1.8162462711334229, -2.1467232704162598]\n",
      "[0.6364482641220093, -1.8932512998580933]\n",
      "[-4.072309970855713, -1.1411681175231934]\n",
      "[1.8211889266967773, -1.971452236175537]\n",
      "[-0.06693166494369507, -1.9749348163604736]\n",
      "[2.2674319744110107, -2.365750551223755]\n",
      "[1.3715183734893799, -1.9127720594406128]\n",
      "[1.7977490425109863, -1.9547226428985596]\n",
      "[-0.09295070171356201, -1.588867425918579]\n",
      "[-1.5022754669189453, -1.2842611074447632]\n",
      "[-1.5214378833770752, -1.294358491897583]\n",
      "[2.469595193862915, -2.4374945163726807]\n",
      "[-2.3095204830169678, -1.106804609298706]\n",
      "[1.059849500656128, -1.7529562711715698]\n",
      "[-0.3861728310585022, -1.4645202159881592]\n",
      "[1.6317312717437744, -2.0192818641662598]\n",
      "[0.33828651905059814, -1.7257353067398071]\n",
      "[-2.416696071624756, -1.138527750968933]\n",
      "[0.18911141157150269, -1.4740406274795532]\n",
      "[0.3965425491333008, -1.695119857788086]\n",
      "[1.2342872619628906, -1.7881131172180176]\n",
      "[0.9977614283561707, -1.6049952507019043]\n",
      "[-1.8181545734405518, -1.308046579360962]\n",
      "[1.186180591583252, -1.8585416078567505]\n",
      "[-0.5535703301429749, -1.2717746496200562]\n",
      "[-1.0561656951904297, -1.3226262331008911]\n",
      "[2.0245449542999268, -2.2169384956359863]\n",
      "[-2.5542960166931152, -1.2040557861328125]\n",
      "[1.4162189960479736, -2.071458101272583]\n",
      "[0.19192826747894287, -1.3682745695114136]\n",
      "[2.020142078399658, -2.254484176635742]\n",
      "[1.5607025623321533, -2.018754720687866]\n",
      "[-2.0313501358032227, -1.154581069946289]\n",
      "[1.8477745056152344, -1.8180243968963623]\n",
      "[0.33192455768585205, -0.5979886054992676]\n",
      "[-0.3502781093120575, -0.5024356842041016]\n",
      "[-0.36719241738319397, -0.4196520149707794]\n",
      "[0.24532431364059448, -0.4759387671947479]\n",
      "[-1.4118731021881104, -0.47201189398765564]\n",
      "[0.4888347387313843, -0.8255916833877563]\n",
      "[0.3838896155357361, -0.7836503982543945]\n",
      "[1.3335731029510498, -1.3773019313812256]\n",
      "[0.05767863988876343, -0.5700912475585938]\n",
      "[-0.8732383847236633, -1.3334627151489258]\n",
      "[-0.4272298216819763, -0.523161768913269]\n",
      "[0.8388918042182922, -0.9533101320266724]\n",
      "[0.19462168216705322, -0.500757098197937]\n",
      "[-0.26049306988716125, -0.4119320809841156]\n",
      "[0.27101755142211914, -0.5259716510772705]\n",
      "[0.8786711096763611, -0.9874050617218018]\n",
      "[0.6750109791755676, -0.8128478527069092]\n",
      "[-0.3701152503490448, -0.47489461302757263]\n",
      "[0.8294082283973694, -1.0627851486206055]\n",
      "[-2.1004769802093506, -0.8029453754425049]\n",
      "[0.928053081035614, -1.0297304391860962]\n",
      "[-0.3404655456542969, -0.43799206614494324]\n",
      "[0.6304757595062256, -0.7746766805648804]\n",
      "[0.4868336319923401, -0.6515607833862305]\n",
      "[-1.6995751857757568, -1.232763648033142]\n",
      "[-1.7755472660064697, -0.25979164242744446]\n",
      "[1.0123112201690674, -1.1019481420516968]\n",
      "[0.611918568611145, -0.7587711811065674]\n",
      "[-0.7080205678939819, -0.3703644275665283]\n",
      "[0.5816569924354553, -0.732833981513977]\n",
      "[0.7158473134040833, -0.847848653793335]\n",
      "[-1.2998576164245605, -1.054413914680481]\n",
      "[0.8240700364112854, -0.9406063556671143]\n",
      "[-0.5419559478759766, -0.4493781626224518]\n",
      "[-1.6916532516479492, -0.24133405089378357]\n",
      "[-1.5448932647705078, -0.4001924693584442]\n",
      "[-0.1418817639350891, -0.49946048855781555]\n",
      "[-0.30891528725624084, -0.46182456612586975]\n",
      "[0.6784147620201111, -0.8773964643478394]\n",
      "[1.0474278926849365, -1.1320465803146362]\n",
      "[0.28965526819229126, -0.9033559560775757]\n",
      "[1.2596888542175293, -1.3139758110046387]\n",
      "[-1.8982703685760498, -0.3465535640716553]\n",
      "[-0.6528186798095703, -0.5510406494140625]\n",
      "[-0.3550620377063751, -0.461748331785202]\n",
      "[0.35164546966552734, -1.2908674478530884]\n",
      "[0.8040518164634705, -1.3017679452896118]\n",
      "[-0.32852035760879517, -0.47267070412635803]\n",
      "[0.061173468828201294, -0.4899872839450836]\n",
      "[0.07056328654289246, -0.6523455381393433]\n",
      "[-0.5516748428344727, -0.8257163763046265]\n",
      "[-1.7358477115631104, -0.28076818585395813]\n",
      "[-1.6289150714874268, -1.107359766960144]\n",
      "[1.392510175704956, -1.4278168678283691]\n",
      "[0.29247796535491943, -0.7148041725158691]\n",
      "[0.21938353776931763, -0.705866813659668]\n",
      "[-0.23238056898117065, -0.8325122594833374]\n",
      "[0.9114142060279846, -1.0154691934585571]\n",
      "[0.6947624087333679, -0.8297767639160156]\n",
      "[0.5266557335853577, -0.6856924295425415]\n",
      "[-1.7399184703826904, -1.730334997177124]\n",
      "[-1.6642065048217773, -0.4960024654865265]\n",
      "[0.689060628414154, -0.8248897790908813]\n",
      "[-2.0285768508911133, -0.8640316724777222]\n",
      "[0.48723870515823364, -0.7183877229690552]\n",
      "[-0.37945520877838135, -0.5569703578948975]\n",
      "[0.14183354377746582, -0.5098110437393188]\n",
      "[0.8656236529350281, -0.976222038269043]\n",
      "[0.6356424689292908, -0.8933504819869995]\n",
      "[-1.680464506149292, -1.2905428409576416]\n",
      "[0.7829646468162537, -0.9053750038146973]\n",
      "[0.20697027444839478, -1.0820419788360596]\n",
      "[1.009171485900879, -1.099256992340088]\n",
      "[0.4406713843345642, -0.6438512802124023]\n",
      "[0.753070056438446, -0.8797522783279419]\n",
      "[-0.10692772269248962, -0.6004126071929932]\n",
      "[-0.41695040464401245, -0.7752376794815063]\n",
      "[-1.5961744785308838, -0.2875280976295471]\n",
      "[1.0918176174163818, -1.170093059539795]\n",
      "[-1.3750574588775635, -0.5657286643981934]\n",
      "[0.2258768081665039, -0.4836525619029999]\n",
      "[-0.385237455368042, -0.4848819673061371]\n",
      "[0.5948395729064941, -0.7441327571868896]\n",
      "[0.037171393632888794, -0.7059681415557861]\n",
      "[-0.8923466205596924, -0.8398967981338501]\n",
      "[0.3344520926475525, -0.5570272207260132]\n",
      "[-0.06428003311157227, -0.6568716764450073]\n",
      "[0.025200843811035156, -0.531409502029419]\n",
      "[0.46670663356781006, -0.6343098878860474]\n",
      "[-1.527651309967041, -0.4780398905277252]\n",
      "[0.32085949182510376, -0.6157704591751099]\n",
      "[-0.32240816950798035, -0.3509722650051117]\n",
      "[-1.2121326923370361, -0.28965139389038086]\n",
      "[0.869819700717926, -0.9798184633255005]\n",
      "[-0.9668504595756531, -0.9491561651229858]\n",
      "[0.9976720213890076, -1.0894008874893188]\n",
      "[0.21281540393829346, -0.45705661177635193]\n",
      "[0.8525123000144958, -0.9649842977523804]\n",
      "[0.6512818932533264, -0.7925095558166504]\n",
      "[-1.2233014106750488, -0.5615959167480469]\n",
      "[-1.160356044769287, -2.075570583343506]\n",
      "[-1.4881012439727783, -1.5040172338485718]\n",
      "[0.3572748899459839, -1.4323532581329346]\n",
      "[-0.7250233292579651, -1.596028208732605]\n",
      "[-1.0166113376617432, -1.3894057273864746]\n",
      "[1.1232752799987793, -1.1970555782318115]\n",
      "[0.751956045627594, -1.83918035030365]\n",
      "[-0.2717496156692505, -1.574771523475647]\n",
      "[-1.6994497776031494, -1.8117603063583374]\n",
      "[-0.7431830763816833, -1.4413551092147827]\n",
      "[2.0195677280426025, -1.9652683734893799]\n",
      "[0.07307958602905273, -1.7200031280517578]\n",
      "[0.709919273853302, -1.8240890502929688]\n",
      "[-1.4713006019592285, -1.397046685218811]\n",
      "[-0.47121214866638184, -1.3299293518066406]\n",
      "[-1.463139533996582, -1.2748801708221436]\n",
      "[-1.602175235748291, -1.6291027069091797]\n",
      "[-2.0475761890411377, -1.2470234632492065]\n",
      "[0.6350824236869812, -1.1788651943206787]\n",
      "[-0.5731549859046936, -1.703721523284912]\n",
      "[2.225782632827759, -2.1420156955718994]\n",
      "[-1.2695364952087402, -1.6094945669174194]\n",
      "[-1.040939450263977, -1.3883543014526367]\n",
      "[-2.5741426944732666, -1.1533094644546509]\n",
      "[-1.8253989219665527, -1.447798252105713]\n",
      "[2.307929754257202, -2.2412776947021484]\n",
      "[1.0123114585876465, -1.1019482612609863]\n",
      "[-3.1241166591644287, -1.4590281248092651]\n",
      "[-1.8692638874053955, -1.4553524255752563]\n",
      "[0.7781025767326355, -1.199697494506836]\n",
      "[-1.8366036415100098, -1.4824879169464111]\n",
      "[-0.6198220252990723, -1.1447278261184692]\n",
      "[2.392960786819458, -2.285304069519043]\n",
      "[-1.8581559658050537, -1.193773627281189]\n",
      "[-0.07842424511909485, -1.6092580556869507]\n",
      "[1.0703985691070557, -1.1517348289489746]\n",
      "[1.0967490673065186, -1.174319863319397]\n",
      "[0.42407989501953125, -1.2902345657348633]\n",
      "[0.4278786778450012, -1.2872166633605957]\n",
      "[0.364812970161438, -1.8213086128234863]\n",
      "[-2.976541042327881, -1.376994013786316]\n",
      "[1.456505298614502, -1.8627803325653076]\n",
      "[-0.8038156032562256, -1.6756889820098877]\n",
      "[1.0208253860473633, -1.1092455387115479]\n",
      "[1.2936224937438965, -1.5281680822372437]\n",
      "[-1.1033148765563965, -1.3788625001907349]\n",
      "[2.0048983097076416, -1.952695369720459]\n",
      "[1.8277833461761475, -2.1181468963623047]\n",
      "[-0.5300580263137817, -1.6871378421783447]\n",
      "[0.1951414942741394, -1.2596219778060913]\n",
      "[0.5701366662979126, -1.0771633386611938]\n",
      "[1.0356721878051758, -1.121970772743225]\n",
      "[0.9793551564216614, -1.0737013816833496]\n",
      "[2.439204692840576, -2.324939727783203]\n",
      "[-0.838461697101593, -1.911865472793579]\n",
      "[0.3421170115470886, -1.6795951128005981]\n",
      "[0.4431043863296509, -1.7456434965133667]\n",
      "[0.9949391484260559, -1.0870585441589355]\n",
      "[-2.911426067352295, -1.340798020362854]\n",
      "[-1.6088817119598389, -1.2261580228805542]\n",
      "[-1.4942612648010254, -1.5769604444503784]\n",
      "[2.5000269412994385, -2.748847246170044]\n",
      "[1.010129690170288, -1.1000783443450928]\n",
      "[-1.9138708114624023, -1.5536975860595703]\n",
      "[2.1072113513946533, -2.0403881072998047]\n",
      "[-0.961348831653595, -1.6639654636383057]\n",
      "[0.28999143838882446, -1.704893708229065]\n",
      "[-0.2336430549621582, -1.3243387937545776]\n",
      "[-0.9890912175178528, -1.7556041479110718]\n",
      "[0.26976656913757324, -1.7667913436889648]\n",
      "[2.464064121246338, -2.398960828781128]\n",
      "[-2.3605966567993164, -1.231337547302246]\n",
      "[1.7836453914642334, -1.7630592584609985]\n",
      "[-1.8006272315979004, -1.6413471698760986]\n",
      "[-1.1147005558013916, -1.6103415489196777]\n",
      "[-2.421389579772949, -1.1709140539169312]\n",
      "[-0.03816518187522888, -1.509093999862671]\n",
      "[0.9479716420173645, -1.0468025207519531]\n",
      "[1.1155741214752197, -1.1904548406600952]\n",
      "[-2.461606979370117, -1.4234122037887573]\n",
      "[1.0238304138183594, -1.111821174621582]\n",
      "[-1.378770351409912, -1.4528923034667969]\n",
      "[0.41129690408706665, -1.4027694463729858]\n",
      "[-1.4716603755950928, -1.5543943643569946]\n",
      "[0.18884307146072388, -1.7054072618484497]\n",
      "[1.1203711032867432, -1.1945663690567017]\n",
      "[-0.39276322722435, -1.304545283317566]\n",
      "[-0.09836193919181824, -1.6333024501800537]\n",
      "[-1.1333301067352295, -1.5153056383132935]\n",
      "[-1.2310700416564941, -1.271043062210083]\n",
      "[1.4749770164489746, -1.4984993934631348]\n",
      "[-1.1451445817947388, -1.5393471717834473]\n",
      "[0.45810508728027344, -1.266514539718628]\n",
      "[1.1623847484588623, -1.2305761575698853]\n",
      "[-1.8193566799163818, -1.6051414012908936]\n",
      "[1.3452258110046387, -1.3872895240783691]\n",
      "[-0.005286663770675659, -1.7779825925827026]\n",
      "[-0.3545636236667633, -1.2818483114242554]\n",
      "[-2.251065254211426, -1.4550786018371582]\n",
      "[-1.7439987659454346, -1.4410279989242554]\n",
      "[0.9329444766044617, -1.033922791481018]\n",
      "[-1.4075450897216797, -1.8132822513580322]\n",
      "[-2.5881340503692627, -1.161086916923523]\n",
      "[-0.3488939702510834, -1.2227351665496826]\n",
      "[-1.6869370937347412, -1.17523992061615]\n",
      "[-1.0185692310333252, -1.3638794422149658]\n",
      "[0.6795129179954529, -0.8167064189910889]\n",
      "[-1.415027379989624, -1.0283875465393066]\n",
      "[-1.7767298221588135, -0.929711103439331]\n",
      "[-2.2865798473358154, -1.4270870685577393]\n",
      "[-1.2069811820983887, -1.2015982866287231]\n",
      "[2.0731089115142822, -2.0111587047576904]\n",
      "[-1.1607040166854858, -1.1900477409362793]\n",
      "[-2.256105422973633, -0.9765194654464722]\n",
      "[-1.8582696914672852, -1.1533465385437012]\n",
      "[-0.7356007099151611, -1.2488220930099487]\n",
      "[-1.2498271465301514, -1.2584728002548218]\n",
      "[-2.7017974853515625, -1.224269986152649]\n",
      "[-1.113585352897644, -1.4564425945281982]\n",
      "[0.22178751230239868, -1.1041498184204102]\n",
      "[-2.4117062091827393, -1.0630145072937012]\n",
      "[1.4550049304962158, -1.4883766174316406]\n",
      "[-2.6094541549682617, -1.172938346862793]\n",
      "[-1.3674359321594238, -1.1965657472610474]\n",
      "[-1.7428984642028809, -1.237499713897705]\n",
      "[-2.555002212524414, -1.1426697969436646]\n",
      "[1.847465991973877, -1.995373010635376]\n",
      "[0.611918568611145, -0.7587711811065674]\n",
      "[-1.8692643642425537, -1.4553523063659668]\n",
      "[-2.8417327404022217, -1.3020570278167725]\n",
      "[0.21542102098464966, -1.0883780717849731]\n",
      "[-2.6862223148345947, -1.2156120538711548]\n",
      "[0.21182584762573242, -1.3752033710479736]\n",
      "[1.858652114868164, -1.8273476362228394]\n",
      "[-0.6962544918060303, -1.4847874641418457]\n",
      "[-1.030855655670166, -1.2031095027923584]\n",
      "[0.6864369511604309, -0.8226410150527954]\n",
      "[0.648252546787262, -0.7899130582809448]\n",
      "[-0.010265827178955078, -1.2214757204055786]\n",
      "[-0.03569558262825012, -1.1937429904937744]\n",
      "[-1.8390369415283203, -0.9763094186782837]\n",
      "[-1.554837942123413, -1.5173076391220093]\n",
      "[-0.8868542909622192, -1.0588207244873047]\n",
      "[-2.5497024059295654, -1.139723777770996]\n",
      "[0.6118469834327698, -0.7587099075317383]\n",
      "[-0.03261449933052063, -1.0871907472610474]\n",
      "[-1.8811585903167725, -0.9844781160354614]\n",
      "[0.8825854659080505, -1.1279600858688354]\n",
      "[-0.7985314130783081, -1.2300688028335571]\n",
      "[-1.84596586227417, -1.1088894605636597]\n",
      "[-0.012607544660568237, -1.245563268661499]\n",
      "[0.5645789504051208, -1.0620906352996826]\n",
      "[0.8341655135154724, -0.9492592811584473]\n",
      "[0.5780616402626038, -0.7297524213790894]\n",
      "[1.847637414932251, -1.9100216627120972]\n",
      "[-2.176170825958252, -1.2519440650939941]\n",
      "[-1.7051873207092285, -0.9081902503967285]\n",
      "[-1.4804556369781494, -1.0010040998458862]\n",
      "[0.8386957049369812, -0.9531420469284058]\n",
      "[-1.8634815216064453, -1.3908478021621704]\n",
      "[-0.695837140083313, -1.4719891548156738]\n",
      "[-2.6095662117004395, -1.173000693321228]\n",
      "[1.9830899238586426, -2.4174625873565674]\n",
      "[0.892842710018158, -0.99955153465271]\n",
      "[-2.7508111000061035, -1.2515156269073486]\n",
      "[1.596733570098877, -1.6863411664962769]\n",
      "[-2.4159016609191895, -1.0653467178344727]\n",
      "[-1.108109474182129, -1.1053727865219116]\n",
      "[-0.40432262420654297, -1.2988077402114868]\n",
      "[-2.529815912246704, -1.128669261932373]\n",
      "[-1.8379087448120117, -0.953789234161377]\n",
      "[1.9299290180206299, -2.084312677383423]\n",
      "[-1.2180430889129639, -1.4853837490081787]\n",
      "[0.3739672303199768, -1.215090036392212]\n",
      "[-2.6985387802124023, -1.2224586009979248]\n",
      "[-2.452440023422241, -1.0856575965881348]\n",
      "[-1.2751600742340088, -1.4169740676879883]\n",
      "[-0.7300276756286621, -1.2371420860290527]\n",
      "[0.7785634398460388, -0.9016026258468628]\n",
      "[0.6728478074073792, -0.8604623079299927]\n",
      "[-2.6048195362091064, -1.170362114906311]\n",
      "[0.7391104102134705, -0.8677874803543091]\n",
      "[-1.682729959487915, -1.2594305276870728]\n",
      "[-0.3032779097557068, -1.2010881900787354]\n",
      "[-2.6537346839904785, -1.197553038597107]\n",
      "[-1.3951821327209473, -1.0460437536239624]\n",
      "[1.007765293121338, -1.098051905632019]\n",
      "[-0.38692066073417664, -1.3306423425674438]\n",
      "[-1.5448424816131592, -1.0053133964538574]\n",
      "[-2.3545589447021484, -1.031247615814209]\n",
      "[-0.7360436916351318, -1.4025225639343262]\n",
      "[0.8047676682472229, -0.9240622520446777]\n",
      "[-2.357903480529785, -1.0331066846847534]\n",
      "[-0.02614256739616394, -1.1834923028945923]\n",
      "[0.339386522769928, -1.0140459537506104]\n",
      "[-2.804151773452759, -1.2811665534973145]\n",
      "[1.3953380584716797, -1.4302407503128052]\n",
      "[-2.500666856765747, -1.1124658584594727]\n",
      "[-0.285966694355011, -1.3376764059066772]\n",
      "[-2.6515283584594727, -1.196326494216919]\n",
      "[-2.547468662261963, -1.1384819746017456]\n",
      "[0.6544596552848816, -0.7952332496643066]\n",
      "[2.097325325012207, -2.031914710998535]\n",
      "[-0.08755481243133545, -0.9556130170822144]\n",
      "[-1.2717839479446411, -0.3315674364566803]\n",
      "[-1.3352173566818237, -0.6221427917480469]\n",
      "[-1.2650959491729736, -0.4899084270000458]\n",
      "[-0.020801842212677002, -0.762362003326416]\n",
      "[0.4670613408088684, -1.0357608795166016]\n",
      "[-0.640558123588562, -0.7507355213165283]\n",
      "[1.4696786403656006, -1.6428041458129883]\n",
      "[-1.6111340522766113, -0.4441002905368805]\n",
      "[-0.4971988797187805, -1.114553451538086]\n",
      "[-1.1279933452606201, -0.6557552814483643]\n",
      "[1.5446550846099854, -1.5582205057144165]\n",
      "[-1.0194194316864014, -0.6326161623001099]\n",
      "[-1.5966637134552002, -0.43326470255851746]\n",
      "[-1.2733559608459473, -0.5124416351318359]\n",
      "[0.7250441908836365, -1.3321375846862793]\n",
      "[0.038229405879974365, -0.7637689113616943]\n",
      "[-1.119154930114746, -0.2689126431941986]\n",
      "[0.5512394309043884, -1.1212834119796753]\n",
      "[-0.7052100300788879, -1.225278615951538]\n",
      "[0.5480946898460388, -1.1398837566375732]\n",
      "[-1.711723804473877, -0.47148415446281433]\n",
      "[-0.014341920614242554, -0.7854404449462891]\n",
      "[0.0019854307174682617, -0.9272706508636475]\n",
      "[-0.8912516236305237, -1.2961899042129517]\n",
      "[-0.7080205678939819, -0.3703644275665283]\n",
      "[0.7781025767326355, -1.1996973752975464]\n",
      "[0.21542131900787354, -1.0883780717849731]\n",
      "[-1.1659003496170044, -0.2715098559856415]\n",
      "[0.1539602279663086, -1.0268681049346924]\n",
      "[-0.07588720321655273, -0.6511459350585938]\n",
      "[0.09706538915634155, -1.3876287937164307]\n",
      "[0.31804561614990234, -0.8032665252685547]\n",
      "[-1.4168832302093506, -0.5201245546340942]\n",
      "[-0.6086755394935608, -0.3322073817253113]\n",
      "[-0.1756112277507782, -0.6254631280899048]\n",
      "[-1.1821632385253906, -0.27606311440467834]\n",
      "[-1.2344698905944824, -0.30397921800613403]\n",
      "[0.37265992164611816, -1.052030086517334]\n",
      "[0.8856878876686096, -1.1731152534484863]\n",
      "[1.251755714416504, -1.307176113128662]\n",
      "[1.002213478088379, -1.281414270401001]\n",
      "[-0.6185051798820496, -0.5397531986236572]\n",
      "[0.13725465536117554, -0.6463487148284912]\n",
      "[-1.690445899963379, -0.5174999237060547]\n",
      "[1.5420784950256348, -1.7278157472610474]\n",
      "[1.7763478755950928, -1.7568045854568481]\n",
      "[-1.0503313541412354, -0.7644528150558472]\n",
      "[-1.2662173509597778, -0.31254610419273376]\n",
      "[-0.9427725076675415, -0.277566522359848]\n",
      "[-0.5945101976394653, -0.3913842439651489]\n",
      "[-0.7080994248390198, -0.3529168963432312]\n",
      "[-0.32686513662338257, -1.4053096771240234]\n",
      "[1.5073199272155762, -1.5942800045013428]\n",
      "[0.02832934260368347, -0.8867863416671753]\n",
      "[-0.27383309602737427, -0.853449821472168]\n",
      "[-0.42646902799606323, -0.3790673315525055]\n",
      "[0.7485753893852234, -1.1215189695358276]\n",
      "[0.05138969421386719, -0.7182469367980957]\n",
      "[0.14744722843170166, -1.010606288909912]\n",
      "[-0.8314576745033264, -1.879352331161499]\n",
      "[-1.401472806930542, -0.3052959442138672]\n",
      "[0.2934827208518982, -1.108332633972168]\n",
      "[-0.9019789695739746, -1.0937482118606567]\n",
      "[-0.07432550191879272, -0.9913620948791504]\n",
      "[-1.0232133865356445, -0.6753107309341431]\n",
      "[-1.456627368927002, -0.3749387860298157]\n",
      "[0.6466178297996521, -1.2744920253753662]\n",
      "[0.3871843218803406, -1.0326507091522217]\n",
      "[-0.69538813829422, -1.4400285482406616]\n",
      "[0.3264177441596985, -0.8738216161727905]\n",
      "[1.088735818862915, -1.1972719430923462]\n",
      "[0.9534456133842468, -1.407483696937561]\n",
      "[-0.20347744226455688, -0.9510542154312134]\n",
      "[0.25123822689056396, -0.8346956968307495]\n",
      "[-1.4607279300689697, -0.4064003527164459]\n",
      "[-0.5426057577133179, -0.3435799479484558]\n",
      "[-0.6379865407943726, -0.33139166235923767]\n",
      "[0.967052161693573, -1.3991026878356934]\n",
      "[-0.8888209462165833, -0.3770582973957062]\n",
      "[-0.9767754673957825, -0.6370800733566284]\n",
      "[-1.2295703887939453, -0.31440362334251404]\n",
      "[0.25486016273498535, -1.058613657951355]\n",
      "[-0.7581721544265747, -0.753466010093689]\n",
      "[-0.8509488105773926, -0.4492582380771637]\n",
      "[-1.3392629623413086, -0.3922894597053528]\n",
      "[-1.055826187133789, -0.6859133243560791]\n",
      "[-0.7028450965881348, -0.8251175880432129]\n",
      "[-0.7952617406845093, -0.5463405847549438]\n",
      "[-0.03713792562484741, -0.9452400207519531]\n",
      "[-0.5792636871337891, -0.8336969614028931]\n",
      "[-1.2933449745178223, -0.3252504765987396]\n",
      "[-0.9921079277992249, -0.24548736214637756]\n",
      "[0.6358944773674011, -1.2825872898101807]\n",
      "[-0.9124285578727722, -0.6024909019470215]\n",
      "[1.270921230316162, -1.3236030340194702]\n",
      "[-1.3848872184753418, -0.4058895409107208]\n",
      "[0.7034787535667419, -1.2065811157226562]\n",
      "[0.19831126928329468, -0.9627952575683594]\n",
      "[-0.9114482402801514, -0.29612767696380615]\n",
      "[-0.9785577058792114, -1.975494623184204]\n",
      "[-2.5701210498809814, -1.151073932647705]\n",
      "[-0.42015716433525085, -1.1991795301437378]\n",
      "[-1.728384017944336, -1.1937369108200073]\n",
      "[-1.1676959991455078, -1.3192986249923706]\n",
      "[0.8731009364128113, -0.9826308488845825]\n",
      "[-0.7470875978469849, -1.2333076000213623]\n",
      "[-1.525655746459961, -1.0383731126785278]\n",
      "[-1.8370418548583984, -1.611574649810791]\n",
      "[-1.3266243934631348, -1.1853227615356445]\n",
      "[2.0219316482543945, -1.967294454574585]\n",
      "[-1.0895905494689941, -1.2526766061782837]\n",
      "[-1.4800281524658203, -1.0453721284866333]\n",
      "[-2.005857467651367, -1.1346595287322998]\n",
      "[-0.8585197925567627, -1.2010499238967896]\n",
      "[-1.403566598892212, -1.2202935218811035]\n",
      "[-2.503012180328369, -1.1629900932312012]\n",
      "[-1.2363148927688599, -1.416364073753357]\n",
      "[0.13864195346832275, -1.0365203619003296]\n",
      "[-1.8864901065826416, -1.0982787609100342]\n",
      "[1.382357120513916, -1.5940423011779785]\n",
      "[-2.4718668460845947, -1.096456527709961]\n",
      "[-1.4999244213104248, -1.1738054752349854]\n",
      "[-1.845109462738037, -1.2220183610916138]\n",
      "[-2.6433398723602295, -1.1917747259140015]\n",
      "[1.6959376335144043, -2.0004544258117676]\n",
      "[0.5816569924354553, -0.732833981513977]\n",
      "[-1.836604118347168, -1.4824877977371216]\n",
      "[-2.6862223148345947, -1.2156121730804443]\n",
      "[0.1539602279663086, -1.0268681049346924]\n",
      "[-2.7666244506835938, -1.2603060007095337]\n",
      "[0.0864589512348175, -1.294499397277832]\n",
      "[1.855747938156128, -1.9017826318740845]\n",
      "[-0.8025485277175903, -1.4350192546844482]\n",
      "[-1.0503125190734863, -1.2272107601165771]\n",
      "[0.6424471735954285, -0.7849373817443848]\n",
      "[0.673016369342804, -0.8111382722854614]\n",
      "[-0.08723834156990051, -1.1488169431686401]\n",
      "[-0.12213610112667084, -1.132117509841919]\n",
      "[-1.1268771886825562, -1.1973339319229126]\n",
      "[-1.5368342399597168, -1.5319304466247559]\n",
      "[-0.19032488763332367, -1.2695187330245972]\n",
      "[-1.961416482925415, -1.1144784688949585]\n",
      "[0.6462246775627136, -0.7881749868392944]\n",
      "[0.13954943418502808, -1.1508673429489136]\n",
      "[-1.9700965881347656, -0.9937744140625]\n",
      "[1.3614232540130615, -1.4011722803115845]\n",
      "[0.12232613563537598, -1.5147813558578491]\n",
      "[-1.760413408279419, -1.1810115575790405]\n",
      "[-0.10887730121612549, -1.168265461921692]\n",
      "[0.4467136263847351, -0.9856250286102295]\n",
      "[0.7803391814231873, -0.9031246900558472]\n",
      "[0.5487759709358215, -0.7046517133712769]\n",
      "[1.7450823783874512, -1.9592065811157227]\n",
      "[-1.5771784782409668, -1.4976965188980103]\n",
      "[-1.109894871711731, -1.0886479616165161]\n",
      "[-0.8885451555252075, -1.1798558235168457]\n",
      "[0.7793359160423279, -0.9022647142410278]\n",
      "[-1.855497121810913, -1.4092912673950195]\n",
      "[-0.8164004683494568, -1.412521481513977]\n",
      "[-2.638622522354126, -1.1891523599624634]\n",
      "[1.753434658050537, -2.4843692779541016]\n",
      "[0.9560779929161072, -1.0547773838043213]\n",
      "[-2.8023149967193604, -1.280145525932312]\n",
      "[1.4677209854125977, -1.711005449295044]\n",
      "[-2.304844856262207, -1.089827299118042]\n",
      "[-0.9076314568519592, -1.2015373706817627]\n",
      "[-0.5259608030319214, -1.2339909076690674]\n",
      "[-2.2030417919158936, -1.1893295049667358]\n",
      "[-1.1567391157150269, -1.1630903482437134]\n",
      "[1.791450023651123, -2.100031852722168]\n",
      "[-1.3160545825958252, -1.4552438259124756]\n",
      "[0.8843916058540344, -1.3409887552261353]\n",
      "[-2.363093614578247, -1.300412654876709]\n",
      "[-2.424267053604126, -1.069996953010559]\n",
      "[-1.3738420009613037, -1.3893814086914062]\n",
      "[-0.8061830997467041, -1.2283551692962646]\n",
      "[0.7187061905860901, -0.8502990007400513]\n",
      "[0.6981073021888733, -0.8326436281204224]\n",
      "[-2.286616086959839, -1.31182861328125]\n",
      "[0.7247031331062317, -0.8554389476776123]\n",
      "[-1.831380844116211, -1.2334814071655273]\n",
      "[-0.3739829659461975, -1.1758369207382202]\n",
      "[-2.58864688873291, -1.1613720655441284]\n",
      "[-1.057939052581787, -1.1731059551239014]\n",
      "[0.9886767268180847, -1.081691026687622]\n",
      "[-0.5147088766098022, -1.2596346139907837]\n",
      "[-1.3238551616668701, -1.108129858970642]\n",
      "[-2.3482041358947754, -1.0277150869369507]\n",
      "[-0.8793606162071228, -1.3412678241729736]\n",
      "[1.0294620990753174, -1.1166480779647827]\n",
      "[-2.3735597133636475, -1.0418097972869873]\n",
      "[-0.09397736191749573, -1.114293098449707]\n",
      "[0.24806839227676392, -0.9692674875259399]\n",
      "[-2.661877155303955, -1.2020792961120605]\n",
      "[1.414682388305664, -1.446820855140686]\n",
      "[-1.899035930633545, -1.0529582500457764]\n",
      "[-0.39865392446517944, -1.2569360733032227]\n",
      "[-2.5494463443756104, -1.2288365364074707]\n",
      "[-2.638667106628418, -1.18917715549469]\n",
      "[0.6156817674636841, -0.7619967460632324]\n",
      "[1.7575948238372803, -2.152662515640259]\n",
      "[0.23856407403945923, -1.308194875717163]\n",
      "[0.22391372919082642, -0.8339383602142334]\n",
      "[0.06693992018699646, -1.0728545188903809]\n",
      "[-1.053328037261963, -0.687423825263977]\n",
      "[1.1379919052124023, -1.2096691131591797]\n",
      "[1.645141363143921, -1.6443474292755127]\n",
      "[0.7735404372215271, -1.2550994157791138]\n",
      "[1.0830531120300293, -1.8617042303085327]\n",
      "[-0.4952412545681, -0.8347822427749634]\n",
      "[0.6056720018386841, -1.0733157396316528]\n",
      "[0.6784206032752991, -1.233758568763733]\n",
      "[2.207972764968872, -2.1267507076263428]\n",
      "[-0.5611520409584045, -0.9176403284072876]\n",
      "[-1.4537765979766846, -0.6507004499435425]\n",
      "[-1.365628957748413, -0.6531320810317993]\n",
      "[0.7161793112754822, -1.6249029636383057]\n",
      "[-1.6581859588623047, -0.6538330316543579]\n",
      "[-0.18100324273109436, -0.6014819145202637]\n",
      "[1.1157562732696533, -1.5749047994613647]\n",
      "[1.628983736038208, -1.75591242313385]\n",
      "[0.7294067740440369, -1.475719690322876]\n",
      "[-1.1331849098205566, -0.752301812171936]\n",
      "[-1.2742841243743896, -0.765205979347229]\n",
      "[-0.0518527626991272, -1.186850666999817]\n",
      "[1.0058941841125488, -1.474294900894165]\n",
      "[0.7158473134040833, -0.847848653793335]\n",
      "[-0.6198220252990723, -1.1447278261184692]\n",
      "[0.2118256688117981, -1.3752033710479736]\n",
      "[-0.07588714361190796, -0.6511459350585938]\n",
      "[0.0864589512348175, -1.294499397277832]\n",
      "[-2.2032132148742676, -0.8616329431533813]\n",
      "[2.0646471977233887, -2.00390625]\n",
      "[-1.7520751953125, -0.6963399648666382]\n",
      "[0.1185230016708374, -1.049949288368225]\n",
      "[0.6814566254615784, -0.818372368812561]\n",
      "[0.9453807473182678, -1.0445820093154907]\n",
      "[-0.24678365886211395, -0.6196029186248779]\n",
      "[-0.3575315773487091, -0.6385655403137207]\n",
      "[1.5955424308776855, -1.601836085319519]\n",
      "[-0.7994784116744995, -1.0564433336257935]\n",
      "[1.9952237606048584, -1.9444034099578857]\n",
      "[1.25189208984375, -1.6571800708770752]\n",
      "[0.7809252142906189, -0.9036270380020142]\n",
      "[1.2364282608032227, -1.2940391302108765]\n",
      "[-0.3541707396507263, -0.9330590963363647]\n",
      "[2.428248882293701, -2.315549612045288]\n",
      "[2.545022487640381, -2.4156363010406494]\n",
      "[0.527359664440155, -1.2628973722457886]\n",
      "[-0.8036320805549622, -0.5854319334030151]\n",
      "[-0.6750547885894775, -0.5102226734161377]\n",
      "[0.14154809713363647, -0.5450730323791504]\n",
      "[0.6575077176094055, -0.797845721244812]\n",
      "[1.8115105628967285, -1.9073644876480103]\n",
      "[1.6466403007507324, -1.9517822265625]\n",
      "[1.317948341369629, -1.4048482179641724]\n",
      "[1.3755295276641846, -1.4310652017593384]\n",
      "[-0.2547481060028076, -0.5013960599899292]\n",
      "[-0.5018298625946045, -1.1101486682891846]\n",
      "[-1.708977460861206, -0.6723828315734863]\n",
      "[0.2704392671585083, -1.327430248260498]\n",
      "[0.9551813006401062, -2.007563352584839]\n",
      "[0.5630505084991455, -0.7168864011764526]\n",
      "[0.13432884216308594, -1.3474963903427124]\n",
      "[1.2866179943084717, -1.5203595161437988]\n",
      "[0.6402860283851624, -1.411575436592102]\n",
      "[0.8248118758201599, -1.2824251651763916]\n",
      "[-1.1838966608047485, -0.612714409828186]\n",
      "[1.011932373046875, -1.6632636785507202]\n",
      "[1.4944868087768555, -1.5310741662979126]\n",
      "[1.299654245376587, -1.6978204250335693]\n",
      "[-1.4619522094726562, -0.7233892679214478]\n",
      "[1.8017215728759766, -1.7785524129867554]\n",
      "[0.7337988018989563, -1.6499886512756348]\n",
      "[0.48432642221450806, -1.3477917909622192]\n",
      "[-1.490272045135498, -0.7137452363967896]\n",
      "[-0.05744791030883789, -0.8989017009735107]\n",
      "[-0.11083680391311646, -0.49746033549308777]\n",
      "[0.7024665474891663, -0.836379885673523]\n",
      "[0.31160712242126465, -1.530387282371521]\n",
      "[0.5349125862121582, -0.6927692890167236]\n",
      "[-0.5905392169952393, -0.9055360555648804]\n",
      "[0.28282594680786133, -0.8145482540130615]\n",
      "[0.40499013662338257, -1.3875443935394287]\n",
      "[1.0571599006652832, -1.338765263557434]\n",
      "[0.41654348373413086, -0.6077755689620972]\n",
      "[-1.3597620725631714, -0.5822639465332031]\n",
      "[0.7673384547233582, -1.2605130672454834]\n",
      "[0.2895410656929016, -1.2601741552352905]\n",
      "[-1.6258008480072021, -0.626146674156189]\n",
      "[1.658452033996582, -1.6557559967041016]\n",
      "[0.2876013517379761, -1.238325595855713]\n",
      "[-0.5017465949058533, -0.6437369585037231]\n",
      "[0.5962569713592529, -0.7453476190567017]\n",
      "[0.4961116909980774, -1.533996343612671]\n",
      "[0.41560274362564087, -0.6401314735412598]\n",
      "[1.7556686401367188, -1.7681957483291626]\n",
      "[-1.642009973526001, -0.56402587890625]\n",
      "[0.20375871658325195, -1.374122977256775]\n",
      "[0.0898018479347229, -1.2313485145568848]\n",
      "[0.41592586040496826, -0.5907857418060303]\n",
      "[3.0131759643554688, -2.8168909549713135]\n",
      "[1.6285545825958252, -1.7315707206726074]\n",
      "[0.29011160135269165, -1.431857705116272]\n",
      "[1.0995807647705078, -1.5015177726745605]\n",
      "[1.476792335510254, -1.740685224533081]\n",
      "[-1.623666524887085, -0.914886474609375]\n",
      "[0.14451736211776733, -1.1980503797531128]\n",
      "[0.5528276562690735, -1.3784693479537964]\n",
      "[2.555593729019165, -2.424696922302246]\n",
      "[1.0516557693481445, -1.5891990661621094]\n",
      "[-1.2743704319000244, -1.7790074348449707]\n",
      "[0.4296855926513672, -1.3346174955368042]\n",
      "[0.43658220767974854, -1.2766079902648926]\n",
      "[1.5048635005950928, -1.7570734024047852]\n",
      "[0.9812352061271667, -1.4317823648452759]\n",
      "[1.6333136558532715, -1.7626827955245972]\n",
      "[2.0479094982147217, -1.9895603656768799]\n",
      "[2.1043848991394043, -2.0379655361175537]\n",
      "[0.29808908700942993, -1.429552435874939]\n",
      "[1.3035898208618164, -1.5697702169418335]\n",
      "[-3.5711257457733154, -0.7799901962280273]\n",
      "[1.7191803455352783, -1.930126667022705]\n",
      "[1.2582125663757324, -1.5143741369247437]\n",
      "[2.1345596313476562, -2.063828229904175]\n",
      "[1.7857351303100586, -1.89015793800354]\n",
      "[-2.449007272720337, -1.469929575920105]\n",
      "[-1.2998576164245605, -1.0544142723083496]\n",
      "[2.392960786819458, -2.285304069519043]\n",
      "[1.858652114868164, -1.8273476362228394]\n",
      "[0.09706562757492065, -1.387628436088562]\n",
      "[1.855747938156128, -1.9017826318740845]\n",
      "[2.0646471977233887, -2.00390625]\n",
      "[-3.4040300846099854, -0.6824214458465576]\n",
      "[2.2786126136779785, -2.187296152114868]\n",
      "[0.4741959571838379, -1.3691680431365967]\n",
      "[-1.0956096649169922, -1.0968859195709229]\n",
      "[-1.574927568435669, -0.95036780834198]\n",
      "[0.5698455572128296, -1.5137230157852173]\n",
      "[0.41600918769836426, -1.4573633670806885]\n",
      "[0.48895472288131714, -1.3186169862747192]\n",
      "[2.4928653240203857, -2.3709323406219482]\n",
      "[-0.3110398054122925, -1.1377406120300293]\n",
      "[1.6216177940368652, -1.7866294384002686]\n",
      "[-1.6321251392364502, -1.0359705686569214]\n",
      "[-0.6777157187461853, -1.063358187675476]\n",
      "[1.1563892364501953, -1.4449208974838257]\n",
      "[-0.892042338848114, -1.2167630195617676]\n",
      "[0.13074833154678345, -1.5097472667694092]\n",
      "[1.0161051750183105, -1.4819679260253906]\n",
      "[0.8506781458854675, -1.503661870956421]\n",
      "[0.6166511178016663, -1.5701771974563599]\n",
      "[-0.299002468585968, -1.6027909517288208]\n",
      "[-1.295879602432251, -1.0666738748550415]\n",
      "[-3.2722673416137695, -0.9166736602783203]\n",
      "[2.2898952960968018, -2.2499125003814697]\n",
      "[0.12708976864814758, -1.1712331771850586]\n",
      "[0.15692973136901855, -1.179459571838379]\n",
      "[0.08621281385421753, -1.6426111459732056]\n",
      "[2.45332932472229, -2.337045907974243]\n",
      "[2.1404759883880615, -2.068898916244507]\n",
      "[1.6995673179626465, -1.8675248622894287]\n",
      "[-2.9605700969696045, -1.731484293937683]\n",
      "[-1.2496051788330078, -1.261763572692871]\n",
      "[1.9790263175964355, -1.9510807991027832]\n",
      "[-2.7268199920654297, -1.106197714805603]\n",
      "[1.431239366531372, -1.7041212320327759]\n",
      "[0.15883326530456543, -1.185253381729126]\n",
      "[1.0117874145507812, -1.550481915473938]\n",
      "[1.7419970035552979, -1.8465975522994995]\n",
      "[0.44299596548080444, -1.3423830270767212]\n",
      "[-2.8075079917907715, -1.3503632545471191]\n",
      "[2.240382432937622, -2.154529094696045]\n",
      "[-0.7452589869499207, -1.170515537261963]\n",
      "[2.262795925140381, -2.1737396717071533]\n",
      "[1.4685533046722412, -1.716212272644043]\n",
      "[2.2304999828338623, -2.1460587978363037]\n",
      "[0.5002166628837585, -1.494859218597412]\n",
      "[-0.059372782707214355, -1.589216947555542]\n",
      "[-1.0907447338104248, -1.1378133296966553]\n",
      "[2.398074150085449, -2.289686679840088]\n",
      "[-1.1391592025756836, -1.2814651727676392]\n",
      "[1.5249838829040527, -1.7690602540969849]\n",
      "[0.25449150800704956, -1.4059414863586426]\n",
      "[1.7110846042633057, -1.8532958030700684]\n",
      "[0.2870098948478699, -1.2684612274169922]\n",
      "[-0.8082140684127808, -1.5120034217834473]\n",
      "[1.256289005279541, -1.6279716491699219]\n",
      "[0.4505234360694885, -1.293518304824829]\n",
      "[1.3409323692321777, -1.5539793968200684]\n",
      "[1.8019006252288818, -1.879311442375183]\n",
      "[-2.2432937622070312, -0.6827176809310913]\n",
      "[1.380418300628662, -1.667884111404419]\n",
      "[0.4653131365776062, -1.3724052906036377]\n",
      "[-0.46991944313049316, -1.235939621925354]\n",
      "[2.119816541671753, -2.058900833129883]\n",
      "[-0.9248324632644653, -1.5907052755355835]\n",
      "[1.1513252258300781, -1.4160902500152588]\n",
      "[1.3261613845825195, -1.6118744611740112]\n",
      "[2.2260806560516357, -2.191279888153076]\n",
      "[1.7485520839691162, -1.9429988861083984]\n",
      "[-0.8501624464988708, -1.3366559743881226]\n",
      "[0.5298446416854858, -2.1767022609710693]\n",
      "[-0.5466824769973755, -1.453844428062439]\n",
      "[0.23835045099258423, -1.0133075714111328]\n",
      "[-0.35655057430267334, -1.2842497825622559]\n",
      "[-1.3468303680419922, -0.890105128288269]\n",
      "[1.1810147762298584, -1.2465441226959229]\n",
      "[1.399294376373291, -1.7273532152175903]\n",
      "[0.3026937246322632, -1.4158862829208374]\n",
      "[-0.07546639442443848, -1.9213974475860596]\n",
      "[-0.7821212410926819, -1.0299152135849]\n",
      "[1.5768711566925049, -1.5858328342437744]\n",
      "[0.45025748014450073, -1.4432721138000488]\n",
      "[1.92771315574646, -1.9118951559066772]\n",
      "[-1.1634464263916016, -1.0885542631149292]\n",
      "[-0.9128005504608154, -0.8309518098831177]\n",
      "[-1.8219540119171143, -0.7730305194854736]\n",
      "[-0.25550493597984314, -1.7156896591186523]\n",
      "[-2.311561107635498, -1.0073460340499878]\n",
      "[0.11759614944458008, -0.7491614818572998]\n",
      "[0.41129374504089355, -1.703782558441162]\n",
      "[1.907500982284546, -1.9255421161651611]\n",
      "[-0.13752490282058716, -1.6117308139801025]\n",
      "[-1.148031234741211, -0.9504544734954834]\n",
      "[-2.1815850734710693, -0.9350951910018921]\n",
      "[-0.9020242691040039, -1.3370680809020996]\n",
      "[1.7167975902557373, -1.8979885578155518]\n",
      "[0.8240700364112854, -0.9406063556671143]\n",
      "[-1.8581559658050537, -1.193773627281189]\n",
      "[-0.6962552070617676, -1.4847873449325562]\n",
      "[0.3180456757545471, -0.8032665252685547]\n",
      "[-0.8025480508804321, -1.4350193738937378]\n",
      "[-1.7520751953125, -0.6963399648666382]\n",
      "[2.2786126136779785, -2.187296152114868]\n",
      "[-2.470839500427246, -1.0958855152130127]\n",
      "[0.0778556764125824, -1.2565213441848755]\n",
      "[0.7974914908409119, -0.9178259372711182]\n",
      "[1.0074427127838135, -1.0977753400802612]\n",
      "[-0.025080472230911255, -0.8098791837692261]\n",
      "[0.007444709539413452, -0.8332597017288208]\n",
      "[1.104217529296875, -1.746028184890747]\n",
      "[-2.0354928970336914, -1.086138367652893]\n",
      "[1.9417893886566162, -1.8986047506332397]\n",
      "[0.4074122905731201, -1.7484838962554932]\n",
      "[0.9310557246208191, -1.0323039293289185]\n",
      "[1.366896390914917, -1.4058634042739868]\n",
      "[-0.8203104734420776, -1.0997830629348755]\n",
      "[2.25062894821167, -2.16331148147583]\n",
      "[2.4554312229156494, -2.3388476371765137]\n",
      "[0.03426074981689453, -1.4740406274795532]\n",
      "[-0.4309992790222168, -0.7512335777282715]\n",
      "[-0.2478083074092865, -0.6067866086959839]\n",
      "[0.6512364745140076, -0.7924706935882568]\n",
      "[0.798214852809906, -0.9184459447860718]\n",
      "[2.1652684211730957, -2.145299196243286]\n",
      "[0.6209273934364319, -2.0195813179016113]\n",
      "[0.9139893651008606, -1.5433051586151123]\n",
      "[0.9983920454978943, -1.5979108810424805]\n",
      "[0.4695731997489929, -0.6367669105529785]\n",
      "[-1.6689000129699707, -1.163249135017395]\n",
      "[-2.2667760848999023, -0.9824509620666504]\n",
      "[-0.5510613918304443, -1.491998314857483]\n",
      "[1.8077058792114258, -2.358243227005005]\n",
      "[0.8542618155479431, -0.9664838314056396]\n",
      "[-0.8056596517562866, -1.4899256229400635]\n",
      "[1.7638425827026367, -1.809110164642334]\n",
      "[-0.07667040824890137, -1.5873432159423828]\n",
      "[0.6689656376838684, -1.4753282070159912]\n",
      "[-0.7870292663574219, -0.8073315620422363]\n",
      "[0.16775333881378174, -1.7870196104049683]\n",
      "[0.9932565093040466, -1.6819292306900024]\n",
      "[1.9298651218414307, -2.0843355655670166]\n",
      "[-2.309147596359253, -1.0060044527053833]\n",
      "[1.8660101890563965, -1.8336541652679443]\n",
      "[-0.32958686351776123, -1.7346137762069702]\n",
      "[-0.24509868025779724, -1.5236198902130127]\n",
      "[-2.3203883171081543, -1.012252926826477]\n",
      "[-0.0832226574420929, -1.0985076427459717]\n",
      "[0.4562367796897888, -0.6253361701965332]\n",
      "[0.8391801714897156, -0.9535572528839111]\n",
      "[-0.8398535251617432, -1.5707100629806519]\n",
      "[0.7795817255973816, -0.9024754762649536]\n",
      "[-1.1500730514526367, -1.0919331312179565]\n",
      "[0.2877126932144165, -0.9850726127624512]\n",
      "[-0.4305275082588196, -1.5279626846313477]\n",
      "[0.6846833825111389, -1.522153377532959]\n",
      "[0.7911399006843567, -0.9123820066452026]\n",
      "[-1.077444314956665, -0.7489591836929321]\n",
      "[0.3767938017845154, -1.4406479597091675]\n",
      "[-0.36669445037841797, -1.4182106256484985]\n",
      "[-1.935194492340088, -0.7981319427490234]\n",
      "[1.7276694774627686, -1.7150821685791016]\n",
      "[-0.3916708827018738, -1.415162205696106]\n",
      "[-0.014515846967697144, -0.8113070726394653]\n",
      "[0.8135859370231628, -0.9316204786300659]\n",
      "[-0.5038588047027588, -1.644657850265503]\n",
      "[0.961115300655365, -1.058068037033081]\n",
      "[1.0511577129364014, -1.8381918668746948]\n",
      "[-1.1847071647644043, -0.7031670808792114]\n",
      "[-0.8637439608573914, -1.4767440557479858]\n",
      "[-0.7685242891311646, -1.3717948198318481]\n",
      "[0.6638949513435364, -0.8033202886581421]\n",
      "[1.0338337421417236, -2.1894967555999756]\n",
      "[-1.2777503728866577, -1.0662333965301514]\n",
      "[-1.201966404914856, -0.744192361831665]\n",
      "[-1.466594934463501, -0.8893798589706421]\n",
      "[-1.1105411052703857, -1.0140057802200317]\n",
      "[-0.04149031639099121, -0.5935769081115723]\n",
      "[-0.9087932109832764, -0.8543510437011719]\n",
      "[-1.5383079051971436, -0.6965725421905518]\n",
      "[0.2163938283920288, -1.8713620901107788]\n",
      "[-1.3790311813354492, -0.8218436241149902]\n",
      "[0.7444748282432556, -1.5533167123794556]\n",
      "[-1.4254145622253418, -0.7767709493637085]\n",
      "[-0.43021661043167114, -1.0597712993621826]\n",
      "[-1.3714971542358398, -0.9620620012283325]\n",
      "[-1.4422383308410645, -0.8642038106918335]\n",
      "[-1.3511860370635986, -0.9402385950088501]\n",
      "[-0.567672848701477, -1.431140661239624]\n",
      "[-0.33513393998146057, -1.2449102401733398]\n",
      "[-1.273201823234558, -0.5302197933197021]\n",
      "[-0.9353463053703308, -1.0731956958770752]\n",
      "[-0.31822264194488525, -1.0860092639923096]\n",
      "[-0.7733217477798462, -1.259054183959961]\n",
      "[-1.4737966060638428, -0.881313681602478]\n",
      "[-0.5940924882888794, -1.2052937746047974]\n",
      "[-1.0846753120422363, -1.1862945556640625]\n",
      "[0.08323144912719727, -1.5639840364456177]\n",
      "[-0.5419559478759766, -0.4493781626224518]\n",
      "[-0.07842424511909485, -1.6092580556869507]\n",
      "[-1.030855655670166, -1.2031095027923584]\n",
      "[-1.4168832302093506, -0.5201245546340942]\n",
      "[-1.0503126382827759, -1.2272106409072876]\n",
      "[0.1185230016708374, -1.049949288368225]\n",
      "[0.47419577836990356, -1.3691679239273071]\n",
      "[0.07785555720329285, -1.2565213441848755]\n",
      "[-1.5934481620788574, -0.649915337562561]\n",
      "[-0.9419501423835754, -0.35250213742256165]\n",
      "[-0.2536444664001465, -0.6074918508529663]\n",
      "[-1.1381354331970215, -0.666214108467102]\n",
      "[-1.2231682538986206, -0.67664635181427]\n",
      "[-1.080775260925293, -0.8913286924362183]\n",
      "[0.1284116506576538, -1.6148616075515747]\n",
      "[-0.46011102199554443, -0.9396499395370483]\n",
      "[-0.5395999550819397, -1.2424911260604858]\n",
      "[-0.08500081300735474, -0.6839892864227295]\n",
      "[-0.6234762668609619, -0.6461488008499146]\n",
      "[-1.8293745517730713, -0.7393089532852173]\n",
      "[0.9701835513114929, -1.1169074773788452]\n",
      "[0.2455022931098938, -1.31782865524292]\n",
      "[-1.7964565753936768, -0.8287036418914795]\n",
      "[-1.1696052551269531, -0.7119669914245605]\n",
      "[-0.606045663356781, -0.5579051971435547]\n",
      "[0.35595160722732544, -0.6252175569534302]\n",
      "[-0.5303025841712952, -0.45433273911476135]\n",
      "[0.20555084943771362, -1.4532192945480347]\n",
      "[0.16809356212615967, -1.698049545288086]\n",
      "[-1.3041608333587646, -0.7068952322006226]\n",
      "[-1.2716336250305176, -0.7324438095092773]\n",
      "[0.35360056161880493, -0.5373666286468506]\n",
      "[-0.11969897150993347, -1.5256829261779785]\n",
      "[-0.09786254167556763, -1.1822304725646973]\n",
      "[-1.1208642721176147, -1.202669382095337]\n",
      "[0.03411990404129028, -2.0536322593688965]\n",
      "[-0.3411203622817993, -0.6246161460876465]\n",
      "[-0.9200614094734192, -1.3650867938995361]\n",
      "[-0.16508018970489502, -1.2148702144622803]\n",
      "[-1.2446056604385376, -1.0816367864608765]\n",
      "[-1.6402263641357422, -0.6345466375350952]\n",
      "[-1.2031030654907227, -0.8534654378890991]\n",
      "[-0.7222340703010559, -1.3221062421798706]\n",
      "[-1.0402307510375977, -0.8821450471878052]\n",
      "[0.15827888250350952, -1.6474148035049438]\n",
      "[-0.16636022925376892, -1.3524913787841797]\n",
      "[0.17622911930084229, -0.9625179767608643]\n",
      "[-0.30990496277809143, -1.5842761993408203]\n",
      "[-1.2794411182403564, -1.067694902420044]\n",
      "[-0.24036139249801636, -1.3007960319519043]\n",
      "[-1.2879211902618408, -0.7853668928146362]\n",
      "[0.1922479271888733, -0.5027998685836792]\n",
      "[-0.9849715232849121, -0.37110233306884766]\n",
      "[-0.16443738341331482, -1.6080007553100586]\n",
      "[0.10537353157997131, -0.6972731351852417]\n",
      "[-1.2170300483703613, -1.0349198579788208]\n",
      "[-1.212836503982544, -0.7172355651855469]\n",
      "[-1.0320477485656738, -1.187954306602478]\n",
      "[-1.4624686241149902, -0.7073671817779541]\n",
      "[0.4456189274787903, -0.918447732925415]\n",
      "[-1.149775505065918, -0.870303750038147]\n",
      "[-1.6988236904144287, -0.6667386293411255]\n",
      "[-1.5473449230194092, -0.8855245113372803]\n",
      "[-0.7829331159591675, -1.0221115350723267]\n",
      "[-0.06779727339744568, -0.5652261972427368]\n",
      "[-1.423888921737671, -0.9668562412261963]\n",
      "[-1.3928043842315674, -0.6397241353988647]\n",
      "[-1.3970541954040527, -0.41519081592559814]\n",
      "[-0.6172417402267456, -1.4671542644500732]\n",
      "[0.44073373079299927, -1.0743104219436646]\n",
      "[-0.9732795357704163, -1.031799554824829]\n",
      "[-1.2489724159240723, -0.8495728969573975]\n",
      "[-0.4561406373977661, -1.4931340217590332]\n",
      "[-0.9798516035079956, -1.1818888187408447]\n",
      "[-0.02128174901008606, -0.608812689781189]\n",
      "[1.9173779487609863, -1.8776816129684448]\n",
      "[0.2894361615180969, -0.6699801683425903]\n",
      "[-0.3943895101547241, -0.41664546728134155]\n",
      "[-0.6012383699417114, -0.40841466188430786]\n",
      "[0.08621194958686829, -0.46517816185951233]\n",
      "[-1.2463839054107666, -0.49604877829551697]\n",
      "[0.5768716931343079, -0.8062927722930908]\n",
      "[0.27938956022262573, -0.7588506937026978]\n",
      "[1.4488978385925293, -1.476146936416626]\n",
      "[-0.30173736810684204, -0.43440917134284973]\n",
      "[-0.7619615197181702, -1.325254201889038]\n",
      "[-0.9037384986877441, -0.4080725312232971]\n",
      "[0.8545545935630798, -0.966734766960144]\n",
      "[0.022330522537231445, -0.5078932046890259]\n",
      "[-0.5848103165626526, -0.39521586894989014]\n",
      "[0.10558927059173584, -0.5201761722564697]\n",
      "[0.9769375920295715, -1.071629285812378]\n",
      "[0.6632410883903503, -0.8027597665786743]\n",
      "[-0.40238156914711, -0.39570143818855286]\n",
      "[0.8533515334129333, -1.0634052753448486]\n",
      "[-1.8989789485931396, -0.8409014940261841]\n",
      "[0.842939555644989, -0.9567794799804688]\n",
      "[-0.5335588455200195, -0.42827990651130676]\n",
      "[0.6218824982643127, -0.7673113346099854]\n",
      "[0.5300277471542358, -0.6885825395584106]\n",
      "[-1.5490365028381348, -1.2490986585617065]\n",
      "[-1.691652774810791, -0.2413339614868164]\n",
      "[1.0703985691070557, -1.1517348289489746]\n",
      "[0.6864370703697205, -0.822641134262085]\n",
      "[-0.6086753010749817, -0.3322073221206665]\n",
      "[0.642447292804718, -0.7849373817443848]\n",
      "[0.6814566254615784, -0.818372368812561]\n",
      "[-1.0956096649169922, -1.0968859195709229]\n",
      "[0.797491729259491, -0.9178260564804077]\n",
      "[-0.9419500827789307, -0.35250216722488403]\n",
      "[-1.5954687595367432, -0.2065882384777069]\n",
      "[-1.3614943027496338, -0.42572566866874695]\n",
      "[-0.19989031553268433, -0.4031107425689697]\n",
      "[-0.36076706647872925, -0.3704786002635956]\n",
      "[0.7125949263572693, -0.8749446868896484]\n",
      "[1.094926357269287, -1.172757625579834]\n",
      "[0.36032551527023315, -0.8717966079711914]\n",
      "[1.195169448852539, -1.2586759328842163]\n",
      "[-1.6961631774902344, -0.3793545663356781]\n",
      "[-0.5489911437034607, -0.5319602489471436]\n",
      "[-0.5420196652412415, -0.4504448473453522]\n",
      "[0.5058925747871399, -1.2891387939453125]\n",
      "[0.874662458896637, -1.2696030139923096]\n",
      "[-0.7097086310386658, -0.4252970516681671]\n",
      "[0.0013045668601989746, -0.369304895401001]\n",
      "[0.05838534235954285, -0.5785312652587891]\n",
      "[-0.5048016309738159, -0.7792848348617554]\n",
      "[-1.6117162704467773, -0.278473824262619]\n",
      "[-1.429244041442871, -1.147714614868164]\n",
      "[1.3752329349517822, -1.413008689880371]\n",
      "[0.3539122939109802, -0.7047003507614136]\n",
      "[0.21385329961776733, -0.6740825176239014]\n",
      "[-0.20027583837509155, -0.7792659997940063]\n",
      "[0.9642812609672546, -1.0607815980911255]\n",
      "[0.6659155488014221, -0.8050520420074463]\n",
      "[0.5856413245201111, -0.7373340129852295]\n",
      "[-1.5769000053405762, -1.7542340755462646]\n",
      "[-1.6306476593017578, -0.4610423147678375]\n",
      "[0.7698845267295837, -0.894163966178894]\n",
      "[-1.8475065231323242, -0.8953797817230225]\n",
      "[0.3220384120941162, -0.6925852298736572]\n",
      "[-0.4893862009048462, -0.48790696263313293]\n",
      "[-0.024135708808898926, -0.40005072951316833]\n",
      "[0.873526394367218, -0.9829955101013184]\n",
      "[0.6738894581794739, -0.8881963491439819]\n",
      "[-1.5096290111541748, -1.3162517547607422]\n",
      "[0.7908458113670349, -0.9121299982070923]\n",
      "[0.33879369497299194, -1.0663048028945923]\n",
      "[1.0970451831817627, -1.1745736598968506]\n",
      "[0.23013067245483398, -0.6500939130783081]\n",
      "[0.7473180890083313, -0.8748222589492798]\n",
      "[-0.3016854226589203, -0.49761196970939636]\n",
      "[-0.39046281576156616, -0.7233031988143921]\n",
      "[-1.4299802780151367, -0.2943323254585266]\n",
      "[1.1735551357269287, -1.2401503324508667]\n",
      "[-1.287515640258789, -0.5452109575271606]\n",
      "[0.05931568145751953, -0.4970800578594208]\n",
      "[-0.4260881543159485, -0.4027637243270874]\n",
      "[0.6648356318473816, -0.804126501083374]\n",
      "[-0.05349910259246826, -0.6515462398529053]\n",
      "[-0.8165346384048462, -0.8106578588485718]\n",
      "[0.13925713300704956, -0.4348627030849457]\n",
      "[-0.18351495265960693, -0.5980708599090576]\n",
      "[-0.17444610595703125, -0.5319715738296509]\n",
      "[0.4303285479545593, -0.6031302213668823]\n",
      "[-1.3525359630584717, -0.49606606364250183]\n",
      "[0.10209214687347412, -0.5801433324813843]\n",
      "[-0.3496054708957672, -0.3089545667171478]\n",
      "[-1.1349141597747803, -0.253095805644989]\n",
      "[0.9733580946922302, -1.068561315536499]\n",
      "[-0.8938712477684021, -0.9244309663772583]\n",
      "[1.0309984683990479, -1.1179651021957397]\n",
      "[0.025301218032836914, -0.44873008131980896]\n",
      "[0.916467010974884, -1.0197999477386475]\n",
      "[0.6395570635795593, -0.7824602127075195]\n",
      "[-1.1762490272521973, -0.5237466096878052]\n",
      "[1.9125335216522217, -1.873529314994812]\n",
      "[0.5424342155456543, -0.7187693119049072]\n",
      "[0.03155401349067688, -0.7038450241088867]\n",
      "[-0.20177608728408813, -0.5131254196166992]\n",
      "[0.6211799383163452, -0.7667092084884644]\n",
      "[-1.8169605731964111, -0.28978031873703003]\n",
      "[0.12813794612884521, -0.6073566675186157]\n",
      "[0.4102303981781006, -0.7510474920272827]\n",
      "[1.3579602241516113, -1.3982040882110596]\n",
      "[0.4650428295135498, -0.8405263423919678]\n",
      "[-0.579657256603241, -1.5086193084716797]\n",
      "[-0.38860028982162476, -0.5776649713516235]\n",
      "[0.4586983323097229, -0.7398720979690552]\n",
      "[0.579465925693512, -0.7309560775756836]\n",
      "[0.16232389211654663, -0.5612931251525879]\n",
      "[0.5625544786453247, -0.7164613008499146]\n",
      "[0.9394922852516174, -1.0395349264144897]\n",
      "[0.8172513842582703, -0.9347621202468872]\n",
      "[0.110918790102005, -0.7174279689788818]\n",
      "[0.861265242099762, -0.9724863767623901]\n",
      "[-2.3186609745025635, -0.6974896192550659]\n",
      "[1.0516979694366455, -1.1357064247131348]\n",
      "[-0.06807255744934082, -0.5108492374420166]\n",
      "[0.7799221873283386, -0.9027673006057739]\n",
      "[0.6165951490402222, -0.7627794742584229]\n",
      "[-1.533156394958496, -1.340573787689209]\n",
      "[-1.5448932647705078, -0.4001924693584442]\n",
      "[1.0967490673065186, -1.174319863319397]\n",
      "[0.6482523083686829, -0.7899129390716553]\n",
      "[-0.17561155557632446, -0.6254632472991943]\n",
      "[0.6730162501335144, -0.8111381530761719]\n",
      "[0.9453808665275574, -1.0445820093154907]\n",
      "[-1.5749270915985107, -0.9503679275512695]\n",
      "[1.0074427127838135, -1.0977753400802612]\n",
      "[-0.25364434719085693, -0.6074918508529663]\n",
      "[-1.3614943027496338, -0.42572566866874695]\n",
      "[-1.9045336246490479, -0.28808942437171936]\n",
      "[0.34269487857818604, -0.7413694858551025]\n",
      "[0.17721664905548096, -0.7049078941345215]\n",
      "[0.41120433807373047, -0.6894174814224243]\n",
      "[1.145622730255127, -1.2162095308303833]\n",
      "[-0.15650497376918793, -0.6395876407623291]\n",
      "[1.1975135803222656, -1.2606853246688843]\n",
      "[-1.6763601303100586, -0.4739992320537567]\n",
      "[-0.9138832688331604, -0.39123982191085815]\n",
      "[0.0041595399379730225, -0.5450277328491211]\n",
      "[-0.09194502234458923, -1.0193051099777222]\n",
      "[0.3445212244987488, -1.0305094718933105]\n",
      "[-0.18236511945724487, -0.6378024816513062]\n",
      "[0.5133189558982849, -0.7805622816085815]\n",
      "[0.5392817854881287, -0.8998377323150635]\n",
      "[-0.10391485691070557, -1.0638973712921143]\n",
      "[-1.4093034267425537, -0.46226605772972107]\n",
      "[-1.774155616760254, -1.0634586811065674]\n",
      "[1.4766948223114014, -1.499971628189087]\n",
      "[0.049843162298202515, -0.559563398361206]\n",
      "[-0.024449825286865234, -0.5309122800827026]\n",
      "[0.22635698318481445, -1.0762935876846313]\n",
      "[1.059647798538208, -1.1425204277038574]\n",
      "[0.9068108201026917, -1.0115236043930054]\n",
      "[0.7043824791908264, -0.8380221128463745]\n",
      "[-1.6707336902618408, -1.7886708974838257]\n",
      "[-1.234268307685852, -0.7149648666381836]\n",
      "[0.7651744484901428, -0.8901270627975464]\n",
      "[-1.9540221691131592, -0.9187694787979126]\n",
      "[0.5825480818748474, -0.9048348665237427]\n",
      "[-0.4252791106700897, -0.42934170365333557]\n",
      "[0.6081252098083496, -0.8373873233795166]\n",
      "[1.029329538345337, -1.1165344715118408]\n",
      "[0.3757503032684326, -0.7209397554397583]\n",
      "[-1.613560438156128, -1.3507614135742188]\n",
      "[0.9132738709449768, -1.0170631408691406]\n",
      "[-0.27965518832206726, -0.8222261667251587]\n",
      "[1.0616717338562012, -1.1442550420761108]\n",
      "[0.5512717366218567, -0.8276416063308716]\n",
      "[0.8904263377189636, -0.9974805116653442]\n",
      "[0.26535505056381226, -0.7944861650466919]\n",
      "[0.035479962825775146, -1.0165547132492065]\n",
      "[-1.3230032920837402, -0.4461635649204254]\n",
      "[1.1069774627685547, -1.183086633682251]\n",
      "[-0.9574707746505737, -0.7840216159820557]\n",
      "[0.5358604192733765, -0.6935817003250122]\n",
      "[-0.004183679819107056, -0.6850860118865967]\n",
      "[0.7955141663551331, -0.9161312580108643]\n",
      "[-0.026545584201812744, -0.5799318552017212]\n",
      "[-0.4881862998008728, -1.0580476522445679]\n",
      "[0.8075689673423767, -0.9264633655548096]\n",
      "[-0.007928818464279175, -0.5840386152267456]\n",
      "[0.2847554087638855, -0.6558215618133545]\n",
      "[0.7440747618675232, -0.8720424175262451]\n",
      "[-1.8293728828430176, -0.34276410937309265]\n",
      "[0.48519766330718994, -0.8085548877716064]\n",
      "[0.04835185408592224, -0.5407031774520874]\n",
      "[-0.7955236434936523, -0.5020154714584351]\n",
      "[0.9365969300270081, -1.0370532274246216]\n",
      "[-0.5805656313896179, -1.1629496812820435]\n",
      "[0.7914294600486755, -0.9126302003860474]\n",
      "[0.4983026385307312, -0.6613909006118774]\n",
      "[0.9445357918739319, -1.0438576936721802]\n",
      "[0.8488790392875671, -0.9618701934814453]\n",
      "[-0.7671686410903931, -0.7961515188217163]\n",
      "[1.9752283096313477, -2.045487642288208]\n",
      "[-0.18118947744369507, -1.0943210124969482]\n",
      "[-1.3238694667816162, -0.35142281651496887]\n",
      "[-1.1242599487304688, -0.7633203268051147]\n",
      "[-1.5011825561523438, -0.5188069343566895]\n",
      "[0.41778355836868286, -0.8556557893753052]\n",
      "[0.5120588541030884, -1.160641074180603]\n",
      "[-0.6500177383422852, -0.8553493022918701]\n",
      "[1.1889729499816895, -1.753997564315796]\n",
      "[-1.5434482097625732, -0.5253815650939941]\n",
      "[-0.011475324630737305, -1.2272814512252808]\n",
      "[-0.7613650560379028, -0.8427658081054688]\n",
      "[1.6895959377288818, -1.6824493408203125]\n",
      "[-1.039928674697876, -0.7108240127563477]\n",
      "[-1.7411525249481201, -0.42668434977531433]\n",
      "[-1.5522449016571045, -0.516618013381958]\n",
      "[0.5024939775466919, -1.4616048336029053]\n",
      "[-0.30940359830856323, -0.7963920831680298]\n",
      "[-1.1717486381530762, -0.25098395347595215]\n",
      "[0.39450907707214355, -1.2654471397399902]\n",
      "[-0.2320268750190735, -1.349703311920166]\n",
      "[0.3602895140647888, -1.2664858102798462]\n",
      "[-1.7666723728179932, -0.5133758783340454]\n",
      "[-0.31440597772598267, -0.8443901538848877]\n",
      "[-0.17346937954425812, -1.0430845022201538]\n",
      "[-0.3628832697868347, -1.436466097831726]\n",
      "[-0.14188188314437866, -0.4994606077671051]\n",
      "[0.4240800142288208, -1.2902346849441528]\n",
      "[-0.010265976190567017, -1.2214757204055786]\n",
      "[-1.1821632385253906, -0.27606311440467834]\n",
      "[-0.08723855018615723, -1.1488168239593506]\n",
      "[-0.24678336083889008, -0.6196029186248779]\n",
      "[0.5698457956314087, -1.5137230157852173]\n",
      "[-0.025080472230911255, -0.8098791837692261]\n",
      "[-1.1381354331970215, -0.666214108467102]\n",
      "[-0.19989046454429626, -0.4031108319759369]\n",
      "[0.34269487857818604, -0.7413694858551025]\n",
      "[-1.2681915760040283, -0.24638935923576355]\n",
      "[-1.3333630561828613, -0.29509615898132324]\n",
      "[0.31130343675613403, -1.1848434209823608]\n",
      "[0.5299133062362671, -1.2578372955322266]\n",
      "[1.4352030754089355, -1.4644091129302979]\n",
      "[0.7331351637840271, -1.3943653106689453]\n",
      "[-0.020248323678970337, -0.6871674060821533]\n",
      "[0.5176653861999512, -0.8352271318435669]\n",
      "[-1.5500683784484863, -0.6271299123764038]\n",
      "[1.7285130023956299, -1.851637601852417]\n",
      "[1.9372351169586182, -1.8947011232376099]\n",
      "[-0.7966526746749878, -0.9155899286270142]\n",
      "[-1.38075590133667, -0.30348068475723267]\n",
      "[-1.0158557891845703, -0.24881350994110107]\n",
      "[-0.29764869809150696, -0.42222896218299866]\n",
      "[-0.26213783025741577, -0.4425320327281952]\n",
      "[0.16922509670257568, -1.5409893989562988]\n",
      "[1.308974027633667, -1.7263948917388916]\n",
      "[-0.03362107276916504, -1.0131664276123047]\n",
      "[-0.22269952297210693, -0.9888781309127808]\n",
      "[-0.2257569432258606, -0.36899739503860474]\n",
      "[0.4031323790550232, -1.2087267637252808]\n",
      "[-0.23859518766403198, -0.7275304794311523]\n",
      "[-0.06046724319458008, -1.1380200386047363]\n",
      "[-0.35970765352249146, -2.0024569034576416]\n",
      "[-0.7802400588989258, -0.46436217427253723]\n",
      "[0.040689051151275635, -1.2241079807281494]\n",
      "[-0.35450470447540283, -1.2426562309265137]\n",
      "[0.017779290676116943, -1.1419742107391357]\n",
      "[-0.7242281436920166, -0.8431878089904785]\n",
      "[-1.5990631580352783, -0.3677619397640228]\n",
      "[0.4931607246398926, -1.418914794921875]\n",
      "[0.329164981842041, -1.1549850702285767]\n",
      "[-0.16552472114562988, -1.5841420888900757]\n",
      "[-0.033085137605667114, -0.9303233623504639]\n",
      "[1.2715544700622559, -1.324145793914795]\n",
      "[0.6971495747566223, -1.5256870985031128]\n",
      "[-0.06995239853858948, -1.0943881273269653]\n",
      "[-0.11136859655380249, -0.8803036212921143]\n",
      "[-1.3579869270324707, -0.48764660954475403]\n",
      "[-0.3486538529396057, -0.3371000289916992]\n",
      "[-0.11375509202480316, -0.3676401376724243]\n",
      "[0.6563556790351868, -1.5083664655685425]\n",
      "[-0.3718024790287018, -0.49404093623161316]\n",
      "[-1.0527982711791992, -0.7111015319824219]\n",
      "[-1.3242642879486084, -0.3172527849674225]\n",
      "[0.05422848463058472, -1.1922637224197388]\n",
      "[-0.6199868321418762, -0.8979712724685669]\n",
      "[-0.31258124113082886, -0.5665264129638672]\n",
      "[-1.6150174140930176, -0.36266809701919556]\n",
      "[-0.8139315247535706, -0.8511866331100464]\n",
      "[-0.42799779772758484, -0.9756866693496704]\n",
      "[-1.0053215026855469, -0.5543527603149414]\n",
      "[0.3450513482093811, -1.0262354612350464]\n",
      "[-0.26395413279533386, -0.9735935926437378]\n",
      "[-1.3454346656799316, -0.31578686833381653]\n",
      "[-0.6741870641708374, -0.2643260061740875]\n",
      "[0.38866549730300903, -1.4014484882354736]\n",
      "[-0.4303126335144043, -0.7032270431518555]\n",
      "[1.039815902709961, -1.424246072769165]\n",
      "[-1.6355252265930176, -0.37658682465553284]\n",
      "[0.411345899105072, -1.3088494539260864]\n",
      "[-0.04875636100769043, -1.0795460939407349]\n",
      "[-0.5816773772239685, -0.3518139123916626]\n",
      "[1.985879898071289, -2.0327138900756836]\n",
      "[-0.22506240010261536, -1.067809820175171]\n",
      "[-1.4150128364562988, -0.3649809658527374]\n",
      "[-1.2065069675445557, -0.7660518884658813]\n",
      "[-1.5344643592834473, -0.5426712036132812]\n",
      "[0.2577516436576843, -0.8149756193161011]\n",
      "[0.8110843300819397, -1.1593455076217651]\n",
      "[-0.5885477066040039, -0.8706644773483276]\n",
      "[1.1897430419921875, -1.7440752983093262]\n",
      "[-1.5876030921936035, -0.5452255010604858]\n",
      "[-0.14669620990753174, -1.2014718055725098]\n",
      "[-0.8898639678955078, -0.8312933444976807]\n",
      "[1.511246681213379, -1.5295860767364502]\n",
      "[-1.2300806045532227, -0.7021993398666382]\n",
      "[-1.7668812274932861, -0.4595845639705658]\n",
      "[-1.6617207527160645, -0.534311056137085]\n",
      "[0.48460787534713745, -1.4393326044082642]\n",
      "[-0.28773337602615356, -0.8063958883285522]\n",
      "[-1.2171108722686768, -0.2776278853416443]\n",
      "[0.4010183811187744, -1.246836543083191]\n",
      "[-0.40343910455703735, -1.2452666759490967]\n",
      "[0.3177341818809509, -1.2538114786148071]\n",
      "[-1.8031806945800781, -0.5334459543228149]\n",
      "[-0.304118275642395, -0.8464870452880859]\n",
      "[-0.20962247252464294, -1.0296061038970947]\n",
      "[-0.5199809670448303, -1.380652666091919]\n",
      "[-0.30891522765159607, -0.46182456612586975]\n",
      "[0.4278789758682251, -1.2872167825698853]\n",
      "[-0.03569534420967102, -1.1937428712844849]\n",
      "[-1.2344698905944824, -0.30397921800613403]\n",
      "[-0.12213598191738129, -1.132117509841919]\n",
      "[-0.3575315475463867, -0.6385655403137207]\n",
      "[0.41600948572158813, -1.457363486289978]\n",
      "[0.0074446797370910645, -0.8332598209381104]\n",
      "[-1.2231683731079102, -0.6766464710235596]\n",
      "[-0.3607671856880188, -0.3704786002635956]\n",
      "[0.17721682786941528, -0.7049078941345215]\n",
      "[-1.3333630561828613, -0.29509615898132324]\n",
      "[-1.3469691276550293, -0.2985995411872864]\n",
      "[0.6543753743171692, -1.1924519538879395]\n",
      "[0.5373600721359253, -1.2561007738113403]\n",
      "[1.3221070766448975, -1.3674744367599487]\n",
      "[0.7932228446006775, -1.3893978595733643]\n",
      "[-0.194394052028656, -0.6463634967803955]\n",
      "[0.46969276666641235, -0.8106945753097534]\n",
      "[-1.6123006343841553, -0.6327564716339111]\n",
      "[1.6230521202087402, -1.7323449850082397]\n",
      "[1.8344295024871826, -1.8065863847732544]\n",
      "[-0.8900095820426941, -0.9169158935546875]\n",
      "[-1.4106557369232178, -0.3332047164440155]\n",
      "[-1.0858542919158936, -0.2692073583602905]\n",
      "[-0.3857012093067169, -0.3986767530441284]\n",
      "[-0.43030664324760437, -0.40593618154525757]\n",
      "[0.012480497360229492, -1.4806749820709229]\n",
      "[1.2912037372589111, -1.7051856517791748]\n",
      "[0.33700627088546753, -1.0042047500610352]\n",
      "[-0.01274031400680542, -0.9849942922592163]\n",
      "[-0.2837139368057251, -0.36602410674095154]\n",
      "[0.405150830745697, -1.2071082592010498]\n",
      "[-0.23205766081809998, -0.7505283355712891]\n",
      "[-0.10907840728759766, -1.1218466758728027]\n",
      "[-0.5068216919898987, -1.9332025051116943]\n",
      "[-0.9715978503227234, -0.4122384786605835]\n",
      "[0.006928801536560059, -1.2120678424835205]\n",
      "[-0.5255832672119141, -1.1766462326049805]\n",
      "[-0.05266407132148743, -1.1174019575119019]\n",
      "[-0.7792963981628418, -0.8307099342346191]\n",
      "[-1.6250627040863037, -0.4000028073787689]\n",
      "[0.4557878375053406, -1.3930184841156006]\n",
      "[0.6285626292228699, -1.1677528619766235]\n",
      "[-0.3297094702720642, -1.522091031074524]\n",
      "[-0.016360700130462646, -0.9334132671356201]\n",
      "[1.208601713180542, -1.2701889276504517]\n",
      "[0.6856345534324646, -1.5119205713272095]\n",
      "[-0.13834166526794434, -1.0729891061782837]\n",
      "[-0.09192261099815369, -0.8873560428619385]\n",
      "[-1.4071855545043945, -0.5135887861251831]\n",
      "[-0.39839863777160645, -0.32775774598121643]\n",
      "[-0.2505311667919159, -0.3674633204936981]\n",
      "[0.6668896079063416, -1.4923940896987915]\n",
      "[-0.5329315662384033, -0.4622018039226532]\n",
      "[-1.236860752105713, -0.7032712697982788]\n",
      "[-1.370903730392456, -0.3453369438648224]\n",
      "[0.015400052070617676, -1.1704235076904297]\n",
      "[-0.6157664060592651, -0.8944227695465088]\n",
      "[-0.44906875491142273, -0.5485326051712036]\n",
      "[-1.640352487564087, -0.3953548073768616]\n",
      "[-0.8302011489868164, -0.8439517021179199]\n",
      "[-0.49373340606689453, -0.9424713850021362]\n",
      "[-1.09967041015625, -0.5792554616928101]\n",
      "[0.1817624568939209, -0.9181824922561646]\n",
      "[-0.48402610421180725, -0.9519388675689697]\n",
      "[-1.3978948593139648, -0.3443211317062378]\n",
      "[-0.8651638627052307, -0.2685683071613312]\n",
      "[0.36759763956069946, -1.3882455825805664]\n",
      "[-0.5590604543685913, -0.682098388671875]\n",
      "[1.1937012672424316, -1.4181846380233765]\n",
      "[-1.6638948917388916, -0.4088253378868103]\n",
      "[0.39545774459838867, -1.305360198020935]\n",
      "[-0.0859430730342865, -1.0670801401138306]\n",
      "[-0.6768434047698975, -0.33881744742393494]\n",
      "[0.3520594835281372, -2.01813006401062]\n",
      "[-1.9240317344665527, -0.8873952627182007]\n",
      "[0.34985917806625366, -1.135424256324768]\n",
      "[-1.0432239770889282, -1.0399614572525024]\n",
      "[0.20157229900360107, -1.4498612880706787]\n",
      "[-0.061583518981933594, -0.5153520107269287]\n",
      "[-2.5625109672546387, -1.146843671798706]\n",
      "[-2.036679983139038, -0.8545455932617188]\n",
      "[-0.3334306478500366, -1.6966001987457275]\n",
      "[-0.18497148156166077, -1.1989284753799438]\n",
      "[1.6618146896362305, -2.118042230606079]\n",
      "[-1.863431453704834, -0.7582404613494873]\n",
      "[-2.666616916656494, -1.2047139406204224]\n",
      "[-0.46126776933670044, -1.2671048641204834]\n",
      "[0.08977687358856201, -1.2207963466644287]\n",
      "[0.143144428730011, -1.392788290977478]\n",
      "[-1.7753901481628418, -1.0803163051605225]\n",
      "[0.6577968001365662, -1.7048932313919067]\n",
      "[0.8627390265464783, -1.1337987184524536]\n",
      "[-2.5476598739624023, -1.138588309288025]\n",
      "[-0.17151916027069092, -1.102346420288086]\n",
      "[-1.6911721229553223, -0.9931958913803101]\n",
      "[-0.37462615966796875, -1.195716142654419]\n",
      "[0.21646934747695923, -1.5327236652374268]\n",
      "[-0.784022331237793, -1.2738208770751953]\n",
      "[0.7453976273536682, -1.9074105024337769]\n",
      "[0.6784148812294006, -0.8773965835571289]\n",
      "[0.36481285095214844, -1.8213084936141968]\n",
      "[-1.839036464691162, -0.9763095378875732]\n",
      "[0.37265998125076294, -1.0520299673080444]\n",
      "[-1.1268773078918457, -1.1973336935043335]\n",
      "[1.5955424308776855, -1.6018362045288086]\n",
      "[0.48895519971847534, -1.3186169862747192]\n",
      "[1.104217529296875, -1.746028184890747]\n",
      "[-1.080775260925293, -0.8913285732269287]\n",
      "[0.7125949263572693, -0.8749446868896484]\n",
      "[0.41120433807373047, -0.6894174814224243]\n",
      "[0.31130415201187134, -1.18484365940094]\n",
      "[0.6543747782707214, -1.19245183467865]\n",
      "[-2.9183578491210938, -1.344651222229004]\n",
      "[0.6498488783836365, -1.8752132654190063]\n",
      "[-1.9973206520080566, -0.8326665163040161]\n",
      "[-2.3633811473846436, -1.0361517667770386]\n",
      "[0.730356752872467, -0.9994068145751953]\n",
      "[-1.2390697002410889, -0.5491014719009399]\n",
      "[-1.3028552532196045, -0.880257248878479]\n",
      "[-1.1245806217193604, -0.5146019458770752]\n",
      "[-1.7046444416046143, -1.0250563621520996]\n",
      "[-1.9504172801971436, -0.8065940141677856]\n",
      "[0.5244848728179932, -1.2385849952697754]\n",
      "[1.2662603855133057, -1.3196080923080444]\n",
      "[1.5687706470489502, -1.5788899660110474]\n",
      "[0.8527715802192688, -1.064307689666748]\n",
      "[0.4415590763092041, -1.552014708518982]\n",
      "[-1.6092545986175537, -1.193304181098938]\n",
      "[-2.42368221282959, -1.0696717500686646]\n",
      "[-2.4402098655700684, -1.0788592100143433]\n",
      "[1.5245089530944824, -1.540953278541565]\n",
      "[0.32510340213775635, -1.739811658859253]\n",
      "[0.9787982106208801, -1.7011451721191406]\n",
      "[-1.1858948469161987, -1.1670681238174438]\n",
      "[0.5919162631034851, -2.3404998779296875]\n",
      "[0.7045711874961853, -1.0901765823364258]\n",
      "[-0.8135741949081421, -1.390373706817627]\n",
      "[0.34220170974731445, -1.4618650674819946]\n",
      "[-2.0686144828796387, -0.8722972869873047]\n",
      "[-2.0436270236968994, -0.8584072589874268]\n",
      "[0.5774808526039124, -1.342314600944519]\n",
      "[-2.2503185272216797, -0.9733026027679443]\n",
      "[-2.7035181522369385, -1.2252265214920044]\n",
      "[0.6929762959480286, -1.928222417831421]\n",
      "[0.6910993456840515, -1.7673096656799316]\n",
      "[-1.4097936153411865, -0.5729131698608398]\n",
      "[-1.0349631309509277, -1.3620445728302002]\n",
      "[-1.8708724975585938, -0.8912500143051147]\n",
      "[0.6430409550666809, -1.703417420387268]\n",
      "[-0.028595447540283203, -1.1522843837738037]\n",
      "[1.4869227409362793, -1.5087381601333618]\n",
      "[0.5348026156425476, -0.9623016119003296]\n",
      "[-0.6461125612258911, -1.4834110736846924]\n",
      "[1.2178046703338623, -1.4355674982070923]\n",
      "[-0.32423701882362366, -1.3684238195419312]\n",
      "[0.34160804748535156, -1.0977627038955688]\n",
      "[-1.6875650882720947, -0.9983818531036377]\n",
      "[-2.209725856781006, -0.9507379531860352]\n",
      "[1.526867389678955, -1.7060883045196533]\n",
      "[0.697808563709259, -1.4138225317001343]\n",
      "[-2.1206958293914795, -0.9012480974197388]\n",
      "[-2.1007485389709473, -0.8901599645614624]\n",
      "[0.679701030254364, -1.5600509643554688]\n",
      "[-0.24594002962112427, -0.3713030517101288]\n",
      "[-1.7049758434295654, -0.8940036296844482]\n",
      "[-0.21646738052368164, -1.0800747871398926]\n",
      "[0.5571553111076355, -0.9184014797210693]\n",
      "[-1.0640232563018799, -1.317289113998413]\n",
      "[1.4420835971832275, -1.7382055521011353]\n",
      "[-2.8912107944488525, -1.3295607566833496]\n",
      "[0.5931333899497986, -1.3941434621810913]\n",
      "[-0.40491002798080444, -1.523125410079956]\n",
      "[-0.9469742774963379, -1.1953991651535034]\n",
      "[1.2032113075256348, -1.3821582794189453]\n",
      "[-0.952606737613678, -2.0996761322021484]\n",
      "[-1.1916918754577637, -1.5569630861282349]\n",
      "[0.5282022953033447, -1.413132667541504]\n",
      "[-0.4975060820579529, -1.6086690425872803]\n",
      "[-0.9180876612663269, -1.3511683940887451]\n",
      "[1.1746649742126465, -1.2411015033721924]\n",
      "[1.028320550918579, -1.8858914375305176]\n",
      "[-0.00858980417251587, -1.614015817642212]\n",
      "[-1.4196441173553467, -1.8585208654403687]\n",
      "[-0.5841012001037598, -1.432215690612793]\n",
      "[2.0443334579467773, -1.9864952564239502]\n",
      "[0.3129732608795166, -1.7409169673919678]\n",
      "[1.1230509281158447, -1.9152323007583618]\n",
      "[-1.280419945716858, -1.4036855697631836]\n",
      "[-0.3705473244190216, -1.2933748960494995]\n",
      "[-1.3911993503570557, -1.2364528179168701]\n",
      "[-1.2842011451721191, -1.694856882095337]\n",
      "[-2.1157777309417725, -1.1669036149978638]\n",
      "[0.7172144055366516, -1.1529332399368286]\n",
      "[-0.2700899839401245, -1.765927791595459]\n",
      "[2.3103795051574707, -2.2145235538482666]\n",
      "[-0.9594482183456421, -1.666213035583496]\n",
      "[-0.8990728259086609, -1.3721407651901245]\n",
      "[-2.5208888053894043, -1.1237068176269531]\n",
      "[-1.5435469150543213, -1.4874231815338135]\n",
      "[2.3570642471313477, -2.2835094928741455]\n",
      "[1.0474278926849365, -1.1320465803146362]\n",
      "[-2.976541042327881, -1.376994013786316]\n",
      "[-1.5548381805419922, -1.5173076391220093]\n",
      "[0.8856876492500305, -1.1731150150299072]\n",
      "[-1.5368340015411377, -1.5319303274154663]\n",
      "[-0.7994786500930786, -1.056443214416504]\n",
      "[2.4928653240203857, -2.3709323406219482]\n",
      "[-2.0354928970336914, -1.086138367652893]\n",
      "[0.1284116506576538, -1.6148616075515747]\n",
      "[1.094926357269287, -1.172757625579834]\n",
      "[1.145622730255127, -1.2162095308303833]\n",
      "[0.5299133062362671, -1.2578372955322266]\n",
      "[0.5373597145080566, -1.2561006546020508]\n",
      "[0.6498487591743469, -1.8752131462097168]\n",
      "[-3.0754306316375732, -1.431964635848999]\n",
      "[1.7266316413879395, -1.8975635766983032]\n",
      "[-0.4899846315383911, -1.7436645030975342]\n",
      "[1.0907607078552246, -1.169187307357788]\n",
      "[1.521345853805542, -1.538242220878601]\n",
      "[-0.8928653597831726, -1.3957436084747314]\n",
      "[2.090242385864258, -2.025843858718872]\n",
      "[2.132530450820923, -2.1707603931427]\n",
      "[-0.27109163999557495, -1.7186764478683472]\n",
      "[0.25655221939086914, -1.2197211980819702]\n",
      "[0.5689958333969116, -1.0513266324996948]\n",
      "[1.0577726364135742, -1.1409131288528442]\n",
      "[1.0214910507202148, -1.1098161935806274]\n",
      "[2.523468017578125, -2.3971619606018066]\n",
      "[-0.5348554849624634, -1.973191738128662]\n",
      "[0.6087384223937988, -1.723829984664917]\n",
      "[0.7087308764457703, -1.7870540618896484]\n",
      "[0.9935696721076965, -1.085884690284729]\n",
      "[-2.7662107944488525, -1.2600759267807007]\n",
      "[-1.721296787261963, -1.135370135307312]\n",
      "[-1.20173978805542, -1.62226140499115]\n",
      "[2.5412120819091797, -2.7837002277374268]\n",
      "[1.0671968460083008, -1.1489907503128052]\n",
      "[-1.610081672668457, -1.5998022556304932]\n",
      "[2.179161548614502, -2.1020565032958984]\n",
      "[-0.6679681539535522, -1.7135506868362427]\n",
      "[0.5344760417938232, -1.732987880706787]\n",
      "[-0.1588819921016693, -1.2832143306732178]\n",
      "[-0.6760367155075073, -1.8189175128936768]\n",
      "[0.5523404479026794, -1.8178976774215698]\n",
      "[2.5229578018188477, -2.448166608810425]\n",
      "[-2.453317880630493, -1.1472456455230713]\n",
      "[1.8915352821350098, -1.8555318117141724]\n",
      "[-1.487513542175293, -1.7011574506759644]\n",
      "[-0.8246951103210449, -1.6574275493621826]\n",
      "[-2.4981143474578857, -1.1110469102859497]\n",
      "[0.1391165852546692, -1.4954661130905151]\n",
      "[0.9482473731040955, -1.0470389127731323]\n",
      "[1.1431338787078857, -1.2140763998031616]\n",
      "[-2.1736111640930176, -1.4766755104064941]\n",
      "[1.0718603134155273, -1.1529875993728638]\n",
      "[-1.1939679384231567, -1.4418179988861084]\n",
      "[0.5815464854240417, -1.3835797309875488]\n",
      "[-1.1673775911331177, -1.6088064908981323]\n",
      "[0.44636791944503784, -1.7409826517105103]\n",
      "[1.1455802917480469, -1.2161731719970703]\n",
      "[-0.3489491939544678, -1.256030559539795]\n",
      "[0.15524327754974365, -1.6680089235305786]\n",
      "[-0.8520815372467041, -1.5635151863098145]\n",
      "[-1.2492296695709229, -1.2037688493728638]\n",
      "[1.6356089115142822, -1.6361770629882812]\n",
      "[-0.8689945340156555, -1.5808532238006592]\n",
      "[0.5616514086723328, -1.233837366104126]\n",
      "[1.2136919498443604, -1.2745516300201416]\n",
      "[-1.5008540153503418, -1.6631340980529785]\n",
      "[1.3667621612548828, -1.4057483673095703]\n",
      "[0.3025144338607788, -1.8524994850158691]\n",
      "[-0.3388291597366333, -1.2278013229370117]\n",
      "[-1.9310684204101562, -1.507220983505249]\n",
      "[-1.45273756980896, -1.4866588115692139]\n",
      "[0.9763503670692444, -1.0711259841918945]\n",
      "[1.3187594413757324, -2.084587812423706]\n",
      "[-0.8933851718902588, -0.995236873626709]\n",
      "[0.6511456370353699, -1.1150652170181274]\n",
      "[-0.19673043489456177, -1.0795961618423462]\n",
      "[1.3145544528961182, -1.4801170825958252]\n",
      "[-0.5088248252868652, -0.4328993856906891]\n",
      "[-2.172844409942627, -0.9302364587783813]\n",
      "[-0.9163424372673035, -0.7959682941436768]\n",
      "[0.5846291184425354, -1.7548840045928955]\n",
      "[0.5981807708740234, -1.2506804466247559]\n",
      "[1.246596097946167, -2.1765048503875732]\n",
      "[-1.2122344970703125, -0.7343677282333374]\n",
      "[-2.1420609951019287, -0.9131245613098145]\n",
      "[0.6413847804069519, -1.3504633903503418]\n",
      "[0.8877533078193665, -1.2588919401168823]\n",
      "[1.189969539642334, -1.4199069738388062]\n",
      "[-0.7575953006744385, -1.1837059259414673]\n",
      "[1.599410057067871, -1.673345685005188]\n",
      "[1.3628506660461426, -1.4023957252502441]\n",
      "[-1.4678981304168701, -0.8851968050003052]\n",
      "[-0.928896963596344, -0.9402405023574829]\n",
      "[-0.29962751269340515, -1.2119907140731812]\n",
      "[0.36723244190216064, -1.1706868410110474]\n",
      "[1.2552967071533203, -1.5594940185546875]\n",
      "[0.18306875228881836, -1.333616018295288]\n",
      "[0.175520122051239, -1.866851568222046]\n",
      "[0.28965550661087036, -0.9033560752868652]\n",
      "[1.456505298614502, -1.8627803325653076]\n",
      "[-0.8868542909622192, -1.0588208436965942]\n",
      "[1.2517554759979248, -1.3071759939193726]\n",
      "[-0.1903248131275177, -1.2695187330245972]\n",
      "[1.9952237606048584, -1.9444034099578857]\n",
      "[-0.3110398054122925, -1.1377406120300293]\n",
      "[1.9417898654937744, -1.8986049890518188]\n",
      "[-0.46011102199554443, -0.9396499395370483]\n",
      "[0.36032551527023315, -0.8717966079711914]\n",
      "[-0.15650470554828644, -0.63958740234375]\n",
      "[1.4352033138275146, -1.464409351348877]\n",
      "[1.3221070766448975, -1.3674744367599487]\n",
      "[-1.9973206520080566, -0.8326666355133057]\n",
      "[1.7266316413879395, -1.8975635766983032]\n",
      "[-2.3982937335968018, -1.0555589199066162]\n",
      "[-0.9288129806518555, -1.0394588708877563]\n",
      "[0.23662471771240234, -0.9763288497924805]\n",
      "[-1.324629306793213, -0.4587320387363434]\n",
      "[-0.29760563373565674, -0.9740253686904907]\n",
      "[-1.5326204299926758, -0.5743497610092163]\n",
      "[-2.1955838203430176, -0.9428768157958984]\n",
      "[-0.9761165976524353, -0.8675934076309204]\n",
      "[1.5482990741729736, -1.561343789100647]\n",
      "[1.606330156326294, -1.6110823154449463]\n",
      "[1.5197670459747314, -1.8102226257324219]\n",
      "[0.5324026942253113, -1.0400047302246094]\n",
      "[-0.29703086614608765, -1.399475336074829]\n",
      "[-0.3565348982810974, -1.363747239112854]\n",
      "[-1.8525817394256592, -0.7522093057632446]\n",
      "[-1.939730167388916, -0.8006532192230225]\n",
      "[1.698406457901001, -1.8404896259307861]\n",
      "[1.5670971870422363, -1.810081124305725]\n",
      "[1.8469228744506836, -1.8172944784164429]\n",
      "[-0.16729088127613068, -1.2647886276245117]\n",
      "[-0.05467468500137329, -2.257568359375]\n",
      "[0.3262506127357483, -1.11819326877594]\n",
      "[0.23206061124801636, -1.4578220844268799]\n",
      "[-0.2703136205673218, -1.3759872913360596]\n",
      "[-0.9022783041000366, -0.9896236658096313]\n",
      "[-1.671170711517334, -0.6513668298721313]\n",
      "[1.4216229915618896, -1.4527696371078491]\n",
      "[-1.1628953218460083, -1.0074968338012695]\n",
      "[-1.8291568756103516, -0.7539224624633789]\n",
      "[0.05459856986999512, -1.8526865243911743]\n",
      "[1.6859769821166992, -1.747585415840149]\n",
      "[-1.7064456939697266, -0.6709754467010498]\n",
      "[-0.05996870994567871, -1.4440964460372925]\n",
      "[-0.7086776494979858, -1.0382680892944336]\n",
      "[1.6744678020477295, -1.6978029012680054]\n",
      "[0.33607006072998047, -1.1211129426956177]\n",
      "[1.5294229984283447, -1.7495839595794678]\n",
      "[0.6634532809257507, -1.146439790725708]\n",
      "[0.2955661416053772, -1.5568292140960693]\n",
      "[0.9456995129585266, -1.4314157962799072]\n",
      "[0.8801564574241638, -1.435071587562561]\n",
      "[0.7004117369651794, -1.095609426498413]\n",
      "[-0.5177258849143982, -1.149113655090332]\n",
      "[-1.6275758743286133, -0.6803046464920044]\n",
      "[1.3375234603881836, -1.7682898044586182]\n",
      "[1.5708410739898682, -1.5806646347045898]\n",
      "[-1.309516191482544, -0.7207945585250854]\n",
      "[-1.2015453577041626, -0.8264250755310059]\n",
      "[1.6894445419311523, -1.6823196411132812]\n",
      "[-0.9368822574615479, -0.26612985134124756]\n",
      "[-0.489526629447937, -1.0570155382156372]\n",
      "[1.107269287109375, -1.2863091230392456]\n",
      "[0.8060274720191956, -0.9868457317352295]\n",
      "[-0.09669190645217896, -1.3996461629867554]\n",
      "[1.1765003204345703, -1.7810602188110352]\n",
      "[-1.9098668098449707, -0.9157830476760864]\n",
      "[1.5156216621398926, -1.5483444929122925]\n",
      "[0.7311825156211853, -1.6071335077285767]\n",
      "[0.25851738452911377, -1.3488554954528809]\n",
      "[0.9981275200843811, -1.3694149255752563]\n",
      "[-1.175852656364441, -1.7459379434585571]\n",
      "[-2.3786120414733887, -1.0446181297302246]\n",
      "[0.441650927066803, -1.364508032798767]\n",
      "[-1.007361888885498, -1.2635066509246826]\n",
      "[-0.05921238660812378, -1.569087266921997]\n",
      "[1.0289058685302734, -1.1161713600158691]\n",
      "[-2.0317938327789307, -0.8530358076095581]\n",
      "[-2.0057153701782227, -0.837333083152771]\n",
      "[-1.8802111148834229, -1.4077156782150269]\n",
      "[-0.37434709072113037, -1.3487017154693604]\n",
      "[2.2622785568237305, -2.405137538909912]\n",
      "[-1.101208209991455, -1.1167991161346436]\n",
      "[-2.5643577575683594, -1.1478701829910278]\n",
      "[-0.8904131650924683, -1.3379608392715454]\n",
      "[0.12135913968086243, -1.4389817714691162]\n",
      "[-0.22682633996009827, -1.4805734157562256]\n",
      "[-2.757549524307251, -1.2552614212036133]\n",
      "[-0.013011723756790161, -1.710112452507019]\n",
      "[1.047562599182129, -1.3010388612747192]\n",
      "[-2.827721118927002, -1.294268250465393]\n",
      "[1.0357770919799805, -1.485906720161438]\n",
      "[-2.4592678546905518, -1.089453101158142]\n",
      "[-0.472532719373703, -1.3697106838226318]\n",
      "[-0.61854088306427, -1.4830557107925415]\n",
      "[-1.6486289501190186, -1.2108831405639648]\n",
      "[1.755577802658081, -2.1973822116851807]\n",
      "[1.2596890926361084, -1.3139759302139282]\n",
      "[-0.8038156032562256, -1.6756892204284668]\n",
      "[-2.5497024059295654, -1.139723777770996]\n",
      "[1.002213478088379, -1.2814143896102905]\n",
      "[-1.961416482925415, -1.1144784688949585]\n",
      "[1.25189208984375, -1.6571800708770752]\n",
      "[1.621617317199707, -1.7866294384002686]\n",
      "[0.40741217136383057, -1.748483657836914]\n",
      "[-0.5395998954772949, -1.2424912452697754]\n",
      "[1.195169448852539, -1.2586759328842163]\n",
      "[1.1975135803222656, -1.2606853246688843]\n",
      "[0.7331351637840271, -1.3943653106689453]\n",
      "[0.7932228446006775, -1.3893978595733643]\n",
      "[-2.3633809089660645, -1.0361515283584595]\n",
      "[-0.4899846315383911, -1.7436645030975342]\n",
      "[-0.9288129806518555, -1.0394588708877563]\n",
      "[-3.208183765411377, -1.5057592391967773]\n",
      "[1.4089686870574951, -1.4419234991073608]\n",
      "[0.14475369453430176, -1.088374376296997]\n",
      "[-1.2332614660263062, -1.08574640750885]\n",
      "[0.5936486124992371, -1.0760154724121094]\n",
      "[-0.8690109848976135, -1.2790842056274414]\n",
      "[-1.588655710220337, -1.0752512216567993]\n",
      "[0.8182020783424377, -1.4524530172348022]\n",
      "[1.3363349437713623, -1.379669189453125]\n",
      "[1.4937753677368164, -1.5146114826202393]\n",
      "[1.2422802448272705, -1.299054741859436]\n",
      "[1.604926586151123, -1.9377882480621338]\n",
      "[-2.7775185108184814, -1.2663617134094238]\n",
      "[-2.0323307514190674, -0.8521280288696289]\n",
      "[-1.9875926971435547, -0.8272590637207031]\n",
      "[1.2478916645050049, -1.3038643598556519]\n",
      "[-0.7928111553192139, -1.6106172800064087]\n",
      "[0.388782799243927, -1.7308275699615479]\n",
      "[-1.9091968536376953, -1.126636028289795]\n",
      "[1.6639304161071777, -2.6676390171051025]\n",
      "[1.342912197113037, -1.3982244729995728]\n",
      "[-1.869516134262085, -1.2670598030090332]\n",
      "[1.4204578399658203, -1.7880438566207886]\n",
      "[-2.3432517051696777, -1.0249621868133545]\n",
      "[-1.4864590167999268, -0.9415907859802246]\n",
      "[0.47205495834350586, -1.5078706741333008]\n",
      "[-2.9249801635742188, -1.3483325242996216]\n",
      "[-2.2807912826538086, -0.9902417659759521]\n",
      "[1.778017520904541, -2.25636887550354]\n",
      "[-0.11281149089336395, -1.7358405590057373]\n",
      "[0.4943273067474365, -1.2073320150375366]\n",
      "[-2.470046043395996, -1.0954443216323853]\n",
      "[-2.258340358734131, -0.9777617454528809]\n",
      "[-0.16195207834243774, -1.669632077217102]\n",
      "[0.07603877782821655, -1.370318055152893]\n",
      "[1.290461778640747, -1.3403512239456177]\n",
      "[1.0467479228973389, -1.1314637660980225]\n",
      "[-2.0156333446502686, -1.235217571258545]\n",
      "[1.5996158123016357, -1.6053274869918823]\n",
      "[-0.7207040190696716, -1.4475412368774414]\n",
      "[0.45512551069259644, -1.33449387550354]\n",
      "[-2.384249210357666, -1.047751784324646]\n",
      "[-1.816434621810913, -0.8667675256729126]\n",
      "[1.8781764507293701, -1.8440818786621094]\n",
      "[0.5320786833763123, -1.5521823167800903]\n",
      "[-1.893428087234497, -0.851646900177002]\n",
      "[-2.3193624019622803, -1.0116825103759766]\n",
      "[0.29984188079833984, -1.6479108333587646]\n",
      "[0.7284365296363831, -0.8586388826370239]\n",
      "[-1.9987046718597412, -0.9820709228515625]\n",
      "[0.6960934996604919, -1.365478515625]\n",
      "[1.1320207118988037, -1.204551100730896]\n",
      "[-2.308283567428589, -1.1050137281417847]\n",
      "[1.969022512435913, -1.9680194854736328]\n",
      "[-3.0966742038726807, -1.4437735080718994]\n",
      "[0.6379875540733337, -1.575006365776062]\n",
      "[-1.673421859741211, -1.3364986181259155]\n",
      "[-1.7158563137054443, -1.1364691257476807]\n",
      "[1.4997210502624512, -1.5197075605392456]\n",
      "[1.7989015579223633, -1.7761353254318237]\n",
      "[0.6712537407875061, -0.8096274137496948]\n",
      "[-0.20541352033615112, -0.6990795135498047]\n",
      "[0.15794426202774048, -0.6716182231903076]\n",
      "[0.6762024760246277, -0.8214820623397827]\n",
      "[-1.5159344673156738, -0.552686333656311]\n",
      "[0.5030683279037476, -0.9402652978897095]\n",
      "[0.5616286396980286, -0.9426287412643433]\n",
      "[1.259411096572876, -1.3137376308441162]\n",
      "[0.4830161929130554, -0.859541654586792]\n",
      "[-1.1095566749572754, -1.367197871208191]\n",
      "[0.018960684537887573, -0.7417144775390625]\n",
      "[0.8784369826316833, -1.0559710264205933]\n",
      "[0.657500684261322, -0.8548169136047363]\n",
      "[0.012631088495254517, -0.5445427894592285]\n",
      "[0.5323871374130249, -0.6906048059463501]\n",
      "[0.8617461323738098, -0.9728986024856567]\n",
      "[0.763275682926178, -0.8884994983673096]\n",
      "[-0.2832232415676117, -0.6461631059646606]\n",
      "[1.0897183418273926, -1.2178775072097778]\n",
      "[-2.450312614440918, -0.7710789442062378]\n",
      "[1.235713243484497, -1.2970452308654785]\n",
      "[-0.03090953826904297, -0.5036555528640747]\n",
      "[0.7509438395500183, -0.8779299259185791]\n",
      "[0.730775773525238, -0.8606438636779785]\n",
      "[-2.0031917095184326, -1.2294049263000488]\n",
      "[-1.8982703685760498, -0.3465535640716553]\n",
      "[1.0208253860473633, -1.1092455387115479]\n",
      "[0.6118469834327698, -0.7587099075317383]\n",
      "[-0.6185051202774048, -0.5397530794143677]\n",
      "[0.6462246775627136, -0.7881749868392944]\n",
      "[0.7809252142906189, -0.9036270380020142]\n",
      "[-1.6321251392364502, -1.0359703302383423]\n",
      "[0.93105548620224, -1.032303810119629]\n",
      "[-0.08500081300735474, -0.6839892864227295]\n",
      "[-1.6961631774902344, -0.3793545663356781]\n",
      "[-1.6763601303100586, -0.4739992320537567]\n",
      "[-0.02024856209754944, -0.6871674060821533]\n",
      "[-0.19439396262168884, -0.646363377571106]\n",
      "[0.7303569912910461, -0.9994068145751953]\n",
      "[1.0907607078552246, -1.169187307357788]\n",
      "[0.23662489652633667, -0.9763286113739014]\n",
      "[1.4089686870574951, -1.4419234991073608]\n",
      "[-2.200077533721924, -0.3459712862968445]\n",
      "[-0.6499646902084351, -0.670221209526062]\n",
      "[0.16528445482254028, -0.6889530420303345]\n",
      "[0.1876627802848816, -1.3281298875808716]\n",
      "[0.7721514105796814, -1.392180323600769]\n",
      "[0.24516171216964722, -0.776519775390625]\n",
      "[0.2072334885597229, -0.6616405248641968]\n",
      "[0.1057199239730835, -0.8064109086990356]\n",
      "[-0.6480177640914917, -0.9242504835128784]\n",
      "[-1.9479432106018066, -0.33136799931526184]\n",
      "[-1.9854328632354736, -1.0772687196731567]\n",
      "[1.5621302127838135, -1.5731984376907349]\n",
      "[0.312691867351532, -0.8268551826477051]\n",
      "[0.3392104506492615, -0.842139482498169]\n",
      "[-0.29892414808273315, -0.9454725980758667]\n",
      "[1.009753704071045, -1.0997560024261475]\n",
      "[0.8117485642433167, -0.9300456047058105]\n",
      "[0.7824689745903015, -0.9049501419067383]\n",
      "[-2.0613250732421875, -1.7133716344833374]\n",
      "[-1.6836626529693604, -0.6181750297546387]\n",
      "[0.6497957110404968, -0.7912356853485107]\n",
      "[-2.37931752204895, -0.8367596864700317]\n",
      "[0.7955746054649353, -1.0383137464523315]\n",
      "[-0.056986331939697266, -0.7132790088653564]\n",
      "[0.33329641819000244, -0.7339824438095093]\n",
      "[1.0640473365783691, -1.1462911367416382]\n",
      "[0.6671062111854553, -1.0144470930099487]\n",
      "[-2.0184075832366943, -1.269803762435913]\n",
      "[0.8656730055809021, -0.9762643575668335]\n",
      "[0.06193971633911133, -1.1384563446044922]\n",
      "[0.9625517725944519, -1.0592992305755615]\n",
      "[0.7681463360786438, -0.9911887645721436]\n",
      "[0.8506646752357483, -0.9634007215499878]\n",
      "[0.06979334354400635, -0.8048522472381592]\n",
      "[-0.46881192922592163, -0.8927865028381348]\n",
      "[-1.8564379215240479, -0.32085561752319336]\n",
      "[1.0204923152923584, -1.1089600324630737]\n",
      "[-1.5315001010894775, -0.6339362859725952]\n",
      "[0.6409755349159241, -0.783676028251648]\n",
      "[-0.24115166068077087, -0.6803206205368042]\n",
      "[0.9090556502342224, -1.013447642326355]\n",
      "[0.298123836517334, -0.8570277690887451]\n",
      "[-1.0485129356384277, -0.9102195501327515]\n",
      "[0.5386960506439209, -0.8277015686035156]\n",
      "[0.33969587087631226, -0.8320482969284058]\n",
      "[0.5076940059661865, -0.8427363634109497]\n",
      "[0.6853957772254944, -0.8217486143112183]\n",
      "[-1.7144575119018555, -0.5188068151473999]\n",
      "[0.7166152596473694, -1.010103702545166]\n",
      "[-0.3584635555744171, -0.4710254371166229]\n",
      "[-1.0969280004501343, -0.47008463740348816]\n",
      "[0.8236169219017029, -0.9402179718017578]\n",
      "[-1.1196351051330566, -1.020010232925415]\n",
      "[1.1051535606384277, -1.181523323059082]\n",
      "[0.4194193482398987, -0.605708122253418]\n",
      "[0.9286848902702332, -1.0302718877792358]\n",
      "[1.0618391036987305, -1.1443984508514404]\n",
      "[-1.2932767868041992, -0.6640655994415283]\n",
      "[1.9939024448394775, -1.9978199005126953]\n",
      "[-0.2037338763475418, -0.9700020551681519]\n",
      "[0.18533313274383545, -0.7588684558868408]\n",
      "[-0.7459276914596558, -0.7969970703125]\n",
      "[0.23277336359024048, -1.0194754600524902]\n",
      "[-1.16927170753479, -0.2184610366821289]\n",
      "[-1.4085993766784668, -0.5054092407226562]\n",
      "[-0.7608572840690613, -0.6299196481704712]\n",
      "[1.169435977935791, -1.6954514980316162]\n",
      "[0.1364791989326477, -0.8975987434387207]\n",
      "[0.3896142840385437, -1.7609843015670776]\n",
      "[-1.1419942378997803, -0.5006752014160156]\n",
      "[-0.9196105003356934, -0.708135724067688]\n",
      "[-0.038678139448165894, -0.9859328269958496]\n",
      "[-0.2969464063644409, -0.7953081130981445]\n",
      "[0.2236979603767395, -0.978961706161499]\n",
      "[0.2579907774925232, -1.2645479440689087]\n",
      "[1.0692873001098633, -1.2427136898040771]\n",
      "[0.5528835654258728, -0.8128468990325928]\n",
      "[-0.46629300713539124, -0.8556290864944458]\n",
      "[-1.392888069152832, -0.8035120964050293]\n",
      "[0.44472944736480713, -1.200545310974121]\n",
      "[-0.806810200214386, -0.7647632360458374]\n",
      "[0.86263507604599, -1.170867681503296]\n",
      "[0.20535701513290405, -1.119189739227295]\n",
      "[-0.5483367443084717, -1.5625029802322388]\n",
      "[-0.6528186798095703, -0.5510406494140625]\n",
      "[1.2936222553253174, -1.528167963027954]\n",
      "[-0.03261479735374451, -1.0871906280517578]\n",
      "[0.13725468516349792, -0.6463488340377808]\n",
      "[0.13954973220825195, -1.1508674621582031]\n",
      "[1.2364282608032227, -1.2940391302108765]\n",
      "[-0.6777152419090271, -1.063357949256897]\n",
      "[1.3668968677520752, -1.4058637619018555]\n",
      "[-0.6234762668609619, -0.6461488008499146]\n",
      "[-0.5489909648895264, -0.5319602489471436]\n",
      "[-0.9138832688331604, -0.39123982191085815]\n",
      "[0.5176653861999512, -0.8352271318435669]\n",
      "[0.4696927070617676, -0.8106945753097534]\n",
      "[-1.2390695810317993, -0.5491015911102295]\n",
      "[1.521345615386963, -1.5382421016693115]\n",
      "[-1.324629306793213, -0.4587320387363434]\n",
      "[0.1447540521621704, -1.0883744955062866]\n",
      "[-0.6499646902084351, -0.670221209526062]\n",
      "[-1.1919806003570557, -0.3849954605102539]\n",
      "[-0.5501570701599121, -0.6854790449142456]\n",
      "[-0.14576363563537598, -0.7082874774932861]\n",
      "[-0.7527183890342712, -0.823195219039917]\n",
      "[-0.7888071537017822, -0.7122008800506592]\n",
      "[0.5965927839279175, -0.867223858833313]\n",
      "[0.9431068301200867, -1.0426329374313354]\n",
      "[0.7332431674003601, -1.2123992443084717]\n",
      "[-0.456849068403244, -0.675122857093811]\n",
      "[-0.802010178565979, -1.2332484722137451]\n",
      "[0.8608096241950989, -1.5001506805419922]\n",
      "[-1.387132167816162, -0.493476003408432]\n",
      "[-1.4721646308898926, -0.5407437086105347]\n",
      "[0.9205763936042786, -1.238875150680542]\n",
      "[1.3093650341033936, -1.4525667428970337]\n",
      "[1.2447936534881592, -1.3012090921401978]\n",
      "[0.11306688189506531, -1.1354559659957886]\n",
      "[-0.7013296484947205, -1.9914915561676025]\n",
      "[-0.4968121647834778, -0.7828159332275391]\n",
      "[0.26851165294647217, -1.272422432899475]\n",
      "[-0.9585865139961243, -1.1018985509872437]\n",
      "[-0.17362825572490692, -0.9784426689147949]\n",
      "[-1.5175795555114746, -0.5659888982772827]\n",
      "[0.5806667804718018, -0.935562252998352]\n",
      "[-0.057469338178634644, -1.120832085609436]\n",
      "[-1.0843441486358643, -0.5810494422912598]\n",
      "[-0.6039712429046631, -1.581599473953247]\n",
      "[1.2743372917175293, -1.326530933380127]\n",
      "[-0.6197208762168884, -0.49998244643211365]\n",
      "[0.6877098679542542, -1.442197561264038]\n",
      "[-0.07868599891662598, -0.9924473762512207]\n",
      "[1.2146074771881104, -1.2761085033416748]\n",
      "[0.04774549603462219, -0.8258211612701416]\n",
      "[0.7754494547843933, -1.1534721851348877]\n",
      "[-0.33235228061676025, -0.6466084718704224]\n",
      "[0.9616071581840515, -1.5039654970169067]\n",
      "[-0.002895534038543701, -1.0107895135879517]\n",
      "[-0.23297202587127686, -1.0095943212509155]\n",
      "[0.21097666025161743, -0.7322597503662109]\n",
      "[0.18197906017303467, -1.1267282962799072]\n",
      "[-1.4152591228485107, -0.5091111660003662]\n",
      "[0.4274185299873352, -1.2675830125808716]\n",
      "[0.7569946646690369, -1.0472850799560547]\n",
      "[-1.3328948020935059, -0.4633266031742096]\n",
      "[-0.5999147891998291, -0.7741456031799316]\n",
      "[0.89947909116745, -1.120740532875061]\n",
      "[-1.2233493328094482, -0.2116556167602539]\n",
      "[-0.05162057280540466, -0.9477293491363525]\n",
      "[-0.27883636951446533, -0.6127852201461792]\n",
      "[-0.21755892038345337, -0.5281801223754883]\n",
      "[0.43808090686798096, -1.3472932577133179]\n",
      "[0.30127376317977905, -1.3727507591247559]\n",
      "[-0.5618788003921509, -0.7769216299057007]\n",
      "[0.42078661918640137, -0.9113078117370605]\n",
      "[0.76602703332901, -1.3904783725738525]\n",
      "[0.569161593914032, -1.1948657035827637]\n",
      "[0.08171731233596802, -0.9830049276351929]\n",
      "[0.22164714336395264, -2.023925542831421]\n",
      "[-2.067272186279297, -0.8715511560440063]\n",
      "[-1.5040440559387207, -0.683975338935852]\n",
      "[-2.189995765686035, -0.9397705793380737]\n",
      "[-1.7802319526672363, -0.8248610496520996]\n",
      "[0.3008546829223633, -0.6413404941558838]\n",
      "[-1.1111562252044678, -0.8480784893035889]\n",
      "[-1.7474915981292725, -0.6937921047210693]\n",
      "[-0.6242151260375977, -1.6783123016357422]\n",
      "[-1.9763424396514893, -0.8210052251815796]\n",
      "[1.15287184715271, -1.5493892431259155]\n",
      "[-1.6802527904510498, -0.7732009887695312]\n",
      "[-1.0410040616989136, -0.9289737939834595]\n",
      "[-2.1440632343292236, -0.9142376184463501]\n",
      "[-2.011277198791504, -0.7098363637924194]\n",
      "[-1.8482677936553955, -0.7738206386566162]\n",
      "[-1.357313632965088, -1.2316981554031372]\n",
      "[-1.2591193914413452, -1.0813699960708618]\n",
      "[-1.6381831169128418, -0.5040479898452759]\n",
      "[-1.4866023063659668, -0.957361102104187]\n",
      "[0.38517695665359497, -1.1356207132339478]\n",
      "[-1.5282566547393799, -1.0732693672180176]\n",
      "[-2.043227434158325, -0.8581851720809937]\n",
      "[-1.5851452350616455, -0.9816617965698242]\n",
      "[-2.0525918006896973, -0.944556474685669]\n",
      "[0.692852795124054, -1.574236512184143]\n",
      "[-0.3550620377063751, -0.461748331785202]\n",
      "[-1.1033155918121338, -1.3788623809814453]\n",
      "[-1.8811585903167725, -0.9844781160354614]\n",
      "[-1.690445899963379, -0.5174998044967651]\n",
      "[-1.9700965881347656, -0.9937744140625]\n",
      "[-0.3541707992553711, -0.9330589771270752]\n",
      "[1.1563892364501953, -1.4449211359024048]\n",
      "[-0.820310115814209, -1.099783182144165]\n",
      "[-1.8293745517730713, -0.7393089532852173]\n",
      "[-0.5420198440551758, -0.4504447877407074]\n",
      "[0.004159480333328247, -0.5450277328491211]\n",
      "[-1.5500686168670654, -0.6271297931671143]\n",
      "[-1.6123008728027344, -0.6327564716339111]\n",
      "[-1.3028552532196045, -0.880257248878479]\n",
      "[-0.8928653597831726, -1.3957436084747314]\n",
      "[-0.29760628938674927, -0.9740251302719116]\n",
      "[-1.2332618236541748, -1.0857462882995605]\n",
      "[0.16528445482254028, -0.6889530420303345]\n",
      "[-0.5501570701599121, -0.6854790449142456]\n",
      "[-2.2497336864471436, -0.9729773998260498]\n",
      "[1.0996687412261963, -1.1768224239349365]\n",
      "[0.4359790086746216, -1.3520585298538208]\n",
      "[-2.0170602798461914, -0.8436393737792969]\n",
      "[-1.6581857204437256, -0.6459670066833496]\n",
      "[-0.9358643293380737, -0.5290296077728271]\n",
      "[0.29499274492263794, -0.5458277463912964]\n",
      "[-0.31757357716560364, -0.47914549708366394]\n",
      "[0.9047366976737976, -1.5073480606079102]\n",
      "[-0.49578630924224854, -1.5441185235977173]\n",
      "[-1.5084295272827148, -0.6981334686279297]\n",
      "[-1.3478344678878784, -0.767109751701355]\n",
      "[0.18167614936828613, -0.5260238647460938]\n",
      "[-1.1411455869674683, -1.2997947931289673]\n",
      "[-0.9633092284202576, -1.0392601490020752]\n",
      "[-1.985560655593872, -0.9859058856964111]\n",
      "[0.6469835638999939, -2.066171169281006]\n",
      "[0.11485555768013, -0.592867374420166]\n",
      "[-1.85831618309021, -1.133016586303711]\n",
      "[0.5104060173034668, -1.2549567222595215]\n",
      "[-1.8620820045471191, -0.9342172145843506]\n",
      "[-1.4373304843902588, -0.7418535947799683]\n",
      "[-1.7846837043762207, -0.7307075262069702]\n",
      "[-1.3523547649383545, -1.1696652173995972]\n",
      "[-1.3513085842132568, -0.8406591415405273]\n",
      "[0.8092419505119324, -1.6771445274353027]\n",
      "[-1.1456470489501953, -1.1757745742797852]\n",
      "[0.3159588575363159, -0.979449987411499]\n",
      "[-1.1533055305480957, -1.3775122165679932]\n",
      "[-1.977679967880249, -0.8968355655670166]\n",
      "[-1.2058491706848145, -1.1118391752243042]\n",
      "[-1.7037551403045654, -0.6694799661636353]\n",
      "[0.01664745807647705, -0.4877413809299469]\n",
      "[-0.4805091917514801, -0.5000401735305786]\n",
      "[-1.1032164096832275, -1.3751749992370605]\n",
      "[0.1001177728176117, -0.5629366636276245]\n",
      "[-2.135655641555786, -0.9095640182495117]\n",
      "[-1.5021588802337646, -0.669800877571106]\n",
      "[-1.8426249027252197, -0.9828180074691772]\n",
      "[-1.430600643157959, -0.7633624076843262]\n",
      "[0.5330101847648621, -0.7955397367477417]\n",
      "[-1.7857606410980225, -0.7429276704788208]\n",
      "[-1.7359495162963867, -0.6873760223388672]\n",
      "[-2.0470376014709473, -0.8603031635284424]\n",
      "[-1.2926784753799438, -0.8995290994644165]\n",
      "[0.4540674090385437, -0.6234768629074097]\n",
      "[-2.0701122283935547, -0.8731298446655273]\n",
      "[-1.7170734405517578, -0.6243548393249512]\n",
      "[-1.5282299518585205, -0.45317092537879944]\n",
      "[-1.4838099479675293, -1.2516367435455322]\n",
      "[0.6911972165107727, -1.0409811735153198]\n",
      "[-1.5124318599700928, -0.9230877161026001]\n",
      "[-1.793433427810669, -0.7435929775238037]\n",
      "[-1.4149162769317627, -1.2643327713012695]\n",
      "[-1.9222729206085205, -0.9446219205856323]\n",
      "[-0.11983591318130493, -0.49682357907295227]\n",
      "[2.4221177101135254, -2.3102943897247314]\n",
      "[0.8438470959663391, -1.0956040620803833]\n",
      "[1.3077518939971924, -1.46587336063385]\n",
      "[1.2194409370422363, -1.2794792652130127]\n",
      "[1.8865480422973633, -1.8512572050094604]\n",
      "[-0.42596668004989624, -0.8193109035491943]\n",
      "[-1.3606228828430176, -0.47874006628990173]\n",
      "[0.29191845655441284, -0.8801310062408447]\n",
      "[1.961329698562622, -1.9153525829315186]\n",
      "[1.5496807098388672, -1.5625280141830444]\n",
      "[1.1056814193725586, -2.479991912841797]\n",
      "[0.2259628176689148, -0.8910338878631592]\n",
      "[-1.6283149719238281, -0.6275442838668823]\n",
      "[1.6076643466949463, -1.6122257709503174]\n",
      "[1.7159478664398193, -1.7050355672836304]\n",
      "[1.830967903137207, -1.803619384765625]\n",
      "[1.102754831314087, -1.2778314352035522]\n",
      "[2.0701987743377686, -2.008664608001709]\n",
      "[1.6218910217285156, -1.7705646753311157]\n",
      "[-0.02658793330192566, -0.8665589094161987]\n",
      "[-1.3991031646728516, -1.0774670839309692]\n",
      "[1.2710819244384766, -1.3237407207489014]\n",
      "[1.515568733215332, -1.5332906246185303]\n",
      "[1.930119276046753, -1.8886022567749023]\n",
      "[1.5357708930969238, -1.5506058931350708]\n",
      "[-0.06805306673049927, -2.1268258094787598]\n",
      "[0.351645827293396, -1.2908674478530884]\n",
      "[2.0048980712890625, -1.9526951313018799]\n",
      "[0.882585346698761, -1.127960205078125]\n",
      "[1.5420784950256348, -1.7278157472610474]\n",
      "[1.3614232540130615, -1.401172399520874]\n",
      "[2.428248882293701, -2.315549612045288]\n",
      "[-0.892042338848114, -1.2167630195617676]\n",
      "[2.2506296634674072, -2.163311719894409]\n",
      "[0.9701835513114929, -1.1169074773788452]\n",
      "[0.505892813205719, -1.2891390323638916]\n",
      "[-0.0919441282749176, -1.0193052291870117]\n",
      "[1.728513479232788, -1.8516374826431274]\n",
      "[1.6230521202087402, -1.732344627380371]\n",
      "[-1.1245806217193604, -0.5146018266677856]\n",
      "[2.090242624282837, -2.025844097137451]\n",
      "[-1.5326204299926758, -0.5743497610092163]\n",
      "[0.5936486124992371, -1.0760154724121094]\n",
      "[0.1876627802848816, -1.3281298875808716]\n",
      "[-0.14576388895511627, -0.7082871198654175]\n",
      "[1.0996689796447754, -1.176822543144226]\n",
      "[-2.3000025749206543, -0.7275960445404053]\n",
      "[-1.8087353706359863, -0.7278361320495605]\n",
      "[0.5135527849197388, -0.9981092214584351]\n",
      "[1.9609260559082031, -1.917771339416504]\n",
      "[1.9691622257232666, -2.0476887226104736]\n",
      "[1.6838841438293457, -2.1365575790405273]\n",
      "[0.547427773475647, -1.4148553609848022]\n",
      "[-0.7536775469779968, -1.5436080694198608]\n",
      "[1.4602556228637695, -1.4858816862106323]\n",
      "[-0.8331230878829956, -0.5295511484146118]\n",
      "[-0.9650163054466248, -0.4990626871585846]\n",
      "[1.9054455757141113, -2.2093398571014404]\n",
      "[2.1048126220703125, -2.038331985473633]\n",
      "[2.2280666828155518, -2.1439731121063232]\n",
      "[1.355696439743042, -1.3962639570236206]\n",
      "[-0.4739028811454773, -2.4289305210113525]\n",
      "[0.4196310043334961, -1.5415512323379517]\n",
      "[1.5217869281768799, -1.538620114326477]\n",
      "[-0.49971508979797363, -1.6365967988967896]\n",
      "[0.696908175945282, -1.0900275707244873]\n",
      "[-0.4295855164527893, -0.6626896858215332]\n",
      "[1.893721342086792, -1.857405424118042]\n",
      "[0.554231584072113, -1.1132560968399048]\n",
      "[-0.7921079397201538, -0.5951036214828491]\n",
      "[-0.30240780115127563, -2.0554375648498535]\n",
      "[2.1140940189361572, -2.0462870597839355]\n",
      "[-1.2983516454696655, -0.43974778056144714]\n",
      "[1.5783288478851318, -1.5870823860168457]\n",
      "[0.9272823929786682, -1.139360785484314]\n",
      "[2.086376905441284, -2.0225307941436768]\n",
      "[1.3447411060333252, -1.4303213357925415]\n",
      "[1.7403473854064941, -2.10142183303833]\n",
      "[0.7604338526725769, -1.4980920553207397]\n",
      "[1.747950792312622, -1.732465386390686]\n",
      "[0.9280020594596863, -1.7990864515304565]\n",
      "[1.6914045810699463, -1.6839995384216309]\n",
      "[1.2988429069519043, -1.465777039527893]\n",
      "[1.163365125656128, -1.2366496324539185]\n",
      "[-0.4015992283821106, -0.6860337257385254]\n",
      "[1.3526012897491455, -2.1052703857421875]\n",
      "[2.0049970149993896, -1.95278000831604]\n",
      "[-0.07632437348365784, -0.7705279588699341]\n",
      "[0.4619213342666626, -0.939495325088501]\n",
      "[2.1091179847717285, -2.0420219898223877]\n",
      "[-1.069737195968628, -0.5475146770477295]\n",
      "[1.0854151248931885, -1.1646054983139038]\n",
      "[1.7345554828643799, -1.7581212520599365]\n",
      "[0.9864113926887512, -1.409192442893982]\n",
      "[1.4786605834960938, -1.5016565322875977]\n",
      "[1.1798744201660156, -2.165229320526123]\n",
      "[-0.7940502166748047, -0.6733012199401855]\n",
      "[2.025738000869751, -1.9705569744110107]\n",
      "[1.8469676971435547, -1.8173328638076782]\n",
      "[1.5640084743499756, -1.5748083591461182]\n",
      "[1.055903434753418, -1.7754169702529907]\n",
      "[1.1308200359344482, -2.186147928237915]\n",
      "[-0.5448901057243347, -1.2493138313293457]\n",
      "[1.2332816123962402, -1.4871628284454346]\n",
      "[0.5641407370567322, -1.470154047012329]\n",
      "[1.958287000656128, -1.9127446413040161]\n",
      "[-0.01889103651046753, -0.8168582916259766]\n",
      "[-2.0107626914978027, -0.8940610885620117]\n",
      "[-0.36641961336135864, -1.1242626905441284]\n",
      "[0.4577346444129944, -1.8666222095489502]\n",
      "[1.278972864151001, -1.614175796508789]\n",
      "[1.8781085014343262, -2.6601176261901855]\n",
      "[-0.42677512764930725, -1.1499240398406982]\n",
      "[-2.449636697769165, -1.084099292755127]\n",
      "[1.29249906539917, -1.702540397644043]\n",
      "[1.5928537845611572, -1.6235384941101074]\n",
      "[1.8537178039550781, -1.8231184482574463]\n",
      "[-0.8126990795135498, -1.306945562362671]\n",
      "[2.174253463745117, -2.0978498458862305]\n",
      "[1.8633341789245605, -1.8313605785369873]\n",
      "[-1.2149791717529297, -1.168217658996582]\n",
      "[-0.4094018340110779, -1.365698218345642]\n",
      "[-0.03179338574409485, -1.441022276878357]\n",
      "[1.1856105327606201, -1.578416109085083]\n",
      "[1.7818455696105957, -1.8818261623382568]\n",
      "[0.6284454464912415, -1.624507188796997]\n",
      "[0.7962430119514465, -2.3671157360076904]\n",
      "[0.8040516972541809, -1.3017677068710327]\n",
      "[1.8277838230133057, -2.1181468963623047]\n",
      "[-0.7985314130783081, -1.2300688028335571]\n",
      "[1.7763478755950928, -1.7568045854568481]\n",
      "[0.12232613563537598, -1.5147813558578491]\n",
      "[2.54502272605896, -2.4156365394592285]\n",
      "[0.13074880838394165, -1.50974702835083]\n",
      "[2.4554312229156494, -2.3388476371765137]\n",
      "[0.2455022931098938, -1.31782865524292]\n",
      "[0.8746625781059265, -1.2696031332015991]\n",
      "[0.3445212244987488, -1.0305094718933105]\n",
      "[1.9372351169586182, -1.8947011232376099]\n",
      "[1.8344299793243408, -1.806586742401123]\n",
      "[-1.7046444416046143, -1.0250563621520996]\n",
      "[2.1325302124023438, -2.1707603931427]\n",
      "[-2.1955838203430176, -0.9428766965866089]\n",
      "[-0.8690109252929688, -1.2790840864181519]\n",
      "[0.77215176820755, -1.3921802043914795]\n",
      "[-0.7527186274528503, -0.823195219039917]\n",
      "[0.43597882986068726, -1.3520585298538208]\n",
      "[-1.8087348937988281, -0.7278358936309814]\n",
      "[-2.7711849212646484, -1.262840986251831]\n",
      "[-0.24044013023376465, -1.2492921352386475]\n",
      "[2.0520527362823486, -1.9931113719940186]\n",
      "[2.116966724395752, -2.0487492084503174]\n",
      "[2.0495526790618896, -2.2955260276794434]\n",
      "[1.0683395862579346, -1.4581996202468872]\n",
      "[0.2277895212173462, -1.8259845972061157]\n",
      "[-0.5725506544113159, -1.4420214891433716]\n",
      "[-1.3776929378509521, -0.9641711711883545]\n",
      "[-1.499154806137085, -0.9450690746307373]\n",
      "[2.210495710372925, -2.2880501747131348]\n",
      "[2.0422935485839844, -2.104309320449829]\n",
      "[2.4132606983184814, -2.3027031421661377]\n",
      "[0.2274312973022461, -1.5353305339813232]\n",
      "[0.5255486369132996, -2.737637519836426]\n",
      "[0.9526844620704651, -1.6003514528274536]\n",
      "[0.43707966804504395, -1.6713714599609375]\n",
      "[0.3351970314979553, -1.8537802696228027]\n",
      "[-0.49011868238449097, -1.263041615486145]\n",
      "[-0.9805128574371338, -0.9620431661605835]\n",
      "[1.9225022792816162, -1.8820736408233643]\n",
      "[-1.0763944387435913, -1.2231521606445312]\n",
      "[-1.3965604305267334, -1.0428858995437622]\n",
      "[0.6463595032691956, -2.3383121490478516]\n",
      "[2.237941265106201, -2.1524367332458496]\n",
      "[-1.7041049003601074, -0.669674277305603]\n",
      "[-0.09763443470001221, -1.5779829025268555]\n",
      "[-0.2679387927055359, -1.3219276666641235]\n",
      "[2.2171998023986816, -2.1346590518951416]\n",
      "[0.983241617679596, -1.4963486194610596]\n",
      "[2.059262752532959, -2.24364972114563]\n",
      "[1.1752173900604248, -1.5469404458999634]\n",
      "[0.3011106252670288, -1.7069816589355469]\n",
      "[1.5165395736694336, -1.9387266635894775]\n",
      "[1.5540008544921875, -1.7965434789657593]\n",
      "[1.2832839488983154, -1.4714984893798828]\n",
      "[-0.2065637856721878, -1.390097975730896]\n",
      "[-0.9751634001731873, -1.0102370977401733]\n",
      "[1.8796703815460205, -2.284512519836426]\n",
      "[2.0761115550994873, -2.0137321949005127]\n",
      "[-0.6721231937408447, -1.05135178565979]\n",
      "[-0.7004935145378113, -1.1288673877716064]\n",
      "[2.2508535385131836, -2.163503885269165]\n",
      "[-0.46151968836784363, -0.6386520862579346]\n",
      "[0.03749963641166687, -1.3671772480010986]\n",
      "[1.853325366973877, -1.822782039642334]\n",
      "[1.2740800380706787, -1.3771229982376099]\n",
      "[-0.015917301177978516, -1.5700323581695557]\n",
      "[1.795781135559082, -2.294597864151001]\n",
      "[-1.9493789672851562, -1.137007236480713]\n",
      "[2.16542387008667, -2.0902819633483887]\n",
      "[0.995721161365509, -1.843625545501709]\n",
      "[0.6925775408744812, -1.6323786973953247]\n",
      "[1.5487382411956787, -1.903642177581787]\n",
      "[0.216458261013031, -2.1472816467285156]\n",
      "[-2.0944676399230957, -0.9717268943786621]\n",
      "[-0.833815336227417, -0.9817603826522827]\n",
      "[-2.1402831077575684, -0.9121363162994385]\n",
      "[-0.9046884775161743, -1.2137616872787476]\n",
      "[0.14203977584838867, -0.6384234428405762]\n",
      "[-1.7724535465240479, -0.7449362277984619]\n",
      "[-2.0818703174591064, -0.8796659708023071]\n",
      "[-0.602150559425354, -1.8072537183761597]\n",
      "[-1.4287142753601074, -0.9853527545928955]\n",
      "[1.451174259185791, -1.7896288633346558]\n",
      "[-1.9985411167144775, -0.8333450555801392]\n",
      "[-1.8195691108703613, -0.8614503145217896]\n",
      "[-1.5802857875823975, -1.0898003578186035]\n",
      "[-1.2012388706207275, -1.067833662033081]\n",
      "[-0.9929628372192383, -1.167285680770874]\n",
      "[-1.4420037269592285, -1.3155415058135986]\n",
      "[-0.427461177110672, -1.4588580131530762]\n",
      "[-0.8480443954467773, -0.7874515056610107]\n",
      "[-2.0073113441467285, -0.8943008184432983]\n",
      "[0.2783685326576233, -1.2495851516723633]\n",
      "[-1.644880771636963, -1.1514935493469238]\n",
      "[-1.499596357345581, -1.004523515701294]\n",
      "[-0.8444085717201233, -1.3408695459365845]\n",
      "[-1.6990067958831787, -1.1810109615325928]\n",
      "[0.8412087559700012, -1.7585618495941162]\n",
      "[-0.32852035760879517, -0.47267070412635803]\n",
      "[-0.5300581455230713, -1.6871376037597656]\n",
      "[-1.84596586227417, -1.1088894605636597]\n",
      "[-1.0503313541412354, -0.7644528150558472]\n",
      "[-1.760413408279419, -1.1810115575790405]\n",
      "[0.527359664440155, -1.2628973722457886]\n",
      "[1.0161051750183105, -1.4819679260253906]\n",
      "[0.03426074981689453, -1.4740406274795532]\n",
      "[-1.7964565753936768, -0.8287036418914795]\n",
      "[-0.7097085118293762, -0.42529699206352234]\n",
      "[-0.18236565589904785, -0.637802243232727]\n",
      "[-0.7966526746749878, -0.9155899286270142]\n",
      "[-0.8900098204612732, -0.916915774345398]\n",
      "[-1.9504170417785645, -0.8065938949584961]\n",
      "[-0.2710915207862854, -1.7186765670776367]\n",
      "[-0.9761161208152771, -0.8675936460494995]\n",
      "[-1.5886554718017578, -1.0752513408660889]\n",
      "[0.24516171216964722, -0.776519775390625]\n",
      "[-0.7888071537017822, -0.7122008800506592]\n",
      "[-2.0170602798461914, -0.8436393737792969]\n",
      "[0.513553261756897, -0.9981094598770142]\n",
      "[-0.24044013023376465, -1.2492921352386475]\n",
      "[-2.343357801437378, -1.0250210762023926]\n",
      "[-0.7738406658172607, -0.9538240432739258]\n",
      "[0.049003928899765015, -0.8122998476028442]\n",
      "[0.6420472264289856, -0.7845945358276367]\n",
      "[-0.17639777064323425, -0.4877679646015167]\n",
      "[0.818300187587738, -1.5999077558517456]\n",
      "[-0.7434981465339661, -1.5751692056655884]\n",
      "[-1.9372262954711914, -0.7992614507675171]\n",
      "[-1.8707680702209473, -0.7623187303543091]\n",
      "[0.5605488419532776, -0.7147421836853027]\n",
      "[-0.5718538761138916, -1.6051151752471924]\n",
      "[-0.08790460228919983, -1.4131879806518555]\n",
      "[-1.8705949783325195, -1.1437478065490723]\n",
      "[0.7640005946159363, -2.253690242767334]\n",
      "[0.4902344346046448, -0.8490530252456665]\n",
      "[-1.6134865283966064, -1.33650541305542]\n",
      "[0.5430980920791626, -1.405665636062622]\n",
      "[-2.1677398681640625, -0.9500894546508789]\n",
      "[-1.850722074508667, -0.7511755228042603]\n",
      "[-0.9795693755149841, -1.047908902168274]\n",
      "[-1.661085844039917, -1.1767029762268066]\n",
      "[-1.949211835861206, -0.8059239387512207]\n",
      "[0.8786739706993103, -1.8275221586227417]\n",
      "[-0.35551220178604126, -1.5412702560424805]\n",
      "[-0.314579576253891, -0.9136595726013184]\n",
      "[-1.139093279838562, -1.5002293586730957]\n",
      "[-2.1505279541015625, -0.9580892324447632]\n",
      "[-0.4228321611881256, -1.4844952821731567]\n",
      "[-1.212855577468872, -0.9373289346694946]\n",
      "[0.5089967250823975, -0.670556902885437]\n",
      "[-0.7730217576026917, -0.46363452076911926]\n",
      "[-0.8972225189208984, -1.560902714729309]\n",
      "[0.48829418420791626, -0.7778708934783936]\n",
      "[-1.4458982944488525, -1.1753060817718506]\n",
      "[-0.8324953317642212, -0.9647445678710938]\n",
      "[-1.8579719066619873, -1.0934293270111084]\n",
      "[-1.9059183597564697, -0.781857967376709]\n",
      "[1.0116987228393555, -1.111296534538269]\n",
      "[-0.6733608245849609, -1.0964547395706177]\n",
      "[-2.033353567123413, -0.852696418762207]\n",
      "[-2.2050657272338867, -0.9481475353240967]\n",
      "[-0.4160555601119995, -1.280296802520752]\n",
      "[0.372982919216156, -0.6449780464172363]\n",
      "[-2.1627566814422607, -0.9246288537979126]\n",
      "[-1.0104658603668213, -0.8974350690841675]\n",
      "[-1.033155918121338, -0.6663855314254761]\n",
      "[-1.4189345836639404, -1.3917317390441895]\n",
      "[1.1225824356079102, -1.3303691148757935]\n",
      "[-2.0842273235321045, -0.8809760808944702]\n",
      "[-0.6520371437072754, -1.0925213098526]\n",
      "[-1.1311300992965698, -1.4800599813461304]\n",
      "[-1.6442193984985352, -1.15483820438385]\n",
      "[0.38096827268600464, -0.677354097366333]\n",
      "[1.9502737522125244, -2.0817320346832275]\n",
      "[-0.1501910537481308, -1.1269742250442505]\n",
      "[-1.3220577239990234, -0.41434845328330994]\n",
      "[-1.2158851623535156, -0.7830973863601685]\n",
      "[-1.7501187324523926, -0.5047332048416138]\n",
      "[0.6464876532554626, -0.8905030488967896]\n",
      "[0.6285186409950256, -1.2026017904281616]\n",
      "[-0.47337931394577026, -0.9113262891769409]\n",
      "[1.1681456565856934, -1.7883200645446777]\n",
      "[-1.6553864479064941, -0.5471251010894775]\n",
      "[-0.026544392108917236, -1.0925990343093872]\n",
      "[-0.8160593509674072, -0.8762309551239014]\n",
      "[1.7794325351715088, -1.7594484090805054]\n",
      "[-1.1412267684936523, -0.718393087387085]\n",
      "[-1.8508260250091553, -0.4627368748188019]\n",
      "[-1.9753618240356445, -0.47740182280540466]\n",
      "[0.5197626352310181, -1.498578429222107]\n",
      "[-0.6713900566101074, -0.7401150465011597]\n",
      "[-1.258404016494751, -0.2876014709472656]\n",
      "[0.5101046562194824, -1.3264975547790527]\n",
      "[0.042224228382110596, -1.3445099592208862]\n",
      "[0.40248537063598633, -1.3105642795562744]\n",
      "[-1.919825553894043, -0.5219603776931763]\n",
      "[-0.5856994986534119, -0.8080236911773682]\n",
      "[-0.21931518614292145, -1.0579339265823364]\n",
      "[-0.2361963987350464, -1.361485242843628]\n",
      "[0.061173439025878906, -0.4899872839450836]\n",
      "[0.19514137506484985, -1.2596219778060913]\n",
      "[-0.012607544660568237, -1.245563268661499]\n",
      "[-1.2662172317504883, -0.31254610419273376]\n",
      "[-0.10887730121612549, -1.168265461921692]\n",
      "[-0.8036321401596069, -0.5854319334030151]\n",
      "[0.8506785035133362, -1.503661870956421]\n",
      "[-0.4309992790222168, -0.7512335777282715]\n",
      "[-1.1696052551269531, -0.7119669914245605]\n",
      "[0.0013045668601989746, -0.369304895401001]\n",
      "[0.5133188366889954, -0.7805622816085815]\n",
      "[-1.38075590133667, -0.30348071455955505]\n",
      "[-1.4106554985046387, -0.3332047462463379]\n",
      "[0.5244845747947693, -1.2385849952697754]\n",
      "[0.25655215978622437, -1.2197211980819702]\n",
      "[1.5482988357543945, -1.5613434314727783]\n",
      "[0.8182020783424377, -1.4524530172348022]\n",
      "[0.2072334885597229, -0.6616405248641968]\n",
      "[0.5965927243232727, -0.867223858833313]\n",
      "[-1.6581857204437256, -0.6459670066833496]\n",
      "[1.9609260559082031, -1.917771339416504]\n",
      "[2.0520527362823486, -1.9931113719940186]\n",
      "[-0.7738407254219055, -0.9538240432739258]\n",
      "[-1.488738775253296, -0.31261590123176575]\n",
      "[-1.2407586574554443, -0.26970934867858887]\n",
      "[-0.32866841554641724, -0.38302353024482727]\n",
      "[-0.05698898434638977, -0.4093267023563385]\n",
      "[0.42399418354034424, -1.519880771636963]\n",
      "[1.3688867092132568, -1.78019118309021]\n",
      "[0.09679055213928223, -1.0587079524993896]\n",
      "[-0.03827929496765137, -1.050033688545227]\n",
      "[-0.48284628987312317, -0.32539209723472595]\n",
      "[0.18625140190124512, -1.1862472295761108]\n",
      "[-0.7126108407974243, -0.6612539291381836]\n",
      "[-0.05048650503158569, -1.1666977405548096]\n",
      "[-0.2217053771018982, -1.9301097393035889]\n",
      "[-0.6039684414863586, -0.4240417182445526]\n",
      "[0.004987806081771851, -1.2412664890289307]\n",
      "[-0.13906294107437134, -1.2074456214904785]\n",
      "[0.09829932451248169, -1.1893961429595947]\n",
      "[-0.7499553561210632, -0.8827064037322998]\n",
      "[-1.731431007385254, -0.4025329053401947]\n",
      "[0.5701313018798828, -1.4716508388519287]\n",
      "[0.48196858167648315, -1.192164659500122]\n",
      "[-0.0047917962074279785, -1.5235874652862549]\n",
      "[-0.3689122498035431, -0.8785879611968994]\n",
      "[1.3510677814483643, -1.3922966718673706]\n",
      "[0.685918390750885, -1.5579270124435425]\n",
      "[-0.008601248264312744, -1.1362029314041138]\n",
      "[-0.44687625765800476, -0.8306009769439697]\n",
      "[-1.3771274089813232, -0.5419285297393799]\n",
      "[-0.4355960786342621, -0.3012271523475647]\n",
      "[-0.04477822780609131, -0.40292981266975403]\n",
      "[0.5746830105781555, -1.5204992294311523]\n",
      "[-0.250031054019928, -0.43514296412467957]\n",
      "[-1.1800282001495361, -0.7138625383377075]\n",
      "[-1.3218464851379395, -0.38066282868385315]\n",
      "[0.07766261696815491, -1.2257972955703735]\n",
      "[-0.4540204703807831, -0.9601044654846191]\n",
      "[-0.36183544993400574, -0.4653567969799042]\n",
      "[-1.7565641403198242, -0.3962451219558716]\n",
      "[-0.7894037961959839, -0.8955479860305786]\n",
      "[-0.34993284940719604, -1.0192137956619263]\n",
      "[-1.609886884689331, -0.48525145649909973]\n",
      "[0.6689383387565613, -1.0396478176116943]\n",
      "[-0.21458317339420319, -1.011987328529358]\n",
      "[-1.432159423828125, -0.35353079438209534]\n",
      "[-0.5652202367782593, -0.33845481276512146]\n",
      "[0.3762495517730713, -1.430837869644165]\n",
      "[-0.4956042766571045, -0.5615218877792358]\n",
      "[1.1061103343963623, -1.47754967212677]\n",
      "[-1.7823741436004639, -0.41025277972221375]\n",
      "[0.3352823257446289, -1.3233691453933716]\n",
      "[-0.08434587717056274, -1.0988091230392456]\n",
      "[-0.37092649936676025, -0.3521941900253296]\n",
      "[2.142735242843628, -2.070835590362549]\n",
      "[0.4270898103713989, -0.9496208429336548]\n",
      "[-0.5800092220306396, -0.35926300287246704]\n",
      "[-0.5120211839675903, -0.6319526433944702]\n",
      "[-1.135169506072998, -0.41074392199516296]\n",
      "[0.609592616558075, -1.0008888244628906]\n",
      "[1.34488844871521, -1.387000322341919]\n",
      "[0.6214753985404968, -0.8639073371887207]\n",
      "[1.6358535289764404, -1.636386752128601]\n",
      "[-1.080780267715454, -0.43269088864326477]\n",
      "[-0.486366868019104, -1.0861337184906006]\n",
      "[0.013889998197555542, -0.7369540929794312]\n",
      "[1.8508591651916504, -1.820668339729309]\n",
      "[-0.5365013480186462, -0.5786285400390625]\n",
      "[-1.5206327438354492, -0.36854588985443115]\n",
      "[-1.1549277305603027, -0.44989171624183655]\n",
      "[1.0971901416778564, -1.314620852470398]\n",
      "[-0.40624356269836426, -0.5805026292800903]\n",
      "[-0.972452700138092, -0.21873009204864502]\n",
      "[1.1269960403442383, -1.2002445459365845]\n",
      "[-0.21748793125152588, -1.3755342960357666]\n",
      "[0.963190495967865, -1.1298071146011353]\n",
      "[-1.4127554893493652, -0.4274460971355438]\n",
      "[-0.22838972508907318, -0.6562144756317139]\n",
      "[0.3137304186820984, -0.8800468444824219]\n",
      "[-0.6228576898574829, -1.3528070449829102]\n",
      "[0.07056337594985962, -0.6523455381393433]\n",
      "[0.5701366662979126, -1.0771633386611938]\n",
      "[0.5645789504051208, -1.0620906352996826]\n",
      "[-0.9427725076675415, -0.277566522359848]\n",
      "[0.4467136263847351, -0.9856250286102295]\n",
      "[-0.6750547885894775, -0.5102226734161377]\n",
      "[0.6166505813598633, -1.5701773166656494]\n",
      "[-0.24780814349651337, -0.6067867279052734]\n",
      "[-0.6060457229614258, -0.5579051971435547]\n",
      "[0.058385252952575684, -0.5785313844680786]\n",
      "[0.5392817854881287, -0.8998377323150635]\n",
      "[-1.0158557891845703, -0.24881350994110107]\n",
      "[-1.0858542919158936, -0.2692073583602905]\n",
      "[1.2662603855133057, -1.3196080923080444]\n",
      "[0.5689957141876221, -1.0513267517089844]\n",
      "[1.606330156326294, -1.6110823154449463]\n",
      "[1.3363349437713623, -1.3796690702438354]\n",
      "[0.1057199239730835, -0.8064109086990356]\n",
      "[0.9431069493293762, -1.042633056640625]\n",
      "[-0.9358643293380737, -0.5290296077728271]\n",
      "[1.9691622257232666, -2.0476887226104736]\n",
      "[2.116966724395752, -2.0487492084503174]\n",
      "[0.04900386929512024, -0.8122998476028442]\n",
      "[-1.2407586574554443, -0.26970934867858887]\n",
      "[-1.0234642028808594, -0.17956864833831787]\n",
      "[-0.5783323049545288, -0.38921716809272766]\n",
      "[-0.09962952136993408, -0.5752440690994263]\n",
      "[0.15184056758880615, -1.5643565654754639]\n",
      "[1.7191908359527588, -1.7078152894973755]\n",
      "[1.1114931106567383, -1.1869571208953857]\n",
      "[1.0400407314300537, -1.1257150173187256]\n",
      "[-0.5553848743438721, -0.3307902216911316]\n",
      "[0.562680184841156, -1.0199532508850098]\n",
      "[-0.42402660846710205, -0.5361371040344238]\n",
      "[0.5132221579551697, -0.9870699644088745]\n",
      "[-0.6483020186424255, -1.8877825736999512]\n",
      "[-0.7405292987823486, -0.5293867588043213]\n",
      "[0.5559279322624207, -1.0561200380325317]\n",
      "[-0.43089228868484497, -1.2399917840957642]\n",
      "[0.6804085373878479, -1.0140143632888794]\n",
      "[0.30845028162002563, -0.8064365386962891]\n",
      "[-1.409193992614746, -0.31536921858787537]\n",
      "[1.1584746837615967, -1.2886499166488647]\n",
      "[1.2369592189788818, -1.2944940328598022]\n",
      "[-0.3707846999168396, -1.5225257873535156]\n",
      "[-0.08357596397399902, -0.7083848714828491]\n",
      "[1.5604000091552734, -1.5717153549194336]\n",
      "[1.2500393390655518, -1.3694038391113281]\n",
      "[0.5664820075035095, -0.9611881971359253]\n",
      "[-0.1606014519929886, -0.672966718673706]\n",
      "[-0.7947776913642883, -0.4262970983982086]\n",
      "[-0.6368216276168823, -0.29779544472694397]\n",
      "[0.17409873008728027, -0.5881128311157227]\n",
      "[1.1077680587768555, -1.327719807624817]\n",
      "[-0.45801985263824463, -0.5358625650405884]\n",
      "[-0.5507216453552246, -0.5720330476760864]\n",
      "[-0.5257126092910767, -0.34825441241264343]\n",
      "[0.6505191922187805, -1.0461350679397583]\n",
      "[0.6762669682502747, -0.9210587739944458]\n",
      "[-0.6956456899642944, -0.48882856965065]\n",
      "[-1.4517958164215088, -0.3111664652824402]\n",
      "[0.29954594373703003, -0.8152338266372681]\n",
      "[0.19961392879486084, -0.8539084196090698]\n",
      "[-1.0351653099060059, -0.4308513104915619]\n",
      "[0.5137147307395935, -1.1285200119018555]\n",
      "[0.3513461947441101, -0.8447582721710205]\n",
      "[-1.1866168975830078, -0.28406476974487305]\n",
      "[-0.05050736665725708, -0.3450932502746582]\n",
      "[0.9445065855979919, -1.244992733001709]\n",
      "[-0.8936685919761658, -0.5699608325958252]\n",
      "[1.4931578636169434, -1.5140821933746338]\n",
      "[-1.5103039741516113, -0.32728302478790283]\n",
      "[0.8398815989494324, -1.1404348611831665]\n",
      "[0.44863224029541016, -0.9234999418258667]\n",
      "[-0.644156813621521, -0.3866179585456848]\n",
      "[1.9224586486816406, -1.8820362091064453]\n",
      "[0.74542635679245, -0.8732008934020996]\n",
      "[0.07277566194534302, -0.6165310144424438]\n",
      "[0.4476807713508606, -0.6180028915405273]\n",
      "[0.22403442859649658, -0.4650033414363861]\n",
      "[0.01408451795578003, -1.1837137937545776]\n",
      "[1.5385351181030273, -1.6271463632583618]\n",
      "[1.2271032333374023, -1.2889660596847534]\n",
      "[1.4189372062683105, -1.450467586517334]\n",
      "[0.24566036462783813, -0.498383492231369]\n",
      "[-1.7880713939666748, -0.967204213142395]\n",
      "[0.7572261691093445, -0.8882502317428589]\n",
      "[1.9740653038024902, -2.056197166442871]\n",
      "[0.44860541820526123, -0.6187953948974609]\n",
      "[-0.5591176748275757, -0.3764776587486267]\n",
      "[0.172807514667511, -0.48281142115592957]\n",
      "[1.0537774562835693, -1.1374889612197876]\n",
      "[0.5688353776931763, -0.7218445539474487]\n",
      "[-0.5401914715766907, -0.3907076418399811]\n",
      "[1.2694568634033203, -1.3223479986190796]\n",
      "[-1.1471266746520996, -1.415147304534912]\n",
      "[0.9939867854118347, -1.0862421989440918]\n",
      "[-0.08323335647583008, -0.4550040066242218]\n",
      "[0.6360147595405579, -0.7794240713119507]\n",
      "[0.7050378918647766, -0.8385839462280273]\n",
      "[-1.8517537117004395, -1.2526317834854126]\n",
      "[-0.5516748428344727, -0.8257163763046265]\n",
      "[1.0356721878051758, -1.121970772743225]\n",
      "[0.8341655135154724, -0.9492592811584473]\n",
      "[-0.5945101976394653, -0.3913842439651489]\n",
      "[0.7803391814231873, -0.9031246900558472]\n",
      "[0.14154815673828125, -0.5450730323791504]\n",
      "[-0.299002468585968, -1.6027909517288208]\n",
      "[0.6512365937232971, -0.7924708127975464]\n",
      "[0.35595160722732544, -0.6252175569534302]\n",
      "[-0.5048015117645264, -0.7792847156524658]\n",
      "[-0.10391485691070557, -1.0638973712921143]\n",
      "[-0.2976488173007965, -0.42222893238067627]\n",
      "[-0.38570117950439453, -0.3986767530441284]\n",
      "[1.5687706470489502, -1.5788899660110474]\n",
      "[1.0577723979949951, -1.1409130096435547]\n",
      "[1.5197677612304688, -1.8102226257324219]\n",
      "[1.4937756061553955, -1.5146116018295288]\n",
      "[-0.6480177640914917, -0.9242504835128784]\n",
      "[0.7332431674003601, -1.2123992443084717]\n",
      "[0.29499274492263794, -0.5458277463912964]\n",
      "[1.6838841438293457, -2.1365573406219482]\n",
      "[2.0495526790618896, -2.2955260276794434]\n",
      "[0.6420472264289856, -0.7845945358276367]\n",
      "[-0.32866841554641724, -0.38302353024482727]\n",
      "[-0.5783325433731079, -0.38921719789505005]\n",
      "[-1.6729531288146973, -0.3019324839115143]\n",
      "[-0.7959558963775635, -0.7161009311676025]\n",
      "[-0.8442375063896179, -1.5612155199050903]\n",
      "[1.4030749797821045, -1.436872124671936]\n",
      "[1.3170018196105957, -1.47339928150177]\n",
      "[1.3289713859558105, -1.4238158464431763]\n",
      "[-1.3427269458770752, -0.3655158579349518]\n",
      "[0.9590288996696472, -1.0562797784805298]\n",
      "[0.5100438594818115, -0.6714543104171753]\n",
      "[0.7768240571022034, -0.9001117944717407]\n",
      "[-1.875349760055542, -1.7848304510116577]\n",
      "[-1.362806797027588, -0.7033569812774658]\n",
      "[0.8143433928489685, -0.9322696924209595]\n",
      "[-1.4542126655578613, -1.2292810678482056]\n",
      "[0.8416337370872498, -0.9556602239608765]\n",
      "[0.8773905634880066, -1.0825254917144775]\n",
      "[-0.31698179244995117, -0.3778613209724426]\n",
      "[1.0493080615997314, -1.1336581707000732]\n",
      "[1.5259275436401367, -1.5680029392242432]\n",
      "[-1.5574347972869873, -1.437562108039856]\n",
      "[0.712537944316864, -0.8450121879577637]\n",
      "[1.4234519004821777, -1.762427568435669]\n",
      "[1.132399559020996, -1.2048760652542114]\n",
      "[0.7862892746925354, -0.9082244634628296]\n",
      "[0.6848320364952087, -0.8212653398513794]\n",
      "[0.26381176710128784, -0.6833041906356812]\n",
      "[-1.4052188396453857, -0.3584662973880768]\n",
      "[-0.412691593170166, -0.7585666179656982]\n",
      "[1.201958417892456, -1.2644948959350586]\n",
      "[-1.418166160583496, -0.5715481042861938]\n",
      "[0.43305695056915283, -0.60546875]\n",
      "[0.04095771908760071, -0.6138507127761841]\n",
      "[0.8446317315101624, -0.9582298994064331]\n",
      "[1.1623663902282715, -1.260110855102539]\n",
      "[-1.8828303813934326, -0.42476925253868103]\n",
      "[-0.23454295098781586, -0.3855818212032318]\n",
      "[1.0315656661987305, -1.1395668983459473]\n",
      "[0.6844646334648132, -0.8209505081176758]\n",
      "[0.18949782848358154, -0.4789731800556183]\n",
      "[-0.12493646144866943, -1.2935394048690796]\n",
      "[0.7030078768730164, -0.8368439674377441]\n",
      "[-0.5323182344436646, -0.3656661808490753]\n",
      "[-0.3528103828430176, -0.6308974027633667]\n",
      "[0.9969771504402161, -1.0888053178787231]\n",
      "[-2.215970516204834, -0.44737932085990906]\n",
      "[1.814749002456665, -1.7897182703018188]\n",
      "[-0.4717695116996765, -0.36302244663238525]\n",
      "[0.9514654278755188, -1.0497970581054688]\n",
      "[0.7820317149162292, -0.9045754671096802]\n",
      "[-1.5594451427459717, -0.44791826605796814]\n",
      "[1.8040308952331543, -1.7805317640304565]\n",
      "[0.427034854888916, -0.606682538986206]\n",
      "[-0.3645709753036499, -0.4903753697872162]\n",
      "[-0.2927247881889343, -0.442570298910141]\n",
      "[0.2065562605857849, -0.47551724314689636]\n",
      "[-1.2473965883255005, -0.5592037439346313]\n",
      "[0.7200555205345154, -0.9928380250930786]\n",
      "[0.48930805921554565, -0.9005107879638672]\n",
      "[1.301788330078125, -1.350059151649475]\n",
      "[-0.02992209792137146, -0.51958167552948]\n",
      "[-1.0979191064834595, -1.231046438217163]\n",
      "[-0.28033721446990967, -0.5552778244018555]\n",
      "[1.0572326183319092, -1.14493727684021]\n",
      "[0.21780502796173096, -0.5145988464355469]\n",
      "[-0.40141698718070984, -0.409392774105072]\n",
      "[0.22121703624725342, -0.5315995216369629]\n",
      "[0.8501289486885071, -0.9629415273666382]\n",
      "[0.6543067097663879, -0.7951021194458008]\n",
      "[-0.5057368278503418, -0.41437914967536926]\n",
      "[0.8729068636894226, -1.0699529647827148]\n",
      "[-2.113003969192505, -0.8208127021789551]\n",
      "[0.8486023545265198, -0.9616330862045288]\n",
      "[-0.3590799868106842, -0.4486362040042877]\n",
      "[0.6286017894744873, -0.7730704545974731]\n",
      "[0.4853159785270691, -0.6502599716186523]\n",
      "[-1.8852195739746094, -1.1551116704940796]\n",
      "[-1.7358477115631104, -0.28076818585395813]\n",
      "[0.9793551564216614, -1.0737015008926392]\n",
      "[0.5780616402626038, -0.7297524213790894]\n",
      "[-0.7080996036529541, -0.3529169261455536]\n",
      "[0.5487759709358215, -0.7046517133712769]\n",
      "[0.6575077176094055, -0.797845721244812]\n",
      "[-1.2958805561065674, -1.0666738748550415]\n",
      "[0.798214852809906, -0.9184459447860718]\n",
      "[-0.5303025841712952, -0.45433273911476135]\n",
      "[-1.6117162704467773, -0.278473824262619]\n",
      "[-1.4093034267425537, -0.46226605772972107]\n",
      "[-0.2621379792690277, -0.4425322115421295]\n",
      "[-0.43030673265457153, -0.40593624114990234]\n",
      "[0.8527715802192688, -1.064307689666748]\n",
      "[1.0214910507202148, -1.1098161935806274]\n",
      "[0.5324026346206665, -1.0400046110153198]\n",
      "[1.2422802448272705, -1.299054741859436]\n",
      "[-1.9479429721832275, -0.33136799931526184]\n",
      "[-0.4568489193916321, -0.6751229763031006]\n",
      "[-0.31757357716560364, -0.47914549708366394]\n",
      "[0.5474282503128052, -1.4148552417755127]\n",
      "[1.0683388710021973, -1.458199381828308]\n",
      "[-0.17639771103858948, -0.4877679646015167]\n",
      "[-0.05698898434638977, -0.4093267023563385]\n",
      "[-0.09962952136993408, -0.5752440690994263]\n",
      "[-0.7959558963775635, -0.7161009311676025]\n",
      "[-1.769531488418579, -0.24477314949035645]\n",
      "[-1.6801707744598389, -1.0938293933868408]\n",
      "[1.3209140300750732, -1.3664518594741821]\n",
      "[0.4981837272644043, -0.8347873687744141]\n",
      "[0.4194837212562561, -0.8396670818328857]\n",
      "[-0.4591636061668396, -0.7302652597427368]\n",
      "[0.8909077048301697, -0.9978929758071899]\n",
      "[0.6676378846168518, -0.8065283298492432]\n",
      "[0.5042114853858948, -0.6664553880691528]\n",
      "[-1.8868474960327148, -1.6720162630081177]\n",
      "[-1.8197181224822998, -0.43759891390800476]\n",
      "[0.6529231667518616, -0.7939163446426392]\n",
      "[-2.1736061573028564, -0.8115804195404053]\n",
      "[0.4890748858451843, -0.6534818410873413]\n",
      "[-0.20783549547195435, -0.6276544332504272]\n",
      "[-0.02242034673690796, -0.42696109414100647]\n",
      "[0.7602983117103577, -0.885947585105896]\n",
      "[0.8122069239616394, -1.0737334489822388]\n",
      "[-1.8313536643981934, -1.2277220487594604]\n",
      "[0.760707676410675, -0.8862985372543335]\n",
      "[0.3731192946434021, -1.1637464761734009]\n",
      "[0.9806111454963684, -1.0747779607772827]\n",
      "[0.42155003547668457, -0.5956062078475952]\n",
      "[0.7368181347846985, -0.8658227920532227]\n",
      "[-0.12116396427154541, -0.5895447731018066]\n",
      "[-0.6270053386688232, -0.6822057962417603]\n",
      "[-1.565699577331543, -0.29134050011634827]\n",
      "[1.057551383972168, -1.1407233476638794]\n",
      "[-1.6515090465545654, -0.4509238302707672]\n",
      "[0.24263566732406616, -0.4965972602367401]\n",
      "[-0.39821386337280273, -0.47424444556236267]\n",
      "[0.5536614060401917, -0.708838939666748]\n",
      "[0.20410913228988647, -0.8004438877105713]\n",
      "[-1.1483814716339111, -0.7279236316680908]\n",
      "[0.16576272249221802, -0.45818135142326355]\n",
      "[0.07968524098396301, -0.7317447662353516]\n",
      "[0.18254858255386353, -0.5606813430786133]\n",
      "[0.441336452960968, -0.6125651597976685]\n",
      "[-1.3793156147003174, -0.5782798528671265]\n",
      "[0.37353962659835815, -0.6074504852294922]\n",
      "[-0.3863479197025299, -0.3412543535232544]\n",
      "[-1.1588819026947021, -0.30472442507743835]\n",
      "[0.8368931412696838, -0.9515970945358276]\n",
      "[-1.202101707458496, -0.8471941947937012]\n",
      "[1.1635329723358154, -1.2315603494644165]\n",
      "[0.13814127445220947, -0.45526668429374695]\n",
      "[0.8216702342033386, -0.9385495185852051]\n",
      "[0.5967016220092773, -0.7457287311553955]\n",
      "[-1.4566891193389893, -0.4631482660770416]\n",
      "[3.2248313426971436, -2.998906135559082]\n",
      "[1.5357739925384521, -1.8012362718582153]\n",
      "[-0.04767048358917236, -1.480711579322815]\n",
      "[0.8419191241264343, -1.5904431343078613]\n",
      "[1.2288930416107178, -1.7226265668869019]\n",
      "[-1.767249584197998, -1.0588984489440918]\n",
      "[0.11354506015777588, -1.437336802482605]\n",
      "[0.3818128705024719, -1.525874376296997]\n",
      "[2.6596992015838623, -2.5468297004699707]\n",
      "[0.7242185473442078, -1.6420258283615112]\n",
      "[-1.8954002857208252, -1.698879599571228]\n",
      "[0.23775535821914673, -1.4563778638839722]\n",
      "[0.460827112197876, -1.5502853393554688]\n",
      "[1.3283469676971436, -1.7589106559753418]\n",
      "[0.5778763890266418, -1.4551305770874023]\n",
      "[1.4056925773620605, -1.7275917530059814]\n",
      "[2.054763078689575, -2.101088285446167]\n",
      "[1.9991176128387451, -2.013015031814575]\n",
      "[-0.12052863836288452, -1.445691466331482]\n",
      "[1.2289488315582275, -1.754797101020813]\n",
      "[-3.8594765663146973, -0.8472574949264526]\n",
      "[1.63932466506958, -2.003216505050659]\n",
      "[0.9997215867042542, -1.513407588005066]\n",
      "[1.9928226470947266, -2.054267644882202]\n",
      "[1.6552927494049072, -1.925415277481079]\n",
      "[-3.0758543014526367, -1.3845365047454834]\n",
      "[-1.6289148330688477, -1.1073596477508545]\n",
      "[2.439204692840576, -2.324939727783203]\n",
      "[1.8476371765136719, -1.9100216627120972]\n",
      "[-0.32686513662338257, -1.4053096771240234]\n",
      "[1.7450823783874512, -1.9592065811157227]\n",
      "[1.8115108013153076, -1.907364010810852]\n",
      "[-3.2722668647766113, -0.9166738986968994]\n",
      "[2.1652684211730957, -2.1452999114990234]\n",
      "[0.20555007457733154, -1.4532190561294556]\n",
      "[-1.429244041442871, -1.1477148532867432]\n",
      "[-1.7741551399230957, -1.0634583234786987]\n",
      "[0.16922509670257568, -1.5409893989562988]\n",
      "[0.012480497360229492, -1.4806749820709229]\n",
      "[0.4415590763092041, -1.552014708518982]\n",
      "[2.523468017578125, -2.3971619606018066]\n",
      "[-0.29703086614608765, -1.399475336074829]\n",
      "[1.604926586151123, -1.9377882480621338]\n",
      "[-1.9854326248168945, -1.0772689580917358]\n",
      "[-0.8020092248916626, -1.2332487106323242]\n",
      "[0.9047366976737976, -1.5073480606079102]\n",
      "[-0.7536775469779968, -1.5436080694198608]\n",
      "[0.2277895212173462, -1.8259845972061157]\n",
      "[0.8183003067970276, -1.5999077558517456]\n",
      "[0.42399418354034424, -1.519880771636963]\n",
      "[0.15184056758880615, -1.5643565654754639]\n",
      "[-0.8442375063896179, -1.5612155199050903]\n",
      "[-1.6801707744598389, -1.0938293933868408]\n",
      "[-3.8768937587738037, -0.8155148029327393]\n",
      "[2.2789323329925537, -2.401231527328491]\n",
      "[0.033157289028167725, -1.3773465156555176]\n",
      "[0.07334238290786743, -1.3908421993255615]\n",
      "[-0.453921377658844, -1.6032564640045166]\n",
      "[2.305020332336426, -2.3618319034576416]\n",
      "[1.9369797706604004, -2.0241198539733887]\n",
      "[1.5926687717437744, -1.9230811595916748]\n",
      "[-3.5435609817504883, -1.6664997339248657]\n",
      "[-1.680354356765747, -1.2667889595031738]\n",
      "[1.8660471439361572, -2.0092556476593018]\n",
      "[-3.279386281967163, -1.0553494691848755]\n",
      "[1.3500525951385498, -1.7884079217910767]\n",
      "[0.01776522397994995, -1.3546217679977417]\n",
      "[0.5982116460800171, -1.5708963871002197]\n",
      "[1.708846092224121, -1.9731321334838867]\n",
      "[0.3696547746658325, -1.5593652725219727]\n",
      "[-3.430054187774658, -1.2669638395309448]\n",
      "[2.154092788696289, -2.1424314975738525]\n",
      "[-0.6514201760292053, -1.4655412435531616]\n",
      "[2.258538246154785, -2.271881341934204]\n",
      "[1.3790733814239502, -1.781016230583191]\n",
      "[2.1343088150024414, -2.1289708614349365]\n",
      "[0.17621004581451416, -1.5516971349716187]\n",
      "[-0.5784466862678528, -1.5585763454437256]\n",
      "[-1.4386353492736816, -1.1855183839797974]\n",
      "[2.506570816040039, -2.3826792240142822]\n",
      "[-1.6472175121307373, -1.2532802820205688]\n",
      "[1.3440783023834229, -1.7693159580230713]\n",
      "[-0.08330738544464111, -1.4547486305236816]\n",
      "[1.6227662563323975, -1.9280569553375244]\n",
      "[0.15721577405929565, -1.4505313634872437]\n",
      "[-1.3720602989196777, -1.4593526124954224]\n",
      "[0.8496140837669373, -1.63711416721344]\n",
      "[0.29013097286224365, -1.4588767290115356]\n",
      "[1.2447633743286133, -1.635960340499878]\n",
      "[1.5475058555603027, -1.8278249502182007]\n",
      "[-2.2916624546051025, -0.8806681632995605]\n",
      "[1.2386302947998047, -1.7352908849716187]\n",
      "[0.0484929084777832, -1.3964751958847046]\n",
      "[-0.8131377696990967, -1.28434157371521]\n",
      "[2.036221981048584, -2.144068717956543]\n",
      "[-1.4930646419525146, -1.5342328548431396]\n",
      "[1.114577293395996, -1.658907413482666]\n",
      "[0.9714280962944031, -1.5743780136108398]\n",
      "[2.106715679168701, -2.2421674728393555]\n",
      "[1.6255426406860352, -1.983459711074829]\n",
      "[-1.3404393196105957, -1.3161927461624146]\n",
      "[-1.9798495769500732, -1.7556509971618652]\n",
      "[-1.6517341136932373, -1.3577417135238647]\n",
      "[0.8959901928901672, -1.723509669303894]\n",
      "[-0.3924294710159302, -1.7035702466964722]\n",
      "[0.39493393898010254, -1.9277033805847168]\n",
      "[1.628354787826538, -1.6299597024917603]\n",
      "[-1.166218638420105, -1.2472755908966064]\n",
      "[-1.000747561454773, -1.323702096939087]\n",
      "[-2.4466986656188965, -1.4894825220108032]\n",
      "[0.16728544235229492, -1.7518339157104492]\n",
      "[2.829754114151001, -2.659679889678955]\n",
      "[-0.13531902432441711, -1.6296961307525635]\n",
      "[-2.398266315460205, -1.1332529783248901]\n",
      "[-0.42675694823265076, -1.7253108024597168]\n",
      "[0.6820885539054871, -1.8183449506759644]\n",
      "[0.19173574447631836, -1.840930461883545]\n",
      "[-2.677032470703125, -1.2105036973953247]\n",
      "[0.23943614959716797, -2.0119166374206543]\n",
      "[1.5431339740753174, -1.612141489982605]\n",
      "[-2.380913734436035, -1.0917729139328003]\n",
      "[1.858137607574463, -1.9463536739349365]\n",
      "[-1.8791930675506592, -1.3192495107650757]\n",
      "[0.06243187189102173, -1.7737683057785034]\n",
      "[-0.4107266664505005, -1.7891182899475098]\n",
      "[-1.264482021331787, -1.5861095190048218]\n",
      "[2.4061269760131836, -2.5959346294403076]\n",
      "[1.392510175704956, -1.4278171062469482]\n",
      "[-0.838461697101593, -1.911865472793579]\n",
      "[-2.176170825958252, -1.2519440650939941]\n",
      "[1.507319688796997, -1.5942800045013428]\n",
      "[-1.577178955078125, -1.4976965188980103]\n",
      "[1.6466405391693115, -1.951781988143921]\n",
      "[2.2898952960968018, -2.2499125003814697]\n",
      "[0.6209273934364319, -2.0195813179016113]\n",
      "[0.16809356212615967, -1.698049545288086]\n",
      "[1.3752329349517822, -1.413008689880371]\n",
      "[1.4766948223114014, -1.499971628189087]\n",
      "[1.3089735507965088, -1.7263944149017334]\n",
      "[1.2912039756774902, -1.7051856517791748]\n",
      "[-1.609255075454712, -1.1933039426803589]\n",
      "[-0.5348552465438843, -1.9731919765472412]\n",
      "[-0.35653480887413025, -1.363747239112854]\n",
      "[-2.7775185108184814, -1.2663617134094238]\n",
      "[1.5621304512023926, -1.5731985569000244]\n",
      "[0.8608096241950989, -1.5001506805419922]\n",
      "[-0.49578648805618286, -1.5441185235977173]\n",
      "[1.4602549076080322, -1.485880970954895]\n",
      "[-0.5725507140159607, -1.442021369934082]\n",
      "[-0.7434981465339661, -1.5751692056655884]\n",
      "[1.3688867092132568, -1.7801910638809204]\n",
      "[1.7191905975341797, -1.7078149318695068]\n",
      "[1.4030749797821045, -1.436872124671936]\n",
      "[1.3209140300750732, -1.3664518594741821]\n",
      "[2.278932571411133, -2.401231527328491]\n",
      "[-3.1684837341308594, -1.483690857887268]\n",
      "[-1.198931097984314, -1.2115951776504517]\n",
      "[-1.016993761062622, -1.2907313108444214]\n",
      "[1.462871789932251, -1.4881240129470825]\n",
      "[-0.7696202993392944, -1.8619824647903442]\n",
      "[0.6682246327400208, -2.0137720108032227]\n",
      "[-1.4369709491729736, -1.536963701248169]\n",
      "[2.5534236431121826, -3.0084736347198486]\n",
      "[1.8203465938568115, -1.7945159673690796]\n",
      "[-1.554569959640503, -1.6238133907318115]\n",
      "[2.1039774417877197, -2.2448437213897705]\n",
      "[-1.4789834022521973, -1.412282109260559]\n",
      "[-0.4030274748802185, -1.477063536643982]\n",
      "[1.0042221546173096, -1.8445850610733032]\n",
      "[-2.519193172454834, -1.1753742694854736]\n",
      "[-1.5184471607208252, -1.199106216430664]\n",
      "[2.4549248218536377, -2.6604177951812744]\n",
      "[0.06819376349449158, -2.021122932434082]\n",
      "[1.054185390472412, -1.565096139907837]\n",
      "[-2.464195728302002, -1.3253533840179443]\n",
      "[-1.42978835105896, -1.4343791007995605]\n",
      "[0.02104055881500244, -1.9588322639465332]\n",
      "[0.5339248180389404, -1.7463412284851074]\n",
      "[1.3983650207519531, -1.4328352212905884]\n",
      "[1.3884620666503906, -1.4243474006652832]\n",
      "[-2.1232876777648926, -1.4334475994110107]\n",
      "[1.518721103668213, -1.5359925031661987]\n",
      "[-0.2614642083644867, -1.8265076875686646]\n",
      "[0.9405465722084045, -1.699586272239685]\n",
      "[-1.7300541400909424, -1.379608392715454]\n",
      "[-0.7559444904327393, -1.395486831665039]\n",
      "[1.834594964981079, -1.8067282438278198]\n",
      "[1.0369892120361328, -1.879818081855774]\n",
      "[-0.7886399626731873, -1.3949143886566162]\n",
      "[-1.5119357109069824, -1.3258525133132935]\n",
      "[0.6863078474998474, -1.9616360664367676]\n",
      "[1.3235211372375488, -1.3686864376068115]\n",
      "[-1.1803090572357178, -1.4646762609481812]\n",
      "[1.3137881755828857, -1.711406946182251]\n",
      "[1.455984354019165, -1.4822208881378174]\n",
      "[-2.086191415786743, -1.4312551021575928]\n",
      "[2.244964599609375, -2.158456325531006]\n",
      "[-2.7892520427703857, -1.2728842496871948]\n",
      "[1.1540532112121582, -1.9009205102920532]\n",
      "[-1.6079998016357422, -1.6124464273452759]\n",
      "[-1.34206223487854, -1.5117440223693848]\n",
      "[1.4412429332733154, -1.4695857763290405]\n",
      "[0.5971062183380127, -1.9735832214355469]\n",
      "[-1.8658583164215088, -0.7946219444274902]\n",
      "[-0.0018130838871002197, -0.9361382722854614]\n",
      "[-1.301608681678772, -0.8397473096847534]\n",
      "[-0.032222747802734375, -1.2419359683990479]\n",
      "[-0.259591668844223, -0.3936103284358978]\n",
      "[-2.2432384490966797, -0.9693669080734253]\n",
      "[-1.9419975280761719, -0.8019137382507324]\n",
      "[-0.1102154552936554, -1.6505203247070312]\n",
      "[-0.3670639097690582, -1.0265814065933228]\n",
      "[1.244384527206421, -1.955488681793213]\n",
      "[-1.8762848377227783, -0.7653853893280029]\n",
      "[-2.164306879043579, -0.9254906177520752]\n",
      "[-0.6312475204467773, -1.0890847444534302]\n",
      "[-0.1202973872423172, -1.0187705755233765]\n",
      "[-0.0979812741279602, -1.1833196878433228]\n",
      "[-1.523364782333374, -1.0513052940368652]\n",
      "[0.4718390703201294, -1.5075528621673584]\n",
      "[0.6867467761039734, -0.9847177267074585]\n",
      "[-2.2700917720794678, -0.9842941761016846]\n",
      "[-0.5690685510635376, -0.9332057237625122]\n",
      "[-1.5048258304595947, -0.942068338394165]\n",
      "[-0.6650196313858032, -0.9770983457565308]\n",
      "[0.08462285995483398, -1.3575702905654907]\n",
      "[-0.8236626386642456, -1.141929030418396]\n",
      "[0.290030300617218, -1.722226619720459]\n",
      "[0.29247820377349854, -0.7148044109344482]\n",
      "[0.3421170115470886, -1.6795951128005981]\n",
      "[-1.7051873207092285, -0.9081902503967285]\n",
      "[0.028329461812973022, -0.8867864608764648]\n",
      "[-1.109894871711731, -1.0886478424072266]\n",
      "[1.3179478645324707, -1.4048480987548828]\n",
      "[0.12708920240402222, -1.171233057975769]\n",
      "[0.9139894843101501, -1.5433050394058228]\n",
      "[-1.3041608333587646, -0.7068952322006226]\n",
      "[0.3539122939109802, -0.704700231552124]\n",
      "[0.049843281507492065, -0.5595632791519165]\n",
      "[-0.03362089395523071, -1.0131666660308838]\n",
      "[0.337006151676178, -1.0042047500610352]\n",
      "[-2.42368221282959, -1.069671869277954]\n",
      "[0.6087386608123779, -1.7238301038742065]\n",
      "[-1.85258150100708, -0.7522091865539551]\n",
      "[-2.0323307514190674, -0.8521280288696289]\n",
      "[0.31269168853759766, -0.8268551826477051]\n",
      "[-1.387132167816162, -0.49347594380378723]\n",
      "[-1.5084295272827148, -0.6981334686279297]\n",
      "[-0.8331230878829956, -0.5295511484146118]\n",
      "[-1.3776936531066895, -0.9641709327697754]\n",
      "[-1.9372262954711914, -0.7992614507675171]\n",
      "[0.09679055213928223, -1.0587079524993896]\n",
      "[1.1114931106567383, -1.1869568824768066]\n",
      "[1.3170018196105957, -1.47339928150177]\n",
      "[0.4981837272644043, -0.8347877264022827]\n",
      "[0.03315633535385132, -1.3773465156555176]\n",
      "[-1.198931097984314, -1.2115951776504517]\n",
      "[-2.4156434535980225, -1.0652031898498535]\n",
      "[-2.387852430343628, -1.0497548580169678]\n",
      "[1.4026434421539307, -1.4365020990371704]\n",
      "[0.29552096128463745, -1.5957378149032593]\n",
      "[0.765720784664154, -1.490918517112732]\n",
      "[-1.1801451444625854, -1.0550097227096558]\n",
      "[0.14957767724990845, -2.1583502292633057]\n",
      "[0.22209930419921875, -0.8724410533905029]\n",
      "[-0.793891429901123, -1.2781133651733398]\n",
      "[-0.10947448015213013, -1.2631183862686157]\n",
      "[-1.9571459293365479, -0.810334324836731]\n",
      "[-2.050891637802124, -0.862445592880249]\n",
      "[0.4061107039451599, -1.1550387144088745]\n",
      "[-2.0366389751434326, -0.854522705078125]\n",
      "[-2.337907314300537, -1.0219913721084595]\n",
      "[0.25377899408340454, -1.750657558441162]\n",
      "[0.545599639415741, -1.5825908184051514]\n",
      "[-1.3153061866760254, -0.5117992162704468]\n",
      "[-0.8275401592254639, -1.3147449493408203]\n",
      "[-1.83341383934021, -0.7908066511154175]\n",
      "[0.49413001537323, -1.5182225704193115]\n",
      "[-0.215312659740448, -0.9822196960449219]\n",
      "[1.2932586669921875, -1.4059244394302368]\n",
      "[0.17796629667282104, -0.8246411085128784]\n",
      "[-0.5103042125701904, -1.4114338159561157]\n",
      "[0.875143826007843, -1.23333740234375]\n",
      "[-0.530000627040863, -1.1762430667877197]\n",
      "[-0.0052762627601623535, -0.8999917507171631]\n",
      "[-1.595700979232788, -0.916350245475769]\n",
      "[-2.2017946243286133, -0.9463291168212891]\n",
      "[1.1995162963867188, -1.5324420928955078]\n",
      "[0.5262721180915833, -1.2268840074539185]\n",
      "[-2.0899107456207275, -0.8841354846954346]\n",
      "[-2.0217597484588623, -0.8462517261505127]\n",
      "[0.45898371934890747, -1.3512948751449585]\n",
      "[-0.5526492595672607, -0.26891180872917175]\n",
      "[-1.7505073547363281, -0.7659878730773926]\n",
      "[-0.6495635509490967, -0.8898531198501587]\n",
      "[0.13989567756652832, -0.7397830486297607]\n",
      "[-0.924881637096405, -1.2485214471817017]\n",
      "[1.0895154476165771, -1.5251283645629883]\n",
      "[-2.423750877380371, -1.0697098970413208]\n",
      "[-0.019684016704559326, -1.1881327629089355]\n",
      "[-0.34595680236816406, -1.417864203453064]\n",
      "[-0.953156590461731, -1.0790412425994873]\n",
      "[0.8561397194862366, -1.1538727283477783]\n",
      "[0.7968254685401917, -2.057797431945801]\n",
      "[-1.64060640335083, -0.8873974084854126]\n",
      "[-0.07939553260803223, -0.9305474758148193]\n",
      "[-1.229783535003662, -0.9014877080917358]\n",
      "[0.002052217721939087, -1.284483790397644]\n",
      "[-0.33931300044059753, -0.37182748317718506]\n",
      "[-2.3137624263763428, -1.0085697174072266]\n",
      "[-1.9100847244262695, -0.7841739654541016]\n",
      "[0.05867186188697815, -1.732892394065857]\n",
      "[-0.37480103969573975, -1.050929069519043]\n",
      "[1.298454999923706, -1.9424529075622559]\n",
      "[-1.8650221824645996, -0.759124755859375]\n",
      "[-2.207170009613037, -0.949317216873169]\n",
      "[-0.5702387094497681, -1.1462230682373047]\n",
      "[-0.4106226861476898, -1.031988501548767]\n",
      "[-0.035688579082489014, -1.2360138893127441]\n",
      "[-1.3034331798553467, -1.1429612636566162]\n",
      "[0.5541995167732239, -1.562771201133728]\n",
      "[0.3051455616950989, -0.9480479955673218]\n",
      "[-2.211366653442383, -0.9516500234603882]\n",
      "[-0.532370388507843, -0.9449256658554077]\n",
      "[-1.3100121021270752, -1.024398684501648]\n",
      "[-0.606799304485321, -1.0316736698150635]\n",
      "[0.17545199394226074, -1.4207462072372437]\n",
      "[-0.7236505746841431, -1.2131407260894775]\n",
      "[0.3472411632537842, -1.7273696660995483]\n",
      "[0.21938347816467285, -0.705866813659668]\n",
      "[0.4431040287017822, -1.7456434965133667]\n",
      "[-1.4804556369781494, -1.0010040998458862]\n",
      "[-0.27383309602737427, -0.8534497022628784]\n",
      "[-0.8885450959205627, -1.1798559427261353]\n",
      "[1.3755295276641846, -1.4310652017593384]\n",
      "[0.15693068504333496, -1.179459571838379]\n",
      "[0.9983920454978943, -1.5979108810424805]\n",
      "[-1.2716336250305176, -0.7324438095092773]\n",
      "[0.21385329961776733, -0.6740825176239014]\n",
      "[-0.024449646472930908, -0.5309121608734131]\n",
      "[-0.2226991057395935, -0.9888781309127808]\n",
      "[-0.012740105390548706, -0.9849944114685059]\n",
      "[-2.4402101039886475, -1.0788592100143433]\n",
      "[0.7087305188179016, -1.7870539426803589]\n",
      "[-1.939730167388916, -0.8006532192230225]\n",
      "[-1.9875926971435547, -0.8272590637207031]\n",
      "[0.33921051025390625, -0.842139482498169]\n",
      "[-1.4721646308898926, -0.5407437086105347]\n",
      "[-1.3478344678878784, -0.767109751701355]\n",
      "[-0.9650163054466248, -0.49906280636787415]\n",
      "[-1.499154806137085, -0.9450690746307373]\n",
      "[-1.8707680702209473, -0.7623187303543091]\n",
      "[-0.03827929496765137, -1.050033688545227]\n",
      "[1.0400407314300537, -1.1257150173187256]\n",
      "[1.3289713859558105, -1.4238159656524658]\n",
      "[0.41948401927948, -0.8396672010421753]\n",
      "[0.07334262132644653, -1.3908421993255615]\n",
      "[-1.0169938802719116, -1.2907315492630005]\n",
      "[-2.387852430343628, -1.0497548580169678]\n",
      "[-2.458406925201416, -1.0889744758605957]\n",
      "[1.3477632999420166, -1.3894644975662231]\n",
      "[0.3962048888206482, -1.660944938659668]\n",
      "[0.8458927273750305, -1.543247938156128]\n",
      "[-0.9587919116020203, -1.1462868452072144]\n",
      "[0.1987348198890686, -2.161911964416504]\n",
      "[0.3147382140159607, -0.9083330631256104]\n",
      "[-0.6916409730911255, -1.3504620790481567]\n",
      "[-0.055208802223205566, -1.2764756679534912]\n",
      "[-1.8255243301391602, -0.8241555690765381]\n",
      "[-2.0519087314605713, -0.8630110025405884]\n",
      "[0.03212669491767883, -1.153645634651184]\n",
      "[-1.8493473529815674, -0.9205803871154785]\n",
      "[-2.3368358612060547, -1.0213956832885742]\n",
      "[0.3007034659385681, -1.7507622241973877]\n",
      "[0.6347357034683228, -1.6400272846221924]\n",
      "[-1.445425033569336, -0.5258797407150269]\n",
      "[-0.608604371547699, -1.4057488441467285]\n",
      "[-1.6123919486999512, -0.8815590143203735]\n",
      "[0.5827556252479553, -1.5762107372283936]\n",
      "[-0.22757676243782043, -0.9921125173568726]\n",
      "[1.315035104751587, -1.3629266023635864]\n",
      "[0.02532818913459778, -0.7582322359085083]\n",
      "[-0.29131704568862915, -1.5019506216049194]\n",
      "[0.8859382271766663, -1.238020420074463]\n",
      "[-0.4495255649089813, -1.2383545637130737]\n",
      "[-0.08041983842849731, -0.8946031332015991]\n",
      "[-1.3715245723724365, -1.0089349746704102]\n",
      "[-2.178715944290161, -0.9335002899169922]\n",
      "[1.213371992111206, -1.512129545211792]\n",
      "[0.193448007106781, -1.2303060293197632]\n",
      "[-2.0505847930908203, -0.8622748851776123]\n",
      "[-1.9512202739715576, -0.8070404529571533]\n",
      "[0.49157774448394775, -1.391711711883545]\n",
      "[-0.5518209934234619, -0.25825929641723633]\n",
      "[-1.5359368324279785, -0.8542678356170654]\n",
      "[-0.8058174848556519, -0.8814932107925415]\n",
      "[-0.03379485011100769, -0.7196762561798096]\n",
      "[-0.7030231952667236, -1.3403115272521973]\n",
      "[1.1431918144226074, -1.535520315170288]\n",
      "[-2.4191935062408447, -1.0671766996383667]\n",
      "[-0.14001721143722534, -1.1983754634857178]\n",
      "[-0.24258196353912354, -1.4894269704818726]\n",
      "[-0.7480056881904602, -1.16449773311615]\n",
      "[0.887995183467865, -1.1772830486297607]\n",
      "[2.0694611072540283, -2.0080320835113525]\n",
      "[0.7099440693855286, -0.8427890539169312]\n",
      "[0.13588911294937134, -0.5104422569274902]\n",
      "[0.372408926486969, -0.5534873008728027]\n",
      "[7.25090503692627e-05, -0.41817331314086914]\n",
      "[0.3322984576225281, -1.1926668882369995]\n",
      "[1.598076343536377, -1.6040079593658447]\n",
      "[1.1298854351043701, -1.2027209997177124]\n",
      "[1.568004846572876, -1.5782337188720703]\n",
      "[0.030434459447860718, -0.4373038113117218]\n",
      "[-1.5165677070617676, -0.9550371170043945]\n",
      "[0.5795629620552063, -0.7310391664505005]\n",
      "[2.117109537124634, -2.058134078979492]\n",
      "[0.3496384024620056, -0.5393052101135254]\n",
      "[-0.7153076529502869, -0.3572278320789337]\n",
      "[-0.057623982429504395, -0.46638110280036926]\n",
      "[1.1278131008148193, -1.2009449005126953]\n",
      "[0.3827190399169922, -0.5926727056503296]\n",
      "[-0.3919420540332794, -0.3636842370033264]\n",
      "[1.0405659675598145, -1.1261652708053589]\n",
      "[-0.7615358233451843, -1.451780080795288]\n",
      "[0.9329066872596741, -1.0338904857635498]\n",
      "[-0.227609783411026, -0.443699449300766]\n",
      "[0.5162608623504639, -0.6767829656600952]\n",
      "[0.675403892993927, -0.8131844997406006]\n",
      "[-1.4763784408569336, -1.286940574645996]\n",
      "[-0.23238056898117065, -0.8325122594833374]\n",
      "[0.9949391484260559, -1.0870585441589355]\n",
      "[0.8386954665184021, -0.9531419277191162]\n",
      "[-0.42646902799606323, -0.3790673315525055]\n",
      "[0.7793360352516174, -0.9022648334503174]\n",
      "[-0.2547481060028076, -0.5013960599899292]\n",
      "[0.08621281385421753, -1.6426111459732056]\n",
      "[0.4695731997489929, -0.6367669105529785]\n",
      "[0.35360056161880493, -0.5373666286468506]\n",
      "[-0.20027583837509155, -0.7792659997940063]\n",
      "[0.22635698318481445, -1.0762935876846313]\n",
      "[-0.2257569283246994, -0.36899733543395996]\n",
      "[-0.2837139368057251, -0.36602410674095154]\n",
      "[1.5245089530944824, -1.540953278541565]\n",
      "[0.9935696721076965, -1.085884690284729]\n",
      "[1.698406457901001, -1.8404896259307861]\n",
      "[1.2478916645050049, -1.3038643598556519]\n",
      "[-0.29892414808273315, -0.9454725980758667]\n",
      "[0.9205765128135681, -1.238875150680542]\n",
      "[0.18167614936828613, -0.5260238647460938]\n",
      "[1.9054458141326904, -2.2093398571014404]\n",
      "[2.2104952335357666, -2.2880499362945557]\n",
      "[0.5605488419532776, -0.7147421836853027]\n",
      "[-0.48284628987312317, -0.32539209723472595]\n",
      "[-0.5553849339485168, -0.33079031109809875]\n",
      "[-1.3427269458770752, -0.3655158579349518]\n",
      "[-0.4591636061668396, -0.7302652597427368]\n",
      "[-0.453921377658844, -1.6032564640045166]\n",
      "[1.462871789932251, -1.4881240129470825]\n",
      "[1.4026434421539307, -1.43650221824646]\n",
      "[1.3477632999420166, -1.3894644975662231]\n",
      "[-1.3174231052398682, -0.2532486617565155]\n",
      "[0.926776111125946, -1.0286359786987305]\n",
      "[0.2742840647697449, -0.5430635213851929]\n",
      "[0.7681980729103088, -0.8927185535430908]\n",
      "[-1.5428998470306396, -1.798409342765808]\n",
      "[-1.0804016590118408, -0.6964181661605835]\n",
      "[0.856528103351593, -0.9684263467788696]\n",
      "[-1.060736894607544, -1.2716883420944214]\n",
      "[0.7756480574607849, -0.8991039991378784]\n",
      "[0.8455092310905457, -0.958981990814209]\n",
      "[-0.5227012634277344, -0.3296624720096588]\n",
      "[1.069427490234375, -1.1509026288986206]\n",
      "[1.5078771114349365, -1.5266979932785034]\n",
      "[-1.1751618385314941, -1.4749572277069092]\n",
      "[0.5873632431030273, -0.7377249002456665]\n",
      "[1.6300272941589355, -1.814582109451294]\n",
      "[1.2289090156555176, -1.2875943183898926]\n",
      "[0.7202524542808533, -0.8516242504119873]\n",
      "[0.5486598014831543, -0.7045520544052124]\n",
      "[0.24041181802749634, -0.4963352382183075]\n",
      "[-1.265295147895813, -0.3070189654827118]\n",
      "[-0.07583039999008179, -0.766796350479126]\n",
      "[1.2808079719543457, -1.332076907157898]\n",
      "[-1.0385135412216187, -0.6017429828643799]\n",
      "[0.33007359504699707, -0.5215469598770142]\n",
      "[0.11366504430770874, -0.5200052261352539]\n",
      "[0.8342763781547546, -0.9493542909622192]\n",
      "[1.1157066822052002, -1.1905685663223267]\n",
      "[-1.498187780380249, -0.45861396193504333]\n",
      "[-0.481702983379364, -0.33622312545776367]\n",
      "[0.8559866547584534, -0.9679621458053589]\n",
      "[0.6129971742630005, -0.7596957683563232]\n",
      "[-0.030485987663269043, -0.4460359513759613]\n",
      "[0.20931971073150635, -1.3091458082199097]\n",
      "[0.6145124435424805, -0.7609944343566895]\n",
      "[-0.6069661378860474, -0.3327580690383911]\n",
      "[-0.11308637261390686, -0.5996478796005249]\n",
      "[1.075441598892212, -1.1560572385787964]\n",
      "[-1.855452299118042, -0.47235962748527527]\n",
      "[1.6462981700897217, -1.645338773727417]\n",
      "[-0.787428081035614, -0.3242560923099518]\n",
      "[1.00105881690979, -1.0923035144805908]\n",
      "[0.7412802577018738, -0.8696472644805908]\n",
      "[-1.2368907928466797, -0.45606759190559387]\n",
      "[-0.9515938758850098, -2.0600109100341797]\n",
      "[-1.510026454925537, -1.4304996728897095]\n",
      "[0.32494646310806274, -1.3434348106384277]\n",
      "[-0.7665876150131226, -1.5141443014144897]\n",
      "[-1.0071611404418945, -1.3133410215377808]\n",
      "[1.1997358798980713, -1.262589931488037]\n",
      "[0.7068527340888977, -1.7550479173660278]\n",
      "[-0.31608232855796814, -1.491808533668518]\n",
      "[-1.5583863258361816, -1.7823402881622314]\n",
      "[-0.7692314982414246, -1.3633577823638916]\n",
      "[2.1421360969543457, -2.070322036743164]\n",
      "[0.026861071586608887, -1.6350361108779907]\n",
      "[0.7344387173652649, -1.7679823637008667]\n",
      "[-1.5045511722564697, -1.3189786672592163]\n",
      "[-0.4772775173187256, -1.2514541149139404]\n",
      "[-1.4461727142333984, -1.2065247297286987]\n",
      "[-1.5611152648925781, -1.5742560625076294]\n",
      "[-1.9270575046539307, -1.1987875699996948]\n",
      "[0.6066798567771912, -1.1044716835021973]\n",
      "[-0.5917418003082275, -1.630106806755066]\n",
      "[2.006418466567993, -2.108370780944824]\n",
      "[-1.27492356300354, -1.5388685464859009]\n",
      "[-1.0593760013580322, -1.3123425245285034]\n",
      "[-2.504502773284912, -1.1145981550216675]\n",
      "[-1.8574895858764648, -1.37019681930542]\n",
      "[2.0920817852020264, -2.280799150466919]\n",
      "[0.9114142060279846, -1.0154691934585571]\n",
      "[-2.911426067352295, -1.340798020362854]\n",
      "[-1.8634815216064453, -1.3908478021621704]\n",
      "[0.7485756278038025, -1.1215190887451172]\n",
      "[-1.855497121810913, -1.4092912673950195]\n",
      "[-0.5018298625946045, -1.1101486682891846]\n",
      "[2.45332932472229, -2.337045907974243]\n",
      "[-1.6689002513885498, -1.1632490158081055]\n",
      "[-0.11969897150993347, -1.5256829261779785]\n",
      "[0.9642812609672546, -1.0607815980911255]\n",
      "[1.059647798538208, -1.1425203084945679]\n",
      "[0.4031323790550232, -1.2087267637252808]\n",
      "[0.4051506519317627, -1.2071082592010498]\n",
      "[0.32510340213775635, -1.739811658859253]\n",
      "[-2.7662107944488525, -1.2600759267807007]\n",
      "[1.5670971870422363, -1.810081124305725]\n",
      "[-0.7928111553192139, -1.6106172800064087]\n",
      "[1.009753704071045, -1.0997560024261475]\n",
      "[1.30936598777771, -1.4525671005249023]\n",
      "[-1.1411455869674683, -1.2997947931289673]\n",
      "[2.1048128604888916, -2.038332223892212]\n",
      "[2.0422935485839844, -2.104309320449829]\n",
      "[-0.5718540549278259, -1.605115294456482]\n",
      "[0.18625110387802124, -1.1862471103668213]\n",
      "[0.5626801252365112, -1.0199531316757202]\n",
      "[0.9590290188789368, -1.0562798976898193]\n",
      "[0.8909077048301697, -0.9978930950164795]\n",
      "[2.305020570755005, -2.3618316650390625]\n",
      "[-0.7696201801300049, -1.8619825839996338]\n",
      "[0.2955213189125061, -1.5957379341125488]\n",
      "[0.3962053060531616, -1.660944938659668]\n",
      "[0.9267763495445251, -1.02863609790802]\n",
      "[-2.8657639026641846, -1.3154155015945435]\n",
      "[-1.4771678447723389, -1.1821409463882446]\n",
      "[-1.5206751823425293, -1.5007139444351196]\n",
      "[2.26888370513916, -2.747666120529175]\n",
      "[1.1659281253814697, -1.2336132526397705]\n",
      "[-1.929126262664795, -1.4808984994888306]\n",
      "[1.965423583984375, -2.0733392238616943]\n",
      "[-0.9892039895057678, -1.586680293083191]\n",
      "[0.24353748559951782, -1.6204525232315063]\n",
      "[-0.23707972466945648, -1.24861741065979]\n",
      "[-0.9868443608283997, -1.6876602172851562]\n",
      "[0.22915232181549072, -1.6846299171447754]\n",
      "[2.241476535797119, -2.4284164905548096]\n",
      "[-2.210871458053589, -1.1926441192626953]\n",
      "[1.8199443817138672, -1.7941712141036987]\n",
      "[-1.7273623943328857, -1.595000982284546]\n",
      "[-1.1440796852111816, -1.5330183506011963]\n",
      "[-2.2776341438293457, -1.1377325057983398]\n",
      "[-0.07061529159545898, -1.422133445739746]\n",
      "[0.8773061633110046, -0.9862351417541504]\n",
      "[1.0142874717712402, -1.1036419868469238]\n",
      "[-2.3101766109466553, -1.4019274711608887]\n",
      "[0.960765540599823, -1.0577682256698608]\n",
      "[-1.4047460556030273, -1.3732109069824219]\n",
      "[0.3778877258300781, -1.3136171102523804]\n",
      "[-1.4861698150634766, -1.4820915460586548]\n",
      "[0.14224636554718018, -1.6208425760269165]\n",
      "[1.111405849456787, -1.1868822574615479]\n",
      "[-0.38225987553596497, -1.2327288389205933]\n",
      "[-0.14325453341007233, -1.5500694513320923]\n",
      "[-1.1657949686050415, -1.4382566213607788]\n",
      "[-1.1640403270721436, -1.2102373838424683]\n",
      "[1.768352746963501, -1.749951958656311]\n",
      "[-1.1811020374298096, -1.460048794746399]\n",
      "[0.43821918964385986, -1.1874979734420776]\n",
      "[1.0792179107666016, -1.1592938899993896]\n",
      "[-1.7923309803009033, -1.544942855834961]\n",
      "[1.4501569271087646, -1.4772261381149292]\n",
      "[-0.01679888367652893, -1.708019495010376]\n",
      "[-0.3350513279438019, -1.213706135749817]\n",
      "[-2.221653699874878, -1.3953131437301636]\n",
      "[-1.7711193561553955, -1.3649253845214844]\n",
      "[0.8512400984764099, -0.9638938903808594]\n",
      "[0.711710512638092, -2.2006845474243164]\n",
      "[-0.5967813730239868, -1.4259889125823975]\n",
      "[0.041719019412994385, -0.9318307638168335]\n",
      "[-0.499168336391449, -1.218622088432312]\n",
      "[-1.5343213081359863, -0.8127363920211792]\n",
      "[1.1111445426940918, -1.1866581439971924]\n",
      "[1.2428851127624512, -1.6658997535705566]\n",
      "[0.17269349098205566, -1.361870288848877]\n",
      "[0.051456063985824585, -1.9334063529968262]\n",
      "[-0.9579060077667236, -0.9558095932006836]\n",
      "[1.3709666728973389, -1.4740558862686157]\n",
      "[0.2933908700942993, -1.3739442825317383]\n",
      "[1.8528542518615723, -1.8736211061477661]\n",
      "[-1.2751579284667969, -1.0308109521865845]\n",
      "[-1.3817765712738037, -0.7453778982162476]\n",
      "[-1.9029605388641357, -0.7802138328552246]\n",
      "[-0.22433066368103027, -1.7097324132919312]\n",
      "[-2.2894186973571777, -0.9950375556945801]\n",
      "[-0.10306823253631592, -0.6640849113464355]\n",
      "[0.3422396779060364, -1.6732131242752075]\n",
      "[1.6757125854492188, -1.7952014207839966]\n",
      "[-0.1539953649044037, -1.5889283418655396]\n",
      "[-1.3288466930389404, -0.8752084970474243]\n",
      "[-2.127192258834839, -0.9048594236373901]\n",
      "[-0.9193150997161865, -1.3026124238967896]\n",
      "[1.4847514629364014, -1.7773826122283936]\n",
      "[0.6947624087333679, -0.8297767639160156]\n",
      "[-1.6088817119598389, -1.2261580228805542]\n",
      "[-0.6958364248275757, -1.4719892740249634]\n",
      "[0.051389724016189575, -0.7182469367980957]\n",
      "[-0.8164003491401672, -1.412521481513977]\n",
      "[-1.708977460861206, -0.6723828315734863]\n",
      "[2.1404759883880615, -2.068899154663086]\n",
      "[-2.2667760848999023, -0.9824509620666504]\n",
      "[-0.09786254167556763, -1.1822304725646973]\n",
      "[0.6659156680107117, -0.8050522804260254]\n",
      "[0.9068108201026917, -1.0115236043930054]\n",
      "[-0.23859521746635437, -0.7275304794311523]\n",
      "[-0.23205767571926117, -0.7505283355712891]\n",
      "[0.9787984490394592, -1.7011451721191406]\n",
      "[-1.7212965488433838, -1.1353702545166016]\n",
      "[1.8469228744506836, -1.8172944784164429]\n",
      "[0.38878268003463745, -1.7308276891708374]\n",
      "[0.8117483258247375, -0.930045485496521]\n",
      "[1.2447936534881592, -1.3012090921401978]\n",
      "[-0.963309109210968, -1.0392601490020752]\n",
      "[2.22806715965271, -2.1439735889434814]\n",
      "[2.4132606983184814, -2.3027031421661377]\n",
      "[-0.08790463209152222, -1.4131877422332764]\n",
      "[-0.7126109600067139, -0.6612539291381836]\n",
      "[-0.42402634024620056, -0.5361371040344238]\n",
      "[0.5100438594818115, -0.6714543104171753]\n",
      "[0.6676378846168518, -0.8065283298492432]\n",
      "[1.9369797706604004, -2.0241198539733887]\n",
      "[0.6682245135307312, -2.0137717723846436]\n",
      "[0.765720784664154, -1.490918517112732]\n",
      "[0.8458927273750305, -1.543247938156128]\n",
      "[0.2742839455604553, -0.5430635213851929]\n",
      "[-1.4771678447723389, -1.1821409463882446]\n",
      "[-2.307696580886841, -1.0051977634429932]\n",
      "[-0.5813085436820984, -1.4552561044692993]\n",
      "[1.5004804134368896, -2.2828619480133057]\n",
      "[0.7704264521598816, -0.8946284055709839]\n",
      "[-0.7906205654144287, -1.4633899927139282]\n",
      "[1.5336389541625977, -1.686113715171814]\n",
      "[-0.14590784907341003, -1.547967791557312]\n",
      "[0.502640962600708, -1.4108095169067383]\n",
      "[-1.0294873714447021, -0.7200430631637573]\n",
      "[0.14151865243911743, -1.764868974685669]\n",
      "[0.8672448992729187, -1.6358197927474976]\n",
      "[1.6991536617279053, -1.961921215057373]\n",
      "[-2.2085816860198975, -0.9501020908355713]\n",
      "[1.8058857917785645, -1.7821214199066162]\n",
      "[-0.25811389088630676, -1.7359004020690918]\n",
      "[-0.30609068274497986, -1.4823890924453735]\n",
      "[-2.2188539505004883, -0.9558120965957642]\n",
      "[-0.2718404531478882, -1.020047664642334]\n",
      "[0.2599524259567261, -0.5252348184585571]\n",
      "[0.7073130011558533, -0.8405338525772095]\n",
      "[-0.6965540647506714, -1.5951565504074097]\n",
      "[0.6481221318244934, -0.7898013591766357]\n",
      "[-1.27449631690979, -1.0309734344482422]\n",
      "[0.08795467019081116, -0.905367374420166]\n",
      "[-0.461198091506958, -1.5040549039840698]\n",
      "[0.5302939414978027, -1.464536190032959]\n",
      "[0.6823859810829163, -0.8191689252853394]\n",
      "[-1.2872636318206787, -0.6613138914108276]\n",
      "[0.22407811880111694, -1.3841887712478638]\n",
      "[-0.4566175639629364, -1.3805800676345825]\n",
      "[-2.001854419708252, -0.8351867198944092]\n",
      "[1.5940954685211182, -1.6005959510803223]\n",
      "[-0.46840107440948486, -1.369484543800354]\n",
      "[-0.39992010593414307, -0.7290511131286621]\n",
      "[0.6755263209342957, -0.8132894039154053]\n",
      "[-0.4687076807022095, -1.6363526582717896]\n",
      "[0.8541303277015686, -0.9663711786270142]\n",
      "[0.9835018515586853, -1.8112261295318604]\n",
      "[-1.482611894607544, -0.6156210899353027]\n",
      "[-0.7751452326774597, -1.4681541919708252]\n",
      "[-0.7842186093330383, -1.3434785604476929]\n",
      "[0.5261551141738892, -0.6852632761001587]\n",
      "[-0.7224013805389404, -2.042848825454712]\n",
      "[-2.6007463932037354, -1.168097972869873]\n",
      "[-0.4508298635482788, -1.184369683265686]\n",
      "[-1.7863600254058838, -1.1731163263320923]\n",
      "[-1.05841064453125, -1.3418914079666138]\n",
      "[0.9467901587486267, -1.0696213245391846]\n",
      "[-0.8167388439178467, -1.1988333463668823]\n",
      "[-1.641235113143921, -0.9989570379257202]\n",
      "[-1.5702812671661377, -1.6873743534088135]\n",
      "[-1.3025699853897095, -1.188855528831482]\n",
      "[1.9285144805908203, -1.9817354679107666]\n",
      "[-1.195633888244629, -1.2168675661087036]\n",
      "[-1.4505751132965088, -1.0423189401626587]\n",
      "[-1.9129414558410645, -1.1588504314422607]\n",
      "[-0.8177491426467896, -1.2042664289474487]\n",
      "[-1.264014482498169, -1.2554435729980469]\n",
      "[-2.3180999755859375, -1.2186641693115234]\n",
      "[-1.0002014636993408, -1.4745709896087646]\n",
      "[0.15951591730117798, -1.0259654521942139]\n",
      "[-1.9467649459838867, -1.076019287109375]\n",
      "[1.1742174625396729, -1.5804204940795898]\n",
      "[-2.444977283477783, -1.0815093517303467]\n",
      "[-1.4526703357696533, -1.1837654113769531]\n",
      "[-1.5743284225463867, -1.2967220544815063]\n",
      "[-2.555001735687256, -1.142669439315796]\n",
      "[1.5677225589752197, -1.983189582824707]\n",
      "[0.5266558527946472, -0.6856924295425415]\n",
      "[-1.4942615032196045, -1.576960563659668]\n",
      "[-2.6095664501190186, -1.1730008125305176]\n",
      "[0.14744722843170166, -1.010606288909912]\n",
      "[-2.638622522354126, -1.1891523599624634]\n",
      "[0.27043938636779785, -1.3274301290512085]\n",
      "[1.6995673179626465, -1.8675248622894287]\n",
      "[-0.55106121301651, -1.4919984340667725]\n",
      "[-1.1208643913269043, -1.2026695013046265]\n",
      "[0.5856412053108215, -0.7373340129852295]\n",
      "[0.7043824791908264, -0.8380221128463745]\n",
      "[-0.06046724319458008, -1.1380200386047363]\n",
      "[-0.10907849669456482, -1.1218467950820923]\n",
      "[-1.1858948469161987, -1.1670681238174438]\n",
      "[-1.2017402648925781, -1.62226140499115]\n",
      "[-0.16729119420051575, -1.2647885084152222]\n",
      "[-1.9091966152191162, -1.1266359090805054]\n",
      "[0.7824689745903015, -0.9049501419067383]\n",
      "[0.11306756734848022, -1.1354563236236572]\n",
      "[-1.985560655593872, -0.9859058856964111]\n",
      "[1.355696678161621, -1.3962640762329102]\n",
      "[0.2274312973022461, -1.5353305339813232]\n",
      "[-1.8705947399139404, -1.1437474489212036]\n",
      "[-0.05048683285713196, -1.16669762134552]\n",
      "[0.5132224559783936, -0.9870702028274536]\n",
      "[0.7768240571022034, -0.9001117944717407]\n",
      "[0.50421142578125, -0.6664553880691528]\n",
      "[1.5926687717437744, -1.9230809211730957]\n",
      "[-1.4369711875915527, -1.536963701248169]\n",
      "[-1.180145263671875, -1.0550096035003662]\n",
      "[-0.9587919116020203, -1.1462868452072144]\n",
      "[0.7681980729103088, -0.8927185535430908]\n",
      "[-1.5206754207611084, -1.50071382522583]\n",
      "[-0.5813085436820984, -1.4552561044692993]\n",
      "[-2.667663335800171, -1.2052955627441406]\n",
      "[1.5479040145874023, -2.501573324203491]\n",
      "[0.972669780254364, -1.0924243927001953]\n",
      "[-2.659623622894287, -1.2008265256881714]\n",
      "[1.3253402709960938, -1.68173348903656]\n",
      "[-2.401416778564453, -1.0572948455810547]\n",
      "[-1.019605040550232, -1.1632766723632812]\n",
      "[-0.4627421498298645, -1.236750841140747]\n",
      "[-2.1771063804626465, -1.1945456266403198]\n",
      "[-1.2187645435333252, -1.1319520473480225]\n",
      "[1.6545162200927734, -2.0755677223205566]\n",
      "[-1.0479793548583984, -1.5236079692840576]\n",
      "[0.8058952689170837, -1.3182040452957153]\n",
      "[-2.108769655227661, -1.3756728172302246]\n",
      "[-2.4635820388793945, -1.0918512344360352]\n",
      "[-1.1057021617889404, -1.459611415863037]\n",
      "[-0.8351517915725708, -1.2156736850738525]\n",
      "[0.7083759903907776, -0.8414449691772461]\n",
      "[0.614974856376648, -0.7847323417663574]\n",
      "[-1.9556710720062256, -1.4102526903152466]\n",
      "[0.7444718480110168, -0.8723827600479126]\n",
      "[-1.7351915836334229, -1.257662057876587]\n",
      "[-0.403747022151947, -1.1599838733673096]\n",
      "[-2.5772275924682617, -1.1550244092941284]\n",
      "[-1.174471139907837, -1.1330190896987915]\n",
      "[1.0532193183898926, -1.1370104551315308]\n",
      "[-0.4194124937057495, -1.2678521871566772]\n",
      "[-1.4372122287750244, -1.0693204402923584]\n",
      "[-2.3903069496154785, -1.0511192083358765]\n",
      "[-0.7091209888458252, -1.3729546070098877]\n",
      "[0.995952308177948, -1.0879268646240234]\n",
      "[-2.4130337238311768, -1.06375253200531]\n",
      "[-0.0806901752948761, -1.101210355758667]\n",
      "[0.04704397916793823, -0.9421368837356567]\n",
      "[-2.431874990463257, -1.2531423568725586]\n",
      "[1.448518991470337, -1.4758222103118896]\n",
      "[-1.8904292583465576, -1.0436993837356567]\n",
      "[-0.2989055812358856, -1.265694260597229]\n",
      "[-2.196786403656006, -1.3314818143844604]\n",
      "[-2.5319385528564453, -1.1298491954803467]\n",
      "[0.6287198662757874, -0.7731716632843018]\n",
      "[3.4765520095825195, -3.694445848464966]\n",
      "[1.5258276462554932, -2.3797104358673096]\n",
      "[-0.37710827589035034, -2.0022151470184326]\n",
      "[0.6596422791481018, -2.1837472915649414]\n",
      "[0.7125470042228699, -2.1805381774902344]\n",
      "[-1.6336333751678467, -1.8110960721969604]\n",
      "[0.2934454083442688, -2.235166549682617]\n",
      "[0.3717910647392273, -2.220691204071045]\n",
      "[2.8585212230682373, -3.136483669281006]\n",
      "[0.3446860909461975, -2.138720750808716]\n",
      "[-3.553264856338501, -1.615876317024231]\n",
      "[0.20868533849716187, -2.1262094974517822]\n",
      "[0.7114962935447693, -2.4569904804229736]\n",
      "[1.012573003768921, -2.2875254154205322]\n",
      "[0.014826476573944092, -1.8765288591384888]\n",
      "[0.9116567969322205, -2.126437187194824]\n",
      "[2.2608582973480225, -2.6543960571289062]\n",
      "[1.7195978164672852, -2.3034491539001465]\n",
      "[-0.6891812682151794, -1.8726847171783447]\n",
      "[1.2753865718841553, -2.4865269660949707]\n",
      "[-3.790316581726074, -1.4588416814804077]\n",
      "[1.619112491607666, -2.614534854888916]\n",
      "[0.56207275390625, -1.9943370819091797]\n",
      "[1.8121204376220703, -2.3861634731292725]\n",
      "[1.5780043601989746, -2.4470736980438232]\n",
      "[-4.326892852783203, -1.4758652448654175]\n",
      "[-1.7399184703826904, -1.730334997177124]\n",
      "[2.5000267028808594, -2.7488467693328857]\n",
      "[1.9830894470214844, -2.4174623489379883]\n",
      "[-0.8314576745033264, -1.879352331161499]\n",
      "[1.753434658050537, -2.4843690395355225]\n",
      "[0.9551817774772644, -2.007563352584839]\n",
      "[-2.9605700969696045, -1.731484293937683]\n",
      "[1.8077058792114258, -2.358243227005005]\n",
      "[0.03411990404129028, -2.0536322593688965]\n",
      "[-1.5769000053405762, -1.7542340755462646]\n",
      "[-1.6707336902618408, -1.7886708974838257]\n",
      "[-0.3597075343132019, -2.0024569034576416]\n",
      "[-0.5068216919898987, -1.9332025051116943]\n",
      "[0.5919162631034851, -2.3404998779296875]\n",
      "[2.5412120819091797, -2.7837002277374268]\n",
      "[-0.05467468500137329, -2.257568359375]\n",
      "[1.6639304161071777, -2.6676387786865234]\n",
      "[-2.0613250732421875, -1.7133716344833374]\n",
      "[-0.7013296484947205, -1.9914915561676025]\n",
      "[0.6469835638999939, -2.066171169281006]\n",
      "[-0.4739028811454773, -2.4289307594299316]\n",
      "[0.5255486369132996, -2.737637519836426]\n",
      "[0.7640005946159363, -2.253690242767334]\n",
      "[-0.2217053771018982, -1.9301097393035889]\n",
      "[-0.6483020186424255, -1.8877825736999512]\n",
      "[-1.875349760055542, -1.7848304510116577]\n",
      "[-1.886847734451294, -1.6720161437988281]\n",
      "[-3.5435609817504883, -1.6665000915527344]\n",
      "[2.5534234046936035, -3.0084733963012695]\n",
      "[0.14957767724990845, -2.1583502292633057]\n",
      "[0.1987350583076477, -2.161912202835083]\n",
      "[-1.5428991317749023, -1.7984098196029663]\n",
      "[2.26888370513916, -2.747666120529175]\n",
      "[1.5004801750183105, -2.2828621864318848]\n",
      "[1.5479040145874023, -2.501573324203491]\n",
      "[-5.898819446563721, -1.291127324104309]\n",
      "[-2.147545337677002, -1.7239841222763062]\n",
      "[1.936694860458374, -2.5238161087036133]\n",
      "[-3.5651450157165527, -1.5952479839324951]\n",
      "[1.2999744415283203, -2.446667432785034]\n",
      "[0.05499619245529175, -2.0672271251678467]\n",
      "[-0.0203741192817688, -1.9781622886657715]\n",
      "[1.834822654724121, -2.614030361175537]\n",
      "[0.4945588707923889, -2.3311221599578857]\n",
      "[-4.588194370269775, -1.401318073272705]\n",
      "[1.9722270965576172, -2.445608377456665]\n",
      "[-0.3764339089393616, -2.3295083045959473]\n",
      "[2.438343048095703, -2.811565637588501]\n",
      "[1.3056986331939697, -2.4183030128479004]\n",
      "[1.9342246055603027, -2.4187190532684326]\n",
      "[-0.13273149728775024, -2.084878921508789]\n",
      "[-1.544792890548706, -1.8015193939208984]\n",
      "[-1.5273218154907227, -1.8364664316177368]\n",
      "[2.650580644607544, -2.9013359546661377]\n",
      "[-2.250091314315796, -1.6532659530639648]\n",
      "[1.0192570686340332, -2.297595977783203]\n",
      "[-0.41195017099380493, -1.976464033126831]\n",
      "[1.613659381866455, -2.5155460834503174]\n",
      "[0.21612399816513062, -2.174839973449707]\n",
      "[-2.41762113571167, -1.6623256206512451]\n",
      "[0.17108768224716187, -2.0238747596740723]\n",
      "[0.29582077264785767, -2.155056953430176]\n",
      "[1.1626334190368652, -2.2791197299957275]\n",
      "[0.949330747127533, -2.159061908721924]\n",
      "[-2.0597550868988037, -1.692541480064392]\n",
      "[1.1208148002624512, -2.356403350830078]\n",
      "[-0.481151282787323, -1.8744230270385742]\n",
      "[-1.0576001405715942, -1.85191011428833]\n",
      "[2.1945409774780273, -2.66389536857605]\n",
      "[-2.6187705993652344, -1.6914724111557007]\n",
      "[1.2795555591583252, -2.5094897747039795]\n",
      "[0.21016496419906616, -1.952789306640625]\n",
      "[2.1734585762023926, -2.7127766609191895]\n",
      "[1.5085651874542236, -2.5374791622161865]\n",
      "[-2.0001137256622314, -1.6870923042297363]\n",
      "[2.0228044986724854, -1.9680428504943848]\n",
      "[0.8120619654655457, -1.011052131652832]\n",
      "[-0.7836313247680664, -0.5337673425674438]\n",
      "[0.1940355896949768, -0.6922348737716675]\n",
      "[0.3297317624092102, -0.6767243146896362]\n",
      "[-1.1928808689117432, -0.7327470779418945]\n",
      "[0.5078339576721191, -1.0297229290008545]\n",
      "[0.2794206738471985, -0.8638448715209961]\n",
      "[1.4769985675811768, -1.5002319812774658]\n",
      "[-0.014994919300079346, -0.683037519454956]\n",
      "[-1.5044581890106201, -1.2625535726547241]\n",
      "[0.04250037670135498, -0.7757225036621094]\n",
      "[0.8900807499885559, -1.1740416288375854]\n",
      "[0.546798050403595, -0.8245060443878174]\n",
      "[-0.535926342010498, -0.3702102601528168]\n",
      "[0.3222206234931946, -0.648554801940918]\n",
      "[1.2101671695709229, -1.2715305089950562]\n",
      "[0.7133710980415344, -0.8457262516021729]\n",
      "[-1.105987787246704, -0.38337695598602295]\n",
      "[1.0668706893920898, -1.2127485275268555]\n",
      "[-2.11000657081604, -0.9704641103744507]\n",
      "[1.2385928630828857, -1.3207652568817139]\n",
      "[-0.197233647108078, -0.45396479964256287]\n",
      "[0.778136670589447, -0.901236891746521]\n",
      "[0.9037116169929504, -1.0432101488113403]\n",
      "[-2.133143186569214, -1.2209346294403076]\n",
      "[-1.6642065048217773, -0.4960027039051056]\n",
      "[1.0101299285888672, -1.1000784635543823]\n",
      "[0.8928428292274475, -0.9995516538619995]\n",
      "[-1.401472806930542, -0.3052959442138672]\n",
      "[0.9560779929161072, -1.0547771453857422]\n",
      "[0.5630505084991455, -0.7168864011764526]\n",
      "[-1.2496051788330078, -1.261763095855713]\n",
      "[0.8542618155479431, -0.9664838314056396]\n",
      "[-0.3411204516887665, -0.6246160268783569]\n",
      "[-1.6306476593017578, -0.4610423147678375]\n",
      "[-1.2342681884765625, -0.7149648666381836]\n",
      "[-0.7802401781082153, -0.464362233877182]\n",
      "[-0.9715979099273682, -0.41223815083503723]\n",
      "[0.7045709490776062, -1.0901764631271362]\n",
      "[1.0671968460083008, -1.1489907503128052]\n",
      "[0.3262506127357483, -1.11819326877594]\n",
      "[1.342912197113037, -1.3982244729995728]\n",
      "[-1.6836626529693604, -0.6181749105453491]\n",
      "[-0.49681198596954346, -0.7828161716461182]\n",
      "[0.11485555768013, -0.5928674936294556]\n",
      "[0.4196310043334961, -1.5415512323379517]\n",
      "[0.9526844620704651, -1.6003514528274536]\n",
      "[0.49023449420928955, -0.849052906036377]\n",
      "[-0.6039683222770691, -0.4240417182445526]\n",
      "[-0.7405292987823486, -0.5293867588043213]\n",
      "[-1.362807273864746, -0.7033569812774658]\n",
      "[-1.819718837738037, -0.4375990331172943]\n",
      "[-1.6803545951843262, -1.2667887210845947]\n",
      "[1.8203465938568115, -1.7945159673690796]\n",
      "[0.22209960222244263, -0.8724406957626343]\n",
      "[0.3147382140159607, -0.9083330631256104]\n",
      "[-1.0804016590118408, -0.6964181661605835]\n",
      "[1.1659281253814697, -1.2336132526397705]\n",
      "[0.7704265713691711, -0.894628643989563]\n",
      "[0.9726698994636536, -1.0924242734909058]\n",
      "[-2.147545099258423, -1.7239845991134644]\n",
      "[-2.3263306617736816, -0.4351053535938263]\n",
      "[0.936755359172821, -1.0371891260147095]\n",
      "[-2.2153830528259277, -0.9532759189605713]\n",
      "[0.9826052784919739, -1.122276782989502]\n",
      "[0.014576226472854614, -0.7638107538223267]\n",
      "[-0.4039298892021179, -0.4879296123981476]\n",
      "[1.29457426071167, -1.3438758850097656]\n",
      "[0.5640367865562439, -1.0541372299194336]\n",
      "[-1.9835309982299805, -1.3369674682617188]\n",
      "[0.8720033764839172, -0.9816901683807373]\n",
      "[0.3011782169342041, -1.3475933074951172]\n",
      "[1.2884750366210938, -1.3386484384536743]\n",
      "[0.9378873705863953, -1.0836257934570312]\n",
      "[0.8373865485191345, -0.9520200490951538]\n",
      "[-0.48678508400917053, -0.6456421613693237]\n",
      "[-1.2837023735046387, -0.620956301689148]\n",
      "[-1.3882973194122314, -0.5894252061843872]\n",
      "[1.1964893341064453, -1.2598073482513428]\n",
      "[-1.9350590705871582, -0.5245009660720825]\n",
      "[0.5220522284507751, -0.7978131771087646]\n",
      "[-0.8213678002357483, -0.5093752145767212]\n",
      "[1.0821912288665771, -1.1735007762908936]\n",
      "[0.23831015825271606, -0.8820569515228271]\n",
      "[-1.6265733242034912, -0.7351117134094238]\n",
      "[-0.11292704939842224, -0.5492514371871948]\n",
      "[0.22639888525009155, -0.8200438022613525]\n",
      "[0.6962869763374329, -0.9292792081832886]\n",
      "[0.584221601486206, -0.7350322008132935]\n",
      "[-1.368943214416504, -0.7250756025314331]\n",
      "[0.8319583535194397, -1.014023780822754]\n",
      "[-0.9970869421958923, -0.307643324136734]\n",
      "[-1.4430980682373047, -0.39673909544944763]\n",
      "[1.1526436805725098, -1.2222270965576172]\n",
      "[-1.7441487312316895, -0.8110471963882446]\n",
      "[1.1323745250701904, -1.2048543691635132]\n",
      "[-0.13932493329048157, -0.44680896401405334]\n",
      "[1.217378854751587, -1.2777116298675537]\n",
      "[1.0817503929138184, -1.1785492897033691]\n",
      "[-2.034418821334839, -0.40645724534988403]\n",
      "[-1.0267486572265625, -2.065760374069214]\n",
      "[-2.5967235565185547, -1.16586172580719]\n",
      "[-0.2963271737098694, -1.2992218732833862]\n",
      "[-1.600193738937378, -1.3331457376480103]\n",
      "[-1.094578742980957, -1.4011973142623901]\n",
      "[0.859962522983551, -0.9713699817657471]\n",
      "[-0.41167622804641724, -1.426255464553833]\n",
      "[-1.3801839351654053, -1.197258710861206]\n",
      "[-1.879408836364746, -1.708938717842102]\n",
      "[-1.2341606616973877, -1.3054872751235962]\n",
      "[1.9915142059326172, -1.9412238597869873]\n",
      "[-0.9467295408248901, -1.402461290359497]\n",
      "[-0.9769585728645325, -1.2696400880813599]\n",
      "[-1.9391660690307617, -1.2529205083847046]\n",
      "[-0.760631799697876, -1.2885085344314575]\n",
      "[-1.3656973838806152, -1.3165466785430908]\n",
      "[-2.479994535446167, -1.2906038761138916]\n",
      "[-1.237902045249939, -1.4821584224700928]\n",
      "[0.268882691860199, -1.114102840423584]\n",
      "[-1.7606213092803955, -1.262571096420288]\n",
      "[1.5344383716583252, -1.7155686616897583]\n",
      "[-2.421189069747925, -1.2061959505081177]\n",
      "[-1.4181733131408691, -1.289344310760498]\n",
      "[-1.8778042793273926, -1.3046562671661377]\n",
      "[-2.696892261505127, -1.221543312072754]\n",
      "[1.8057372570037842, -2.0421016216278076]\n",
      "[0.689060628414154, -0.8248897790908813]\n",
      "[-1.9138705730438232, -1.5536978244781494]\n",
      "[-2.7508111000061035, -1.2515156269073486]\n",
      "[0.29348278045654297, -1.108332633972168]\n",
      "[-2.8023149967193604, -1.280145525932312]\n",
      "[0.13432884216308594, -1.3474963903427124]\n",
      "[1.9790260791778564, -1.951080560684204]\n",
      "[-0.8056596517562866, -1.4899256229400635]\n",
      "[-0.9200615286827087, -1.3650866746902466]\n",
      "[0.7698845267295837, -0.894163966178894]\n",
      "[0.7651744484901428, -0.8901270627975464]\n",
      "[0.040689051151275635, -1.224108099937439]\n",
      "[0.006928801536560059, -1.2120678424835205]\n",
      "[-0.8135740756988525, -1.390373706817627]\n",
      "[-1.610081434249878, -1.599802017211914]\n",
      "[0.23206061124801636, -1.4578220844268799]\n",
      "[-1.8695156574249268, -1.2670599222183228]\n",
      "[0.6497957110404968, -0.7912356853485107]\n",
      "[0.2685115933418274, -1.2724223136901855]\n",
      "[-1.85831618309021, -1.133016586303711]\n",
      "[1.521787166595459, -1.5386203527450562]\n",
      "[0.4370792508125305, -1.671371340751648]\n",
      "[-1.6134867668151855, -1.336505651473999]\n",
      "[0.004988133907318115, -1.2412667274475098]\n",
      "[0.5559280514717102, -1.0561200380325317]\n",
      "[0.8143433928489685, -0.9322696924209595]\n",
      "[0.652923047542572, -0.7939162254333496]\n",
      "[1.8660471439361572, -2.0092556476593018]\n",
      "[-1.554569959640503, -1.6238133907318115]\n",
      "[-0.793891429901123, -1.2781133651733398]\n",
      "[-0.6916408538818359, -1.3504620790481567]\n",
      "[0.856528103351593, -0.9684263467788696]\n",
      "[-1.929126501083374, -1.480898380279541]\n",
      "[-0.7906204462051392, -1.4633901119232178]\n",
      "[-2.659623861312866, -1.2008265256881714]\n",
      "[1.9366950988769531, -2.523815870285034]\n",
      "[0.9367554783821106, -1.037189245223999]\n",
      "[-2.9009227752685547, -1.3349595069885254]\n",
      "[1.586618423461914, -1.7639665603637695]\n",
      "[-2.1607232093811035, -1.2513686418533325]\n",
      "[-0.7625669240951538, -1.3580831289291382]\n",
      "[-0.4201778173446655, -1.3082425594329834]\n",
      "[-2.109569787979126, -1.341070532798767]\n",
      "[-0.9035304188728333, -1.3449273109436035]\n",
      "[1.9079201221466064, -2.151365041732788]\n",
      "[-1.3409217596054077, -1.5214924812316895]\n",
      "[0.8813591599464417, -1.4131488800048828]\n",
      "[-2.3859550952911377, -1.4080959558486938]\n",
      "[-2.3064918518066406, -1.2048163414001465]\n",
      "[-1.404083013534546, -1.462033748626709]\n",
      "[-0.692548394203186, -1.3482836484909058]\n",
      "[0.804993212223053, -0.9242556095123291]\n",
      "[0.8236574530601501, -0.9402527809143066]\n",
      "[-2.358293056488037, -1.4020209312438965]\n",
      "[0.7485384345054626, -0.8758683204650879]\n",
      "[-1.7675037384033203, -1.3448694944381714]\n",
      "[-0.24287930130958557, -1.2703661918640137]\n",
      "[-2.623481273651123, -1.18073570728302]\n",
      "[-0.9125128388404846, -1.332764744758606]\n",
      "[0.9903796315193176, -1.0831505060195923]\n",
      "[-0.4191650450229645, -1.3286714553833008]\n",
      "[-1.1775636672973633, -1.2675716876983643]\n",
      "[-2.2980058193206787, -1.1058335304260254]\n",
      "[-0.8285012245178223, -1.4037103652954102]\n",
      "[1.2148313522338867, -1.2755281925201416]\n",
      "[-2.271677255630493, -1.1606518030166626]\n",
      "[0.041242748498916626, -1.19266676902771]\n",
      "[0.4813470244407654, -1.0530915260314941]\n",
      "[-2.7038190364837646, -1.2863816022872925]\n",
      "[1.3679347038269043, -1.4067533016204834]\n",
      "[-1.380756139755249, -1.2827651500701904]\n",
      "[-0.30486589670181274, -1.3248881101608276]\n",
      "[-2.6348648071289062, -1.3091464042663574]\n",
      "[-2.68617582321167, -1.2155863046646118]\n",
      "[0.6769580245018005, -0.8145166635513306]\n",
      "[3.0050222873687744, -2.8233346939086914]\n",
      "[1.282607078552246, -1.57015061378479]\n",
      "[-0.49889636039733887, -1.205080270767212]\n",
      "[0.4926493763923645, -1.3598634004592896]\n",
      "[0.7525438666343689, -1.43896484375]\n",
      "[-1.888141393661499, -0.9413031339645386]\n",
      "[0.051666438579559326, -1.3548305034637451]\n",
      "[0.13149702548980713, -1.3437153100967407]\n",
      "[2.423760175704956, -2.3460705280303955]\n",
      "[0.27994388341903687, -1.3651264905929565]\n",
      "[-2.3512625694274902, -1.4303621053695679]\n",
      "[-0.023838400840759277, -1.2688477039337158]\n",
      "[0.4512721300125122, -1.5158700942993164]\n",
      "[0.9228240847587585, -1.5019303560256958]\n",
      "[0.01088261604309082, -1.1416985988616943]\n",
      "[0.9713123440742493, -1.4039592742919922]\n",
      "[1.8315916061401367, -1.904054045677185]\n",
      "[1.6102955341339111, -1.6869913339614868]\n",
      "[-0.6737951040267944, -1.1336336135864258]\n",
      "[1.0309572219848633, -1.6156247854232788]\n",
      "[-3.621932029724121, -0.792218804359436]\n",
      "[1.4017009735107422, -1.7805501222610474]\n",
      "[0.4964030385017395, -1.224740982055664]\n",
      "[1.6272056102752686, -1.7420827150344849]\n",
      "[1.3611221313476562, -1.6629031896591187]\n",
      "[-3.438100814819336, -1.1561930179595947]\n",
      "[-2.0285768508911133, -0.8640316724777222]\n",
      "[2.1072113513946533, -2.0403881072998047]\n",
      "[1.5967330932617188, -1.6863411664962769]\n",
      "[-0.9019789695739746, -1.0937482118606567]\n",
      "[1.4677209854125977, -1.711005449295044]\n",
      "[1.2866177558898926, -1.5203595161437988]\n",
      "[-2.7268199920654297, -1.106197714805603]\n",
      "[1.7638425827026367, -1.8091098070144653]\n",
      "[-0.1650800108909607, -1.2148703336715698]\n",
      "[-1.8475077152252197, -0.8953795433044434]\n",
      "[-1.9540221691131592, -0.9187694787979126]\n",
      "[-0.35450470447540283, -1.2426562309265137]\n",
      "[-0.5255831480026245, -1.1766459941864014]\n",
      "[0.34220170974731445, -1.4618650674819946]\n",
      "[2.179161548614502, -2.1020565032958984]\n",
      "[-0.2703140377998352, -1.3759872913360596]\n",
      "[1.4204576015472412, -1.7880439758300781]\n",
      "[-2.37931752204895, -0.8367596864700317]\n",
      "[-0.9585861563682556, -1.1018991470336914]\n",
      "[0.5104060173034668, -1.2549567222595215]\n",
      "[-0.49971508979797363, -1.6365967988967896]\n",
      "[0.33519744873046875, -1.8537801504135132]\n",
      "[0.5430980920791626, -1.405665636062622]\n",
      "[-0.13906294107437134, -1.2074456214904785]\n",
      "[-0.43089228868484497, -1.2399916648864746]\n",
      "[-1.4542124271392822, -1.2292811870574951]\n",
      "[-2.1736061573028564, -0.8115804195404053]\n",
      "[-3.279387950897217, -1.0553491115570068]\n",
      "[2.1039774417877197, -2.2448437213897705]\n",
      "[-0.10947459936141968, -1.263117790222168]\n",
      "[-0.055208802223205566, -1.2764756679534912]\n",
      "[-1.0607378482818604, -1.2716881036758423]\n",
      "[1.965423583984375, -2.0733392238616943]\n",
      "[1.5336394309997559, -1.686113953590393]\n",
      "[1.3253402709960938, -1.6817331314086914]\n",
      "[-3.5651450157165527, -1.5952479839324951]\n",
      "[-2.2153830528259277, -0.9532759189605713]\n",
      "[1.586618423461914, -1.7639662027359009]\n",
      "[-3.755168914794922, -0.7873415946960449]\n",
      "[1.0815906524658203, -1.5983567237854004]\n",
      "[-0.19566011428833008, -1.1923315525054932]\n",
      "[0.05032283067703247, -1.261495590209961]\n",
      "[1.5168733596801758, -1.79750394821167]\n",
      "[0.2400762438774109, -1.4462181329727173]\n",
      "[-3.524991512298584, -1.1624253988265991]\n",
      "[1.7741501331329346, -1.8243390321731567]\n",
      "[-0.505713164806366, -1.4987740516662598]\n",
      "[2.017592430114746, -2.060276508331299]\n",
      "[1.0986123085021973, -1.5760631561279297]\n",
      "[1.753227949142456, -1.8081094026565552]\n",
      "[-0.25433236360549927, -1.2823209762573242]\n",
      "[-1.1792917251586914, -1.2251149415969849]\n",
      "[-1.8660180568695068, -0.9389306306838989]\n",
      "[2.242479085922241, -2.1563260555267334]\n",
      "[-2.2736406326293945, -0.9132015705108643]\n",
      "[0.9408162236213684, -1.5065600872039795]\n",
      "[-0.5337381958961487, -1.1793280839920044]\n",
      "[1.372908592224121, -1.701282262802124]\n",
      "[-0.034907758235931396, -1.2982807159423828]\n",
      "[-1.9818756580352783, -1.127272129058838]\n",
      "[0.30047184228897095, -1.327470302581787]\n",
      "[0.052830278873443604, -1.2830047607421875]\n",
      "[0.9548842310905457, -1.435911774635315]\n",
      "[1.1175751686096191, -1.4786525964736938]\n",
      "[-2.261221170425415, -0.8369951248168945]\n",
      "[0.9273971915245056, -1.5221788883209229]\n",
      "[-0.535586953163147, -1.0852868556976318]\n",
      "[-1.2718758583068848, -1.0145870447158813]\n",
      "[1.7829608917236328, -1.9204133749008179]\n",
      "[-2.078148126602173, -1.20600163936615]\n",
      "[1.0025825500488281, -1.5723596811294556]\n",
      "[0.372972309589386, -1.2539713382720947]\n",
      "[1.8167147636413574, -1.987285852432251]\n",
      "[1.338393211364746, -1.7247439622879028]\n",
      "[-1.9420239925384521, -0.9824780225753784]\n",
      "[-0.5169826149940491, -2.0166780948638916]\n",
      "[-2.4408693313598633, -1.0792256593704224]\n",
      "[-0.4593047499656677, -1.1322722434997559]\n",
      "[-1.8026857376098633, -1.083700180053711]\n",
      "[-0.7575541734695435, -1.379980206489563]\n",
      "[0.7875551581382751, -0.933525800704956]\n",
      "[-1.7453505992889404, -0.8545186519622803]\n",
      "[-2.005448579788208, -0.8371847867965698]\n",
      "[-1.3337223529815674, -1.6648452281951904]\n",
      "[-1.137202501296997, -1.166316032409668]\n",
      "[1.867236614227295, -2.0312230587005615]\n",
      "[-1.4979476928710938, -1.0274187326431274]\n",
      "[-2.1956965923309326, -0.9429394006729126]\n",
      "[-1.571810245513916, -1.1840426921844482]\n",
      "[-0.6633086204528809, -1.209902286529541]\n",
      "[-0.9032773375511169, -1.3016847372055054]\n",
      "[-2.1934452056884766, -1.1458953619003296]\n",
      "[-0.5292301177978516, -1.562008261680603]\n",
      "[0.20627456903457642, -1.026484489440918]\n",
      "[-2.4019486904144287, -1.0575906038284302]\n",
      "[0.8035939335823059, -1.4323830604553223]\n",
      "[-2.3445522785186768, -1.0256850719451904]\n",
      "[-1.2429256439208984, -1.1771876811981201]\n",
      "[-1.0595611333847046, -1.3819681406021118]\n",
      "[-2.1144931316375732, -1.1371413469314575]\n",
      "[1.3704750537872314, -1.9421963691711426]\n",
      "[0.4872388243675232, -0.7183877229690552]\n",
      "[-0.961348831653595, -1.6639654636383057]\n",
      "[-2.4159016609191895, -1.0653468370437622]\n",
      "[-0.07432547211647034, -0.9913620948791504]\n",
      "[-2.304844856262207, -1.089827299118042]\n",
      "[0.6402857899665833, -1.4115753173828125]\n",
      "[1.431239366531372, -1.7041212320327759]\n",
      "[-0.07667064666748047, -1.5873430967330933]\n",
      "[-1.2446057796478271, -1.081636905670166]\n",
      "[0.322038471698761, -0.6925853490829468]\n",
      "[0.5825480818748474, -0.9048348665237427]\n",
      "[0.017779231071472168, -1.1419742107391357]\n",
      "[-0.052664220333099365, -1.1174018383026123]\n",
      "[-2.0686140060424805, -0.8722970485687256]\n",
      "[-0.6679681539535522, -1.7135506868362427]\n",
      "[-0.9022783041000366, -0.9896236658096313]\n",
      "[-2.3432517051696777, -1.0249621868133545]\n",
      "[0.7955748438835144, -1.0383137464523315]\n",
      "[-0.1736280471086502, -0.9784426689147949]\n",
      "[-1.86208176612854, -0.9342173337936401]\n",
      "[0.6969080567359924, -1.0900275707244873]\n",
      "[-0.49011868238449097, -1.263041615486145]\n",
      "[-2.1677398681640625, -0.9500894546508789]\n",
      "[0.09829941391944885, -1.1893962621688843]\n",
      "[0.6804084181785583, -1.0140142440795898]\n",
      "[0.8416337370872498, -0.9556602239608765]\n",
      "[0.4890748858451843, -0.6534818410873413]\n",
      "[1.3500525951385498, -1.7884079217910767]\n",
      "[-1.4789838790893555, -1.4122819900512695]\n",
      "[-1.9571459293365479, -0.810334324836731]\n",
      "[-1.8255243301391602, -0.8241555690765381]\n",
      "[0.7756481766700745, -0.8991039991378784]\n",
      "[-0.9892039895057678, -1.586680293083191]\n",
      "[-0.14590784907341003, -1.547967791557312]\n",
      "[-2.401416301727295, -1.0572946071624756]\n",
      "[1.2999744415283203, -2.446667432785034]\n",
      "[0.9826052784919739, -1.122276782989502]\n",
      "[-2.1607232093811035, -1.2513686418533325]\n",
      "[1.0815904140472412, -1.5983566045761108]\n",
      "[-2.5098514556884766, -1.1175713539123535]\n",
      "[-1.4257888793945312, -0.9386394023895264]\n",
      "[-0.28604549169540405, -1.2623387575149536]\n",
      "[-2.3595218658447266, -1.0340063571929932]\n",
      "[-2.057828426361084, -0.8663016557693481]\n",
      "[1.4150750637054443, -2.0162513256073]\n",
      "[-0.5458297729492188, -1.6155426502227783]\n",
      "[0.1783924102783203, -1.1215611696243286]\n",
      "[-1.8697590827941895, -1.3481842279434204]\n",
      "[-2.464320659637451, -1.092261791229248]\n",
      "[-0.6013391017913818, -1.549979567527771]\n",
      "[-0.8245435953140259, -1.1522749662399292]\n",
      "[0.7241893410682678, -0.8549985885620117]\n",
      "[0.23557323217391968, -0.7147071361541748]\n",
      "[-1.5685994625091553, -1.433268666267395]\n",
      "[0.9141317009925842, -1.0177984237670898]\n",
      "[-1.4030365943908691, -1.2859176397323608]\n",
      "[-0.413941353559494, -1.110432744026184]\n",
      "[-2.4068806171417236, -1.060332179069519]\n",
      "[-1.7304372787475586, -0.8734246492385864]\n",
      "[1.272352695465088, -1.3248298168182373]\n",
      "[-0.19275087118148804, -1.3096702098846436]\n",
      "[-1.8579716682434082, -0.8399753570556641]\n",
      "[-2.399724245071411, -1.05635404586792]\n",
      "[-0.34332507848739624, -1.4449865818023682]\n",
      "[0.6762488484382629, -0.8139088153839111]\n",
      "[-2.3367807865142822, -1.0213651657104492]\n",
      "[-0.16636845469474792, -1.095549464225769]\n",
      "[-0.3129682242870331, -0.8855854272842407]\n",
      "[-2.1165177822113037, -1.2504853010177612]\n",
      "[1.551335334777832, -1.5639461278915405]\n",
      "[-2.453547477722168, -1.086273193359375]\n",
      "[-0.07195982336997986, -1.3081501722335815]\n",
      "[-1.7201011180877686, -1.389220952987671]\n",
      "[-2.1262876987457275, -1.0863683223724365]\n",
      "[0.7658845782279968, -0.8907356262207031]\n",
      "[0.9572475552558899, -2.128810167312622]\n",
      "[-1.3139839172363281, -0.9762866497039795]\n",
      "[-0.7414668798446655, -0.8110232353210449]\n",
      "[-1.5531210899353027, -0.8372161388397217]\n",
      "[-0.5503523349761963, -1.1636492013931274]\n",
      "[-0.5606144666671753, -0.305193692445755]\n",
      "[-1.924424171447754, -0.7921450138092041]\n",
      "[-1.8722293376922607, -0.7631310224533081]\n",
      "[0.17532384395599365, -1.800885796546936]\n",
      "[-0.9062222242355347, -0.9147709608078003]\n",
      "[1.0176236629486084, -1.7488343715667725]\n",
      "[-1.871422529220581, -0.7626825571060181]\n",
      "[-1.6647930145263672, -0.908106803894043]\n",
      "[-0.8700511455535889, -1.0737658739089966]\n",
      "[-1.0133614540100098, -0.9639146327972412]\n",
      "[-0.41124194860458374, -1.1227257251739502]\n",
      "[-0.8311565518379211, -1.2821810245513916]\n",
      "[0.2301487922668457, -1.4497528076171875]\n",
      "[-0.7316906452178955, -0.7184371948242188]\n",
      "[-1.9324593544006348, -0.7966116666793823]\n",
      "[-0.5637717247009277, -0.932278037071228]\n",
      "[-0.8979446887969971, -1.1429861783981323]\n",
      "[-1.0076898336410522, -0.9264131784439087]\n",
      "[-0.08891406655311584, -1.3439946174621582]\n",
      "[-0.8524738550186157, -1.2018895149230957]\n",
      "[0.1704900860786438, -1.614975094795227]\n",
      "[-0.3794550895690918, -0.556970477104187]\n",
      "[0.289991557598114, -1.7048934698104858]\n",
      "[-1.108109474182129, -1.1053727865219116]\n",
      "[-1.023213505744934, -0.6753107309341431]\n",
      "[-0.9076311588287354, -1.2015373706817627]\n",
      "[0.8248118758201599, -1.2824251651763916]\n",
      "[0.15883326530456543, -1.185253381729126]\n",
      "[0.6689656376838684, -1.4753282070159912]\n",
      "[-1.6402263641357422, -0.6345465183258057]\n",
      "[-0.48938634991645813, -0.48790696263313293]\n",
      "[-0.42527908086776733, -0.42934176325798035]\n",
      "[-0.7242281436920166, -0.8431878089904785]\n",
      "[-0.7792963981628418, -0.8307099342346191]\n",
      "[-2.0436270236968994, -0.8584072589874268]\n",
      "[0.5344760417938232, -1.732987880706787]\n",
      "[-1.671170711517334, -0.6513668298721313]\n",
      "[-1.4864590167999268, -0.9415906667709351]\n",
      "[-0.05698618292808533, -0.7132790088653564]\n",
      "[-1.5175795555114746, -0.5659888982772827]\n",
      "[-1.4373304843902588, -0.7418535947799683]\n",
      "[-0.4295862317085266, -0.662689208984375]\n",
      "[-0.9805128574371338, -0.9620431661605835]\n",
      "[-1.850722074508667, -0.7511755228042603]\n",
      "[-0.7499553561210632, -0.8827064037322998]\n",
      "[0.30845028162002563, -0.8064366579055786]\n",
      "[0.8773905634880066, -1.082525610923767]\n",
      "[-0.20783549547195435, -0.6276545524597168]\n",
      "[0.01776522397994995, -1.3546217679977417]\n",
      "[-0.40302735567092896, -1.4770636558532715]\n",
      "[-2.050891637802124, -0.862445592880249]\n",
      "[-2.0519087314605713, -0.8630110025405884]\n",
      "[0.8455092310905457, -0.958981990814209]\n",
      "[0.24353748559951782, -1.6204525232315063]\n",
      "[0.5026411414146423, -1.4108096361160278]\n",
      "[-1.019605040550232, -1.1632766723632812]\n",
      "[0.05499619245529175, -2.0672271251678467]\n",
      "[0.014576733112335205, -0.7638106346130371]\n",
      "[-0.762566864490509, -1.3580831289291382]\n",
      "[-0.19565993547439575, -1.1923316717147827]\n",
      "[-1.4257888793945312, -0.9386396408081055]\n",
      "[-2.1062088012695312, -0.8931951522827148]\n",
      "[-0.7340507507324219, -1.0213242769241333]\n",
      "[-1.288065791130066, -1.0911014080047607]\n",
      "[-1.9736781120300293, -0.8195241689682007]\n",
      "[0.15822255611419678, -1.6553317308425903]\n",
      "[0.35336703062057495, -1.5430315732955933]\n",
      "[-1.1496983766555786, -0.5478827953338623]\n",
      "[-0.3418061137199402, -1.5003148317337036]\n",
      "[-1.3216674327850342, -0.968056321144104]\n",
      "[0.2969467043876648, -1.4802511930465698]\n",
      "[-0.8391552567481995, -0.8234635591506958]\n",
      "[0.8338857293128967, -1.0072966814041138]\n",
      "[-1.02811598777771, -0.43478408455848694]\n",
      "[-0.06526464223861694, -1.5735992193222046]\n",
      "[0.45150864124298096, -0.9459037780761719]\n",
      "[-0.7680554389953613, -1.1534875631332397]\n",
      "[-0.78941410779953, -0.779029369354248]\n",
      "[-1.0257501602172852, -1.104752540588379]\n",
      "[-2.0620875358581543, -0.8686690330505371]\n",
      "[0.841932475566864, -1.224381923675537]\n",
      "[-0.6586079597473145, -1.0744647979736328]\n",
      "[-2.011481523513794, -0.8405383825302124]\n",
      "[-1.8050813674926758, -0.7458434104919434]\n",
      "[-0.0012153089046478271, -1.2527503967285156]\n",
      "[-0.5681066513061523, -0.2717151939868927]\n",
      "[-1.3485819101333618, -0.9090911149978638]\n",
      "[-1.038116693496704, -0.7928396463394165]\n",
      "[-1.0272048711776733, -0.5246330499649048]\n",
      "[-0.5956106185913086, -1.4009263515472412]\n",
      "[0.8399394154548645, -1.321636438369751]\n",
      "[-2.068587303161621, -0.8722822666168213]\n",
      "[-0.7867344617843628, -1.0513139963150024]\n",
      "[-0.28840988874435425, -1.4988973140716553]\n",
      "[-0.7949057817459106, -1.1748096942901611]\n",
      "[0.4593358635902405, -0.9054346084594727]\n",
      "[1.568467617034912, -2.1472902297973633]\n",
      "[-0.5402276515960693, -1.1883705854415894]\n",
      "[-1.3762283325195312, -0.54646897315979]\n",
      "[-1.3154804706573486, -0.896109938621521]\n",
      "[-1.9074249267578125, -0.6015257835388184]\n",
      "[0.7377334237098694, -0.9492295980453491]\n",
      "[0.7403339743614197, -1.2999529838562012]\n",
      "[-0.4716549813747406, -1.000299096107483]\n",
      "[0.770138680934906, -1.854132056236267]\n",
      "[-1.7790157794952393, -0.653650164604187]\n",
      "[0.21406173706054688, -1.177706003189087]\n",
      "[-0.8374633193016052, -1.0237088203430176]\n",
      "[1.588172197341919, -1.5955190658569336]\n",
      "[-1.510502815246582, -0.7839380502700806]\n",
      "[-2.0919811725616455, -0.5301418304443359]\n",
      "[-2.1296989917755127, -0.5766911506652832]\n",
      "[0.13282954692840576, -1.553163766860962]\n",
      "[-1.0584914684295654, -0.8028830289840698]\n",
      "[-1.4541621208190918, -0.3475751578807831]\n",
      "[0.17515116930007935, -1.3852450847625732]\n",
      "[0.18804854154586792, -1.3358368873596191]\n",
      "[0.0034293830394744873, -1.380066990852356]\n",
      "[-2.039368152618408, -0.6330565214157104]\n",
      "[-0.9997557401657104, -0.8692463636398315]\n",
      "[-0.6393720507621765, -1.125200867652893]\n",
      "[-0.017043709754943848, -1.4312348365783691]\n",
      "[0.1418333649635315, -0.5098109245300293]\n",
      "[-0.2336430549621582, -1.3243387937545776]\n",
      "[-0.40432268381118774, -1.2988076210021973]\n",
      "[-1.456627368927002, -0.37493881583213806]\n",
      "[-0.5259608030319214, -1.2339909076690674]\n",
      "[-1.1838966608047485, -0.612714409828186]\n",
      "[1.0117874145507812, -1.550481915473938]\n",
      "[-0.7870292663574219, -0.8073315620422363]\n",
      "[-1.2031030654907227, -0.8534655570983887]\n",
      "[-0.024135708808898926, -0.40005072951316833]\n",
      "[0.6081250905990601, -0.837387204170227]\n",
      "[-1.5990629196166992, -0.36776190996170044]\n",
      "[-1.6250627040863037, -0.4000028073787689]\n",
      "[0.5774809122085571, -1.342314600944519]\n",
      "[-0.1588819921016693, -1.2832143306732178]\n",
      "[1.4216229915618896, -1.4527696371078491]\n",
      "[0.4720548391342163, -1.5078709125518799]\n",
      "[0.33329635858535767, -0.7339823246002197]\n",
      "[0.5806668400764465, -0.935562252998352]\n",
      "[-1.7846837043762207, -0.7307076454162598]\n",
      "[1.893721342086792, -1.857405424118042]\n",
      "[1.9225022792816162, -1.8820736408233643]\n",
      "[-0.979569137096405, -1.0479087829589844]\n",
      "[-1.731431007385254, -0.4025329053401947]\n",
      "[-1.409193992614746, -0.31536921858787537]\n",
      "[-0.31698164343833923, -0.3778614103794098]\n",
      "[-0.02242046594619751, -0.4269610345363617]\n",
      "[0.5982116460800171, -1.5708963871002197]\n",
      "[1.0042221546173096, -1.8445850610733032]\n",
      "[0.40611082315444946, -1.155038833618164]\n",
      "[0.032127320766448975, -1.1536455154418945]\n",
      "[-0.5227012634277344, -0.3296624720096588]\n",
      "[-0.2370801568031311, -1.248617172241211]\n",
      "[-1.0294873714447021, -0.7200430631637573]\n",
      "[-0.4627421498298645, -1.236750841140747]\n",
      "[-0.0203741192817688, -1.9781622886657715]\n",
      "[-0.4039299786090851, -0.48792973160743713]\n",
      "[-0.420177698135376, -1.3082427978515625]\n",
      "[0.05032223463058472, -1.261495590209961]\n",
      "[-0.2860453128814697, -1.2623388767242432]\n",
      "[-0.7340507507324219, -1.0213242769241333]\n",
      "[-1.9401028156280518, -0.44769421219825745]\n",
      "[0.20240265130996704, -1.5325888395309448]\n",
      "[0.5462003946304321, -1.3028455972671509]\n",
      "[0.20242953300476074, -1.5864472389221191]\n",
      "[-0.7642773985862732, -0.9415261745452881]\n",
      "[1.3931059837341309, -1.4283275604248047]\n",
      "[0.28363853693008423, -1.6197762489318848]\n",
      "[-0.4027176797389984, -1.210072636604309]\n",
      "[-0.8410409688949585, -0.8905916213989258]\n",
      "[-1.4324212074279785, -0.6748085021972656]\n",
      "[-0.5490187406539917, -0.2979316711425781]\n",
      "[-0.1536581665277481, -0.44298604130744934]\n",
      "[0.16951358318328857, -1.5701885223388672]\n",
      "[-0.11219322681427002, -0.4580531418323517]\n",
      "[-1.5527045726776123, -0.7786799669265747]\n",
      "[-1.3848223686218262, -0.509084939956665]\n",
      "[-0.31807413697242737, -1.2882018089294434]\n",
      "[-0.5582746863365173, -1.0682491064071655]\n",
      "[-0.1408931016921997, -0.5071225166320801]\n",
      "[-1.9984467029571533, -0.4633432924747467]\n",
      "[-0.8263452053070068, -0.985026478767395]\n",
      "[-0.6385713815689087, -1.0809096097946167]\n",
      "[-2.002795457839966, -0.5531352758407593]\n",
      "[0.7639445662498474, -1.0097602605819702]\n",
      "[-0.6070244312286377, -1.0851472616195679]\n",
      "[-1.6379797458648682, -0.4206465184688568]\n",
      "[-0.8511360883712769, -0.38520848751068115]\n",
      "[-0.03297615051269531, -1.4937310218811035]\n",
      "[-0.24413037300109863, -0.6410975456237793]\n",
      "[0.9250476956367493, -1.5375442504882812]\n",
      "[-2.0158815383911133, -0.472608357667923]\n",
      "[-0.09340569376945496, -1.3899375200271606]\n",
      "[-0.5024192333221436, -1.1661978960037231]\n",
      "[-0.26083847880363464, -0.3630799949169159]\n",
      "[-1.1155414581298828, -1.9137721061706543]\n",
      "[-2.3870677947998047, -1.0493186712265015]\n",
      "[0.0558946430683136, -1.3869221210479736]\n",
      "[-1.2735278606414795, -1.3268909454345703]\n",
      "[-0.32591068744659424, -1.617258071899414]\n",
      "[1.0056192874908447, -1.096212387084961]\n",
      "[-1.9117352962493896, -0.904055118560791]\n",
      "[-1.9111499786376953, -0.9286311864852905]\n",
      "[-1.8787481784820557, -1.5628976821899414]\n",
      "[-0.6427881717681885, -1.406907320022583]\n",
      "[2.2604739665985107, -2.2402806282043457]\n",
      "[-1.0316894054412842, -1.243001937866211]\n",
      "[-2.600949764251709, -1.1682108640670776]\n",
      "[-1.1614859104156494, -1.396968960762024]\n",
      "[-0.15400098264217377, -1.4787309169769287]\n",
      "[-0.4991001486778259, -1.5290724039077759]\n",
      "[-2.6475560665130615, -1.1941183805465698]\n",
      "[-0.2615157961845398, -1.7588157653808594]\n",
      "[0.720183789730072, -1.3045690059661865]\n",
      "[-2.7196273803710938, -1.23418128490448]\n",
      "[1.2705130577087402, -1.536122441291809]\n",
      "[-2.4620509147644043, -1.0910000801086426]\n",
      "[-0.7511613368988037, -1.4220337867736816]\n",
      "[-0.8609964847564697, -1.5435891151428223]\n",
      "[-1.9076924324035645, -1.2777533531188965]\n",
      "[1.8162462711334229, -2.146723508834839]\n",
      "[0.8656236529350281, -0.976222038269043]\n",
      "[-0.9890915155410767, -1.7556040287017822]\n",
      "[-2.529815912246704, -1.128669261932373]\n",
      "[0.6466177105903625, -1.2744919061660767]\n",
      "[-2.2030417919158936, -1.1893295049667358]\n",
      "[1.011932134628296, -1.6632635593414307]\n",
      "[1.7419970035552979, -1.8465975522994995]\n",
      "[0.16775310039520264, -1.7870194911956787]\n",
      "[-0.7222342491149902, -1.322106122970581]\n",
      "[0.8735265135765076, -0.9829956293106079]\n",
      "[1.029329538345337, -1.1165344715118408]\n",
      "[0.493160605430603, -1.4189149141311646]\n",
      "[0.4557878375053406, -1.3930184841156006]\n",
      "[-2.2503185272216797, -0.9733026027679443]\n",
      "[-0.6760374307632446, -1.8189176321029663]\n",
      "[-1.162895917892456, -1.0074968338012695]\n",
      "[-2.9249801635742188, -1.3483325242996216]\n",
      "[1.0640473365783691, -1.1462911367416382]\n",
      "[-0.057469338178634644, -1.120832085609436]\n",
      "[-1.3523547649383545, -1.1696653366088867]\n",
      "[0.5542316436767578, -1.1132563352584839]\n",
      "[-1.0763945579528809, -1.2231522798538208]\n",
      "[-1.661086082458496, -1.1767030954360962]\n",
      "[0.5701313018798828, -1.4716508388519287]\n",
      "[1.1584746837615967, -1.2886499166488647]\n",
      "[1.0493080615997314, -1.1336581707000732]\n",
      "[0.7602983117103577, -0.8859477043151855]\n",
      "[1.708845615386963, -1.9731323719024658]\n",
      "[-2.519193649291992, -1.1753743886947632]\n",
      "[-2.0366389751434326, -0.854522705078125]\n",
      "[-1.8493483066558838, -0.9205801486968994]\n",
      "[1.069427490234375, -1.150902509689331]\n",
      "[-0.9868447184562683, -1.6876598596572876]\n",
      "[0.14151853322982788, -1.7648687362670898]\n",
      "[-2.1771066188812256, -1.1945453882217407]\n",
      "[1.8348212242126465, -2.614030599594116]\n",
      "[1.29457426071167, -1.3438758850097656]\n",
      "[-2.109570026397705, -1.341070532798767]\n",
      "[1.5168731212615967, -1.797504186630249]\n",
      "[-2.3595218658447266, -1.0340063571929932]\n",
      "[-1.2880659103393555, -1.0911012887954712]\n",
      "[0.20240247249603271, -1.5325887203216553]\n",
      "[-2.887434244155884, -1.3274614810943604]\n",
      "[-2.2040181159973145, -0.9475651979446411]\n",
      "[1.864957571029663, -2.209923505783081]\n",
      "[-0.34732764959335327, -1.7898141145706177]\n",
      "[0.3019474148750305, -1.2385852336883545]\n",
      "[-2.3515145778656006, -1.2532707452774048]\n",
      "[-2.312238931655884, -1.0416018962860107]\n",
      "[-0.39888131618499756, -1.7238502502441406]\n",
      "[-0.30831944942474365, -1.397814154624939]\n",
      "[1.0144484043121338, -1.103779911994934]\n",
      "[0.926123321056366, -1.028076410293579]\n",
      "[-1.9271838665008545, -1.3872309923171997]\n",
      "[1.0253865718841553, -1.1131551265716553]\n",
      "[-0.9901502728462219, -1.5042264461517334]\n",
      "[0.10047566890716553, -1.3654780387878418]\n",
      "[-2.4240269660949707, -1.0698633193969727]\n",
      "[-1.639587163925171, -1.0087708234786987]\n",
      "[1.3802945613861084, -1.4173470735549927]\n",
      "[0.2691989541053772, -1.5761566162109375]\n",
      "[-1.6966803073883057, -0.9997096061706543]\n",
      "[-2.3195528984069824, -1.0117884874343872]\n",
      "[0.037396132946014404, -1.681567668914795]\n",
      "[0.8120936751365662, -0.9303414821624756]\n",
      "[-2.042017936706543, -1.0829523801803589]\n",
      "[0.4554969072341919, -1.386179804801941]\n",
      "[0.6194905638694763, -1.149836778640747]\n",
      "[-2.4055182933807373, -1.218875527381897]\n",
      "[1.7804977893829346, -1.7603614330291748]\n",
      "[-2.8983726501464844, -1.333541989326477]\n",
      "[0.3780289888381958, -1.5883551836013794]\n",
      "[-1.8549132347106934, -1.4274917840957642]\n",
      "[-1.9691791534423828, -1.2072402238845825]\n",
      "[0.9375742077827454, -1.037890911102295]\n",
      "[0.3816182017326355, -1.9992306232452393]\n",
      "[-1.9412741661071777, -0.8588559627532959]\n",
      "[0.4003397822380066, -1.129820466041565]\n",
      "[-1.1509016752243042, -0.9907007217407227]\n",
      "[0.09358912706375122, -1.3913724422454834]\n",
      "[0.08852586150169373, -0.5546135902404785]\n",
      "[-2.3688716888427734, -1.0392038822174072]\n",
      "[-1.9900169372558594, -0.8286066055297852]\n",
      "[-0.3112141788005829, -1.6775383949279785]\n",
      "[-0.2185303419828415, -1.1675201654434204]\n",
      "[1.5772998332977295, -2.0910933017730713]\n",
      "[-1.8324317932128906, -0.7410084009170532]\n",
      "[-2.4432997703552246, -1.080576777458191]\n",
      "[-0.5735663771629333, -1.2113420963287354]\n",
      "[0.033434659242630005, -1.1771330833435059]\n",
      "[0.031081467866897583, -1.332834243774414]\n",
      "[-1.7411258220672607, -1.0686330795288086]\n",
      "[0.5465148091316223, -1.6426331996917725]\n",
      "[0.863281786441803, -1.1246936321258545]\n",
      "[-2.4923715591430664, -1.1078547239303589]\n",
      "[-0.23802149295806885, -1.1124480962753296]\n",
      "[-1.6865711212158203, -0.971322774887085]\n",
      "[-0.48174190521240234, -1.1418824195861816]\n",
      "[0.10759130120277405, -1.4739127159118652]\n",
      "[-0.883673369884491, -1.2247271537780762]\n",
      "[0.6364482641220093, -1.8932512998580933]\n",
      "[0.6356423497200012, -0.8933502435684204]\n",
      "[0.26976609230041504, -1.7667909860610962]\n",
      "[-1.8379082679748535, -0.953789472579956]\n",
      "[0.3871839642524719, -1.0326505899429321]\n",
      "[-1.1567392349243164, -1.1630902290344238]\n",
      "[1.4944868087768555, -1.5310741662979126]\n",
      "[0.44299596548080444, -1.3423830270767212]\n",
      "[0.9932566285133362, -1.681929349899292]\n",
      "[-1.0402307510375977, -0.8821450471878052]\n",
      "[0.6738893389701843, -0.8881962299346924]\n",
      "[0.3757503628730774, -0.7209398746490479]\n",
      "[0.32916533946990967, -1.1549851894378662]\n",
      "[0.6285629272460938, -1.1677526235580444]\n",
      "[-2.7035181522369385, -1.2252265214920044]\n",
      "[0.5523399114608765, -1.8178974390029907]\n",
      "[-1.8291568756103516, -0.7539224624633789]\n",
      "[-2.2807915210723877, -0.9902418851852417]\n",
      "[0.6671058535575867, -1.0144469738006592]\n",
      "[-1.0843441486358643, -0.5810494422912598]\n",
      "[-1.3513084650039673, -0.8406592607498169]\n",
      "[-0.7921079397201538, -0.5951036214828491]\n",
      "[-1.3965604305267334, -1.0428858995437622]\n",
      "[-1.949211835861206, -0.8059239387512207]\n",
      "[0.48196834325790405, -1.192164659500122]\n",
      "[1.2369592189788818, -1.2944940328598022]\n",
      "[1.5259275436401367, -1.5680029392242432]\n",
      "[0.812207043170929, -1.0737334489822388]\n",
      "[0.3696547746658325, -1.5593652725219727]\n",
      "[-1.5184471607208252, -1.199106216430664]\n",
      "[-2.337907314300537, -1.0219913721084595]\n",
      "[-2.3368358612060547, -1.0213956832885742]\n",
      "[1.5078771114349365, -1.5266979932785034]\n",
      "[0.22915256023406982, -1.6846301555633545]\n",
      "[0.8672448992729187, -1.6358197927474976]\n",
      "[-1.2187645435333252, -1.1319520473480225]\n",
      "[0.4945588707923889, -2.3311221599578857]\n",
      "[0.5640367865562439, -1.0541372299194336]\n",
      "[-0.9035304188728333, -1.3449273109436035]\n",
      "[0.2400762438774109, -1.4462181329727173]\n",
      "[-2.057828664779663, -0.8663016557693481]\n",
      "[-1.9736778736114502, -0.8195241689682007]\n",
      "[0.5462002754211426, -1.3028454780578613]\n",
      "[-2.2040176391601562, -0.947564959526062]\n",
      "[-2.6480422019958496, -1.1943886280059814]\n",
      "[0.5969929695129395, -1.9202066659927368]\n",
      "[0.5823625326156616, -1.705927848815918]\n",
      "[-1.1254169940948486, -0.6423614025115967]\n",
      "[-1.016064167022705, -1.343645453453064]\n",
      "[-1.8945460319519043, -0.8602995872497559]\n",
      "[0.5335429906845093, -1.6417077779769897]\n",
      "[0.013649165630340576, -1.1419354677200317]\n",
      "[1.4687354564666748, -1.493149757385254]\n",
      "[0.5365617871284485, -0.9665545225143433]\n",
      "[-0.6483792662620544, -1.4573036432266235]\n",
      "[1.1445343494415283, -1.4247757196426392]\n",
      "[-0.4367867112159729, -1.310829520225525]\n",
      "[0.3953865170478821, -1.0951842069625854]\n",
      "[-1.6986677646636963, -0.9715343713760376]\n",
      "[-2.144120693206787, -0.9142694473266602]\n",
      "[1.4452028274536133, -1.6868610382080078]\n",
      "[0.6488822102546692, -1.3670238256454468]\n",
      "[-2.0737357139587402, -0.8751441240310669]\n",
      "[-2.0948808193206787, -0.8868981599807739]\n",
      "[0.56876140832901, -1.4969338178634644]\n",
      "[-0.2546992599964142, -0.4148857295513153]\n",
      "[-1.7416884899139404, -0.858600378036499]\n",
      "[-0.26785027980804443, -1.0363754034042358]\n",
      "[0.5786752700805664, -0.9325954914093018]\n",
      "[-1.0650930404663086, -1.2926411628723145]\n",
      "[1.3510212898254395, -1.7000635862350464]\n",
      "[-2.738863945007324, -1.2448744773864746]\n",
      "[0.487510085105896, -1.3309831619262695]\n",
      "[-0.4863925576210022, -1.4776562452316284]\n",
      "[-0.9865385890007019, -1.1574152708053589]\n",
      "[1.1157636642456055, -1.3514103889465332]\n",
      "[3.3582849502563477, -3.2629234790802]\n",
      "[1.6194534301757812, -1.9740760326385498]\n",
      "[-0.2262064814567566, -1.5987342596054077]\n",
      "[0.79889315366745, -1.7653639316558838]\n",
      "[0.9411582350730896, -1.7799127101898193]\n",
      "[-1.557166576385498, -1.3881784677505493]\n",
      "[0.39527857303619385, -1.8244855403900146]\n",
      "[0.4900374412536621, -1.8177309036254883]\n",
      "[2.760300397872925, -2.7517263889312744]\n",
      "[0.5376243591308594, -1.7561566829681396]\n",
      "[-2.9366707801818848, -1.427860975265503]\n",
      "[0.31860047578811646, -1.7104665040969849]\n",
      "[0.8151969313621521, -2.0476911067962646]\n",
      "[1.1928117275238037, -1.884034276008606]\n",
      "[0.20118755102157593, -1.4835788011550903]\n",
      "[1.1492083072662354, -1.7410122156143188]\n",
      "[2.1732754707336426, -2.311720371246338]\n",
      "[1.812981367111206, -1.9898650646209717]\n",
      "[-0.5000154376029968, -1.470352292060852]\n",
      "[1.3843541145324707, -2.0712690353393555]\n",
      "[-3.6686136722564697, -1.0681157112121582]\n",
      "[1.7428529262542725, -2.1906216144561768]\n",
      "[0.7360491156578064, -1.5909284353256226]\n",
      "[1.877455711364746, -2.0752503871917725]\n",
      "[1.6786837577819824, -2.048978805541992]\n",
      "[-4.072309970855713, -1.1411681175231934]\n",
      "[-1.680464506149292, -1.2905428409576416]\n",
      "[2.464064121246338, -2.398960828781128]\n",
      "[1.9299290180206299, -2.084312677383423]\n",
      "[-0.6953877806663513, -1.4400286674499512]\n",
      "[1.791450023651123, -2.100031852722168]\n",
      "[1.299654245376587, -1.6978204250335693]\n",
      "[-2.807509422302246, -1.3503636121749878]\n",
      "[1.9298651218414307, -2.0843355655670166]\n",
      "[0.15827888250350952, -1.6474148035049438]\n",
      "[-1.5096290111541748, -1.3162517547607422]\n",
      "[-1.613560438156128, -1.3507614135742188]\n",
      "[-0.165524423122406, -1.5841420888900757]\n",
      "[-0.32970982789993286, -1.522091031074524]\n",
      "[0.6929762959480286, -1.928222417831421]\n",
      "[2.5229578018188477, -2.448166608810425]\n",
      "[0.05459851026535034, -1.8526862859725952]\n",
      "[1.778017520904541, -2.2563693523406982]\n",
      "[-2.0184075832366943, -1.269803762435913]\n",
      "[-0.6039712429046631, -1.5815993547439575]\n",
      "[0.8092419505119324, -1.6771445274353027]\n",
      "[-0.30240780115127563, -2.0554375648498535]\n",
      "[0.6463600993156433, -2.3383121490478516]\n",
      "[0.8786739706993103, -1.8275221586227417]\n",
      "[-0.0047917962074279785, -1.5235874652862549]\n",
      "[-0.3707846999168396, -1.5225257873535156]\n",
      "[-1.5574347972869873, -1.437562108039856]\n",
      "[-1.8313536643981934, -1.2277220487594604]\n",
      "[-3.430054187774658, -1.2669638395309448]\n",
      "[2.4549243450164795, -2.6604177951812744]\n",
      "[0.25377899408340454, -1.750657558441162]\n",
      "[0.300703763961792, -1.7507624626159668]\n",
      "[-1.1751618385314941, -1.47495698928833]\n",
      "[2.241476535797119, -2.4284164905548096]\n",
      "[1.6991536617279053, -1.961921215057373]\n",
      "[1.6545164585113525, -2.0755679607391357]\n",
      "[-4.588194370269775, -1.401318073272705]\n",
      "[-1.9835312366485596, -1.3369673490524292]\n",
      "[1.9079198837280273, -2.151365280151367]\n",
      "[-3.5249922275543213, -1.1624246835708618]\n",
      "[1.4150748252868652, -2.0162513256073]\n",
      "[0.15822315216064453, -1.6553319692611694]\n",
      "[0.20242983102798462, -1.5864471197128296]\n",
      "[1.8649568557739258, -2.2099227905273438]\n",
      "[0.5969926714897156, -1.9202064275741577]\n",
      "[-4.56329870223999, -0.9403045177459717]\n",
      "[1.9984431266784668, -2.14068865776062]\n",
      "[-0.25040119886398315, -1.9310681819915771]\n",
      "[2.3519845008850098, -2.459249496459961]\n",
      "[1.427847146987915, -1.9922590255737305]\n",
      "[1.976780652999878, -2.122662305831909]\n",
      "[0.029652655124664307, -1.69495689868927]\n",
      "[-1.2166974544525146, -1.4648219347000122]\n",
      "[-1.5061085224151611, -1.3544495105743408]\n",
      "[2.5790610313415527, -2.5527503490448]\n",
      "[-2.127553701400757, -1.238918423652649]\n",
      "[1.2007043361663818, -1.8801888227462769]\n",
      "[-0.2608938217163086, -1.5733983516693115]\n",
      "[1.7085955142974854, -2.1021244525909424]\n",
      "[0.32248806953430176, -1.7649508714675903]\n",
      "[-2.124857187271118, -1.3198577165603638]\n",
      "[0.4241200089454651, -1.6390645503997803]\n",
      "[0.4091114401817322, -1.7479898929595947]\n",
      "[1.2888782024383545, -1.8648563623428345]\n",
      "[1.240149974822998, -1.7768629789352417]\n",
      "[-1.9715511798858643, -1.280522108078003]\n",
      "[1.2534029483795166, -1.9424245357513428]\n",
      "[-0.3415065407752991, -1.4261382818222046]\n",
      "[-0.9655161499977112, -1.4155495166778564]\n",
      "[2.112964630126953, -2.3136613368988037]\n",
      "[-2.253993511199951, -1.387373447418213]\n",
      "[1.3835060596466064, -2.101672649383545]\n",
      "[0.4599061608314514, -1.5476758480072021]\n",
      "[2.1310627460479736, -2.368739366531372]\n",
      "[1.656446933746338, -2.119039297103882]\n",
      "[-1.8156516551971436, -1.2997767925262451]\n",
      "[0.0036148428916931152, -2.195277690887451]\n",
      "[-1.0319880247116089, -1.4700409173965454]\n",
      "[0.09716737270355225, -1.114906668663025]\n",
      "[-0.6879470348358154, -1.3657701015472412]\n",
      "[-1.4597008228302002, -1.0196095705032349]\n",
      "[1.0164985656738281, -1.1055370569229126]\n",
      "[1.0125510692596436, -1.7588192224502563]\n",
      "[-0.0769963264465332, -1.463422417640686]\n",
      "[-0.6277462244033813, -1.9347891807556152]\n",
      "[-0.9873383641242981, -1.1383756399154663]\n",
      "[1.7034144401550293, -1.6942932605743408]\n",
      "[0.1350986361503601, -1.5255322456359863]\n",
      "[1.352308988571167, -1.8698980808258057]\n",
      "[-1.5252351760864258, -1.1612578630447388]\n",
      "[-0.9375348687171936, -0.9580538272857666]\n",
      "[-1.9499309062957764, -0.8995580673217773]\n",
      "[-0.7946432828903198, -1.7117425203323364]\n",
      "[-2.4648947715759277, -1.0925809144973755]\n",
      "[0.13785266876220703, -0.8327585458755493]\n",
      "[-0.06779810786247253, -1.7053667306900024]\n",
      "[1.8919384479522705, -1.9378957748413086]\n",
      "[-0.6553003787994385, -1.6228547096252441]\n",
      "[-1.3333073854446411, -1.0662641525268555]\n",
      "[-2.520596742630005, -1.123544454574585]\n",
      "[-1.4106080532073975, -1.3729217052459717]\n",
      "[1.8211884498596191, -1.9714515209197998]\n",
      "[0.7829646468162537, -0.9053750038146973]\n",
      "[-2.3605966567993164, -1.231337547302246]\n",
      "[-1.2180432081222534, -1.4853837490081787]\n",
      "[0.3264176845550537, -0.8738216161727905]\n",
      "[-1.316054344177246, -1.4552438259124756]\n",
      "[-1.4619524478912354, -0.7233891487121582]\n",
      "[2.240382432937622, -2.154529094696045]\n",
      "[-2.309147596359253, -1.0060044527053833]\n",
      "[-0.16636031866073608, -1.3524913787841797]\n",
      "[0.7908458113670349, -0.9121299982070923]\n",
      "[0.9132739901542664, -1.0170631408691406]\n",
      "[-0.033085256814956665, -0.9303232431411743]\n",
      "[-0.016360938549041748, -0.9334132671356201]\n",
      "[0.6910989880561829, -1.767309546470642]\n",
      "[-2.453317642211914, -1.1472456455230713]\n",
      "[1.6859772205352783, -1.7475852966308594]\n",
      "[-0.11281163990497589, -1.7358405590057373]\n",
      "[0.8656730055809021, -0.9762643575668335]\n",
      "[1.2743372917175293, -1.326530933380127]\n",
      "[-1.1456470489501953, -1.1757745742797852]\n",
      "[2.1140940189361572, -2.0462870597839355]\n",
      "[2.237941265106201, -2.1524367332458496]\n",
      "[-0.35551220178604126, -1.5412702560424805]\n",
      "[-0.36891230940818787, -0.8785878419876099]\n",
      "[-0.0835760235786438, -0.7083848714828491]\n",
      "[0.712537944316864, -0.8450121879577637]\n",
      "[0.760707676410675, -0.8862985372543335]\n",
      "[2.154093027114868, -2.1424317359924316]\n",
      "[0.06819397211074829, -2.021122932434082]\n",
      "[0.5455995798110962, -1.5825908184051514]\n",
      "[0.6347355842590332, -1.6400272846221924]\n",
      "[0.5873632431030273, -0.7377249002456665]\n",
      "[-2.210871696472168, -1.1926440000534058]\n",
      "[-2.2085819244384766, -0.9501020908355713]\n",
      "[-1.0479793548583984, -1.5236079692840576]\n",
      "[1.972226619720459, -2.445608377456665]\n",
      "[0.8720031380653381, -0.9816899299621582]\n",
      "[-1.3409216403961182, -1.521492600440979]\n",
      "[1.7741501331329346, -1.8243390321731567]\n",
      "[-0.5458294749259949, -1.6155426502227783]\n",
      "[0.3533671498298645, -1.5430318117141724]\n",
      "[-0.7642775774002075, -0.9415261745452881]\n",
      "[-0.34732764959335327, -1.7898141145706177]\n",
      "[0.5823626518249512, -1.7059279680252075]\n",
      "[1.9984431266784668, -2.14068865776062]\n",
      "[-2.6408603191375732, -1.1903964281082153]\n",
      "[1.785149097442627, -1.7643481492996216]\n",
      "[-0.8819754719734192, -1.7416808605194092]\n",
      "[-0.7154529094696045, -1.5533397197723389]\n",
      "[-2.6638143062591553, -1.2031561136245728]\n",
      "[-0.2587173581123352, -1.2034850120544434]\n",
      "[0.5502721071243286, -0.7059340476989746]\n",
      "[0.8350754380226135, -0.9500391483306885]\n",
      "[-1.3783390522003174, -1.5745151042938232]\n",
      "[0.7833197712898254, -0.9056793451309204]\n",
      "[-1.4807558059692383, -1.1720070838928223]\n",
      "[0.14968401193618774, -1.0863206386566162]\n",
      "[-0.9350507855415344, -1.5429147481918335]\n",
      "[0.3343626856803894, -1.5778270959854126]\n",
      "[0.848834216594696, -0.96183180809021]\n",
      "[-0.9952486157417297, -0.9046791791915894]\n",
      "[0.02829909324645996, -1.4988892078399658]\n",
      "[-0.809470534324646, -1.443652868270874]\n",
      "[-1.9501681327819824, -0.8346841335296631]\n",
      "[1.6152353286743164, -1.6187149286270142]\n",
      "[-0.8398863673210144, -1.4513639211654663]\n",
      "[-0.0077811479568481445, -0.908306360244751]\n",
      "[0.8436720967292786, -0.9574073553085327]\n",
      "[-1.0479404926300049, -1.6564209461212158]\n",
      "[1.0847578048706055, -1.1640421152114868]\n",
      "[0.5635316967964172, -1.8215537071228027]\n",
      "[-1.0203301906585693, -0.8692618608474731]\n",
      "[-1.4246771335601807, -1.4987212419509888]\n",
      "[-1.276479959487915, -1.3959283828735352]\n",
      "[0.6727098822593689, -0.8108755350112915]\n",
      "[2.186417579650879, -2.1082756519317627]\n",
      "[0.35818374156951904, -1.1496772766113281]\n",
      "[1.035963773727417, -1.1692441701889038]\n",
      "[0.2885752320289612, -1.044682264328003]\n",
      "[1.4425058364868164, -1.47164785861969]\n",
      "[-0.600386381149292, -0.6216816902160645]\n",
      "[-1.579313039779663, -0.6003051996231079]\n",
      "[-0.32066938281059265, -0.815065860748291]\n",
      "[1.6236331462860107, -1.8007556200027466]\n",
      "[1.0787134170532227, -1.2022658586502075]\n",
      "[1.0099806785583496, -2.2008800506591797]\n",
      "[-0.5681539177894592, -0.7645245790481567]\n",
      "[-1.5182178020477295, -0.6463496685028076]\n",
      "[1.109147548675537, -1.258890986442566]\n",
      "[1.1163976192474365, -1.25013267993927]\n",
      "[1.3982350826263428, -1.432723879814148]\n",
      "[0.6146837472915649, -1.3772327899932861]\n",
      "[1.6793529987335205, -1.6736702919006348]\n",
      "[1.2898974418640137, -1.3398674726486206]\n",
      "[-0.23342902958393097, -0.9711477756500244]\n",
      "[-1.2974936962127686, -0.9998878240585327]\n",
      "[0.8962242007255554, -1.3390631675720215]\n",
      "[0.7974284291267395, -1.11797297000885]\n",
      "[1.5328786373138428, -1.5481268167495728]\n",
      "[1.022826910018921, -1.323290467262268]\n",
      "[-0.06693166494369507, -1.9749348163604736]\n",
      "[0.20697027444839478, -1.0820419788360596]\n",
      "[1.7836456298828125, -1.763059377670288]\n",
      "[0.3739675283432007, -1.2150901556015015]\n",
      "[1.088735818862915, -1.1972719430923462]\n",
      "[0.8843916058540344, -1.3409887552261353]\n",
      "[1.8017215728759766, -1.7785524129867554]\n",
      "[-0.7452589869499207, -1.170515537261963]\n",
      "[1.8660101890563965, -1.8336541652679443]\n",
      "[0.17622911930084229, -0.9625179767608643]\n",
      "[0.33879369497299194, -1.0663048028945923]\n",
      "[-0.27965518832206726, -0.8222261667251587]\n",
      "[1.2715544700622559, -1.324145793914795]\n",
      "[1.208601951599121, -1.2701891660690308]\n",
      "[-1.4097936153411865, -0.5729131698608398]\n",
      "[1.8915352821350098, -1.8555318117141724]\n",
      "[-1.7064456939697266, -0.6709754467010498]\n",
      "[0.49432694911956787, -1.2073320150375366]\n",
      "[0.06193947792053223, -1.1384564638137817]\n",
      "[-0.6197206974029541, -0.49998244643211365]\n",
      "[0.31595897674560547, -0.9794502258300781]\n",
      "[-1.298351526260376, -0.43974772095680237]\n",
      "[-1.7041049003601074, -0.669674277305603]\n",
      "[-0.31457963585853577, -0.9136594533920288]\n",
      "[1.3510677814483643, -1.3922966718673706]\n",
      "[1.5604000091552734, -1.5717153549194336]\n",
      "[1.4234516620635986, -1.7624274492263794]\n",
      "[0.3731185793876648, -1.1637464761734009]\n",
      "[-0.6514201760292053, -1.4655412435531616]\n",
      "[1.054185390472412, -1.565096139907837]\n",
      "[-1.3153057098388672, -0.5117994546890259]\n",
      "[-1.445425033569336, -0.5258797407150269]\n",
      "[1.6300272941589355, -1.8145819902420044]\n",
      "[1.8199443817138672, -1.7941712141036987]\n",
      "[1.8058857917785645, -1.7821214199066162]\n",
      "[0.8058953881263733, -1.3182040452957153]\n",
      "[-0.3764339089393616, -2.3295083045959473]\n",
      "[0.3011782169342041, -1.3475933074951172]\n",
      "[0.8813591599464417, -1.4131488800048828]\n",
      "[-0.505713164806366, -1.4987740516662598]\n",
      "[0.17839229106903076, -1.121561050415039]\n",
      "[-1.1496981382369995, -0.5478829145431519]\n",
      "[1.3931059837341309, -1.4283275604248047]\n",
      "[0.3019478917121887, -1.238585352897644]\n",
      "[-1.1254172325134277, -0.6423614025115967]\n",
      "[-0.25040119886398315, -1.9310681819915771]\n",
      "[1.785149097442627, -1.7643481492996216]\n",
      "[-1.5763673782348633, -0.5883402824401855]\n",
      "[1.1423144340515137, -1.5607562065124512]\n",
      "[0.3235034942626953, -1.1441136598587036]\n",
      "[1.7510106563568115, -1.7350879907608032]\n",
      "[0.9791952967643738, -1.1411101818084717]\n",
      "[1.4772312641143799, -1.7407885789871216]\n",
      "[0.4682430624961853, -1.0929672718048096]\n",
      "[1.3611857891082764, -1.621019959449768]\n",
      "[0.7630025744438171, -1.4873086214065552]\n",
      "[1.1329779624938965, -1.2946290969848633]\n",
      "[1.0270695686340332, -1.1746846437454224]\n",
      "[0.7051587700843811, -1.2852373123168945]\n",
      "[-1.0375696420669556, -0.6211197376251221]\n",
      "[1.1852259635925293, -1.7501076459884644]\n",
      "[1.5046517848968506, -1.5239336490631104]\n",
      "[-0.7510945200920105, -0.6976393461227417]\n",
      "[-0.10827335715293884, -0.9642981290817261]\n",
      "[1.686108112335205, -1.6794599294662476]\n",
      "[-1.1888468265533447, -0.36750856041908264]\n",
      "[0.3636144995689392, -1.111396312713623]\n",
      "[0.9806495308876038, -1.1620222330093384]\n",
      "[0.721449077129364, -1.0715510845184326]\n",
      "[1.050490140914917, -1.496954083442688]\n",
      "[1.0450918674468994, -1.871732473373413]\n",
      "[-0.7388982176780701, -0.7837154865264893]\n",
      "[1.4738492965698242, -1.4975327253341675]\n",
      "[1.5304827690124512, -1.563936471939087]\n",
      "[1.1222999095916748, -1.3562231063842773]\n",
      "[0.9083655476570129, -1.5009820461273193]\n",
      "[-2.0124151706695557, -1.7956236600875854]\n",
      "[-2.252664566040039, -1.2260931730270386]\n",
      "[0.3586770296096802, -1.5638058185577393]\n",
      "[-0.9565777778625488, -1.5629866123199463]\n",
      "[-0.394142210483551, -1.6953904628753662]\n",
      "[1.1943669319152832, -1.2579880952835083]\n",
      "[-0.6021645069122314, -1.4139549732208252]\n",
      "[-0.9764938354492188, -1.3311718702316284]\n",
      "[-2.8873281478881836, -1.409760594367981]\n",
      "[-0.5138435363769531, -1.5686887502670288]\n",
      "[2.4126455783843994, -2.302175760269165]\n",
      "[-0.46821850538253784, -1.5707712173461914]\n",
      "[-1.7161874771118164, -1.138489842414856]\n",
      "[-1.202883005142212, -1.5160623788833618]\n",
      "[-0.03686767816543579, -1.5927263498306274]\n",
      "[-0.659690260887146, -1.5933653116226196]\n",
      "[-2.8509833812713623, -1.3071993589401245]\n",
      "[-0.6988891363143921, -1.7326353788375854]\n",
      "[0.9409219622612, -1.4176132678985596]\n",
      "[-1.9044275283813477, -1.2231444120407104]\n",
      "[1.8980810642242432, -1.9318349361419678]\n",
      "[-2.3889174461364746, -1.22021484375]\n",
      "[-0.6782931089401245, -1.5654611587524414]\n",
      "[-1.3743431568145752, -1.5124002695083618]\n",
      "[-2.0679969787597656, -1.3773425817489624]\n",
      "[2.2674319744110107, -2.365750551223755]\n",
      "[1.009171485900879, -1.099256992340088]\n",
      "[-1.8006272315979004, -1.6413471698760986]\n",
      "[-2.6985387802124023, -1.2224586009979248]\n",
      "[0.9534457325935364, -1.4074835777282715]\n",
      "[-2.363093376159668, -1.300412654876709]\n",
      "[0.7337988018989563, -1.6499886512756348]\n",
      "[2.262795925140381, -2.1737396717071533]\n",
      "[-0.3295869827270508, -1.7346136569976807]\n",
      "[-0.309905081987381, -1.5842764377593994]\n",
      "[1.0970451831817627, -1.1745736598968506]\n",
      "[1.0616717338562012, -1.1442550420761108]\n",
      "[0.6971494555473328, -1.5256870985031128]\n",
      "[0.6856345534324646, -1.5119205713272095]\n",
      "[-1.0349631309509277, -1.3620446920394897]\n",
      "[-1.487513542175293, -1.7011573314666748]\n",
      "[-0.05996900796890259, -1.4440964460372925]\n",
      "[-2.470046043395996, -1.0954443216323853]\n",
      "[0.9625517725944519, -1.0592992305755615]\n",
      "[0.6877098679542542, -1.442197561264038]\n",
      "[-1.1533054113388062, -1.3775123357772827]\n",
      "[1.5783286094665527, -1.5870822668075562]\n",
      "[-0.09763443470001221, -1.5779829025268555]\n",
      "[-1.1390926837921143, -1.5002294778823853]\n",
      "[0.6859182715415955, -1.557926893234253]\n",
      "[1.2500388622283936, -1.369403600692749]\n",
      "[1.1324000358581543, -1.2048763036727905]\n",
      "[0.9806111454963684, -1.0747778415679932]\n",
      "[2.258538007736206, -2.271881341934204]\n",
      "[-2.464195728302002, -1.3253533840179443]\n",
      "[-0.827540397644043, -1.3147447109222412]\n",
      "[-0.608604907989502, -1.4057486057281494]\n",
      "[1.2289090156555176, -1.287594199180603]\n",
      "[-1.7273623943328857, -1.595000982284546]\n",
      "[-0.2581135630607605, -1.7359004020690918]\n",
      "[-2.108769655227661, -1.3756728172302246]\n",
      "[2.438343048095703, -2.811565637588501]\n",
      "[1.2884747982025146, -1.3386480808258057]\n",
      "[-2.3859550952911377, -1.4080957174301147]\n",
      "[2.017592430114746, -2.060276508331299]\n",
      "[-1.8697590827941895, -1.3481842279434204]\n",
      "[-0.34180617332458496, -1.5003149509429932]\n",
      "[0.28363853693008423, -1.6197762489318848]\n",
      "[-2.3515141010284424, -1.2532706260681152]\n",
      "[-1.016064167022705, -1.343645453453064]\n",
      "[2.3519842624664307, -2.459249496459961]\n",
      "[-0.8819753527641296, -1.7416808605194092]\n",
      "[1.1423144340515137, -1.5607562065124512]\n",
      "[-3.0365679264068604, -1.4103617668151855]\n",
      "[-1.914703130722046, -1.336735725402832]\n",
      "[-0.9371401071548462, -1.6770086288452148]\n",
      "[-0.02448311448097229, -1.598664402961731]\n",
      "[1.167980432510376, -1.2353723049163818]\n",
      "[1.1707119941711426, -1.2377135753631592]\n",
      "[-2.8743631839752197, -1.3201955556869507]\n",
      "[1.0319526195526123, -1.118782639503479]\n",
      "[-1.0367944240570068, -1.6131998300552368]\n",
      "[0.40428149700164795, -1.5391494035720825]\n",
      "[-2.3442249298095703, -1.2444947957992554]\n",
      "[-0.5557641983032227, -1.4503259658813477]\n",
      "[1.2975738048553467, -1.3464468717575073]\n",
      "[0.27334511280059814, -1.6421127319335938]\n",
      "[-0.755862832069397, -1.407326102256775]\n",
      "[-1.8446016311645508, -1.2537258863449097]\n",
      "[-0.1866346299648285, -1.698014497756958]\n",
      "[1.3355226516723633, -1.3789730072021484]\n",
      "[-1.7345170974731445, -1.3412750959396362]\n",
      "[0.7092418074607849, -1.5053365230560303]\n",
      "[1.063840389251709, -1.3200958967208862]\n",
      "[-2.8384854793548584, -1.300251841545105]\n",
      "[1.685797929763794, -1.6791940927505493]\n",
      "[-2.2313883304595947, -1.0910195112228394]\n",
      "[0.3768768906593323, -1.6467666625976562]\n",
      "[-2.5666017532348633, -1.3566356897354126]\n",
      "[-2.1285321712493896, -1.3126205205917358]\n",
      "[1.0077383518218994, -1.0980287790298462]\n",
      "[-0.5374224781990051, -2.0083184242248535]\n",
      "[-2.4981398582458496, -1.1110610961914062]\n",
      "[-0.5270562171936035, -1.1000341176986694]\n",
      "[-1.8818085193634033, -1.0562679767608643]\n",
      "[-0.8999199867248535, -1.3233274221420288]\n",
      "[0.7594656348228455, -0.9501204490661621]\n",
      "[-1.5047366619110107, -0.9205915927886963]\n",
      "[-1.9479482173919678, -0.805221438407898]\n",
      "[-1.370779275894165, -1.6535382270812988]\n",
      "[-1.254848599433899, -1.1236445903778076]\n",
      "[1.8426072597503662, -1.9923624992370605]\n",
      "[-1.4665143489837646, -1.038291573524475]\n",
      "[-2.0101139545440674, -0.9061957597732544]\n",
      "[-1.7281973361968994, -1.13083815574646]\n",
      "[-0.774653434753418, -1.162389874458313]\n",
      "[-1.058955192565918, -1.244574785232544]\n",
      "[-2.2095582485198975, -1.1473442316055298]\n",
      "[-0.6980805397033691, -1.4990631341934204]\n",
      "[0.08333137631416321, -0.979395866394043]\n",
      "[-2.2899556159973145, -0.9953359365463257]\n",
      "[0.8463509678840637, -1.4229710102081299]\n",
      "[-2.3590610027313232, -1.0337501764297485]\n",
      "[-1.3702325820922852, -1.1310789585113525]\n",
      "[-1.2309527397155762, -1.3219842910766602]\n",
      "[-2.281951904296875, -1.0838385820388794]\n",
      "[1.3715183734893799, -1.9127720594406128]\n",
      "[0.440671443939209, -0.6438513994216919]\n",
      "[-1.1147003173828125, -1.6103416681289673]\n",
      "[-2.452440023422241, -1.0856575965881348]\n",
      "[-0.20347705483436584, -0.9510543346405029]\n",
      "[-2.424267053604126, -1.069996953010559]\n",
      "[0.4843263626098633, -1.3477917909622192]\n",
      "[1.4685533046722412, -1.716212272644043]\n",
      "[-0.24509868025779724, -1.5236198902130127]\n",
      "[-1.2794411182403564, -1.0676947832107544]\n",
      "[0.23013067245483398, -0.6500939130783081]\n",
      "[0.5512717366218567, -0.8276417255401611]\n",
      "[-0.06995236873626709, -1.0943881273269653]\n",
      "[-0.13834166526794434, -1.0729891061782837]\n",
      "[-1.8708724975585938, -0.8912500143051147]\n",
      "[-0.8246951103210449, -1.6574275493621826]\n",
      "[-0.7086776494979858, -1.0382680892944336]\n",
      "[-2.2583401203155518, -0.9777616262435913]\n",
      "[0.7681463360786438, -0.9911890029907227]\n",
      "[-0.07868582010269165, -0.9924474954605103]\n",
      "[-1.977679967880249, -0.8968355655670166]\n",
      "[0.9272823929786682, -1.139360785484314]\n",
      "[-0.2679387927055359, -1.3219276666641235]\n",
      "[-2.1505279541015625, -0.9580892324447632]\n",
      "[-0.00860130786895752, -1.1362029314041138]\n",
      "[0.5664820075035095, -0.9611881971359253]\n",
      "[0.7862892746925354, -0.9082244634628296]\n",
      "[0.42154979705810547, -0.5956059694290161]\n",
      "[1.3790733814239502, -1.781016230583191]\n",
      "[-1.42978835105896, -1.4343791007995605]\n",
      "[-1.83341383934021, -0.7908066511154175]\n",
      "[-1.6123921871185303, -0.8815590143203735]\n",
      "[0.7202524542808533, -0.8516242504119873]\n",
      "[-1.1440796852111816, -1.5330183506011963]\n",
      "[-0.3060911297798157, -1.4823888540267944]\n",
      "[-2.4635820388793945, -1.0918512344360352]\n",
      "[1.3056986331939697, -2.4183030128479004]\n",
      "[0.9378873705863953, -1.0836257934570312]\n",
      "[-2.3064918518066406, -1.2048163414001465]\n",
      "[1.0986127853393555, -1.5760633945465088]\n",
      "[-2.4643208980560303, -1.0922619104385376]\n",
      "[-1.3216674327850342, -0.968056321144104]\n",
      "[-0.4027178883552551, -1.2100725173950195]\n",
      "[-2.312238931655884, -1.0416021347045898]\n",
      "[-1.8945462703704834, -0.8602993488311768]\n",
      "[1.427847146987915, -1.9922590255737305]\n",
      "[-0.7154529094696045, -1.5533397197723389]\n",
      "[0.3235035538673401, -1.1441136598587036]\n",
      "[-1.914703130722046, -1.3367359638214111]\n",
      "[-2.519613742828369, -1.1229979991912842]\n",
      "[-0.7714748382568359, -1.4879437685012817]\n",
      "[-0.8993738293647766, -1.1218763589859009]\n",
      "[0.6666930317878723, -0.8057184219360352]\n",
      "[0.20019620656967163, -0.6853278875350952]\n",
      "[-1.6576788425445557, -1.4060299396514893]\n",
      "[0.8384122252464294, -0.9528990983963013]\n",
      "[-1.5568318367004395, -1.2322006225585938]\n",
      "[-0.4805826246738434, -1.0772762298583984]\n",
      "[-2.4491305351257324, -1.083817958831787]\n",
      "[-1.5519108772277832, -0.9191031455993652]\n",
      "[1.1846308708190918, -1.2496434450149536]\n",
      "[-0.3178389072418213, -1.2523910999298096]\n",
      "[-1.742586612701416, -0.8710132837295532]\n",
      "[-2.431432008743286, -1.0739797353744507]\n",
      "[-0.4968760907649994, -1.3833452463150024]\n",
      "[0.720086395740509, -0.8514819145202637]\n",
      "[-2.3931498527526855, -1.0526994466781616]\n",
      "[-0.27803856134414673, -1.0501500368118286]\n",
      "[-0.38146594166755676, -0.8579230308532715]\n",
      "[-2.193881034851074, -1.228337287902832]\n",
      "[1.4989910125732422, -1.519081711769104]\n",
      "[-2.306331157684326, -1.004438877105713]\n",
      "[-0.19629959762096405, -1.2504929304122925]\n",
      "[-1.8454844951629639, -1.3482229709625244]\n",
      "[-2.2860605716705322, -1.0367106199264526]\n",
      "[0.6811143755912781, -0.8180789947509766]\n",
      "[-0.04638221859931946, -2.140921115875244]\n",
      "[-1.0892295837402344, -1.4021679162979126]\n",
      "[0.021384447813034058, -1.0635498762130737]\n",
      "[-0.7607568502426147, -1.3141586780548096]\n",
      "[-1.5346989631652832, -0.9680308103561401]\n",
      "[1.0246548652648926, -1.112527847290039]\n",
      "[0.964796245098114, -1.696669578552246]\n",
      "[-0.13209211826324463, -1.3995440006256104]\n",
      "[-0.6815525889396667, -1.8761584758758545]\n",
      "[-1.061556100845337, -1.0864036083221436]\n",
      "[1.6915991306304932, -1.6841663122177124]\n",
      "[0.06435024738311768, -1.4739584922790527]\n",
      "[1.2949035167694092, -1.8029950857162476]\n",
      "[-1.5969882011413574, -1.1082571744918823]\n",
      "[-1.0133919715881348, -0.9067211151123047]\n",
      "[-2.0244553089141846, -0.8479011058807373]\n",
      "[-0.8496012687683105, -1.643515944480896]\n",
      "[-2.472552537918091, -1.0968376398086548]\n",
      "[0.060896217823028564, -0.7902929782867432]\n",
      "[-0.11817620694637299, -1.6388306617736816]\n",
      "[1.8579273223876953, -1.8903526067733765]\n",
      "[-0.7091318964958191, -1.5576032400131226]\n",
      "[-1.407106876373291, -1.0145734548568726]\n",
      "[-2.5147767066955566, -1.1203092336654663]\n",
      "[-1.474766731262207, -1.3120472431182861]\n",
      "[1.7977490425109863, -1.9547226428985596]\n",
      "[0.7530698180198669, -0.8797520399093628]\n",
      "[-2.421389579772949, -1.1709140539169312]\n",
      "[-1.2751600742340088, -1.4169740676879883]\n",
      "[0.25123822689056396, -0.8346956968307495]\n",
      "[-1.373842716217041, -1.3893815279006958]\n",
      "[-1.4902722835540771, -0.7137452363967896]\n",
      "[2.2305004596710205, -2.146059036254883]\n",
      "[-2.3203883171081543, -1.012252926826477]\n",
      "[-0.24036139249801636, -1.3007960319519043]\n",
      "[0.7473180890083313, -0.8748222589492798]\n",
      "[0.8904263377189636, -0.9974805116653442]\n",
      "[-0.11136859655380249, -0.8803036212921143]\n",
      "[-0.09192261099815369, -0.8873560428619385]\n",
      "[0.6430409550666809, -1.703417420387268]\n",
      "[-2.4981143474578857, -1.1110469102859497]\n",
      "[1.6744678020477295, -1.6978029012680054]\n",
      "[-0.16195236146450043, -1.669631838798523]\n",
      "[0.8506646752357483, -0.9634007215499878]\n",
      "[1.2146072387695312, -1.2761083841323853]\n",
      "[-1.2058491706848145, -1.1118392944335938]\n",
      "[2.086376905441284, -2.0225307941436768]\n",
      "[2.2171998023986816, -2.1346592903137207]\n",
      "[-0.4228321611881256, -1.4844954013824463]\n",
      "[-0.44687625765800476, -0.8306009769439697]\n",
      "[-0.1606014519929886, -0.672966718673706]\n",
      "[0.6848320364952087, -0.8212653398513794]\n",
      "[0.7368181347846985, -0.8658227920532227]\n",
      "[2.1343088150024414, -2.1289708614349365]\n",
      "[0.021040290594100952, -1.958832025527954]\n",
      "[0.4941299557685852, -1.5182228088378906]\n",
      "[0.5827556848526001, -1.576210856437683]\n",
      "[0.5486598014831543, -0.7045520544052124]\n",
      "[-2.2776341438293457, -1.1377325057983398]\n",
      "[-2.2188539505004883, -0.9558120965957642]\n",
      "[-1.105702519416809, -1.4596116542816162]\n",
      "[1.9342236518859863, -2.4187185764312744]\n",
      "[0.837386429309845, -0.9520199298858643]\n",
      "[-1.404083013534546, -1.4620336294174194]\n",
      "[1.753227949142456, -1.8081094026565552]\n",
      "[-0.6013390421867371, -1.5499792098999023]\n",
      "[0.2969472408294678, -1.4802513122558594]\n",
      "[-0.8410406708717346, -0.8905917406082153]\n",
      "[-0.39888185262680054, -1.7238500118255615]\n",
      "[0.5335429906845093, -1.6417077779769897]\n",
      "[1.976780652999878, -2.122662305831909]\n",
      "[-2.6638143062591553, -1.2031559944152832]\n",
      "[1.7510111331939697, -1.7350884675979614]\n",
      "[-0.9371401071548462, -1.6770086288452148]\n",
      "[-0.771475076675415, -1.4879435300827026]\n",
      "[-2.6547367572784424, -1.1981099843978882]\n",
      "[-0.3337154984474182, -1.151906132698059]\n",
      "[0.510621964931488, -0.6719498634338379]\n",
      "[0.7936620116233826, -0.9145437479019165]\n",
      "[-1.4355876445770264, -1.506665587425232]\n",
      "[0.7593186497688293, -0.8851079940795898]\n",
      "[-1.5541625022888184, -1.1203253269195557]\n",
      "[0.07380688190460205, -1.0349959135055542]\n",
      "[-0.991570234298706, -1.4760702848434448]\n",
      "[0.2800792455673218, -1.5142813920974731]\n",
      "[0.8204790949821472, -0.9375286102294922]\n",
      "[-1.072251319885254, -0.8538684844970703]\n",
      "[-0.027426064014434814, -1.4340639114379883]\n",
      "[-0.8666622638702393, -1.3755643367767334]\n",
      "[-1.979968786239624, -0.8230210542678833]\n",
      "[1.619257926940918, -1.6221625804901123]\n",
      "[-0.8971598148345947, -1.3861298561096191]\n",
      "[-0.08269995450973511, -0.8635556697845459]\n",
      "[0.7849611639976501, -0.9070862531661987]\n",
      "[-1.1044203042984009, -1.5913772583007812]\n",
      "[1.052826166152954, -1.1366734504699707]\n",
      "[0.5193049907684326, -1.7547298669815063]\n",
      "[-1.0979828834533691, -0.8188306093215942]\n",
      "[-1.483605146408081, -1.437733769416809]\n",
      "[-1.3343671560287476, -1.3305375576019287]\n",
      "[0.647098958492279, -0.7889243364334106]\n",
      "[1.2915127277374268, -2.1475303173065186]\n",
      "[-0.9485204815864563, -1.1081098318099976]\n",
      "[-1.3535001277923584, -0.6011819839477539]\n",
      "[-1.300933599472046, -0.9029015302658081]\n",
      "[-1.328449010848999, -0.841161847114563]\n",
      "[0.21023374795913696, -0.7709310054779053]\n",
      "[0.13497281074523926, -1.1197212934494019]\n",
      "[-0.7238631248474121, -0.8920248746871948]\n",
      "[0.48159515857696533, -1.8578312397003174]\n",
      "[-1.5556550025939941, -0.7059866189956665]\n",
      "[0.4820166826248169, -1.5305054187774658]\n",
      "[-0.9294646382331848, -0.9142833948135376]\n",
      "[0.6059945821762085, -1.3050785064697266]\n",
      "[-1.3490962982177734, -0.8743250370025635]\n",
      "[-1.6867618560791016, -0.6817256212234497]\n",
      "[-1.5576562881469727, -0.7808375358581543]\n",
      "[-0.2362803965806961, -1.474759817123413]\n",
      "[-0.4644548296928406, -1.0867180824279785]\n",
      "[-1.4117822647094727, -0.38696181774139404]\n",
      "[-0.36498385667800903, -1.2021435499191284]\n",
      "[-0.3224559426307678, -1.196081519126892]\n",
      "[-0.42626917362213135, -1.301085114479065]\n",
      "[-1.6771893501281738, -0.7538223266601562]\n",
      "[-0.6453450918197632, -1.0821127891540527]\n",
      "[-0.8871197700500488, -1.162900447845459]\n",
      "[-0.09295105934143066, -1.5888676643371582]\n",
      "[-0.10692781209945679, -0.6004127264022827]\n",
      "[-0.03816518187522888, -1.509093999862671]\n",
      "[-0.7300277948379517, -1.2371420860290527]\n",
      "[-1.4607279300689697, -0.40640032291412354]\n",
      "[-0.8061833381652832, -1.2283551692962646]\n",
      "[-0.05744802951812744, -0.8989015817642212]\n",
      "[0.500217080116272, -1.4948595762252808]\n",
      "[-0.0832226574420929, -1.0985076427459717]\n",
      "[-1.28792142868042, -0.7853670120239258]\n",
      "[-0.3016852140426636, -0.4976121485233307]\n",
      "[0.26535487174987793, -0.7944862842559814]\n",
      "[-1.3579869270324707, -0.48764660954475403]\n",
      "[-1.4071857929229736, -0.5135887861251831]\n",
      "[-0.02859559655189514, -1.1522843837738037]\n",
      "[0.1391165852546692, -1.4954661130905151]\n",
      "[0.33607006072998047, -1.1211129426956177]\n",
      "[0.07603877782821655, -1.370318055152893]\n",
      "[0.06979334354400635, -0.8048522472381592]\n",
      "[0.04774576425552368, -0.8258212804794312]\n",
      "[-1.7037551403045654, -0.6694799661636353]\n",
      "[1.3447411060333252, -1.4303213357925415]\n",
      "[0.9832414984703064, -1.4963486194610596]\n",
      "[-1.212855577468872, -0.9373289346694946]\n",
      "[-1.3771274089813232, -0.5419285297393799]\n",
      "[-0.794777512550354, -0.4262971580028534]\n",
      "[0.26381176710128784, -0.6833041906356812]\n",
      "[-0.12116384506225586, -0.5895448923110962]\n",
      "[0.17620956897735596, -1.5516968965530396]\n",
      "[0.5339248180389404, -1.7463412284851074]\n",
      "[-0.21531252562999725, -0.9822198152542114]\n",
      "[-0.22757671773433685, -0.9921126365661621]\n",
      "[0.24041181802749634, -0.4963352382183075]\n",
      "[-0.07061544060707092, -1.4221330881118774]\n",
      "[-0.27184033393859863, -1.0200477838516235]\n",
      "[-0.835151731967926, -1.215673565864563]\n",
      "[-0.13273149728775024, -2.084878921508789]\n",
      "[-0.4867851734161377, -0.6456421613693237]\n",
      "[-0.6925482749938965, -1.3482835292816162]\n",
      "[-0.25433236360549927, -1.2823209762573242]\n",
      "[-0.8245437741279602, -1.1522750854492188]\n",
      "[-0.8391552567481995, -0.8234635591506958]\n",
      "[-1.4324212074279785, -0.6748086214065552]\n",
      "[-0.3083195686340332, -1.3978140354156494]\n",
      "[0.013649165630340576, -1.1419354677200317]\n",
      "[0.029652655124664307, -1.69495689868927]\n",
      "[-0.2587173581123352, -1.2034850120544434]\n",
      "[0.9791952967643738, -1.1411101818084717]\n",
      "[-0.024482935667037964, -1.59866464138031]\n",
      "[-0.8993735313415527, -1.1218763589859009]\n",
      "[-0.33371537923812866, -1.151906132698059]\n",
      "[-1.5032427310943604, -0.6187922954559326]\n",
      "[0.12127292156219482, -0.5160524845123291]\n",
      "[-0.4630630314350128, -0.4025396406650543]\n",
      "[0.040540069341659546, -1.5942243337631226]\n",
      "[0.057944029569625854, -0.7479640245437622]\n",
      "[-1.3084871768951416, -0.9251631498336792]\n",
      "[-1.3826258182525635, -0.5636013746261597]\n",
      "[-0.7094417214393616, -1.2242921590805054]\n",
      "[-0.6109543442726135, -0.9171332120895386]\n",
      "[0.28568917512893677, -0.8954685926437378]\n",
      "[-1.377039909362793, -0.6925538778305054]\n",
      "[-0.9931147694587708, -0.8188283443450928]\n",
      "[-1.1537833213806152, -0.9525102376937866]\n",
      "[-0.9642615914344788, -0.8525292873382568]\n",
      "[0.05783948302268982, -0.7384991645812988]\n",
      "[-1.0806705951690674, -1.006934642791748]\n",
      "[-1.570887565612793, -0.47937169671058655]\n",
      "[-1.23390531539917, -0.36558231711387634]\n",
      "[-0.33947017788887024, -1.4785772562026978]\n",
      "[0.16466295719146729, -1.0395147800445557]\n",
      "[-0.008479595184326172, -1.2597622871398926]\n",
      "[-1.4685180187225342, -0.6760594844818115]\n",
      "[-0.2664862275123596, -1.4560667276382446]\n",
      "[-0.7587810754776001, -1.172300100326538]\n",
      "[-0.11650258302688599, -0.6236114501953125]\n",
      "[2.0231244564056396, -1.9683170318603516]\n",
      "[0.640468418598175, -0.7832412719726562]\n",
      "[-0.024831146001815796, -0.49174997210502625]\n",
      "[0.29487115144729614, -0.502642035484314]\n",
      "[-0.11593510210514069, -0.3832811713218689]\n",
      "[0.12372803688049316, -1.1196393966674805]\n",
      "[1.5326018333435059, -1.5731475353240967]\n",
      "[1.1069977283477783, -1.1831040382385254]\n",
      "[1.5131127834320068, -1.5311856269836426]\n",
      "[-0.1324000358581543, -0.4039738178253174]\n",
      "[-1.4175794124603271, -1.0169082880020142]\n",
      "[0.5921218395233154, -0.7418034076690674]\n",
      "[1.921910285949707, -1.9164215326309204]\n",
      "[0.2329559326171875, -0.4884563386440277]\n",
      "[-0.8561756610870361, -0.3200133740901947]\n",
      "[-0.15843117237091064, -0.43346479535102844]\n",
      "[1.0679411888122559, -1.1496286392211914]\n",
      "[0.34339553117752075, -0.5662455558776855]\n",
      "[-0.51728355884552, -0.32274186611175537]\n",
      "[1.0668082237243652, -1.1486575603485107]\n",
      "[-0.9142818450927734, -1.3780637979507446]\n",
      "[0.8796785473823547, -0.9882684946060181]\n",
      "[-0.37477514147758484, -0.4048406779766083]\n",
      "[0.462979793548584, -0.6311156749725342]\n",
      "[0.6106960773468018, -0.7577234506607056]\n",
      "[-1.502274751663208, -1.2842607498168945]\n",
      "[-0.41695040464401245, -0.7752376794815063]\n",
      "[0.9479716420173645, -1.0468025207519531]\n",
      "[0.7785634398460388, -0.9016026258468628]\n",
      "[-0.5426058173179626, -0.3435799181461334]\n",
      "[0.7187061905860901, -0.8502990007400513]\n",
      "[-0.11083680391311646, -0.49746033549308777]\n",
      "[-0.05937260389328003, -1.5892163515090942]\n",
      "[0.4562368392944336, -0.6253362894058228]\n",
      "[0.1922479271888733, -0.5027998685836792]\n",
      "[-0.39046281576156616, -0.7233031988143921]\n",
      "[0.035479962825775146, -1.0165547132492065]\n",
      "[-0.3486538231372833, -0.337100088596344]\n",
      "[-0.39839863777160645, -0.32775774598121643]\n",
      "[1.4869232177734375, -1.508738398551941]\n",
      "[0.9482473731040955, -1.0470389127731323]\n",
      "[1.5294229984283447, -1.7495839595794678]\n",
      "[1.2904613018035889, -1.340350866317749]\n",
      "[-0.4688120484352112, -0.8927866220474243]\n",
      "[0.7754494547843933, -1.1534721851348877]\n",
      "[0.01664745807647705, -0.4877413809299469]\n",
      "[1.7403473854064941, -2.10142183303833]\n",
      "[2.059262752532959, -2.24364972114563]\n",
      "[0.5089967250823975, -0.670556902885437]\n",
      "[-0.4355960786342621, -0.3012271225452423]\n",
      "[-0.6368216276168823, -0.29779544472694397]\n",
      "[-1.4052188396453857, -0.3584662973880768]\n",
      "[-0.6270052790641785, -0.6822059154510498]\n",
      "[-0.5784465670585632, -1.5585757493972778]\n",
      "[1.3983650207519531, -1.4328352212905884]\n",
      "[1.2932586669921875, -1.4059244394302368]\n",
      "[1.315035104751587, -1.3629266023635864]\n",
      "[-1.265295147895813, -0.3070189654827118]\n",
      "[0.8773061633110046, -0.9862351417541504]\n",
      "[0.2599524259567261, -0.5252348184585571]\n",
      "[0.708375871181488, -0.8414448499679565]\n",
      "[-1.544792890548706, -1.8015193939208984]\n",
      "[-1.2837023735046387, -0.620956301689148]\n",
      "[0.804993212223053, -0.9242556095123291]\n",
      "[-1.1792917251586914, -1.2251149415969849]\n",
      "[0.7241893410682678, -0.8549985885620117]\n",
      "[0.8338857293128967, -1.0072966814041138]\n",
      "[-0.5490187406539917, -0.29793164134025574]\n",
      "[1.014448642730713, -1.1037800312042236]\n",
      "[1.4687354564666748, -1.493149757385254]\n",
      "[-1.2166974544525146, -1.4648219347000122]\n",
      "[0.5502721071243286, -0.7059340476989746]\n",
      "[1.4772312641143799, -1.7407885789871216]\n",
      "[1.167980432510376, -1.2353723049163818]\n",
      "[0.6666931509971619, -0.8057185411453247]\n",
      "[0.510621964931488, -0.6719498634338379]\n",
      "[0.12127304077148438, -0.5160524845123291]\n",
      "[-1.37404465675354, -0.230084627866745]\n",
      "[-0.23333078622817993, -0.7401355504989624]\n",
      "[1.225597858428955, -1.2847561836242676]\n",
      "[-1.1548378467559814, -0.5772558450698853]\n",
      "[0.21694844961166382, -0.4721926748752594]\n",
      "[-0.056785792112350464, -0.48619362711906433]\n",
      "[0.7688316702842712, -0.8932615518569946]\n",
      "[1.1088464260101318, -1.1846885681152344]\n",
      "[-1.5782387256622314, -0.4589989483356476]\n",
      "[-0.5031054615974426, -0.30588653683662415]\n",
      "[0.8762159943580627, -1.0171842575073242]\n",
      "[0.5428736805915833, -0.6995928287506104]\n",
      "[-0.07930579781532288, -0.42084425687789917]\n",
      "[-0.008487284183502197, -1.2138447761535645]\n",
      "[0.5533931255340576, -0.7086089849472046]\n",
      "[-0.6179769039154053, -0.3031117022037506]\n",
      "[-0.31215232610702515, -0.5543904304504395]\n",
      "[1.0165152549743652, -1.1055513620376587]\n",
      "[-1.8168091773986816, -0.5077838897705078]\n",
      "[1.680767297744751, -1.674882411956787]\n",
      "[-0.806807279586792, -0.29145264625549316]\n",
      "[0.9407157301902771, -1.0405834913253784]\n",
      "[0.6730958819389343, -0.8112063407897949]\n",
      "[-1.3944344520568848, -0.41223645210266113]\n",
      "[1.9979450702667236, -1.9467356204986572]\n",
      "[0.2654631733894348, -0.7170044183731079]\n",
      "[-0.3092416822910309, -0.39565205574035645]\n",
      "[-0.6095914244651794, -0.4513624608516693]\n",
      "[0.15233314037322998, -0.5132356882095337]\n",
      "[-1.1007275581359863, -0.5890626907348633]\n",
      "[0.47133785486221313, -0.9920737743377686]\n",
      "[0.12995317578315735, -0.6986192464828491]\n",
      "[1.5281755924224854, -1.5440958738327026]\n",
      "[-0.28690946102142334, -0.46448156237602234]\n",
      "[-0.7095214128494263, -1.3276909589767456]\n",
      "[-1.0892505645751953, -0.3887694776058197]\n",
      "[1.182495355606079, -1.2579010725021362]\n",
      "[0.10271671414375305, -0.5567067861557007]\n",
      "[-0.5835290551185608, -0.44230887293815613]\n",
      "[0.17459845542907715, -0.5691790580749512]\n",
      "[1.061201810836792, -1.1438522338867188]\n",
      "[0.707851231098175, -0.840995192527771]\n",
      "[-0.23887526988983154, -0.39695149660110474]\n",
      "[0.7372190356254578, -0.9163655042648315]\n",
      "[-1.8565173149108887, -0.9686042070388794]\n",
      "[0.8127643465995789, -0.9309163093566895]\n",
      "[-0.4856920540332794, -0.477693110704422]\n",
      "[0.6730393767356873, -0.8111579418182373]\n",
      "[0.5862014293670654, -0.7367290258407593]\n",
      "[-1.5214378833770752, -1.294358491897583]\n",
      "[-1.5961744785308838, -0.2875280976295471]\n",
      "[1.1155741214752197, -1.1904548406600952]\n",
      "[0.6728479266166687, -0.8604623079299927]\n",
      "[-0.637986421585083, -0.33139172196388245]\n",
      "[0.6981071829795837, -0.8326435089111328]\n",
      "[0.7024665474891663, -0.836379885673523]\n",
      "[-1.0907447338104248, -1.1378133296966553]\n",
      "[0.8391801714897156, -0.9535572528839111]\n",
      "[-0.9849715828895569, -0.37110233306884766]\n",
      "[-1.4299805164337158, -0.2943324148654938]\n",
      "[-1.3230031728744507, -0.4461633861064911]\n",
      "[-0.11375509202480316, -0.3676401376724243]\n",
      "[-0.2505311965942383, -0.36746323108673096]\n",
      "[0.5348026156425476, -0.9623016119003296]\n",
      "[1.1431338787078857, -1.2140763998031616]\n",
      "[0.6634532809257507, -1.146439790725708]\n",
      "[1.0467476844787598, -1.131463646888733]\n",
      "[-1.8564379215240479, -0.32085561752319336]\n",
      "[-0.3323523998260498, -0.6466084718704224]\n",
      "[-0.48050907254219055, -0.5000401735305786]\n",
      "[0.7604338526725769, -1.4980919361114502]\n",
      "[1.175217628479004, -1.5469404458999634]\n",
      "[-0.7730217576026917, -0.46363452076911926]\n",
      "[-0.04477822780609131, -0.40292981266975403]\n",
      "[0.17409873008728027, -0.5881128311157227]\n",
      "[-0.412691593170166, -0.7585666179656982]\n",
      "[-1.565699577331543, -0.29134050011634827]\n",
      "[-1.4386353492736816, -1.1855183839797974]\n",
      "[1.3884620666503906, -1.4243474006652832]\n",
      "[0.17796629667282104, -0.824641227722168]\n",
      "[0.025328129529953003, -0.7582322359085083]\n",
      "[-0.07583039999008179, -0.766796350479126]\n",
      "[1.0142877101898193, -1.1036421060562134]\n",
      "[0.7073130011558533, -0.8405338525772095]\n",
      "[0.6149749159812927, -0.784732460975647]\n",
      "[-1.5273208618164062, -1.8364663124084473]\n",
      "[-1.3882970809936523, -0.5894253253936768]\n",
      "[0.8236573338508606, -0.9402527809143066]\n",
      "[-1.8660180568695068, -0.9389306306838989]\n",
      "[0.23557323217391968, -0.7147071361541748]\n",
      "[-1.02811598777771, -0.43478408455848694]\n",
      "[-0.1536581665277481, -0.44298604130744934]\n",
      "[0.926123321056366, -1.028076410293579]\n",
      "[0.5365614295005798, -0.9665542840957642]\n",
      "[-1.5061092376708984, -1.354448914527893]\n",
      "[0.8350754380226135, -0.9500391483306885]\n",
      "[0.46824318170547485, -1.0929672718048096]\n",
      "[1.1707119941711426, -1.2377135753631592]\n",
      "[0.20019620656967163, -0.6853278875350952]\n",
      "[0.7936620116233826, -0.9145437479019165]\n",
      "[-0.4630630314350128, -0.4025396406650543]\n",
      "[-0.23333078622817993, -0.7401355504989624]\n",
      "[-1.3848450183868408, -0.2332700788974762]\n",
      "[1.2408864498138428, -1.2978602647781372]\n",
      "[-1.2786903381347656, -0.5278232097625732]\n",
      "[0.12530922889709473, -0.5437736511230469]\n",
      "[-0.26329153776168823, -0.4125933051109314]\n",
      "[0.7054161429405212, -0.8389080762863159]\n",
      "[-0.3626819849014282, -0.5988529920578003]\n",
      "[-0.7566752433776855, -0.8010475635528564]\n",
      "[0.08634284138679504, -0.4794955551624298]\n",
      "[-0.5659219622612, -0.5123676061630249]\n",
      "[-0.23656368255615234, -0.5793929100036621]\n",
      "[0.4719330668449402, -0.6387895345687866]\n",
      "[-1.1790707111358643, -0.7029273509979248]\n",
      "[0.03445994853973389, -0.6051536798477173]\n",
      "[-0.6610823273658752, -0.33630049228668213]\n",
      "[-0.8898763656616211, -0.31518590450286865]\n",
      "[1.0420584678649902, -1.1274443864822388]\n",
      "[-0.7845853567123413, -0.9534944295883179]\n",
      "[1.127737045288086, -1.234136939048767]\n",
      "[-0.0027066171169281006, -0.494083970785141]\n",
      "[0.9591941237449646, -1.0564213991165161]\n",
      "[0.664347231388092, -0.8037078380584717]\n",
      "[-1.0526013374328613, -0.5588572025299072]\n",
      "[-2.344243049621582, -1.6893142461776733]\n",
      "[-2.0338220596313477, -1.285588026046753]\n",
      "[0.4254268407821655, -1.5586590766906738]\n",
      "[-0.8382805585861206, -1.5790119171142578]\n",
      "[-0.5621879696846008, -1.6237385272979736]\n",
      "[1.1779820919036865, -1.243944764137268]\n",
      "[-0.2232000231742859, -1.532766342163086]\n",
      "[-0.7006193399429321, -1.406104326248169]\n",
      "[-3.005239248275757, -1.392946720123291]\n",
      "[-0.5357264280319214, -1.540355920791626]\n",
      "[2.394265651702881, -2.2864224910736084]\n",
      "[-0.2249736785888672, -1.6289986371994019]\n",
      "[-1.2431232929229736, -1.2579374313354492]\n",
      "[-1.2838994264602661, -1.4711872339248657]\n",
      "[-0.11684495210647583, -1.5444966554641724]\n",
      "[-0.8888406157493591, -1.500025749206543]\n",
      "[-2.684708833694458, -1.214770793914795]\n",
      "[-1.137115478515625, -1.5826128721237183]\n",
      "[0.9068863987922668, -1.398179531097412]\n",
      "[-1.4982032775878906, -1.3509851694107056]\n",
      "[2.1259090900421143, -2.0564138889312744]\n",
      "[-2.0619122982025146, -1.314754843711853]\n",
      "[-0.7474006414413452, -1.5206013917922974]\n",
      "[-1.8176851272583008, -1.357450246810913]\n",
      "[-2.07084059715271, -1.3606406450271606]\n",
      "[2.469595193862915, -2.4374945163726807]\n",
      "[1.0918176174163818, -1.170093059539795]\n",
      "[-2.461606979370117, -1.4234122037887573]\n",
      "[-2.6048195362091064, -1.170362114906311]\n",
      "[0.9670518040657043, -1.3991025686264038]\n",
      "[-2.286616086959839, -1.311828851699829]\n",
      "[0.31160759925842285, -1.530387282371521]\n",
      "[2.398074150085449, -2.289686679840088]\n",
      "[-0.8398533463478088, -1.570710301399231]\n",
      "[-0.16443738341331482, -1.6080007553100586]\n",
      "[1.1735551357269287, -1.2401503324508667]\n",
      "[1.1069774627685547, -1.183086633682251]\n",
      "[0.6563555598258972, -1.5083664655685425]\n",
      "[0.6668892502784729, -1.4923938512802124]\n",
      "[-0.6461125612258911, -1.4834110736846924]\n",
      "[-2.1736111640930176, -1.4766753911972046]\n",
      "[0.2955661416053772, -1.5568292140960693]\n",
      "[-2.015634536743164, -1.2352174520492554]\n",
      "[1.0204920768737793, -1.1089600324630737]\n",
      "[0.9616073966026306, -1.5039653778076172]\n",
      "[-1.1032164096832275, -1.3751749992370605]\n",
      "[1.7479510307312012, -1.7324655055999756]\n",
      "[0.3011106252670288, -1.7069816589355469]\n",
      "[-0.8972225785255432, -1.5609029531478882]\n",
      "[0.5746828317642212, -1.5204991102218628]\n",
      "[1.1077680587768555, -1.327719807624817]\n",
      "[1.201958417892456, -1.2644948959350586]\n",
      "[1.0575511455535889, -1.1407233476638794]\n",
      "[2.506570339202881, -2.382678985595703]\n",
      "[-2.1232879161834717, -1.4334478378295898]\n",
      "[-0.5103047490119934, -1.4114335775375366]\n",
      "[-0.2913169264793396, -1.501950740814209]\n",
      "[1.2808077335357666, -1.3320767879486084]\n",
      "[-2.3101766109466553, -1.4019274711608887]\n",
      "[-0.6965538263320923, -1.5951564311981201]\n",
      "[-1.955671787261963, -1.410252571105957]\n",
      "[2.650580644607544, -2.9013359546661377]\n",
      "[1.1964895725250244, -1.2598074674606323]\n",
      "[-2.358293056488037, -1.4020209312438965]\n",
      "[2.242479085922241, -2.1563260555267334]\n",
      "[-1.5685994625091553, -1.433268666267395]\n",
      "[-0.06526517868041992, -1.5735989809036255]\n",
      "[0.16951358318328857, -1.5701885223388672]\n",
      "[-1.9271838665008545, -1.3872312307357788]\n",
      "[-0.6483792662620544, -1.4573036432266235]\n",
      "[2.5790605545043945, -2.5527498722076416]\n",
      "[-1.3783390522003174, -1.5745153427124023]\n",
      "[1.3611857891082764, -1.621019959449768]\n",
      "[-2.8743631839752197, -1.3201955556869507]\n",
      "[-1.6576793193817139, -1.4060297012329102]\n",
      "[-1.4355876445770264, -1.506665587425232]\n",
      "[0.040540069341659546, -1.5942243337631226]\n",
      "[1.225597858428955, -1.2847561836242676]\n",
      "[1.2408866882324219, -1.2978603839874268]\n",
      "[-3.17230486869812, -1.4858149290084839]\n",
      "[1.0681447982788086, -1.1498030424118042]\n",
      "[-1.1346709728240967, -1.5620512962341309]\n",
      "[0.47098660469055176, -1.5342940092086792]\n",
      "[-2.1059699058532715, -1.3100634813308716]\n",
      "[-0.25201287865638733, -1.5345556735992432]\n",
      "[1.2669482231140137, -1.3201978206634521]\n",
      "[0.10332068800926208, -1.579641580581665]\n",
      "[-0.48332053422927856, -1.481404185295105]\n",
      "[-1.604752779006958, -1.3197563886642456]\n",
      "[-0.49858254194259644, -1.590425729751587]\n",
      "[1.412400722503662, -1.4448652267456055]\n",
      "[-1.5425119400024414, -1.3882226943969727]\n",
      "[0.6842470765113831, -1.4883410930633545]\n",
      "[1.1841866970062256, -1.3374649286270142]\n",
      "[-2.7058448791503906, -1.2844281196594238]\n",
      "[1.579120397567749, -1.5877606868743896]\n",
      "[-1.752920389175415, -1.2369647026062012]\n",
      "[0.17828035354614258, -1.5826761722564697]\n",
      "[-2.795255661010742, -1.2762213945388794]\n",
      "[-2.092529296875, -1.3100990056991577]\n",
      "[1.0568501949310303, -1.1401225328445435]\n",
      "[1.749521017074585, -1.7338111400604248]\n",
      "[0.6839128136634827, -0.8204774856567383]\n",
      "[-0.23573020100593567, -0.6462761163711548]\n",
      "[0.23588794469833374, -0.5701017379760742]\n",
      "[0.3739131689071655, -0.5572715997695923]\n",
      "[-0.8126523494720459, -0.8916943073272705]\n",
      "[1.084625005722046, -1.3874379396438599]\n",
      "[0.9238547682762146, -1.2255436182022095]\n",
      "[1.2784652709960938, -1.3300690650939941]\n",
      "[0.2614668011665344, -0.7079421281814575]\n",
      "[-1.7071492671966553, -1.091719627380371]\n",
      "[0.4035213589668274, -0.8640974760055542]\n",
      "[1.4350883960723877, -1.5927785634994507]\n",
      "[0.4923252463340759, -0.6562676429748535]\n",
      "[-0.3544052243232727, -0.4295141398906708]\n",
      "[0.3487030863761902, -0.5482052564620972]\n",
      "[0.9422357678413391, -1.041886329650879]\n",
      "[0.6624776721000671, -0.8021055459976196]\n",
      "[-0.6730929613113403, -0.45129093527793884]\n",
      "[1.3515267372131348, -1.3926899433135986]\n",
      "[-1.9994893074035645, -1.0376380681991577]\n",
      "[1.2210688591003418, -1.280874490737915]\n",
      "[-0.16529467701911926, -0.4786301553249359]\n",
      "[0.6773388981819153, -0.8148430585861206]\n",
      "[0.6787338852882385, -0.8160387277603149]\n",
      "[-2.3095204830169678, -1.106804609298706]\n",
      "[-1.3750576972961426, -0.5657286643981934]\n",
      "[1.0238304138183594, -1.111821174621582]\n",
      "[0.73911052942276, -0.8677875995635986]\n",
      "[-0.8888211250305176, -0.37705832719802856]\n",
      "[0.7247034907341003, -0.855439305305481]\n",
      "[0.5349125862121582, -0.6927692890167236]\n",
      "[-1.1391592025756836, -1.2814652919769287]\n",
      "[0.7795817255973816, -0.9024754762649536]\n",
      "[0.10537347197532654, -0.6972728967666626]\n",
      "[-1.287515640258789, -0.5452109575271606]\n",
      "[-0.9574707746505737, -0.7840216159820557]\n",
      "[-0.3718024790287018, -0.49404099583625793]\n",
      "[-0.5329316854476929, -0.4622018039226532]\n",
      "[1.217804193496704, -1.4355676174163818]\n",
      "[1.0718603134155273, -1.1529875993728638]\n",
      "[0.9456995129585266, -1.4314157962799072]\n",
      "[1.599616289138794, -1.605327844619751]\n",
      "[-1.5315001010894775, -0.6339362859725952]\n",
      "[-0.0028952360153198242, -1.0107897520065308]\n",
      "[0.1001177728176117, -0.5629366636276245]\n",
      "[0.9280019402503967, -1.799086570739746]\n",
      "[1.5165395736694336, -1.9387266635894775]\n",
      "[0.48829418420791626, -0.7778708934783936]\n",
      "[-0.2500310242176056, -0.4351429045200348]\n",
      "[-0.45801985263824463, -0.5358625650405884]\n",
      "[-1.4181663990020752, -0.5715482234954834]\n",
      "[-1.6515090465545654, -0.4509238302707672]\n",
      "[-1.6472175121307373, -1.2532802820205688]\n",
      "[1.518721103668213, -1.5359925031661987]\n",
      "[0.8751437067985535, -1.23333740234375]\n",
      "[0.8859378695487976, -1.2380201816558838]\n",
      "[-1.0385135412216187, -0.6017429828643799]\n",
      "[0.9607653021812439, -1.0577681064605713]\n",
      "[0.6481221318244934, -0.7898013591766357]\n",
      "[0.7444718480110168, -0.8723827600479126]\n",
      "[-2.250091314315796, -1.6532659530639648]\n",
      "[-1.935058832168579, -0.524500846862793]\n",
      "[0.7485384345054626, -0.8758683204650879]\n",
      "[-2.2736406326293945, -0.9132014513015747]\n",
      "[0.9141313433647156, -1.0177980661392212]\n",
      "[0.4515087604522705, -0.945904016494751]\n",
      "[-0.11219313740730286, -0.4580531418323517]\n",
      "[1.0253863334655762, -1.1131548881530762]\n",
      "[1.1445343494415283, -1.4247757196426392]\n",
      "[-2.127553939819336, -1.2389181852340698]\n",
      "[0.7833197712898254, -0.9056793451309204]\n",
      "[0.7630026936531067, -1.4873085021972656]\n",
      "[1.0319526195526123, -1.1187827587127686]\n",
      "[0.838412344455719, -0.9528992176055908]\n",
      "[0.7593186497688293, -0.8851079940795898]\n",
      "[0.057944029569625854, -0.7479640245437622]\n",
      "[-1.154837965965271, -0.5772558450698853]\n",
      "[-1.2786903381347656, -0.5278233289718628]\n",
      "[1.0681445598602295, -1.1498030424118042]\n",
      "[-2.172999382019043, -0.31671544909477234]\n",
      "[0.45841145515441895, -0.6272001266479492]\n",
      "[-0.2698228657245636, -0.6323710680007935]\n",
      "[0.8476646542549133, -0.9608293771743774]\n",
      "[0.7603251338005066, -1.1387441158294678]\n",
      "[-1.8132827281951904, -0.5677518844604492]\n",
      "[0.09223318099975586, -0.49127086997032166]\n",
      "[0.6996446251869202, -1.0660344362258911]\n",
      "[0.6561505198478699, -0.7966824769973755]\n",
      "[0.4562975764274597, -0.6253883838653564]\n",
      "[-0.9859302043914795, -0.9269850254058838]\n",
      "[0.8378921151161194, -0.9524533748626709]\n",
      "[-0.5303181409835815, -0.3734475076198578]\n",
      "[-0.963311493396759, -0.501258134841919]\n",
      "[0.9058967232704163, -1.0107401609420776]\n",
      "[-1.8430709838867188, -0.6973772048950195]\n",
      "[1.6324996948242188, -1.633512258529663]\n",
      "[0.001579880714416504, -0.4506737291812897]\n",
      "[0.9203841090202332, -1.0231572389602661]\n",
      "[0.9341806769371033, -1.0349823236465454]\n",
      "[-1.9385602474212646, -0.38525649905204773]\n",
      "[0.2485431432723999, -2.1929569244384766]\n",
      "[-1.7588984966278076, -1.172864317893982]\n",
      "[-1.13108491897583, -0.857032060623169]\n",
      "[-1.8360512256622314, -1.0226733684539795]\n",
      "[-1.9379558563232422, -0.8752346038818359]\n",
      "[0.8388710618019104, -0.9658492803573608]\n",
      "[-0.060306400060653687, -1.3456193208694458]\n",
      "[-1.141877293586731, -1.0742433071136475]\n",
      "[-0.565503716468811, -1.881186604499817]\n",
      "[-2.0048232078552246, -0.8368370532989502]\n",
      "[1.3471968173980713, -1.6094931364059448]\n",
      "[-1.008433222770691, -1.1653964519500732]\n",
      "[0.23069143295288086, -1.4428646564483643]\n",
      "[-2.355910539627075, -1.031998872756958]\n",
      "[-1.820589303970337, -0.8117896318435669]\n",
      "[-2.0772666931152344, -0.8771069049835205]\n",
      "[-1.150245189666748, -1.5118285417556763]\n",
      "[-1.5945615768432617, -1.0781971216201782]\n",
      "[-0.9558061361312866, -0.6165437698364258]\n",
      "[-0.8955353498458862, -1.356066107749939]\n",
      "[0.8899051547050476, -1.505314588546753]\n",
      "[-1.2447131872177124, -1.3669006824493408]\n",
      "[-2.037130832672119, -0.8547961711883545]\n",
      "[-1.9466888904571533, -1.0312715768814087]\n",
      "[-1.9731450080871582, -1.1526120901107788]\n",
      "[1.059849500656128, -1.7529562711715698]\n",
      "[0.2258768081665039, -0.4836525619029999]\n",
      "[-1.3787705898284912, -1.4528920650482178]\n",
      "[-1.682729959487915, -1.2594305276870728]\n",
      "[-0.976775586605072, -0.6370800733566284]\n",
      "[-1.831380844116211, -1.2334814071655273]\n",
      "[-0.5905393362045288, -0.9055360555648804]\n",
      "[1.5249838829040527, -1.7690602540969849]\n",
      "[-1.150073528289795, -1.0919331312179565]\n",
      "[-1.2170300483703613, -1.0349199771881104]\n",
      "[0.05931568145751953, -0.4970800578594208]\n",
      "[0.5358601212501526, -0.6935814619064331]\n",
      "[-1.0527981519699097, -0.7111015319824219]\n",
      "[-1.236860752105713, -0.7032711505889893]\n",
      "[-0.32423701882362366, -1.3684238195419312]\n",
      "[-1.1939679384231567, -1.4418179988861084]\n",
      "[0.880156934261322, -1.4350718259811401]\n",
      "[-0.7207040190696716, -1.447541356086731]\n",
      "[0.6409755349159241, -0.783676028251648]\n",
      "[-0.23297131061553955, -1.0095943212509155]\n",
      "[-2.135655641555786, -0.9095640182495117]\n",
      "[1.6914048194885254, -1.68399977684021]\n",
      "[1.5540008544921875, -1.7965434789657593]\n",
      "[-1.4458987712860107, -1.175305962562561]\n",
      "[-1.1800282001495361, -0.7138625383377075]\n",
      "[-0.5507217645645142, -0.5720330476760864]\n",
      "[0.43305695056915283, -0.60546875]\n",
      "[0.24263566732406616, -0.4965972602367401]\n",
      "[1.3440783023834229, -1.7693159580230713]\n",
      "[-0.261464387178421, -1.8265079259872437]\n",
      "[-0.530000627040863, -1.1762430667877197]\n",
      "[-0.4495258629322052, -1.2383543252944946]\n",
      "[0.33007359504699707, -0.5215469598770142]\n",
      "[-1.4047460556030273, -1.3732106685638428]\n",
      "[-1.27449631690979, -1.0309734344482422]\n",
      "[-1.7351915836334229, -1.257662057876587]\n",
      "[1.0192570686340332, -2.297595977783203]\n",
      "[0.5220522284507751, -0.7978131771087646]\n",
      "[-1.7675037384033203, -1.34486985206604]\n",
      "[0.9408162236213684, -1.5065600872039795]\n",
      "[-1.4030365943908691, -1.2859176397323608]\n",
      "[-0.7680554986000061, -1.1534875631332397]\n",
      "[-1.552704095840454, -0.7786799669265747]\n",
      "[-0.9901505708694458, -1.5042263269424438]\n",
      "[-0.43678680062294006, -1.310829520225525]\n",
      "[1.2007033824920654, -1.8801889419555664]\n",
      "[-1.4807558059692383, -1.1720070838928223]\n",
      "[1.1329782009124756, -1.2946292161941528]\n",
      "[-1.0367943048477173, -1.6131999492645264]\n",
      "[-1.5568318367004395, -1.2322006225585938]\n",
      "[-1.5541625022888184, -1.1203253269195557]\n",
      "[-1.3084874153137207, -0.9251632690429688]\n",
      "[0.21694839000701904, -0.4721927344799042]\n",
      "[0.12530922889709473, -0.5437736511230469]\n",
      "[-1.1346709728240967, -1.5620510578155518]\n",
      "[0.45841145515441895, -0.6272001266479492]\n",
      "[-2.3783345222473145, -1.0444639921188354]\n",
      "[-1.1331942081451416, -0.8255561590194702]\n",
      "[-1.5677337646484375, -1.2756766080856323]\n",
      "[-0.7641116976737976, -1.1831310987472534]\n",
      "[0.6718700528144836, -0.8101557493209839]\n",
      "[-1.3835289478302002, -0.7981516122817993]\n",
      "[-1.0734657049179077, -1.1018106937408447]\n",
      "[-1.7278556823730469, -1.0878890752792358]\n",
      "[-1.5412938594818115, -0.912226676940918]\n",
      "[1.0707781314849854, -1.153568148612976]\n",
      "[-1.747586727142334, -1.1145223379135132]\n",
      "[-1.3446404933929443, -0.690069317817688]\n",
      "[-0.5702139735221863, -0.639556884765625]\n",
      "[-1.3487982749938965, -1.4916307926177979]\n",
      "[0.8963595032691956, -1.0824394226074219]\n",
      "[-0.25145888328552246, -1.4472498893737793]\n",
      "[-1.3501288890838623, -0.7956922054290771]\n",
      "[-1.4430866241455078, -1.4344106912612915]\n",
      "[-1.8117318153381348, -1.1714587211608887]\n",
      "[0.3209438920021057, -0.5093765258789062]\n",
      "[1.7167019844055176, -2.0595486164093018]\n",
      "[-0.5240360498428345, -1.0701261758804321]\n",
      "[-1.427964210510254, -0.4190760850906372]\n",
      "[-1.0993692874908447, -0.8757684230804443]\n",
      "[-1.1992669105529785, -0.706484317779541]\n",
      "[-0.0030729472637176514, -0.7159205675125122]\n",
      "[0.26938915252685547, -1.0213968753814697]\n",
      "[-0.42822620272636414, -0.8387356996536255]\n",
      "[0.9149141907691956, -1.7762830257415771]\n",
      "[-1.3809924125671387, -0.6577969789505005]\n",
      "[0.176192045211792, -1.4015849828720093]\n",
      "[-0.862511157989502, -0.8675155639648438]\n",
      "[0.8431209921836853, -1.2254172563552856]\n",
      "[-1.2321782112121582, -0.8047505617141724]\n",
      "[-1.6485278606414795, -0.5302153825759888]\n",
      "[-1.4115574359893799, -0.6793777942657471]\n",
      "[0.18482649326324463, -1.438607931137085]\n",
      "[-0.07271715998649597, -0.9654641151428223]\n",
      "[-1.1914992332458496, -0.28997692465782166]\n",
      "[-0.025367408990859985, -1.15665602684021]\n",
      "[-0.5678858160972595, -1.1229908466339111]\n",
      "[-0.006514608860015869, -1.257954478263855]\n",
      "[-1.5295658111572266, -0.6795313358306885]\n",
      "[-0.22713907063007355, -0.9794446229934692]\n",
      "[-0.4389726519584656, -1.0928913354873657]\n",
      "[-0.38617193698883057, -1.4645202159881592]\n",
      "[-0.385237455368042, -0.4848819673061371]\n",
      "[0.41129690408706665, -1.4027694463729858]\n",
      "[-0.3032781779766083, -1.2010881900787354]\n",
      "[-1.2295703887939453, -0.31440362334251404]\n",
      "[-0.3739829659461975, -1.1758369207382202]\n",
      "[0.28282594680786133, -0.8145482540130615]\n",
      "[0.25449150800704956, -1.4059414863586426]\n",
      "[0.28771257400512695, -0.9850726127624512]\n",
      "[-1.212836503982544, -0.7172355651855469]\n",
      "[-0.4260881543159485, -0.4027637243270874]\n",
      "[-0.004183679819107056, -0.6850860118865967]\n",
      "[-1.3242642879486084, -0.3172527849674225]\n",
      "[-1.370903730392456, -0.3453369140625]\n",
      "[0.34160810708999634, -1.0977628231048584]\n",
      "[0.5815464854240417, -1.3835797309875488]\n",
      "[0.7004117369651794, -1.095609426498413]\n",
      "[0.4551255702972412, -1.3344937562942505]\n",
      "[-0.24115177989006042, -0.6803206205368042]\n",
      "[0.21097660064697266, -0.7322597503662109]\n",
      "[-1.5021591186523438, -0.6698007583618164]\n",
      "[1.2988429069519043, -1.465777039527893]\n",
      "[1.2832834720611572, -1.4714981317520142]\n",
      "[-0.8324953317642212, -0.9647445678710938]\n",
      "[-1.3218464851379395, -0.38066282868385315]\n",
      "[-0.5257126092910767, -0.34825441241264343]\n",
      "[0.04095771908760071, -0.6138507127761841]\n",
      "[-0.39821407198905945, -0.47424444556236267]\n",
      "[-0.08330738544464111, -1.4547486305236816]\n",
      "[0.9405465722084045, -1.699586272239685]\n",
      "[-0.005276352167129517, -0.8999916315078735]\n",
      "[-0.08041974902153015, -0.8946031332015991]\n",
      "[0.11366492509841919, -0.5200052261352539]\n",
      "[0.3778877258300781, -1.3136171102523804]\n",
      "[0.08795464038848877, -0.905367374420166]\n",
      "[-0.40374723076820374, -1.1599839925765991]\n",
      "[-0.4119492173194885, -1.976463794708252]\n",
      "[-0.8213677406311035, -0.5093752145767212]\n",
      "[-0.24287930130958557, -1.2703661918640137]\n",
      "[-0.5337377190589905, -1.1793280839920044]\n",
      "[-0.413941353559494, -1.110432744026184]\n",
      "[-0.7894139289855957, -0.7790292501449585]\n",
      "[-1.3848223686218262, -0.509084939956665]\n",
      "[0.1004757285118103, -1.3654780387878418]\n",
      "[0.395386278629303, -1.095184087753296]\n",
      "[-0.26089411973953247, -1.573398232460022]\n",
      "[0.14968425035476685, -1.0863207578659058]\n",
      "[1.0270698070526123, -1.1746846437454224]\n",
      "[0.4042815566062927, -1.5391494035720825]\n",
      "[-0.4805826246738434, -1.0772762298583984]\n",
      "[0.0738065242767334, -1.0349957942962646]\n",
      "[-1.3826258182525635, -0.5636013746261597]\n",
      "[-0.05678588151931763, -0.4861936867237091]\n",
      "[-0.26329153776168823, -0.4125933051109314]\n",
      "[0.470986545085907, -1.5342940092086792]\n",
      "[-0.2698228657245636, -0.6323710680007935]\n",
      "[-1.1331942081451416, -0.8255562782287598]\n",
      "[-1.446000099182129, -0.3508271276950836]\n",
      "[-0.28539955615997314, -1.1820918321609497]\n",
      "[-0.5771427750587463, -0.8373756408691406]\n",
      "[-0.03321036696434021, -0.7879832983016968]\n",
      "[-1.3056213855743408, -0.5361902713775635]\n",
      "[-0.8416796922683716, -0.7816373109817505]\n",
      "[-0.7390387058258057, -0.917595386505127]\n",
      "[-0.694892406463623, -0.7429436445236206]\n",
      "[-0.1375834345817566, -0.7094888687133789]\n",
      "[-0.6553330421447754, -0.9607594013214111]\n",
      "[-1.3872308731079102, -0.36299726366996765]\n",
      "[-0.9993177056312561, -0.25318020582199097]\n",
      "[0.09095582365989685, -1.4204493761062622]\n",
      "[-0.15181857347488403, -0.9115333557128906]\n",
      "[0.3886866569519043, -1.2050416469573975]\n",
      "[-1.3160631656646729, -0.532991886138916]\n",
      "[0.18476176261901855, -1.3677780628204346]\n",
      "[-0.3250446319580078, -1.1154112815856934]\n",
      "[-0.44025251269340515, -0.5033100843429565]\n",
      "[-0.9449132680892944, -1.9331252574920654]\n",
      "[-2.584214925765991, -1.158908486366272]\n",
      "[-0.3305742144584656, -1.2044135332107544]\n",
      "[-1.6758134365081787, -1.1640362739562988]\n",
      "[-0.8970086574554443, -1.376408576965332]\n",
      "[1.0225462913513184, -1.1107206344604492]\n",
      "[-1.2832348346710205, -1.0427217483520508]\n",
      "[-1.6920363903045654, -0.9295598268508911]\n",
      "[-1.793130874633789, -1.5682687759399414]\n",
      "[-1.1471543312072754, -1.2021489143371582]\n",
      "[2.024524450302124, -2.071223735809326]\n",
      "[-1.1748998165130615, -1.1727492809295654]\n",
      "[-1.9563260078430176, -0.9789838790893555]\n",
      "[-1.744591236114502, -1.1719064712524414]\n",
      "[-0.6663634777069092, -1.2433100938796997]\n",
      "[-1.0994415283203125, -1.2815724611282349]\n",
      "[-2.5254669189453125, -1.1262516975402832]\n",
      "[-0.8674554824829102, -1.503170132637024]\n",
      "[0.27964073419570923, -1.0793436765670776]\n",
      "[-2.313615083694458, -1.0084877014160156]\n",
      "[1.180837869644165, -1.5293138027191162]\n",
      "[-2.519089937210083, -1.1227068901062012]\n",
      "[-1.2909427881240845, -1.2012947797775269]\n",
      "[-1.4606761932373047, -1.302169919013977]\n",
      "[-2.4716930389404297, -1.0963599681854248]\n",
      "[1.6317315101623535, -2.0192818641662598]\n",
      "[0.5948395729064941, -0.7441327571868896]\n",
      "[-1.4716606140136719, -1.554394245147705]\n",
      "[-2.6537346839904785, -1.197553038597107]\n",
      "[0.2548600435256958, -1.0586135387420654]\n",
      "[-2.58864688873291, -1.1613720655441284]\n",
      "[0.4049902558326721, -1.3875442743301392]\n",
      "[1.7110846042633057, -1.8532958030700684]\n",
      "[-0.4305272400379181, -1.5279626846313477]\n",
      "[-1.0320475101470947, -1.1879541873931885]\n",
      "[0.6648356318473816, -0.804126501083374]\n",
      "[0.7955141663551331, -0.9161311388015747]\n",
      "[0.05422845482826233, -1.1922637224197388]\n",
      "[0.0153999924659729, -1.1704236268997192]\n",
      "[-1.6875650882720947, -0.9983818531036377]\n",
      "[-1.1673779487609863, -1.6088063716888428]\n",
      "[-0.5177255272865295, -1.1491138935089111]\n",
      "[-2.384249210357666, -1.047751784324646]\n",
      "[0.9090552926063538, -1.0134474039077759]\n",
      "[0.18197917938232422, -1.1267282962799072]\n",
      "[-1.8426241874694824, -0.9828178882598877]\n",
      "[1.1633648872375488, -1.236649513244629]\n",
      "[-0.20656366646289825, -1.3900980949401855]\n",
      "[-1.8579719066619873, -1.0934293270111084]\n",
      "[0.07766261696815491, -1.2257972955703735]\n",
      "[0.6505191922187805, -1.0461350679397583]\n",
      "[0.8446316123008728, -0.9582297801971436]\n",
      "[0.5536614060401917, -0.708838939666748]\n",
      "[1.6227662563323975, -1.9280569553375244]\n",
      "[-1.7300541400909424, -1.379608392715454]\n",
      "[-1.595700979232788, -0.916350245475769]\n",
      "[-1.3715243339538574, -1.0089350938796997]\n",
      "[0.8342763781547546, -0.9493542909622192]\n",
      "[-1.4861695766448975, -1.4820916652679443]\n",
      "[-0.46119749546051025, -1.5040550231933594]\n",
      "[-2.5772275924682617, -1.1550244092941284]\n",
      "[1.613659381866455, -2.5155460834503174]\n",
      "[1.0821912288665771, -1.173500657081604]\n",
      "[-2.623481273651123, -1.18073570728302]\n",
      "[1.372908115386963, -1.701282024383545]\n",
      "[-2.4068806171417236, -1.060332179069519]\n",
      "[-1.0257503986358643, -1.104752540588379]\n",
      "[-0.31807419657707214, -1.2882016897201538]\n",
      "[-2.4240269660949707, -1.0698633193969727]\n",
      "[-1.6986677646636963, -0.9715343713760376]\n",
      "[1.7085957527160645, -2.1021244525909424]\n",
      "[-0.9350507855415344, -1.5429147481918335]\n",
      "[0.7051586508750916, -1.285237193107605]\n",
      "[-2.344224452972412, -1.2444950342178345]\n",
      "[-2.4491305351257324, -1.083817958831787]\n",
      "[-0.9915703535079956, -1.4760701656341553]\n",
      "[-0.709441602230072, -1.2242921590805054]\n",
      "[0.7688315510749817, -0.8932615518569946]\n",
      "[0.7054161429405212, -0.8389080762863159]\n",
      "[-2.1059699058532715, -1.3100634813308716]\n",
      "[0.8476646542549133, -0.9608293771743774]\n",
      "[-1.5677337646484375, -1.2756766080856323]\n",
      "[-0.2853997051715851, -1.1820918321609497]\n",
      "[-2.6386783123016357, -1.1891834735870361]\n",
      "[-1.3040454387664795, -1.0480897426605225]\n",
      "[1.1759424209594727, -1.2421965599060059]\n",
      "[-0.27693527936935425, -1.3244802951812744]\n",
      "[-1.4607062339782715, -1.0056692361831665]\n",
      "[-2.352433443069458, -1.0300661325454712]\n",
      "[-0.5610966682434082, -1.4209543466567993]\n",
      "[0.8777026534080505, -0.9865750074386597]\n",
      "[-2.351127862930298, -1.029340386390686]\n",
      "[0.02863052487373352, -1.15565025806427]\n",
      "[0.32960325479507446, -0.9757485389709473]\n",
      "[-2.5991761684417725, -1.1672250032424927]\n",
      "[1.5680806636810303, -1.578298568725586]\n",
      "[-2.3594133853912354, -1.03394615650177]\n",
      "[-0.16441872715950012, -1.32646644115448]\n",
      "[-2.266143321990967, -1.265388011932373]\n",
      "[-2.4591917991638184, -1.0894109010696411]\n",
      "[0.7201957106590271, -0.8515757322311401]\n",
      "[0.7228196263313293, -2.08111834526062]\n",
      "[-1.5862276554107666, -0.9221149682998657]\n",
      "[-0.5370546579360962, -0.8618866205215454]\n",
      "[-1.5448672771453857, -0.8601894378662109]\n",
      "[-0.31649839878082275, -1.2215652465820312]\n",
      "[-0.17578375339508057, -0.44914403557777405]\n",
      "[-2.0338492393493652, -0.8529720306396484]\n",
      "[-1.9752836227416992, -0.8204166889190674]\n",
      "[-0.04746714234352112, -1.7519323825836182]\n",
      "[-0.7846932411193848, -0.9772783517837524]\n",
      "[1.2213621139526367, -1.884639024734497]\n",
      "[-1.889814853668213, -0.7729064226150513]\n",
      "[-1.8716247081756592, -0.9208221435546875]\n",
      "[-0.8813145756721497, -1.0958327054977417]\n",
      "[-0.9509177207946777, -0.9744479656219482]\n",
      "[-0.3593040406703949, -1.1718559265136719]\n",
      "[-1.1532973051071167, -1.21016263961792]\n",
      "[0.24159091711044312, -1.495349645614624]\n",
      "[-0.28295132517814636, -0.8266335725784302]\n",
      "[-2.0932867527008057, -0.886012077331543]\n",
      "[-0.431184321641922, -1.0192123651504517]\n",
      "[-1.2003201246261597, -1.0781358480453491]\n",
      "[-0.9285191297531128, -0.9775689840316772]\n",
      "[-0.12302419543266296, -1.3658897876739502]\n",
      "[-0.9734793901443481, -1.186010479927063]\n",
      "[0.3382868766784668, -1.7257351875305176]\n",
      "[0.03717157244682312, -0.7059682607650757]\n",
      "[0.18884307146072388, -1.7054072618484497]\n",
      "[-1.3951818943023682, -1.046043872833252]\n",
      "[-0.7581720352172852, -0.7534661293029785]\n",
      "[-1.057939052581787, -1.1731059551239014]\n",
      "[1.057159662246704, -1.3387653827667236]\n",
      "[0.2870100736618042, -1.2684612274169922]\n",
      "[0.6846831440925598, -1.5221531391143799]\n",
      "[-1.4624686241149902, -0.7073671817779541]\n",
      "[-0.05349934101104736, -0.6515463590621948]\n",
      "[-0.02654588222503662, -0.5799318552017212]\n",
      "[-0.6199868321418762, -0.8979712724685669]\n",
      "[-0.615766167640686, -0.8944227695465088]\n",
      "[-2.209725856781006, -0.9507379531860352]\n",
      "[0.44636791944503784, -1.7409826517105103]\n",
      "[-1.6275758743286133, -0.6803046464920044]\n",
      "[-1.816434621810913, -0.8667675256729126]\n",
      "[0.2981240153312683, -0.8570278882980347]\n",
      "[-1.4152591228485107, -0.5091111660003662]\n",
      "[-1.430600643157959, -0.7633624076843262]\n",
      "[-0.4015992283821106, -0.6860337257385254]\n",
      "[-0.9751634001731873, -1.0102369785308838]\n",
      "[-1.9059183597564697, -0.781857967376709]\n",
      "[-0.45402029156684875, -0.9601045846939087]\n",
      "[0.6762669682502747, -0.9210587739944458]\n",
      "[1.1623661518096924, -1.26011061668396]\n",
      "[0.20410913228988647, -0.8004438877105713]\n",
      "[0.15721583366394043, -1.450531244277954]\n",
      "[-0.7559444904327393, -1.395486831665039]\n",
      "[-2.2017946243286133, -0.9463292360305786]\n",
      "[-2.178715705871582, -0.9335001707077026]\n",
      "[1.1157066822052002, -1.1905685663223267]\n",
      "[0.14224600791931152, -1.620842456817627]\n",
      "[0.530293881893158, -1.464536190032959]\n",
      "[-1.174471378326416, -1.1330188512802124]\n",
      "[0.2161230444908142, -2.174839973449707]\n",
      "[0.23831027746200562, -0.8820569515228271]\n",
      "[-0.9125128388404846, -1.332764744758606]\n",
      "[-0.03490716218948364, -1.2982808351516724]\n",
      "[-1.7304372787475586, -0.8734246492385864]\n",
      "[-2.0620875358581543, -0.8686690330505371]\n",
      "[-0.5582746863365173, -1.0682491064071655]\n",
      "[-1.639587163925171, -1.0087708234786987]\n",
      "[-2.144120693206787, -0.9142694473266602]\n",
      "[0.32248902320861816, -1.7649503946304321]\n",
      "[0.3343626856803894, -1.5778270959854126]\n",
      "[-1.0375696420669556, -0.6211197376251221]\n",
      "[-0.555763840675354, -1.4503260850906372]\n",
      "[-1.5519108772277832, -0.9191031455993652]\n",
      "[0.280079185962677, -1.5142815113067627]\n",
      "[-0.6109544634819031, -0.917133092880249]\n",
      "[1.1088464260101318, -1.1846885681152344]\n",
      "[-0.36268216371536255, -0.5988531112670898]\n",
      "[-0.25201287865638733, -1.5345556735992432]\n",
      "[0.760325014591217, -1.1387439966201782]\n",
      "[-0.7641120553016663, -1.1831310987472534]\n",
      "[-0.5771429538726807, -0.8373756408691406]\n",
      "[-1.30404531955719, -1.048089623451233]\n",
      "[-2.2125236988067627, -0.9522932767868042]\n",
      "[1.0867979526519775, -1.3967812061309814]\n",
      "[-0.31060171127319336, -1.1412711143493652]\n",
      "[-2.1087276935577393, -0.8945953845977783]\n",
      "[-1.9457359313964844, -0.8039916753768921]\n",
      "[0.16767263412475586, -1.3140835762023926]\n",
      "[-0.3994208574295044, -0.35917022824287415]\n",
      "[-1.5225017070770264, -0.876356840133667]\n",
      "[-0.950639545917511, -0.8361437320709229]\n",
      "[-0.7000526189804077, -0.6262916326522827]\n",
      "[-0.7904973030090332, -1.357286810874939]\n",
      "[1.0412163734436035, -1.4629329442977905]\n",
      "[-2.25080943107605, -0.9735754728317261]\n",
      "[-0.45210501551628113, -1.1200098991394043]\n",
      "[-0.4483188986778259, -1.4729974269866943]\n",
      "[-0.9324567914009094, -1.1517738103866577]\n",
      "[0.7532843947410583, -1.0885441303253174]\n",
      "[1.8356609344482422, -1.807641863822937]\n",
      "[0.998816192150116, -1.0903815031051636]\n",
      "[0.0001475811004638672, -0.7998778820037842]\n",
      "[0.6842548251152039, -0.8609263896942139]\n",
      "[0.5166677236557007, -0.6771316528320312]\n",
      "[-0.36298656463623047, -1.169193148612976]\n",
      "[1.4134137630462646, -1.6819987297058105]\n",
      "[1.1793041229248047, -1.446423888206482]\n",
      "[1.4571492671966553, -1.4832192659378052]\n",
      "[0.46360546350479126, -0.8446550369262695]\n",
      "[-2.2343268394470215, -0.9322352409362793]\n",
      "[0.7908993363380432, -1.1304712295532227]\n",
      "[1.7950809001922607, -1.9657082557678223]\n",
      "[0.7199093699455261, -0.8513302803039551]\n",
      "[-0.22699031233787537, -0.47467300295829773]\n",
      "[0.47859370708465576, -0.644498348236084]\n",
      "[1.2411634922027588, -1.2980974912643433]\n",
      "[0.7182286381721497, -0.8498897552490234]\n",
      "[-0.6417056322097778, -0.5195579528808594]\n",
      "[1.6518981456756592, -1.6501384973526]\n",
      "[-1.6753473281860352, -1.28325355052948]\n",
      "[1.5209977626800537, -1.5379438400268555]\n",
      "[0.18407899141311646, -0.5619734525680542]\n",
      "[0.7806809544563293, -0.9034175872802734]\n",
      "[0.9425931572914124, -1.0421926975250244]\n",
      "[-2.4166958332061768, -1.138527750968933]\n",
      "[-0.8923466205596924, -0.8398967981338501]\n",
      "[1.1203711032867432, -1.1945663690567017]\n",
      "[1.007765293121338, -1.098051905632019]\n",
      "[-0.8509489297866821, -0.4492582380771637]\n",
      "[0.9886767268180847, -1.081691026687622]\n",
      "[0.41654348373413086, -0.6077755689620972]\n",
      "[-0.8082140684127808, -1.5120034217834473]\n",
      "[0.7911399006843567, -0.9123820066452026]\n",
      "[0.4456188678741455, -0.918447732925415]\n",
      "[-0.8165345788002014, -0.8106579780578613]\n",
      "[-0.4881862998008728, -1.0580476522445679]\n",
      "[-0.31258130073547363, -0.5665264129638672]\n",
      "[-0.44906875491142273, -0.5485326051712036]\n",
      "[1.526867389678955, -1.7060883045196533]\n",
      "[1.1455802917480469, -1.2161731719970703]\n",
      "[1.3375236988067627, -1.7682899236679077]\n",
      "[1.8781764507293701, -1.8440818786621094]\n",
      "[-1.0485129356384277, -0.9102195501327515]\n",
      "[0.4274185299873352, -1.2675830125808716]\n",
      "[0.5330103039741516, -0.7955397367477417]\n",
      "[1.3526012897491455, -2.1052703857421875]\n",
      "[1.8796703815460205, -2.284512519836426]\n",
      "[1.0116989612579346, -1.111296534538269]\n",
      "[-0.36183544993400574, -0.4653567969799042]\n",
      "[-0.6956456303596497, -0.4888286292552948]\n",
      "[-1.8828301429748535, -0.42476919293403625]\n",
      "[-1.1483814716339111, -0.7279236316680908]\n",
      "[-1.3720602989196777, -1.459352731704712]\n",
      "[1.834594964981079, -1.8067282438278198]\n",
      "[1.1995162963867188, -1.5324420928955078]\n",
      "[1.213371992111206, -1.512129545211792]\n",
      "[-1.498187780380249, -0.45861396193504333]\n",
      "[1.111405849456787, -1.1868822574615479]\n",
      "[0.6823861002922058, -0.8191690444946289]\n",
      "[1.0532195568084717, -1.1370106935501099]\n",
      "[-2.417621612548828, -1.6623255014419556]\n",
      "[-1.6265735626220703, -0.7351117134094238]\n",
      "[0.9903796315193176, -1.0831505060195923]\n",
      "[-1.9818751811981201, -1.1272720098495483]\n",
      "[1.2723522186279297, -1.3248295783996582]\n",
      "[0.8419321179389954, -1.224381685256958]\n",
      "[-0.1408931016921997, -0.5071225166320801]\n",
      "[1.3802947998046875, -1.4173471927642822]\n",
      "[1.4452025890350342, -1.686861276626587]\n",
      "[-2.124857187271118, -1.3198577165603638]\n",
      "[0.848834216594696, -0.96183180809021]\n",
      "[1.1852264404296875, -1.750107765197754]\n",
      "[1.2975738048553467, -1.3464468717575073]\n",
      "[1.1846308708190918, -1.2496434450149536]\n",
      "[0.8204792141914368, -0.9375287294387817]\n",
      "[0.2856892943382263, -0.8954685926437378]\n",
      "[-1.5782387256622314, -0.4589989483356476]\n",
      "[-0.7566752433776855, -0.8010475635528564]\n",
      "[1.2669482231140137, -1.3201978206634521]\n",
      "[-1.8132832050323486, -0.5677520036697388]\n",
      "[0.6718700528144836, -0.8101557493209839]\n",
      "[-0.03321036696434021, -0.7879832983016968]\n",
      "[1.1759421825408936, -1.2421963214874268]\n",
      "[1.0867981910705566, -1.3967812061309814]\n",
      "[-2.3429501056671143, -0.36834079027175903]\n",
      "[0.004099488258361816, -0.5224764347076416]\n",
      "[1.0306479930877686, -1.3147175312042236]\n",
      "[1.015366792678833, -1.104567050933838]\n",
      "[0.5004348158836365, -0.6632183790206909]\n",
      "[-0.5419711470603943, -1.2252545356750488]\n",
      "[1.1693871021270752, -1.236578106880188]\n",
      "[-0.5437002182006836, -0.4341030418872833]\n",
      "[-0.5614374876022339, -0.7273141145706177]\n",
      "[1.172149419784546, -1.2389456033706665]\n",
      "[-2.5381405353546143, -0.4716382920742035]\n",
      "[1.9648385047912598, -1.9183599948883057]\n",
      "[-0.1146242618560791, -0.46516403555870056]\n",
      "[1.1551196575164795, -1.2243493795394897]\n",
      "[1.206254482269287, -1.2681770324707031]\n",
      "[-1.8006031513214111, -0.5070637464523315]\n",
      "[1.5225176811218262, -2.164321184158325]\n",
      "[-0.48844048380851746, -1.228411078453064]\n",
      "[-1.2993258237838745, -0.572428822517395]\n",
      "[-1.3029543161392212, -0.8993161916732788]\n",
      "[-1.9936718940734863, -0.5900921821594238]\n",
      "[0.9468250870704651, -1.0521036386489868]\n",
      "[0.8398147225379944, -1.3745723962783813]\n",
      "[-0.2334783971309662, -1.056937575340271]\n",
      "[0.739264190196991, -1.8719875812530518]\n",
      "[-1.7751843929290771, -0.654613733291626]\n",
      "[0.3537406921386719, -1.1772207021713257]\n",
      "[-0.7831442952156067, -1.0417044162750244]\n",
      "[1.7282838821411133, -1.7156089544296265]\n",
      "[-1.3297114372253418, -0.8056042194366455]\n",
      "[-2.1118690967559814, -0.5220227241516113]\n",
      "[-2.153245687484741, -0.6379091739654541]\n",
      "[0.14987057447433472, -1.5845822095870972]\n",
      "[-1.3085405826568604, -0.7610394954681396]\n",
      "[-1.4700050354003906, -0.34423062205314636]\n",
      "[0.2731091380119324, -1.4399611949920654]\n",
      "[0.4535346031188965, -1.4308913946151733]\n",
      "[0.04856196045875549, -1.416089653968811]\n",
      "[-2.0549168586730957, -0.6286147832870483]\n",
      "[-1.1780340671539307, -0.8454006910324097]\n",
      "[-0.6344713568687439, -1.146787405014038]\n",
      "[0.18911141157150269, -1.4740406274795532]\n",
      "[0.33445215225219727, -0.5570272207260132]\n",
      "[-0.39276352524757385, -1.3045450448989868]\n",
      "[-0.38692066073417664, -1.3306423425674438]\n",
      "[-1.3392629623413086, -0.3922894597053528]\n",
      "[-0.5147086977958679, -1.2596346139907837]\n",
      "[-1.3597620725631714, -0.5822639465332031]\n",
      "[1.256289005279541, -1.6279716491699219]\n",
      "[-1.0774444341659546, -0.7489590644836426]\n",
      "[-1.1497753858566284, -0.8703038692474365]\n",
      "[0.13925713300704956, -0.4348627030849457]\n",
      "[0.8075689673423767, -0.9264633655548096]\n",
      "[-1.6150174140930176, -0.36266809701919556]\n",
      "[-1.640352487564087, -0.3953548073768616]\n",
      "[0.6978086829185486, -1.4138225317001343]\n",
      "[-0.34894901514053345, -1.2560304403305054]\n",
      "[1.5708410739898682, -1.5806646347045898]\n",
      "[0.5320786833763123, -1.5521823167800903]\n",
      "[0.5386960506439209, -0.8277015686035156]\n",
      "[0.7569947838783264, -1.0472851991653442]\n",
      "[-1.7857604026794434, -0.7429277896881104]\n",
      "[2.0049970149993896, -1.95278000831604]\n",
      "[2.0761115550994873, -2.0137321949005127]\n",
      "[-0.6733607053756714, -1.0964547395706177]\n",
      "[-1.7565643787384033, -0.3962450921535492]\n",
      "[-1.4517958164215088, -0.3111664950847626]\n",
      "[-0.23454295098781586, -0.3855818212032318]\n",
      "[0.16576272249221802, -0.45818135142326355]\n",
      "[0.8496138453483582, -1.6371142864227295]\n",
      "[1.0369892120361328, -1.8798179626464844]\n",
      "[0.5262720584869385, -1.2268840074539185]\n",
      "[0.193448007106781, -1.2303060293197632]\n",
      "[-0.481702983379364, -0.33622312545776367]\n",
      "[-0.38226020336151123, -1.2327287197113037]\n",
      "[-1.2872636318206787, -0.6613138914108276]\n",
      "[-0.4194124937057495, -1.2678521871566772]\n",
      "[0.17108803987503052, -2.0238747596740723]\n",
      "[-0.11292681097984314, -0.5492514371871948]\n",
      "[-0.4191650450229645, -1.3286714553833008]\n",
      "[0.30047184228897095, -1.327470302581787]\n",
      "[-0.192750945687294, -1.3096702098846436]\n",
      "[-0.6586079597473145, -1.0744647979736328]\n",
      "[-1.9984467029571533, -0.4633432924747467]\n",
      "[0.2691984176635742, -1.5761563777923584]\n",
      "[0.6488822102546692, -1.3670238256454468]\n",
      "[0.42412030696868896, -1.6390645503997803]\n",
      "[-0.9952487349510193, -0.9046791791915894]\n",
      "[1.5046515464782715, -1.5239335298538208]\n",
      "[0.27334487438201904, -1.6421126127243042]\n",
      "[-0.3178388476371765, -1.2523910999298096]\n",
      "[-1.072251319885254, -0.8538686037063599]\n",
      "[-1.377039909362793, -0.6925538778305054]\n",
      "[-0.5031054615974426, -0.30588653683662415]\n",
      "[0.08634284138679504, -0.4794955551624298]\n",
      "[0.10332068800926208, -1.579641580581665]\n",
      "[0.09223315119743347, -0.4912707507610321]\n",
      "[-1.3835289478302002, -0.7981516122817993]\n",
      "[-1.30562162399292, -0.5361901521682739]\n",
      "[-0.2769351601600647, -1.324480414390564]\n",
      "[-0.3106016218662262, -1.1412711143493652]\n",
      "[0.004099518060684204, -0.522476315498352]\n",
      "[-1.9396076202392578, -0.5135082006454468]\n",
      "[-0.5597946047782898, -1.0534099340438843]\n",
      "[-0.547944188117981, -1.1326147317886353]\n",
      "[-2.06321120262146, -0.6281765699386597]\n",
      "[0.9772576689720154, -1.1592241525650024]\n",
      "[-0.5235463976860046, -1.1279029846191406]\n",
      "[-1.653944969177246, -0.4167259633541107]\n",
      "[-0.5625600814819336, -0.42985329031944275]\n",
      "[-0.03290560841560364, -1.517759084701538]\n",
      "[-0.08709108829498291, -0.6412335634231567]\n",
      "[0.9888260960578918, -1.6008903980255127]\n",
      "[-2.035282850265503, -0.5046894550323486]\n",
      "[-0.13720004260540009, -1.3994394540786743]\n",
      "[-0.49316680431365967, -1.1892204284667969]\n",
      "[-0.10369545221328735, -0.387168288230896]\n",
      "[0.5594829320907593, -2.0489068031311035]\n",
      "[-1.7502429485321045, -0.8764582872390747]\n",
      "[-0.8001291155815125, -0.8045456409454346]\n",
      "[-1.852304458618164, -0.7812674045562744]\n",
      "[-0.6124579310417175, -1.1427353620529175]\n",
      "[0.020622700452804565, -0.48582133650779724]\n",
      "[-1.9213237762451172, -0.7904216051101685]\n",
      "[-2.080700635910034, -0.8790156841278076]\n",
      "[-0.2332441359758377, -1.7131710052490234]\n",
      "[-1.1230627298355103, -0.8874346017837524]\n",
      "[1.2249462604522705, -1.8217856884002686]\n",
      "[-1.9194176197052002, -0.7893620729446411]\n",
      "[-1.7780542373657227, -0.9178587198257446]\n",
      "[-1.1923459768295288, -1.0146021842956543]\n",
      "[-1.1995224952697754, -0.8991378545761108]\n",
      "[-0.6617761850357056, -1.091318130493164]\n",
      "[-1.2590699195861816, -1.1852238178253174]\n",
      "[-0.06620508432388306, -1.4142309427261353]\n",
      "[-0.6546032428741455, -0.7449992895126343]\n",
      "[-2.1018340587615967, -0.8907632827758789]\n",
      "[-0.2866363525390625, -1.034610629081726]\n",
      "[-1.3281421661376953, -1.045737624168396]\n",
      "[-1.21946120262146, -0.9017447233200073]\n",
      "[-0.4316137433052063, -1.284679651260376]\n",
      "[-1.2573280334472656, -1.1146533489227295]\n",
      "[0.3965425491333008, -1.695119857788086]\n",
      "[-0.06427985429763794, -0.6568716764450073]\n",
      "[-0.09836170077323914, -1.6333026885986328]\n",
      "[-1.5448434352874756, -1.0053132772445679]\n",
      "[-1.055826187133789, -0.6859133243560791]\n",
      "[-1.3238556385040283, -1.108129620552063]\n",
      "[0.7673388123512268, -1.2605129480361938]\n",
      "[0.4505237936973572, -1.2935181856155396]\n",
      "[0.37679368257522583, -1.440647840499878]\n",
      "[-1.6988236904144287, -0.6667386293411255]\n",
      "[-0.18351495265960693, -0.5980708599090576]\n",
      "[-0.007928818464279175, -0.5840386152267456]\n",
      "[-0.8139315247535706, -0.8511866331100464]\n",
      "[-0.8302010893821716, -0.8439517021179199]\n",
      "[-2.1206958293914795, -0.9012480974197388]\n",
      "[0.15524327754974365, -1.6680089235305786]\n",
      "[-1.309516191482544, -0.7207945585250854]\n",
      "[-1.8934290409088135, -0.8516467809677124]\n",
      "[0.33969610929489136, -0.8320484161376953]\n",
      "[-1.3328948020935059, -0.4633265435695648]\n",
      "[-1.7359495162963867, -0.6873760223388672]\n",
      "[-0.07632488012313843, -0.770527720451355]\n",
      "[-0.6721231937408447, -1.05135178565979]\n",
      "[-2.033353567123413, -0.852696418762207]\n",
      "[-0.7894037365913391, -0.8955478668212891]\n",
      "[0.29954612255096436, -0.8152338266372681]\n",
      "[1.0315656661987305, -1.1395668983459473]\n",
      "[0.07968524098396301, -0.7317447662353516]\n",
      "[0.29013097286224365, -1.4588767290115356]\n",
      "[-0.7886398434638977, -1.3949145078659058]\n",
      "[-2.0899107456207275, -0.884135365486145]\n",
      "[-2.0505847930908203, -0.8622748851776123]\n",
      "[0.8559866547584534, -0.9679621458053589]\n",
      "[-0.1432546228170395, -1.5500694513320923]\n",
      "[0.22407793998718262, -1.3841886520385742]\n",
      "[-1.4372119903564453, -1.069320559501648]\n",
      "[0.29582077264785767, -2.155056953430176]\n",
      "[0.22639888525009155, -0.8200439214706421]\n",
      "[-1.1775634288787842, -1.2675715684890747]\n",
      "[0.052830278873443604, -1.2830047607421875]\n",
      "[-1.8579716682434082, -0.8399753570556641]\n",
      "[-2.011481523513794, -0.8405383825302124]\n",
      "[-0.8263452053070068, -0.985026478767395]\n",
      "[-1.6966803073883057, -0.9997096061706543]\n",
      "[-2.0737357139587402, -0.8751440048217773]\n",
      "[0.4091114401817322, -1.7479898929595947]\n",
      "[0.02829909324645996, -1.4988892078399658]\n",
      "[-0.7510945200920105, -0.6976393461227417]\n",
      "[-0.755862832069397, -1.407326102256775]\n",
      "[-1.742586612701416, -0.8710132837295532]\n",
      "[-0.027426153421401978, -1.4340639114379883]\n",
      "[-0.9931144714355469, -0.8188284635543823]\n",
      "[0.8762159943580627, -1.0171842575073242]\n",
      "[-0.5659219622612, -0.5123676061630249]\n",
      "[-0.4833201467990875, -1.4814043045043945]\n",
      "[0.6996446251869202, -1.0660345554351807]\n",
      "[-1.0734657049179077, -1.1018106937408447]\n",
      "[-0.8416798710823059, -0.78163743019104]\n",
      "[-1.4607062339782715, -1.0056692361831665]\n",
      "[-2.1087279319763184, -0.8945953845977783]\n",
      "[1.0306479930877686, -1.3147175312042236]\n",
      "[-0.5597949028015137, -1.0534098148345947]\n",
      "[-2.206012487411499, -0.9486738443374634]\n",
      "[-2.0137343406677246, -0.8417906761169434]\n",
      "[-0.12992927432060242, -1.2330983877182007]\n",
      "[-0.2067374587059021, -0.3974522054195404]\n",
      "[-1.7643818855285645, -0.8141014575958252]\n",
      "[-1.106579065322876, -0.8013156652450562]\n",
      "[-0.9814997911453247, -0.5656732320785522]\n",
      "[-1.0144176483154297, -1.3064665794372559]\n",
      "[0.998224675655365, -1.3944828510284424]\n",
      "[-2.224782705307007, -0.9591077566146851]\n",
      "[-0.6730366945266724, -1.0477474927902222]\n",
      "[-0.7033792734146118, -1.4113832712173462]\n",
      "[-1.2058749198913574, -1.0844649076461792]\n",
      "[0.6559545397758484, -0.9990864992141724]\n",
      "[-0.45335274934768677, -1.9391472339630127]\n",
      "[-2.4077677726745605, -1.060825228691101]\n",
      "[-0.784756064414978, -0.9387162923812866]\n",
      "[-2.1138012409210205, -0.8974156379699707]\n",
      "[-1.0923324823379517, -1.1844477653503418]\n",
      "[0.5369293093681335, -0.6944979429244995]\n",
      "[-1.9060587882995605, -0.7819361686706543]\n",
      "[-2.1005656719207764, -0.890058159828186]\n",
      "[-1.2941207885742188, -1.5770903825759888]\n",
      "[-1.479858636856079, -0.961790919303894]\n",
      "[1.726156234741211, -1.8905161619186401]\n",
      "[-1.7283124923706055, -0.851895809173584]\n",
      "[-2.2328784465789795, -0.9636080265045166]\n",
      "[-1.8938074111938477, -0.9843711853027344]\n",
      "[-1.0072296857833862, -1.0133028030395508]\n",
      "[-1.2297239303588867, -1.1062675714492798]\n",
      "[-2.1406803131103516, -1.0593174695968628]\n",
      "[-0.8187084197998047, -1.3815759420394897]\n",
      "[-0.4453355669975281, -0.8560811281204224]\n",
      "[-2.380338430404663, -1.045577883720398]\n",
      "[0.6875311732292175, -1.238246202468872]\n",
      "[-2.255966901779175, -0.9764423370361328]\n",
      "[-1.585402011871338, -0.9734126329421997]\n",
      "[-1.317773699760437, -1.2053037881851196]\n",
      "[-2.2925302982330322, -0.9967671632766724]\n",
      "[1.2342870235443115, -1.7881132364273071]\n",
      "[0.025200843811035156, -0.531409502029419]\n",
      "[-1.133330225944519, -1.5153052806854248]\n",
      "[-2.3545589447021484, -1.031247615814209]\n",
      "[-0.7028447389602661, -0.8251177072525024]\n",
      "[-2.3482038974761963, -1.0277149677276611]\n",
      "[0.2895410656929016, -1.2601741552352905]\n",
      "[1.3409321308135986, -1.553979516029358]\n",
      "[-0.3666943311691284, -1.4182106256484985]\n",
      "[-1.5473451614379883, -0.8855243921279907]\n",
      "[-0.1744459867477417, -0.5319714546203613]\n",
      "[0.28475552797317505, -0.655821681022644]\n",
      "[-0.4279979467391968, -0.9756865501403809]\n",
      "[-0.49373340606689453, -0.9424713850021362]\n",
      "[-2.1007487773895264, -0.890160083770752]\n",
      "[-0.8520815372467041, -1.5635151863098145]\n",
      "[-1.2015453577041626, -0.8264250755310059]\n",
      "[-2.3193624019622803, -1.0116825103759766]\n",
      "[0.5076940655708313, -0.8427363634109497]\n",
      "[-0.5999147295951843, -0.7741456031799316]\n",
      "[-2.0470376014709473, -0.8603031635284424]\n",
      "[0.4619213938713074, -0.9394952058792114]\n",
      "[-0.7004935145378113, -1.1288673877716064]\n",
      "[-2.2050657272338867, -0.9481475353240967]\n",
      "[-0.34993278980255127, -1.0192137956619263]\n",
      "[0.19961434602737427, -0.8539084196090698]\n",
      "[0.6844647526741028, -0.8209506273269653]\n",
      "[0.18254858255386353, -0.5606813430786133]\n",
      "[1.244762659072876, -1.6359606981277466]\n",
      "[-1.5119357109069824, -1.3258525133132935]\n",
      "[-2.0217597484588623, -0.8462517261505127]\n",
      "[-1.9512202739715576, -0.8070404529571533]\n",
      "[0.61299729347229, -0.7596957683563232]\n",
      "[-1.1657954454421997, -1.4382566213607788]\n",
      "[-0.4566176235675812, -1.3805800676345825]\n",
      "[-2.3903064727783203, -1.0511189699172974]\n",
      "[1.1626334190368652, -2.2791197299957275]\n",
      "[0.6962870955467224, -0.9292792081832886]\n",
      "[-2.2980058193206787, -1.1058335304260254]\n",
      "[0.9548842310905457, -1.435911774635315]\n",
      "[-2.399723768234253, -1.0563538074493408]\n",
      "[-1.8050813674926758, -0.7458435297012329]\n",
      "[-0.6385716199874878, -1.0809096097946167]\n",
      "[-2.3195528984069824, -1.0117884874343872]\n",
      "[-2.0948808193206787, -0.8868981599807739]\n",
      "[1.2888784408569336, -1.8648566007614136]\n",
      "[-0.8094701766967773, -1.4436527490615845]\n",
      "[-0.10827308893203735, -0.9642982482910156]\n",
      "[-1.8446016311645508, -1.2537258863449097]\n",
      "[-2.431432008743286, -1.0739797353744507]\n",
      "[-0.8666622042655945, -1.3755643367767334]\n",
      "[-1.1537835597991943, -0.9525103569030762]\n",
      "[0.5428736805915833, -0.6995928287506104]\n",
      "[-0.23656368255615234, -0.5793929100036621]\n",
      "[-1.604752540588379, -1.3197563886642456]\n",
      "[0.6561506390571594, -0.796682596206665]\n",
      "[-1.727855920791626, -1.0878890752792358]\n",
      "[-0.7390387058258057, -0.9175955057144165]\n",
      "[-2.352433443069458, -1.0300661325454712]\n",
      "[-1.9457356929779053, -0.8039916753768921]\n",
      "[1.015366792678833, -1.104567050933838]\n",
      "[-0.547944188117981, -1.1326147317886353]\n",
      "[-2.0137343406677246, -0.8417906761169434]\n",
      "[-2.473041296005249, -1.0971094369888306]\n",
      "[-0.6718119382858276, -1.2628248929977417]\n",
      "[0.4736754298210144, -0.6402828693389893]\n",
      "[-2.383618116378784, -1.047400951385498]\n",
      "[-0.8831570744514465, -0.9282660484313965]\n",
      "[-0.7644819021224976, -0.7295911312103271]\n",
      "[-2.1342177391052246, -1.1419428586959839]\n",
      "[1.3887615203857422, -1.4246039390563965]\n",
      "[-2.4571707248687744, -1.0882872343063354]\n",
      "[-0.4405469298362732, -1.1380043029785156]\n",
      "[-1.8162288665771484, -1.2593244314193726]\n",
      "[-2.250765800476074, -0.9735512733459473]\n",
      "[0.5414241552352905, -0.6983504295349121]\n",
      "[0.9426912665367126, -2.1986279487609863]\n",
      "[-0.7312371730804443, -1.3320986032485962]\n",
      "[-0.7536288499832153, -0.7684624195098877]\n",
      "[-0.8534348607063293, -1.0783787965774536]\n",
      "[-1.9041156768798828, -0.7487426996231079]\n",
      "[1.0604205131530762, -1.143182635307312]\n",
      "[0.9037197232246399, -1.5183829069137573]\n",
      "[-0.14144892990589142, -1.2236403226852417]\n",
      "[0.2060829997062683, -1.915747880935669]\n",
      "[-1.4223995208740234, -0.7984822988510132]\n",
      "[0.954799473285675, -1.3085297346115112]\n",
      "[-0.20820431411266327, -1.2223775386810303]\n",
      "[1.6704766750335693, -1.7602250576019287]\n",
      "[-1.5186290740966797, -0.9181035757064819]\n",
      "[-2.1783597469329834, -0.5940382480621338]\n",
      "[-2.193167209625244, -0.8433475494384766]\n",
      "[-0.22232361137866974, -1.6529232263565063]\n",
      "[-2.07081937789917, -0.8735229969024658]\n",
      "[-0.944997251033783, -0.4903477132320404]\n",
      "[0.1588364839553833, -1.5633212327957153]\n",
      "[1.2514383792877197, -1.5909322500228882]\n",
      "[-0.23966553807258606, -1.5127594470977783]\n",
      "[-2.0828027725219727, -0.7168669700622559]\n",
      "[-1.9814634323120117, -0.8238518238067627]\n",
      "[-1.0034857988357544, -1.2310762405395508]\n",
      "[0.9977615475654602, -1.6049952507019043]\n",
      "[0.46670663356781006, -0.6343098878860474]\n",
      "[-1.2310700416564941, -1.271043062210083]\n",
      "[-0.7360435724258423, -1.4025224447250366]\n",
      "[-0.7952619194984436, -0.5463405847549438]\n",
      "[-0.8793606162071228, -1.3412678241729736]\n",
      "[-1.6258008480072021, -0.6261467933654785]\n",
      "[1.8018999099731445, -1.879311203956604]\n",
      "[-1.935194492340088, -0.7981319427490234]\n",
      "[-0.7829332947731018, -1.0221115350723267]\n",
      "[0.43032848834991455, -0.6031302213668823]\n",
      "[0.7440747618675232, -0.8720424175262451]\n",
      "[-1.0053215026855469, -0.5543527603149414]\n",
      "[-1.099670171737671, -0.5792555809020996]\n",
      "[0.679701030254364, -1.5600509643554688]\n",
      "[-1.249229907989502, -1.2037688493728638]\n",
      "[1.6894445419311523, -1.6823196411132812]\n",
      "[0.2998420000076294, -1.647910714149475]\n",
      "[0.6853958964347839, -0.8217487335205078]\n",
      "[0.89947909116745, -1.1207404136657715]\n",
      "[-1.2926785945892334, -0.899528980255127]\n",
      "[2.1091179847717285, -2.042022228240967]\n",
      "[2.2508533000946045, -2.163503646850586]\n",
      "[-0.41605567932128906, -1.280296802520752]\n",
      "[-1.6098871231079102, -0.48525145649909973]\n",
      "[-1.0351653099060059, -0.4308513104915619]\n",
      "[0.1894979476928711, -0.47897323966026306]\n",
      "[0.441336452960968, -0.6125651597976685]\n",
      "[1.5475058555603027, -1.8278249502182007]\n",
      "[0.6863078474998474, -1.9616363048553467]\n",
      "[0.4589836001396179, -1.351294755935669]\n",
      "[0.491577684879303, -1.3917115926742554]\n",
      "[-0.030485987663269043, -0.4460359513759613]\n",
      "[-1.1640405654907227, -1.2102372646331787]\n",
      "[-2.001854419708252, -0.8351867198944092]\n",
      "[-0.7091214060783386, -1.3729543685913086]\n",
      "[0.949330747127533, -2.159061908721924]\n",
      "[0.584221601486206, -0.7350322008132935]\n",
      "[-0.8285012245178223, -1.4037103652954102]\n",
      "[1.11757493019104, -1.4786525964736938]\n",
      "[-0.3433247208595276, -1.4449865818023682]\n",
      "[-0.0012148618698120117, -1.2527505159378052]\n",
      "[-2.002795457839966, -0.5531352758407593]\n",
      "[0.03739657998085022, -1.681567668914795]\n",
      "[0.56876140832901, -1.4969338178634644]\n",
      "[1.240149974822998, -1.776862621307373]\n",
      "[-1.9501681327819824, -0.8346841335296631]\n",
      "[1.686108112335205, -1.6794599294662476]\n",
      "[-0.1866343468427658, -1.6980148553848267]\n",
      "[-0.4968760907649994, -1.3833452463150024]\n",
      "[-1.979968786239624, -0.8230210542678833]\n",
      "[-0.9642615914344788, -0.8525292873382568]\n",
      "[-0.07930579781532288, -0.42084425687789917]\n",
      "[0.4719330668449402, -0.6387895345687866]\n",
      "[-0.4985824227333069, -1.5904258489608765]\n",
      "[0.45629751682281494, -0.6253882646560669]\n",
      "[-1.5412938594818115, -0.912226676940918]\n",
      "[-0.694892406463623, -0.7429436445236206]\n",
      "[-0.5610966086387634, -1.4209541082382202]\n",
      "[0.16767263412475586, -1.3140835762023926]\n",
      "[0.5004348158836365, -0.6632183790206909]\n",
      "[-2.06321120262146, -0.6281765699386597]\n",
      "[-0.12992924451828003, -1.2330982685089111]\n",
      "[-0.6718118786811829, -1.2628247737884521]\n",
      "[-2.242943525314331, -0.9177778959274292]\n",
      "[1.3765835762023926, -1.4141662120819092]\n",
      "[-0.6818312406539917, -1.2629523277282715]\n",
      "[-1.2640633583068848, -0.5572885274887085]\n",
      "[0.07327547669410706, -0.6345349550247192]\n",
      "[-0.45222947001457214, -1.5846896171569824]\n",
      "[0.5733624696731567, -0.8102452754974365]\n",
      "[0.8024047017097473, -1.692374587059021]\n",
      "[-2.161090850830078, -0.6318142414093018]\n",
      "[-0.682228684425354, -1.441291093826294]\n",
      "[-0.8587521910667419, -1.2733088731765747]\n",
      "[0.24453365802764893, -0.4909789264202118]\n",
      "[2.202885150909424, -2.122390031814575]\n",
      "[0.7162325978279114, -0.8481788635253906]\n",
      "[-0.10555911064147949, -0.7265596389770508]\n",
      "[0.5668136477470398, -0.7678245306015015]\n",
      "[1.1049854755401611, -1.1998709440231323]\n",
      "[-1.7638111114501953, -0.3689279556274414]\n",
      "[-0.5800608992576599, -0.2810001075267792]\n",
      "[-0.09072846174240112, -0.48046019673347473]\n",
      "[1.7180685997009277, -1.7068533897399902]\n",
      "[0.6042026281356812, -0.8424235582351685]\n",
      "[-0.741707444190979, -1.6116364002227783]\n",
      "[-0.2299107015132904, -0.47869595885276794]\n",
      "[-0.2714031934738159, -0.4140860140323639]\n",
      "[0.9556418061256409, -1.0533767938613892]\n",
      "[0.6552413105964661, -0.8407888412475586]\n",
      "[1.0856037139892578, -1.1647671461105347]\n",
      "[1.0337231159210205, -1.1203001737594604]\n",
      "[1.5034873485565186, -1.5229355096817017]\n",
      "[0.10467952489852905, -0.9255071878433228]\n",
      "[0.4542164206504822, -0.6236045360565186]\n",
      "[-2.7041282653808594, -0.6199526786804199]\n",
      "[0.9419334530830383, -1.041627287864685]\n",
      "[0.7283243536949158, -0.8585426807403564]\n",
      "[1.4144630432128906, -1.446632742881775]\n",
      "[1.0796005725860596, -1.1596218347549438]\n",
      "[-1.8181545734405518, -1.308046579360962]\n",
      "[-1.527651309967041, -0.4780398905277252]\n",
      "[1.4749770164489746, -1.4984993934631348]\n",
      "[0.8047677874565125, -0.9240624904632568]\n",
      "[-0.03713792562484741, -0.9452400207519531]\n",
      "[1.0294620990753174, -1.1166480779647827]\n",
      "[1.658452033996582, -1.6557559967041016]\n",
      "[-2.243293046951294, -0.6827170848846436]\n",
      "[1.7276694774627686, -1.7150821685791016]\n",
      "[-0.06779742240905762, -0.5652263164520264]\n",
      "[-1.3525359630584717, -0.49606606364250183]\n",
      "[-1.8293731212615967, -0.3427641987800598]\n",
      "[0.3450513482093811, -1.0262354612350464]\n",
      "[0.18176215887069702, -0.918182373046875]\n",
      "[-0.2459399700164795, -0.3713030219078064]\n",
      "[1.6356089115142822, -1.6361770629882812]\n",
      "[-0.9368823170661926, -0.26612991094589233]\n",
      "[0.7284368872642517, -0.858639121055603]\n",
      "[-1.7144575119018555, -0.5188068151473999]\n",
      "[-1.2233493328094482, -0.2116556167602539]\n",
      "[0.4540674090385437, -0.6234768629074097]\n",
      "[-1.069737195968628, -0.5475142002105713]\n",
      "[-0.461519330739975, -0.638651967048645]\n",
      "[0.37298327684402466, -0.6449778079986572]\n",
      "[0.6689383387565613, -1.0396478176116943]\n",
      "[0.5137147307395935, -1.1285200119018555]\n",
      "[-0.12493628263473511, -1.2935396432876587]\n",
      "[-1.3793156147003174, -0.5782798528671265]\n",
      "[-2.2916624546051025, -0.8806681632995605]\n",
      "[1.323521375656128, -1.3686866760253906]\n",
      "[-0.5526492595672607, -0.26891180872917175]\n",
      "[-0.5518201589584351, -0.2582593858242035]\n",
      "[0.20931971073150635, -1.3091458082199097]\n",
      "[1.768352746963501, -1.749951958656311]\n",
      "[1.5940954685211182, -1.6005959510803223]\n",
      "[0.995952308177948, -1.0879268646240234]\n",
      "[-2.059756278991699, -1.6925417184829712]\n",
      "[-1.368943214416504, -0.7250756025314331]\n",
      "[1.2148313522338867, -1.2755281925201416]\n",
      "[-2.261221170425415, -0.8369951248168945]\n",
      "[0.6762488484382629, -0.8139086961746216]\n",
      "[-0.5681065320968628, -0.2717152237892151]\n",
      "[0.7639448046684265, -1.0097604990005493]\n",
      "[0.8120935559272766, -0.930341362953186]\n",
      "[-0.2546992897987366, -0.4148857295513153]\n",
      "[-1.9715511798858643, -1.280522108078003]\n",
      "[1.6152353286743164, -1.6187149286270142]\n",
      "[-1.1888458728790283, -0.3675084710121155]\n",
      "[1.3355226516723633, -1.3789730072021484]\n",
      "[0.720086395740509, -0.8514819145202637]\n",
      "[1.6192576885223389, -1.6221624612808228]\n",
      "[0.057839542627334595, -0.7384992837905884]\n",
      "[-0.008487462997436523, -1.213844656944275]\n",
      "[-1.1790707111358643, -0.7029273509979248]\n",
      "[1.4124009609222412, -1.444865345954895]\n",
      "[-0.9859302043914795, -0.9269850254058838]\n",
      "[1.0707781314849854, -1.153568148612976]\n",
      "[-0.13758349418640137, -0.7094888687133789]\n",
      "[0.8777027726173401, -0.9865750074386597]\n",
      "[-0.39942091703414917, -0.35917019844055176]\n",
      "[-0.5419711470603943, -1.2252545356750488]\n",
      "[0.9772574305534363, -1.1592243909835815]\n",
      "[-0.2067377269268036, -0.3974522352218628]\n",
      "[0.47367537021636963, -0.6402828693389893]\n",
      "[1.3765833377838135, -1.4141660928726196]\n",
      "[-2.2623393535614014, -0.5253515243530273]\n",
      "[0.6732383370399475, -0.8113285303115845]\n",
      "[0.29743248224258423, -0.9252313375473022]\n",
      "[-0.7756046056747437, -0.6236437559127808]\n",
      "[1.226677417755127, -1.2856816053390503]\n",
      "[-0.6879593133926392, -1.2785627841949463]\n",
      "[0.23233681917190552, -0.49404796957969666]\n",
      "[1.1059348583221436, -1.209360957145691]\n",
      "[1.5009005069732666, -1.5207184553146362]\n",
      "[1.0518090724945068, -1.1358016729354858]\n",
      "[-0.8094878196716309, -0.9200421571731567]\n",
      "[-0.3526698052883148, -2.0063397884368896]\n",
      "[-2.4120805263519287, -1.0632226467132568]\n",
      "[-0.7033867239952087, -0.9836819171905518]\n",
      "[-2.059893846511841, -0.9455292224884033]\n",
      "[-1.0954641103744507, -1.1998234987258911]\n",
      "[0.6905543208122253, -0.9118263721466064]\n",
      "[-1.3814899921417236, -0.9082069396972656]\n",
      "[-1.913461446762085, -0.7860511541366577]\n",
      "[-1.1922085285186768, -1.6539305448532104]\n",
      "[-1.457514762878418, -1.0030186176300049]\n",
      "[1.703169345855713, -1.9237207174301147]\n",
      "[-1.5275228023529053, -0.9664380550384521]\n",
      "[-1.6484777927398682, -0.9350680112838745]\n",
      "[-1.9165523052215576, -1.0154179334640503]\n",
      "[-0.9758005738258362, -1.0380845069885254]\n",
      "[-1.2483285665512085, -1.1250405311584473]\n",
      "[-1.9942140579223633, -1.1680541038513184]\n",
      "[-0.8465533256530762, -1.3886662721633911]\n",
      "[-0.30565503239631653, -0.8577477931976318]\n",
      "[-2.1442861557006836, -0.9143614768981934]\n",
      "[0.6986512541770935, -1.3679990768432617]\n",
      "[-2.1912641525268555, -1.0065903663635254]\n",
      "[-1.573815107345581, -1.009347915649414]\n",
      "[-1.348568081855774, -1.224197506904602]\n",
      "[-2.3288753032684326, -1.0169706344604492]\n",
      "[1.186180591583252, -1.8585416078567505]\n",
      "[0.32085973024368286, -0.6157705783843994]\n",
      "[-1.1451445817947388, -1.5393471717834473]\n",
      "[-2.357903242111206, -1.0331066846847534]\n",
      "[-0.5792635083198547, -0.8336969614028931]\n",
      "[-2.3735597133636475, -1.0418097972869873]\n",
      "[0.2876013517379761, -1.2383257150650024]\n",
      "[1.380418062210083, -1.667884111404419]\n",
      "[-0.3916708827018738, -1.415162205696106]\n",
      "[-1.423888921737671, -0.9668562412261963]\n",
      "[0.1020919680595398, -0.5801433324813843]\n",
      "[0.4851977229118347, -0.8085551261901855]\n",
      "[-0.26395413279533386, -0.9735935926437378]\n",
      "[-0.48402610421180725, -0.9519388675689697]\n",
      "[-1.7049760818481445, -0.8940035104751587]\n",
      "[-0.8689947128295898, -1.58085298538208]\n",
      "[-0.48952698707580566, -1.0570154190063477]\n",
      "[-1.9987046718597412, -0.9820709228515625]\n",
      "[0.7166151404380798, -1.0101035833358765]\n",
      "[-0.05162087082862854, -0.947729229927063]\n",
      "[-2.0701119899749756, -0.8731297254562378]\n",
      "[1.0854151248931885, -1.1646056175231934]\n",
      "[0.03750038146972656, -1.3671773672103882]\n",
      "[-2.1627566814422607, -0.9246288537979126]\n",
      "[-0.2145833522081375, -1.0119872093200684]\n",
      "[0.35134637355804443, -0.844758152961731]\n",
      "[0.7030078768730164, -0.8368439674377441]\n",
      "[0.37353962659835815, -0.6074504852294922]\n",
      "[1.2386302947998047, -1.7352908849716187]\n",
      "[-1.1803088188171387, -1.4646764993667603]\n",
      "[-1.75050687789917, -0.7659881114959717]\n",
      "[-1.5359365940093994, -0.8542677164077759]\n",
      "[0.6145124435424805, -0.7609944343566895]\n",
      "[-1.1811020374298096, -1.460048794746399]\n",
      "[-0.46840107440948486, -1.369484543800354]\n",
      "[-2.4130334854125977, -1.063752293586731]\n",
      "[1.1208152770996094, -2.3564038276672363]\n",
      "[0.8319583535194397, -1.014023780822754]\n",
      "[-2.271677255630493, -1.1606518030166626]\n",
      "[0.9273979067802429, -1.5221786499023438]\n",
      "[-2.3367807865142822, -1.0213651657104492]\n",
      "[-1.3485819101333618, -0.9090911149978638]\n",
      "[-0.6070244908332825, -1.0851470232009888]\n",
      "[-2.042017936706543, -1.0829523801803589]\n",
      "[-1.7416887283325195, -0.858600378036499]\n",
      "[1.2534027099609375, -1.9424245357513428]\n",
      "[-0.8398863673210144, -1.4513639211654663]\n",
      "[0.36361443996429443, -1.111396312713623]\n",
      "[-1.7345168590545654, -1.3412749767303467]\n",
      "[-2.3931498527526855, -1.0526994466781616]\n",
      "[-0.8971596956253052, -1.3861298561096191]\n",
      "[-1.0806705951690674, -1.006934642791748]\n",
      "[0.5533930659294128, -0.7086089849472046]\n",
      "[0.03445994853973389, -0.6051536798477173]\n",
      "[-1.5425114631652832, -1.3882226943969727]\n",
      "[0.8378918766975403, -0.9524531364440918]\n",
      "[-1.747586727142334, -1.1145224571228027]\n",
      "[-0.6553330421447754, -0.9607594013214111]\n",
      "[-2.351127862930298, -1.029340386390686]\n",
      "[-1.5225017070770264, -0.876356840133667]\n",
      "[1.1693871021270752, -1.236578106880188]\n",
      "[-0.5235464572906494, -1.1279029846191406]\n",
      "[-1.7643818855285645, -0.8141014575958252]\n",
      "[-2.3836185932159424, -1.0474011898040771]\n",
      "[-0.6818313598632812, -1.2629523277282715]\n",
      "[0.6732384562492371, -0.8113285303115845]\n",
      "[-2.421565055847168, -1.0684949159622192]\n",
      "[-0.6663655042648315, -0.9297521114349365]\n",
      "[-0.615675687789917, -0.7541531324386597]\n",
      "[-2.041717529296875, -1.2239367961883545]\n",
      "[1.3857462406158447, -1.4539586305618286]\n",
      "[-2.1341652870178223, -0.9087355136871338]\n",
      "[-0.4026016592979431, -1.125975489616394]\n",
      "[-1.7858924865722656, -1.3093907833099365]\n",
      "[-2.282780885696411, -0.9913477897644043]\n",
      "[0.6659846901893616, -0.8051114082336426]\n",
      "[2.0301673412323, -2.043613910675049]\n",
      "[-0.28397229313850403, -1.0531549453735352]\n",
      "[-1.4375452995300293, -0.37998947501182556]\n",
      "[-1.3232964277267456, -0.7466398477554321]\n",
      "[-1.644622802734375, -0.527517557144165]\n",
      "[0.2509257197380066, -0.694193959236145]\n",
      "[-0.1613292694091797, -1.0317741632461548]\n",
      "[-1.093510389328003, -0.7776734828948975]\n",
      "[1.2245724201202393, -1.7442866563796997]\n",
      "[-1.6896109580993652, -0.5316749811172485]\n",
      "[-0.2674488425254822, -1.0242283344268799]\n",
      "[-1.0643129348754883, -0.7911380529403687]\n",
      "[1.6055772304534912, -1.6104369163513184]\n",
      "[-1.369521141052246, -0.6864491701126099]\n",
      "[-1.7771492004394531, -0.4803905189037323]\n",
      "[-1.773207426071167, -0.5206340551376343]\n",
      "[0.49445390701293945, -1.4337643384933472]\n",
      "[-0.384909451007843, -0.7849123477935791]\n",
      "[-1.2541310787200928, -0.29941728711128235]\n",
      "[0.31240636110305786, -1.2184282541275024]\n",
      "[-0.3140110373497009, -1.2527154684066772]\n",
      "[0.34053951501846313, -1.238609790802002]\n",
      "[-1.9036586284637451, -0.5205469131469727]\n",
      "[-0.33511462807655334, -0.8255583047866821]\n",
      "[-0.17322956025600433, -1.010459542274475]\n",
      "[-0.5535703301429749, -1.2717746496200562]\n",
      "[-0.3224080502986908, -0.3509722352027893]\n",
      "[0.45810550451278687, -1.266514539718628]\n",
      "[-0.02614268660545349, -1.1834923028945923]\n",
      "[-1.2933449745178223, -0.3252504765987396]\n",
      "[-0.09397736191749573, -1.1142929792404175]\n",
      "[-0.5017465949058533, -0.6437369585037231]\n",
      "[0.4653131365776062, -1.3724052906036377]\n",
      "[-0.014515787363052368, -0.8113071918487549]\n",
      "[-1.3928043842315674, -0.6397241353988647]\n",
      "[-0.3496054708957672, -0.3089545667171478]\n",
      "[0.04835191369056702, -0.540703296661377]\n",
      "[-1.3454346656799316, -0.31578683853149414]\n",
      "[-1.3978948593139648, -0.3443211317062378]\n",
      "[-0.21646744012832642, -1.0800747871398926]\n",
      "[0.5616514086723328, -1.233837366104126]\n",
      "[1.107269287109375, -1.2863091230392456]\n",
      "[0.6960933804512024, -1.3654783964157104]\n",
      "[-0.35846367478370667, -0.4710254967212677]\n",
      "[-0.27883636951446533, -0.6127852201461792]\n",
      "[-1.717073678970337, -0.6243549585342407]\n",
      "[1.7345554828643799, -1.7581212520599365]\n",
      "[1.853325366973877, -1.822782039642334]\n",
      "[-1.0104658603668213, -0.8974350690841675]\n",
      "[-1.432159423828125, -0.35353079438209534]\n",
      "[-1.1866168975830078, -0.28406476974487305]\n",
      "[-0.5323182344436646, -0.3656661808490753]\n",
      "[-0.3863479197025299, -0.3412543535232544]\n",
      "[0.0484929084777832, -1.3964751958847046]\n",
      "[1.3137881755828857, -1.711406946182251]\n",
      "[-0.6495646834373474, -0.8898528814315796]\n",
      "[-0.8058175444602966, -0.881493330001831]\n",
      "[-0.6069660782814026, -0.3327580690383911]\n",
      "[0.43821918964385986, -1.1874979734420776]\n",
      "[-0.3999205231666565, -0.7290511131286621]\n",
      "[-0.08069011569023132, -1.1012104749679565]\n",
      "[-0.4811508059501648, -1.8744229078292847]\n",
      "[-0.9970869421958923, -0.307643324136734]\n",
      "[0.041242778301239014, -1.19266676902771]\n",
      "[-0.5355868339538574, -1.0852869749069214]\n",
      "[-0.16636863350868225, -1.09554922580719]\n",
      "[-1.038116693496704, -0.7928396463394165]\n",
      "[-1.6379797458648682, -0.4206465184688568]\n",
      "[0.4554970860481262, -1.3861799240112305]\n",
      "[-0.26785027980804443, -1.0363754034042358]\n",
      "[-0.3415065407752991, -1.4261382818222046]\n",
      "[-0.0077811479568481445, -0.908306360244751]\n",
      "[0.9806495308876038, -1.1620222330093384]\n",
      "[0.709242045879364, -1.5053365230560303]\n",
      "[-0.27803856134414673, -1.0501500368118286]\n",
      "[-0.08270007371902466, -0.8635556697845459]\n",
      "[-1.570887565612793, -0.4793716371059418]\n",
      "[-0.6179769039154053, -0.3031117022037506]\n",
      "[-0.6610822677612305, -0.33630046248435974]\n",
      "[0.6842470765113831, -1.4883410930633545]\n",
      "[-0.5303181409835815, -0.3734475076198578]\n",
      "[-1.3446402549743652, -0.6900694370269775]\n",
      "[-1.3872308731079102, -0.36299723386764526]\n",
      "[0.02863052487373352, -1.15565025806427]\n",
      "[-0.9506397247314453, -0.8361437320709229]\n",
      "[-0.5437002182006836, -0.4341030418872833]\n",
      "[-1.653944969177246, -0.4167259633541107]\n",
      "[-1.106579065322876, -0.8013155460357666]\n",
      "[-0.8831570744514465, -0.9282660484313965]\n",
      "[-1.2640633583068848, -0.557288408279419]\n",
      "[0.2974323034286499, -0.9252316951751709]\n",
      "[-0.6663656234741211, -0.9297521114349365]\n",
      "[-1.4058363437652588, -0.3382587432861328]\n",
      "[-1.0775502920150757, -0.2788401246070862]\n",
      "[0.3911352753639221, -1.3764066696166992]\n",
      "[-0.7556924819946289, -0.49796387553215027]\n",
      "[0.8519147038459778, -1.3604621887207031]\n",
      "[-1.6839320659637451, -0.430140882730484]\n",
      "[0.4353069067001343, -1.2892767190933228]\n",
      "[-0.05086275935173035, -1.0481148958206177]\n",
      "[-0.654323935508728, -0.30222249031066895]\n",
      "[2.052403450012207, -1.9934120178222656]\n",
      "[-0.24012604355812073, -0.8752436637878418]\n",
      "[-1.04458749294281, -0.2805391848087311]\n",
      "[-1.2409926652908325, -0.5441920757293701]\n",
      "[-0.5516533851623535, -0.5407662391662598]\n",
      "[-0.7025865316390991, -0.5914390087127686]\n",
      "[0.5477862358093262, -0.8760735988616943]\n",
      "[-0.3023379445075989, -0.6964449882507324]\n",
      "[1.5535573959350586, -1.5658507347106934]\n",
      "[-1.298475742340088, -0.4237763583660126]\n",
      "[-0.39617860317230225, -1.3048982620239258]\n",
      "[-1.1795551776885986, -0.5306822061538696]\n",
      "[1.0029683113098145, -1.093940258026123]\n",
      "[-0.6691615581512451, -0.6248307228088379]\n",
      "[-1.3560471534729004, -0.386673241853714]\n",
      "[-0.5931533575057983, -0.5539687871932983]\n",
      "[0.7969430088996887, -1.2350945472717285]\n",
      "[0.6315889954566956, -0.8330937623977661]\n",
      "[-0.47328102588653564, -0.24985110759735107]\n",
      "[0.5887634754180908, -1.0083757638931274]\n",
      "[-1.2687509059906006, -0.9979890584945679]\n",
      "[0.5935072302818298, -1.038150668144226]\n",
      "[-1.4811406135559082, -0.42718634009361267]\n",
      "[0.5109225511550903, -0.8231770992279053]\n",
      "[0.010412544012069702, -0.8872140645980835]\n",
      "[-1.0561656951904297, -1.3226262331008911]\n",
      "[-1.2121325731277466, -0.28965139389038086]\n",
      "[1.1623847484588623, -1.2305761575698853]\n",
      "[0.33938664197921753, -1.0140458345413208]\n",
      "[-0.9921079277992249, -0.24548736214637756]\n",
      "[0.24806803464889526, -0.9692674875259399]\n",
      "[0.5962569713592529, -0.7453476190567017]\n",
      "[-0.46991968154907227, -1.2359395027160645]\n",
      "[0.8135858178138733, -0.9316203594207764]\n",
      "[-1.3970541954040527, -0.41519081592559814]\n",
      "[-1.1349139213562012, -0.2530957758426666]\n",
      "[-0.7955236434936523, -0.5020154714584351]\n",
      "[-0.674187183380127, -0.26432597637176514]\n",
      "[-0.8651638627052307, -0.2685683071613312]\n",
      "[0.5571553111076355, -0.9184014797210693]\n",
      "[1.2136917114257812, -1.274551510810852]\n",
      "[0.8060272336006165, -0.9868456125259399]\n",
      "[1.1320207118988037, -1.204551339149475]\n",
      "[-1.0969280004501343, -0.47008469700813293]\n",
      "[-0.21755892038345337, -0.5281801223754883]\n",
      "[-1.5282299518585205, -0.45317092537879944]\n",
      "[0.9864113926887512, -1.409192442893982]\n",
      "[1.2740800380706787, -1.3771229982376099]\n",
      "[-1.033155918121338, -0.6663855314254761]\n",
      "[-0.5652201771736145, -0.33845484256744385]\n",
      "[-0.05050736665725708, -0.3450932800769806]\n",
      "[-0.3528103828430176, -0.6308974027633667]\n",
      "[-1.1588819026947021, -0.30472442507743835]\n",
      "[-0.8131375312805176, -1.284341812133789]\n",
      "[1.455984354019165, -1.4822206497192383]\n",
      "[0.13989567756652832, -0.7397830486297607]\n",
      "[-0.03379511833190918, -0.71967613697052]\n",
      "[-0.11308637261390686, -0.5996478796005249]\n",
      "[1.0792179107666016, -1.1592938899993896]\n",
      "[0.6755262017250061, -0.8132894039154053]\n",
      "[0.04704403877258301, -0.9421367645263672]\n",
      "[-1.0576001405715942, -1.85191011428833]\n",
      "[-1.4430983066558838, -0.3967389762401581]\n",
      "[0.4813470244407654, -1.0530914068222046]\n",
      "[-1.2718758583068848, -1.0145869255065918]\n",
      "[-0.3129681348800659, -0.8855855464935303]\n",
      "[-1.027204990386963, -0.5246330499649048]\n",
      "[-0.8511360287666321, -0.38520848751068115]\n",
      "[0.6194905638694763, -1.149836778640747]\n",
      "[0.5786752700805664, -0.9325953722000122]\n",
      "[-0.9655165076255798, -1.415549397468567]\n",
      "[0.8436720967292786, -0.9574073553085327]\n",
      "[0.721449077129364, -1.0715510845184326]\n",
      "[1.063840389251709, -1.3200958967208862]\n",
      "[-0.3814660608768463, -0.857923150062561]\n",
      "[0.7849610447883606, -0.9070860147476196]\n",
      "[-1.23390531539917, -0.36558228731155396]\n",
      "[-0.3121521472930908, -0.554390549659729]\n",
      "[-0.889876127243042, -0.31518590450286865]\n",
      "[1.1841869354248047, -1.3374651670455933]\n",
      "[-0.963311493396759, -0.501258134841919]\n",
      "[-0.5702139735221863, -0.639556884765625]\n",
      "[-0.9993177056312561, -0.25318020582199097]\n",
      "[0.32960325479507446, -0.9757485389709473]\n",
      "[-0.7000524401664734, -0.6262917518615723]\n",
      "[-0.5614374876022339, -0.7273141145706177]\n",
      "[-0.5625599026679993, -0.4298533499240875]\n",
      "[-0.9814996719360352, -0.5656733512878418]\n",
      "[-0.7644820213317871, -0.7295911312103271]\n",
      "[0.07327526807785034, -0.6345349550247192]\n",
      "[-0.7756044864654541, -0.6236438751220703]\n",
      "[-0.615675687789917, -0.7541530132293701]\n",
      "[-1.0775502920150757, -0.2788401246070862]\n",
      "[-0.8425489664077759, -0.14751027524471283]\n",
      "[0.764271080493927, -1.2044838666915894]\n",
      "[-0.6374602317810059, -0.8571068048477173]\n",
      "[1.0251843929290771, -1.1129817962646484]\n",
      "[-0.5636647343635559, -0.44983091950416565]\n",
      "[0.9279438853263855, -1.1531144380569458]\n",
      "[0.2307005524635315, -0.9084742069244385]\n",
      "[-0.9874370098114014, -0.42803725600242615]\n",
      "[-1.5178301334381104, -1.9214779138565063]\n",
      "[-2.5334970951080322, -1.1307154893875122]\n",
      "[0.04526934027671814, -1.4450559616088867]\n",
      "[-1.2750532627105713, -1.4413588047027588]\n",
      "[-0.6948307752609253, -1.5755832195281982]\n",
      "[1.0847804546356201, -1.1640615463256836]\n",
      "[-0.6385960578918457, -1.368515968322754]\n",
      "[-1.2331230640411377, -1.231075644493103]\n",
      "[-2.383721351623535, -1.5472115278244019]\n",
      "[-0.835243821144104, -1.4454127550125122]\n",
      "[2.232696533203125, -2.1479413509368896]\n",
      "[-0.7415003776550293, -1.4647722244262695]\n",
      "[-1.615565538406372, -1.1126843690872192]\n",
      "[-1.518852710723877, -1.3939659595489502]\n",
      "[-0.3615793287754059, -1.4636234045028687]\n",
      "[-0.950076162815094, -1.4737558364868164]\n",
      "[-2.795797824859619, -1.2765228748321533]\n",
      "[-0.9044368863105774, -1.6353875398635864]\n",
      "[0.6263734102249146, -1.2940142154693604]\n",
      "[-1.9093079566955566, -1.1852567195892334]\n",
      "[1.684800624847412, -1.818362832069397]\n",
      "[-2.5798473358154297, -1.1564805507659912]\n",
      "[-1.0038312673568726, -1.4381258487701416]\n",
      "[-1.5638329982757568, -1.424025297164917]\n",
      "[-2.3885927200317383, -1.2546203136444092]\n",
      "[2.0245442390441895, -2.216937780380249]\n",
      "[0.8698195815086365, -0.9798184633255005]\n",
      "[-1.8193566799163818, -1.6051414012908936]\n",
      "[-2.804151773452759, -1.2811665534973145]\n",
      "[0.6358944773674011, -1.2825872898101807]\n",
      "[-2.661877155303955, -1.2020792961120605]\n",
      "[0.4961111545562744, -1.533996343612671]\n",
      "[2.119816541671753, -2.058900833129883]\n",
      "[-0.5038588047027588, -1.644657850265503]\n",
      "[-0.6172419786453247, -1.4671542644500732]\n",
      "[0.9733580946922302, -1.068561315536499]\n",
      "[0.9365969300270081, -1.0370532274246216]\n",
      "[0.38866549730300903, -1.4014484882354736]\n",
      "[0.36759769916534424, -1.3882455825805664]\n",
      "[-1.0640230178833008, -1.317289113998413]\n",
      "[-1.5008540153503418, -1.6631340980529785]\n",
      "[-0.09669190645217896, -1.3996461629867554]\n",
      "[-2.308283567428589, -1.1050137281417847]\n",
      "[0.823617160320282, -0.9402182102203369]\n",
      "[0.43808090686798096, -1.3472930192947388]\n",
      "[-1.483809471130371, -1.2516368627548218]\n",
      "[1.4786608219146729, -1.5016566514968872]\n",
      "[-0.01591739058494568, -1.5700322389602661]\n",
      "[-1.4189348220825195, -1.3917315006256104]\n",
      "[0.3762495517730713, -1.430837869644165]\n",
      "[0.944506824016571, -1.2449928522109985]\n",
      "[0.9969771504402161, -1.0888051986694336]\n",
      "[0.8368931412696838, -0.9515970945358276]\n",
      "[2.036221981048584, -2.144068717956543]\n",
      "[-2.0861916542053223, -1.4312547445297241]\n",
      "[-0.9248809814453125, -1.2485215663909912]\n",
      "[-0.7030238509178162, -1.3403114080429077]\n",
      "[1.075441598892212, -1.1560572385787964]\n",
      "[-1.7923312187194824, -1.5449427366256714]\n",
      "[-0.4687075614929199, -1.636352777481079]\n",
      "[-2.431874990463257, -1.2531423568725586]\n",
      "[2.1945409774780273, -2.66389536857605]\n",
      "[1.1526436805725098, -1.2222270965576172]\n",
      "[-2.7038190364837646, -1.2863816022872925]\n",
      "[1.7829606533050537, -1.9204132556915283]\n",
      "[-2.1165177822113037, -1.2504853010177612]\n",
      "[-0.5956107974052429, -1.4009263515472412]\n",
      "[-0.032975971698760986, -1.4937312602996826]\n",
      "[-2.4055182933807373, -1.218875527381897]\n",
      "[-1.0650933980941772, -1.292641043663025]\n",
      "[2.112964630126953, -2.3136610984802246]\n",
      "[-1.0479402542114258, -1.6564209461212158]\n",
      "[1.050490140914917, -1.496954083442688]\n",
      "[-2.8384854793548584, -1.300251841545105]\n",
      "[-2.1938812732696533, -1.2283374071121216]\n",
      "[-1.1044203042984009, -1.5913772583007812]\n",
      "[-0.3394702970981598, -1.4785770177841187]\n",
      "[1.0165152549743652, -1.1055514812469482]\n",
      "[1.0420584678649902, -1.1274443864822388]\n",
      "[-2.705845355987549, -1.2844280004501343]\n",
      "[0.9058967232704163, -1.0107401609420776]\n",
      "[-1.3487985134124756, -1.4916309118270874]\n",
      "[0.09095582365989685, -1.4204493761062622]\n",
      "[-2.5991756916046143, -1.1672247648239136]\n",
      "[-0.7904968857765198, -1.3572866916656494]\n",
      "[1.172149419784546, -1.2389456033706665]\n",
      "[-0.032905519008636475, -1.517759084701538]\n",
      "[-1.0144176483154297, -1.3064665794372559]\n",
      "[-2.1342177391052246, -1.1419428586959839]\n",
      "[-0.4522296190261841, -1.5846893787384033]\n",
      "[1.226677417755127, -1.2856816053390503]\n",
      "[-2.041717052459717, -1.2239367961883545]\n",
      "[0.39113515615463257, -1.3764067888259888]\n",
      "[0.7642713189125061, -1.2044838666915894]\n",
      "[-2.9383862018585205, -1.35578453540802]\n",
      "[1.5609123706817627, -1.5721545219421387]\n",
      "[-2.101130962371826, -1.0871914625167847]\n",
      "[0.0755099356174469, -1.518613338470459]\n",
      "[-2.6880433559417725, -1.2951643466949463]\n",
      "[-2.4514355659484863, -1.189916729927063]\n",
      "[0.8714630007743835, -0.981226921081543]\n",
      "[2.187248945236206, -2.1089882850646973]\n",
      "[1.4391474723815918, -1.4677897691726685]\n",
      "[-0.11690264940261841, -0.930146336555481]\n",
      "[0.7995923161506653, -1.1095308065414429]\n",
      "[0.4991610646247864, -0.8716393709182739]\n",
      "[-0.4870772361755371, -1.2514113187789917]\n",
      "[1.318131923675537, -1.6783195734024048]\n",
      "[1.0888781547546387, -1.476489543914795]\n",
      "[1.7929418087005615, -1.7710272073745728]\n",
      "[0.4843931198120117, -0.985345721244812]\n",
      "[-2.4540750980377197, -0.9783498048782349]\n",
      "[0.831360399723053, -1.2639431953430176]\n",
      "[1.6970586776733398, -1.9581701755523682]\n",
      "[0.9468989968299866, -1.1315174102783203]\n",
      "[-0.31305229663848877, -0.5578820705413818]\n",
      "[0.5284318923950195, -0.8474797010421753]\n",
      "[1.6522455215454102, -1.6504364013671875]\n",
      "[0.9146211743354797, -1.018217921257019]\n",
      "[-0.7689591646194458, -0.6319141387939453]\n",
      "[1.704310655593872, -1.775625228881836]\n",
      "[-1.8009226322174072, -1.3392362594604492]\n",
      "[1.7543609142303467, -1.737959623336792]\n",
      "[0.2573522925376892, -0.7610301971435547]\n",
      "[1.0489246845245361, -1.1333296298980713]\n",
      "[1.3753306865692139, -1.4130924940109253]\n",
      "[-2.5542960166931152, -1.2040557861328125]\n",
      "[-0.9668504595756531, -0.9491561651229858]\n",
      "[1.3452262878417969, -1.3872898817062378]\n",
      "[1.3953380584716797, -1.4302407503128052]\n",
      "[-0.9124289155006409, -0.6024909019470215]\n",
      "[1.414682388305664, -1.446820855140686]\n",
      "[0.41560274362564087, -0.6401314735412598]\n",
      "[-0.9248324632644653, -1.5907052755355835]\n",
      "[0.9611149430274963, -1.0580676794052124]\n",
      "[0.44073373079299927, -1.0743104219436646]\n",
      "[-0.8938712477684021, -0.9244309663772583]\n",
      "[-0.5805656313896179, -1.1629496812820435]\n",
      "[-0.4303126335144043, -0.7032270431518555]\n",
      "[-0.559060275554657, -0.682098388671875]\n",
      "[1.4420833587646484, -1.7382055521011353]\n",
      "[1.366762399673462, -1.4057486057281494]\n",
      "[1.1765000820159912, -1.7810602188110352]\n",
      "[1.969022512435913, -1.9680194854736328]\n",
      "[-1.1196351051330566, -1.020010232925415]\n",
      "[0.30127328634262085, -1.3727506399154663]\n",
      "[0.6911970973014832, -1.0409810543060303]\n",
      "[1.1798744201660156, -2.165229320526123]\n",
      "[1.795781135559082, -2.294597864151001]\n",
      "[1.1225826740264893, -1.3303691148757935]\n",
      "[-0.4956042766571045, -0.5615220069885254]\n",
      "[-0.8936685919761658, -0.5699608325958252]\n",
      "[-2.215970516204834, -0.44737932085990906]\n",
      "[-1.202101707458496, -0.8471941947937012]\n",
      "[-1.493065357208252, -1.53423273563385]\n",
      "[2.244964599609375, -2.158456325531006]\n",
      "[1.089515209197998, -1.5251281261444092]\n",
      "[1.1431918144226074, -1.535520315170288]\n",
      "[-1.855452299118042, -0.47235962748527527]\n",
      "[1.4501569271087646, -1.4772261381149292]\n",
      "[0.8541303277015686, -0.9663711786270142]\n",
      "[1.448518991470337, -1.4758222103118896]\n",
      "[-2.6187705993652344, -1.6914724111557007]\n",
      "[-1.7441487312316895, -0.8110471963882446]\n",
      "[1.3679347038269043, -1.4067533016204834]\n",
      "[-2.078148126602173, -1.20600163936615]\n",
      "[1.551335096359253, -1.563946008682251]\n",
      "[0.8399391770362854, -1.321636438369751]\n",
      "[-0.24413037300109863, -0.6410975456237793]\n",
      "[1.7804977893829346, -1.7603614330291748]\n",
      "[1.3510212898254395, -1.7000635862350464]\n",
      "[-2.253993272781372, -1.3873730897903442]\n",
      "[1.0847578048706055, -1.1640421152114868]\n",
      "[1.0450923442840576, -1.871732473373413]\n",
      "[1.6857976913452148, -1.6791939735412598]\n",
      "[1.4989910125732422, -1.519081711769104]\n",
      "[1.052826166152954, -1.1366734504699707]\n",
      "[0.16466307640075684, -1.0395146608352661]\n",
      "[-1.8168091773986816, -0.5077837705612183]\n",
      "[-0.78458571434021, -0.953494668006897]\n",
      "[1.57912015914917, -1.5877605676651]\n",
      "[-1.8430712223052979, -0.6973773241043091]\n",
      "[0.8963595032691956, -1.0824393033981323]\n",
      "[-0.15181857347488403, -0.9115333557128906]\n",
      "[1.5680806636810303, -1.578298568725586]\n",
      "[1.0412163734436035, -1.462932825088501]\n",
      "[-2.5381405353546143, -0.4716382920742035]\n",
      "[-0.08709108829498291, -0.6412335634231567]\n",
      "[0.998224675655365, -1.3944828510284424]\n",
      "[1.3887615203857422, -1.4246039390563965]\n",
      "[0.5733624696731567, -0.8102452754974365]\n",
      "[-0.6879593133926392, -1.2785627841949463]\n",
      "[1.3857462406158447, -1.4539586305618286]\n",
      "[-0.7556924819946289, -0.49796387553215027]\n",
      "[-0.6374602317810059, -0.8571068048477173]\n",
      "[1.5609123706817627, -1.5721545219421387]\n",
      "[-2.8395910263061523, -0.4595138132572174]\n",
      "[1.8768703937530518, -1.9515931606292725]\n",
      "[-0.2595234513282776, -0.5183169841766357]\n",
      "[1.5646443367004395, -1.5753533840179443]\n",
      "[1.5184805393218994, -1.5357862710952759]\n",
      "[-1.8804197311401367, -0.6131829023361206]\n",
      "[-0.9542647004127502, -1.7227599620819092]\n",
      "[-2.3967692852020264, -1.0547114610671997]\n",
      "[0.3816715478897095, -1.2421538829803467]\n",
      "[-1.1373322010040283, -1.1118050813674927]\n",
      "[0.36779069900512695, -1.5749361515045166]\n",
      "[0.4188165068626404, -0.6781947612762451]\n",
      "[-2.541841983795166, -1.1353541612625122]\n",
      "[-2.224667549133301, -0.9590437412261963]\n",
      "[-1.5949418544769287, -1.4038692712783813]\n",
      "[-0.2674884796142578, -1.2812871932983398]\n",
      "[2.130337953567505, -2.359448194503784]\n",
      "[-1.8392889499664307, -0.800180196762085]\n",
      "[-3.1611106395721436, -1.4795923233032227]\n",
      "[-0.595999002456665, -1.305071473121643]\n",
      "[0.48075395822525024, -1.4180731773376465]\n",
      "[0.26347941160202026, -1.5008758306503296]\n",
      "[-2.6642351150512695, -1.2033900022506714]\n",
      "[0.6151930093765259, -1.783586025238037]\n",
      "[1.3358204364776611, -1.379228115081787]\n",
      "[-3.09102463722229, -1.440632939338684]\n",
      "[0.5056619048118591, -1.2126452922821045]\n",
      "[-2.4208056926727295, -1.06807279586792]\n",
      "[-0.27292466163635254, -1.30914306640625]\n",
      "[0.07109114527702332, -1.557233214378357]\n",
      "[-1.3458220958709717, -1.190325140953064]\n",
      "[1.4162189960479736, -2.071458101272583]\n",
      "[0.9976720213890076, -1.0894008874893188]\n",
      "[-0.005286663770675659, -1.7779825925827026]\n",
      "[-2.500666856765747, -1.1124658584594727]\n",
      "[1.270921230316162, -1.3236030340194702]\n",
      "[-1.8990364074707031, -1.0529578924179077]\n",
      "[1.7556686401367188, -1.7681955099105835]\n",
      "[1.1513245105743408, -1.4160902500152588]\n",
      "[1.0511577129364014, -1.8381919860839844]\n",
      "[-0.9732791781425476, -1.031799554824829]\n",
      "[1.030998945236206, -1.1179653406143188]\n",
      "[0.7914298176765442, -0.9126304388046265]\n",
      "[1.0398156642913818, -1.4242459535598755]\n",
      "[1.1937012672424316, -1.4181846380233765]\n",
      "[-2.8912110328674316, -1.3295608758926392]\n",
      "[0.3025144338607788, -1.8524994850158691]\n",
      "[-1.9098668098449707, -0.9157830476760864]\n",
      "[-3.0966742038726807, -1.4437735080718994]\n",
      "[1.1051535606384277, -1.181523323059082]\n",
      "[-0.5618788003921509, -0.7769216299057007]\n",
      "[-1.5124320983886719, -0.9230875968933105]\n",
      "[-0.7940502166748047, -0.6733012199401855]\n",
      "[-1.9493796825408936, -1.1370071172714233]\n",
      "[-2.0842273235321045, -0.8809760808944702]\n",
      "[1.1061103343963623, -1.47754967212677]\n",
      "[1.4931578636169434, -1.5140821933746338]\n",
      "[1.814749002456665, -1.7897182703018188]\n",
      "[1.1635327339172363, -1.231560230255127]\n",
      "[1.114577293395996, -1.658907413482666]\n",
      "[-2.7892520427703857, -1.2728842496871948]\n",
      "[-2.423750877380371, -1.0697098970413208]\n",
      "[-2.4191930294036865, -1.067176342010498]\n",
      "[1.6462986469268799, -1.6453392505645752]\n",
      "[-0.01679900288581848, -1.7080193758010864]\n",
      "[0.9835018515586853, -1.8112261295318604]\n",
      "[-1.8904294967651367, -1.0436995029449463]\n",
      "[1.2795555591583252, -2.5094897747039795]\n",
      "[1.1323745250701904, -1.2048543691635132]\n",
      "[-1.38075590133667, -1.28276526927948]\n",
      "[1.0025830268859863, -1.572359561920166]\n",
      "[-2.453547477722168, -1.086273193359375]\n",
      "[-2.068587303161621, -0.8722822666168213]\n",
      "[0.9250478148460388, -1.5375442504882812]\n",
      "[-2.8983728885650635, -1.3335421085357666]\n",
      "[-2.738863945007324, -1.2448744773864746]\n",
      "[1.3835070133209229, -2.101672649383545]\n",
      "[0.5635316967964172, -1.8215537071228027]\n",
      "[-0.7388982772827148, -0.7837154865264893]\n",
      "[-2.231388807296753, -1.091019630432129]\n",
      "[-2.3063313961029053, -1.0044389963150024]\n",
      "[0.5193051695823669, -1.7547301054000854]\n",
      "[-0.008480161428451538, -1.259762167930603]\n",
      "[1.680767297744751, -1.674882411956787]\n",
      "[1.127737283706665, -1.2341370582580566]\n",
      "[-1.752920389175415, -1.2369647026062012]\n",
      "[1.6324996948242188, -1.633512258529663]\n",
      "[-0.25145888328552246, -1.4472498893737793]\n",
      "[0.3886865973472595, -1.2050416469573975]\n",
      "[-2.3594138622283936, -1.0339462757110596]\n",
      "[-2.25080943107605, -0.9735754728317261]\n",
      "[1.9648385047912598, -1.9183599948883057]\n",
      "[0.9888260960578918, -1.6008903980255127]\n",
      "[-2.224782705307007, -0.9591077566146851]\n",
      "[-2.4571707248687744, -1.0882872343063354]\n",
      "[0.8024049401283264, -1.6923747062683105]\n",
      "[0.23233699798583984, -0.4940479099750519]\n",
      "[-2.1341652870178223, -0.9087355136871338]\n",
      "[0.8519148230552673, -1.3604621887207031]\n",
      "[1.0251846313476562, -1.112981915473938]\n",
      "[-2.101130962371826, -1.0871914625167847]\n",
      "[1.8768706321716309, -1.9515933990478516]\n",
      "[-3.8042118549346924, -1.8370784521102905]\n",
      "[0.9651010632514954, -1.6167042255401611]\n",
      "[-1.0356554985046387, -1.3960280418395996]\n",
      "[-1.645388126373291, -1.0775569677352905]\n",
      "[1.5401389598846436, -1.5543497800827026]\n",
      "[1.6226654052734375, -2.1799988746643066]\n",
      "[-0.37620803713798523, -1.2316844463348389]\n",
      "[-1.3858366012573242, -0.5571914911270142]\n",
      "[-1.3166165351867676, -0.8903547525405884]\n",
      "[-2.072352886199951, -0.5716221332550049]\n",
      "[0.8629763722419739, -0.973953127861023]\n",
      "[0.42999833822250366, -1.3256580829620361]\n",
      "[-0.34360843896865845, -1.0573335886001587]\n",
      "[0.8402475714683533, -1.879590392112732]\n",
      "[-1.844970464706421, -0.6428508758544922]\n",
      "[0.19677197933197021, -0.981189489364624]\n",
      "[-0.8809978365898132, -1.0203937292099]\n",
      "[1.7892496585845947, -1.7678626775741577]\n",
      "[-1.2952803373336792, -0.8033251762390137]\n",
      "[-2.1238386631011963, -0.5360363721847534]\n",
      "[-2.239158868789673, -0.6241623163223267]\n",
      "[0.2509768009185791, -1.5946053266525269]\n",
      "[-1.3526315689086914, -0.7210369110107422]\n",
      "[-1.501852035522461, -0.3591998219490051]\n",
      "[0.373019814491272, -1.4556396007537842]\n",
      "[0.6243417859077454, -1.4468151330947876]\n",
      "[0.17183691263198853, -1.4182075262069702]\n",
      "[-2.1253228187561035, -0.6159138679504395]\n",
      "[-1.1502985954284668, -0.8224906921386719]\n",
      "[-0.5198709964752197, -1.1433441638946533]\n",
      "[0.19192826747894287, -1.3682745695114136]\n",
      "[0.21281540393829346, -0.45705661177635193]\n",
      "[-0.3545636236667633, -1.2818483114242554]\n",
      "[-0.28596627712249756, -1.3376765251159668]\n",
      "[-1.3848872184753418, -0.4058895707130432]\n",
      "[-0.39865416288375854, -1.2569361925125122]\n",
      "[-1.64201021194458, -0.56402587890625]\n",
      "[1.3261613845825195, -1.6118748188018799]\n",
      "[-1.1847072839736938, -0.7031669616699219]\n",
      "[-1.2489725351333618, -0.8495727777481079]\n",
      "[0.025301218032836914, -0.44873008131980896]\n",
      "[0.4983026385307312, -0.6613909006118774]\n",
      "[-1.6355252265930176, -0.37658682465553284]\n",
      "[-1.6638948917388916, -0.4088253080844879]\n",
      "[0.5931337475776672, -1.3941434621810913]\n",
      "[-0.3388291895389557, -1.2278014421463013]\n",
      "[1.5156219005584717, -1.5483444929122925]\n",
      "[0.6379875540733337, -1.575006365776062]\n",
      "[0.41941946744918823, -0.605708122253418]\n",
      "[0.42078667879104614, -0.9113078117370605]\n",
      "[-1.7934331893920898, -0.7435929775238037]\n",
      "[2.025738000869751, -1.9705569744110107]\n",
      "[2.165423631668091, -2.0902817249298096]\n",
      "[-0.6520372629165649, -1.0925211906433105]\n",
      "[-1.7823741436004639, -0.41025277972221375]\n",
      "[-1.5103039741516113, -0.32728299498558044]\n",
      "[-0.4717695713043213, -0.36302250623703003]\n",
      "[0.13814127445220947, -0.45526668429374695]\n",
      "[0.9714280962944031, -1.5743780136108398]\n",
      "[1.1540534496307373, -1.9009207487106323]\n",
      "[-0.019684016704559326, -1.1881327629089355]\n",
      "[-0.14001715183258057, -1.1983754634857178]\n",
      "[-0.787428081035614, -0.3242560923099518]\n",
      "[-0.3350514769554138, -1.2137062549591064]\n",
      "[-1.4826116561889648, -0.6156212091445923]\n",
      "[-0.2989055812358856, -1.265694260597229]\n",
      "[0.21016496419906616, -1.952789306640625]\n",
      "[-0.13932475447654724, -0.4468090236186981]\n",
      "[-0.30486589670181274, -1.3248881101608276]\n",
      "[0.3729724884033203, -1.2539713382720947]\n",
      "[-0.07195985317230225, -1.308150053024292]\n",
      "[-0.7867345809936523, -1.051313877105713]\n",
      "[-2.0158815383911133, -0.472608357667923]\n",
      "[0.3780289888381958, -1.5883551836013794]\n",
      "[0.4875097870826721, -1.3309831619262695]\n",
      "[0.4599064588546753, -1.5476758480072021]\n",
      "[-1.0203303098678589, -0.8692618608474731]\n",
      "[1.4738492965698242, -1.4975327253341675]\n",
      "[0.3768767714500427, -1.6467666625976562]\n",
      "[-0.19629988074302673, -1.2504929304122925]\n",
      "[-1.0979828834533691, -0.8188306093215942]\n",
      "[-1.4685180187225342, -0.6760594844818115]\n",
      "[-0.806807279586792, -0.29145264625549316]\n",
      "[-0.0027066171169281006, -0.494083970785141]\n",
      "[0.17828011512756348, -1.5826761722564697]\n",
      "[0.0015798211097717285, -0.4506737291812897]\n",
      "[-1.3501288890838623, -0.7956922054290771]\n",
      "[-1.3160631656646729, -0.532991886138916]\n",
      "[-0.16441872715950012, -1.32646644115448]\n",
      "[-0.45210501551628113, -1.1200098991394043]\n",
      "[-0.11462420225143433, -0.4651639759540558]\n",
      "[-2.035282850265503, -0.5046894550323486]\n",
      "[-0.6730366945266724, -1.0477474927902222]\n",
      "[-0.4405468702316284, -1.138004183769226]\n",
      "[-2.161090850830078, -0.6318142414093018]\n",
      "[1.1059350967407227, -1.2093610763549805]\n",
      "[-0.4026016592979431, -1.125975489616394]\n",
      "[-1.6839323043823242, -0.430140882730484]\n",
      "[-0.5636646151542664, -0.4498309791088104]\n",
      "[0.07551014423370361, -1.5186134576797485]\n",
      "[-0.25952333211898804, -0.5183168649673462]\n",
      "[0.9651013016700745, -1.6167042255401611]\n",
      "[-2.0139856338500977, -0.5261179208755493]\n",
      "[-0.035960853099823, -1.3934954404830933]\n",
      "[-0.378025084733963, -1.1859573125839233]\n",
      "[-0.22942686080932617, -0.3764726519584656]\n",
      "[-1.3887200355529785, -1.9597866535186768]\n",
      "[-2.2311880588531494, -1.2299754619598389]\n",
      "[0.13167405128479004, -1.3976224660873413]\n",
      "[-1.1338520050048828, -1.465367078781128]\n",
      "[-0.8253016471862793, -1.4678211212158203]\n",
      "[1.116520881652832, -1.1912662982940674]\n",
      "[0.009099751710891724, -1.5580013990402222]\n",
      "[-0.9103981256484985, -1.3399791717529297]\n",
      "[-2.1990904808044434, -1.6142187118530273]\n",
      "[-0.8497002124786377, -1.4107930660247803]\n",
      "[2.229895830154419, -2.145540952682495]\n",
      "[-0.46982520818710327, -1.5390411615371704]\n",
      "[-0.5367854237556458, -1.4125847816467285]\n",
      "[-1.5968420505523682, -1.3498698472976685]\n",
      "[-0.40629759430885315, -1.3782432079315186]\n",
      "[-1.1490416526794434, -1.3738945722579956]\n",
      "[-2.45436692237854, -1.3055139780044556]\n",
      "[-1.2409427165985107, -1.482267141342163]\n",
      "[0.6438195109367371, -1.2047549486160278]\n",
      "[-1.405527114868164, -1.3810334205627441]\n",
      "[1.7695209980010986, -1.9311883449554443]\n",
      "[-2.116978645324707, -1.303285002708435]\n",
      "[-1.0584566593170166, -1.3894200325012207]\n",
      "[-1.9308812618255615, -1.288002848625183]\n",
      "[-2.378225088119507, -1.255643606185913]\n",
      "[2.0201423168182373, -2.254484176635742]\n",
      "[0.8525123000144958, -0.9649842977523804]\n",
      "[-2.251065254211426, -1.4550784826278687]\n",
      "[-2.6515281200408936, -1.1963263750076294]\n",
      "[0.7034787535667419, -1.2065811157226562]\n",
      "[-2.5494463443756104, -1.2288368940353394]\n",
      "[0.20375871658325195, -1.374122977256775]\n",
      "[2.226080894470215, -2.191279649734497]\n",
      "[-0.8637434840202332, -1.4767441749572754]\n",
      "[-0.45614081621170044, -1.4931340217590332]\n",
      "[0.916467010974884, -1.0197999477386475]\n",
      "[0.9445357918739319, -1.0438576936721802]\n",
      "[0.411345899105072, -1.3088494539260864]\n",
      "[0.39545780420303345, -1.3053603172302246]\n",
      "[-0.40491002798080444, -1.523125410079956]\n",
      "[-1.9310684204101562, -1.507220983505249]\n",
      "[0.7311822772026062, -1.6071335077285767]\n",
      "[-1.673421859741211, -1.3364986181259155]\n",
      "[0.9286848902702332, -1.0302718877792358]\n",
      "[0.7660272717475891, -1.390478491783142]\n",
      "[-1.4149162769317627, -1.2643327713012695]\n",
      "[1.8469674587249756, -1.8173326253890991]\n",
      "[0.9957208037376404, -1.8436253070831299]\n",
      "[-1.1311300992965698, -1.4800599813461304]\n",
      "[0.3352823257446289, -1.3233691453933716]\n",
      "[0.839881956577301, -1.1404348611831665]\n",
      "[0.9514656662940979, -1.0497972965240479]\n",
      "[0.8216703534126282, -0.9385496377944946]\n",
      "[2.106715679168701, -2.2421674728393555]\n",
      "[-1.6079998016357422, -1.6124464273452759]\n",
      "[-0.34595686197280884, -1.4178643226623535]\n",
      "[-0.2425822913646698, -1.489426851272583]\n",
      "[1.001058578491211, -1.0923033952713013]\n",
      "[-2.221653938293457, -1.395313024520874]\n",
      "[-0.7751452326774597, -1.4681541919708252]\n",
      "[-2.196786403656006, -1.3314818143844604]\n",
      "[2.1734585762023926, -2.7127766609191895]\n",
      "[1.217378854751587, -1.2777116298675537]\n",
      "[-2.6348648071289062, -1.3091464042663574]\n",
      "[1.8167145252227783, -1.987285852432251]\n",
      "[-1.7201011180877686, -1.389220952987671]\n",
      "[-0.28840988874435425, -1.4988973140716553]\n",
      "[-0.09340569376945496, -1.3899375200271606]\n",
      "[-1.8549132347106934, -1.4274917840957642]\n",
      "[-0.4863925576210022, -1.4776562452316284]\n",
      "[2.1310627460479736, -2.368739366531372]\n",
      "[-1.4246773719787598, -1.4987212419509888]\n",
      "[1.5304827690124512, -1.5639365911483765]\n",
      "[-2.5666017532348633, -1.3566356897354126]\n",
      "[-1.8454844951629639, -1.3482229709625244]\n",
      "[-1.483605146408081, -1.43773353099823]\n",
      "[-0.2664862275123596, -1.4560667276382446]\n",
      "[0.9407157301902771, -1.0405834913253784]\n",
      "[0.959194004535675, -1.0564212799072266]\n",
      "[-2.795255661010742, -1.2762213945388794]\n",
      "[0.9203841090202332, -1.0231572389602661]\n",
      "[-1.4430866241455078, -1.4344106912612915]\n",
      "[0.18476170301437378, -1.3677783012390137]\n",
      "[-2.266143321990967, -1.2653878927230835]\n",
      "[-0.4483191668987274, -1.4729974269866943]\n",
      "[1.1551196575164795, -1.2243493795394897]\n",
      "[-0.13720004260540009, -1.3994394540786743]\n",
      "[-0.7033798694610596, -1.4113833904266357]\n",
      "[-1.8162288665771484, -1.2593244314193726]\n",
      "[-0.6822283267974854, -1.441291093826294]\n",
      "[1.5009009838104248, -1.5207188129425049]\n",
      "[-1.785893201828003, -1.3093907833099365]\n",
      "[0.4353069067001343, -1.2892767190933228]\n",
      "[0.9279438853263855, -1.1531144380569458]\n",
      "[-2.6880433559417725, -1.2951643466949463]\n",
      "[1.5646443367004395, -1.5753533840179443]\n",
      "[-1.0356554985046387, -1.3960280418395996]\n",
      "[-0.035960853099823, -1.3934954404830933]\n",
      "[-2.9001471996307373, -1.3345283269882202]\n",
      "[-2.3831400871276855, -1.2148683071136475]\n",
      "[0.8282142281532288, -0.9441584348678589]\n",
      "[-0.7759401798248291, -1.9665913581848145]\n",
      "[-2.4561007022857666, -1.0876924991607666]\n",
      "[-0.37210312485694885, -1.1398143768310547]\n",
      "[-1.6609196662902832, -1.1487228870391846]\n",
      "[-1.1469206809997559, -1.2529726028442383]\n",
      "[1.0869615077972412, -1.165930986404419]\n",
      "[-0.5924354791641235, -1.2204086780548096]\n",
      "[-1.4012997150421143, -1.0163359642028809]\n",
      "[-1.6192188262939453, -1.6136873960494995]\n",
      "[-1.2944862842559814, -1.1272552013397217]\n",
      "[1.912670612335205, -2.0361852645874023]\n",
      "[-0.986782968044281, -1.2200706005096436]\n",
      "[-0.9936357140541077, -1.1433287858963013]\n",
      "[-1.985550880432129, -1.074813961982727]\n",
      "[-0.8302806615829468, -1.1366726160049438]\n",
      "[-1.3867683410644531, -1.1557567119598389]\n",
      "[-2.252568244934082, -1.1850738525390625]\n",
      "[-1.2070783376693726, -1.3535603284835815]\n",
      "[0.17021387815475464, -0.9695736169815063]\n",
      "[-1.683398723602295, -1.1077607870101929]\n",
      "[1.1768598556518555, -1.6137170791625977]\n",
      "[-2.274867057800293, -1.0757701396942139]\n",
      "[-1.473557710647583, -1.113681674003601]\n",
      "[-1.805391550064087, -1.1657615900039673]\n",
      "[-2.5927112102508545, -1.1636313199996948]\n",
      "[1.560701608657837, -2.0187549591064453]\n",
      "[0.651282012462616, -0.7925096750259399]\n",
      "[-1.7439992427825928, -1.4410282373428345]\n",
      "[-2.547468662261963, -1.1384819746017456]\n",
      "[0.19831126928329468, -0.9627952575683594]\n",
      "[-2.638667106628418, -1.18917715549469]\n",
      "[0.0898018479347229, -1.2313485145568848]\n",
      "[1.7485520839691162, -1.9429988861083984]\n",
      "[-0.7685242891311646, -1.3717948198318481]\n",
      "[-0.9798516035079956, -1.1818888187408447]\n",
      "[0.6395571827888489, -0.7824603319168091]\n",
      "[0.8488790392875671, -0.9618701934814453]\n",
      "[-0.04875636100769043, -1.0795460939407349]\n",
      "[-0.0859430730342865, -1.0670801401138306]\n",
      "[-0.9469738006591797, -1.1953994035720825]\n",
      "[-1.4527373313903809, -1.4866589307785034]\n",
      "[0.25851744413375854, -1.3488552570343018]\n",
      "[-1.715855598449707, -1.1364692449569702]\n",
      "[1.0618393421173096, -1.1443986892700195]\n",
      "[0.5691617131233215, -1.1948658227920532]\n",
      "[-1.9222729206085205, -0.9446220397949219]\n",
      "[1.5640084743499756, -1.5748083591461182]\n",
      "[0.6925780177116394, -1.6323786973953247]\n",
      "[-1.644219160079956, -1.1548383235931396]\n",
      "[-0.08434581756591797, -1.0988092422485352]\n",
      "[0.44863224029541016, -0.9234999418258667]\n",
      "[0.7820317149162292, -0.9045754671096802]\n",
      "[0.5967015624046326, -0.7457287311553955]\n",
      "[1.6255426406860352, -1.983459711074829]\n",
      "[-1.34206223487854, -1.5117440223693848]\n",
      "[-0.9531567096710205, -1.0790411233901978]\n",
      "[-0.7480059266090393, -1.1644976139068604]\n",
      "[0.7412802577018738, -0.8696472644805908]\n",
      "[-1.7711193561553955, -1.364925503730774]\n",
      "[-0.7842181921005249, -1.3434786796569824]\n",
      "[-2.5319385528564453, -1.1298490762710571]\n",
      "[1.5085651874542236, -2.5374786853790283]\n",
      "[1.0817503929138184, -1.1785492897033691]\n",
      "[-2.68617582321167, -1.2155863046646118]\n",
      "[1.338393211364746, -1.7247439622879028]\n",
      "[-2.1262876987457275, -1.0863683223724365]\n",
      "[-0.7949057817459106, -1.1748096942901611]\n",
      "[-0.502419114112854, -1.1661978960037231]\n",
      "[-1.969179630279541, -1.207240104675293]\n",
      "[-0.986538290977478, -1.1574152708053589]\n",
      "[1.656446933746338, -2.119039297103882]\n",
      "[-1.276479959487915, -1.3959283828735352]\n",
      "[1.1222999095916748, -1.3562231063842773]\n",
      "[-2.128532648086548, -1.3126202821731567]\n",
      "[-2.286060094833374, -1.0367109775543213]\n",
      "[-1.334367036819458, -1.3305375576019287]\n",
      "[-0.7587810754776001, -1.172300100326538]\n",
      "[0.6730960011482239, -0.8112064599990845]\n",
      "[0.6643471121788025, -0.8037078380584717]\n",
      "[-2.0925285816192627, -1.3100990056991577]\n",
      "[0.9341806769371033, -1.0349823236465454]\n",
      "[-1.8117318153381348, -1.1714587211608887]\n",
      "[-0.32504451274871826, -1.115411400794983]\n",
      "[-2.4591915607452393, -1.089410662651062]\n",
      "[-0.932456910610199, -1.1517738103866577]\n",
      "[1.206254482269287, -1.2681770324707031]\n",
      "[-0.4931665062904358, -1.1892204284667969]\n",
      "[-1.2058749198913574, -1.0844649076461792]\n",
      "[-2.2507660388946533, -0.9735513925552368]\n",
      "[-0.8587521910667419, -1.2733088731765747]\n",
      "[1.0518090724945068, -1.1358016729354858]\n",
      "[-2.282780885696411, -0.9913477897644043]\n",
      "[-0.050862789154052734, -1.0481148958206177]\n",
      "[0.23070037364959717, -0.9084742069244385]\n",
      "[-2.4514355659484863, -1.189916729927063]\n",
      "[1.5184805393218994, -1.5357863903045654]\n",
      "[-1.645388126373291, -1.0775569677352905]\n",
      "[-0.37802523374557495, -1.1859573125839233]\n",
      "[-2.3831396102905273, -1.2148680686950684]\n",
      "[-2.575611114501953, -1.1541258096694946]\n",
      "[0.7378259301185608, -0.8666865825653076]\n",
      "[1.818389892578125, -1.7928388118743896]\n",
      "[0.5826878547668457, -0.7337175607681274]\n",
      "[-0.4062075912952423, -0.5182325839996338]\n",
      "[0.02769094705581665, -0.49457719922065735]\n",
      "[0.04671591520309448, -0.4378783404827118]\n",
      "[-0.6558530926704407, -0.8862165212631226]\n",
      "[1.0830063819885254, -1.3265268802642822]\n",
      "[0.8593378663063049, -1.1334487199783325]\n",
      "[1.305755615234375, -1.3534594774246216]\n",
      "[-0.022077202796936035, -0.5188812017440796]\n",
      "[-1.5726869106292725, -1.0822253227233887]\n",
      "[0.36434364318847656, -0.8083795309066772]\n",
      "[1.420130729675293, -1.5148265361785889]\n",
      "[0.3337450623512268, -0.5302393436431885]\n",
      "[-0.60296630859375, -0.35496291518211365]\n",
      "[0.045582354068756104, -0.47230860590934753]\n",
      "[0.9057336449623108, -1.0106003284454346]\n",
      "[0.5483958125114441, -0.704325795173645]\n",
      "[-0.8002328872680664, -0.3295840620994568]\n",
      "[1.263737678527832, -1.3174458742141724]\n",
      "[-1.707573413848877, -1.0848588943481445]\n",
      "[1.061549186706543, -1.144149899482727]\n",
      "[-0.39984312653541565, -0.42019784450531006]\n",
      "[0.5672006011009216, -0.7204434871673584]\n",
      "[0.5634539723396301, -0.7172321081161499]\n",
      "[-2.0313501358032227, -1.154581069946289]\n",
      "[-1.223301649093628, -0.5615960359573364]\n",
      "[0.9329444766044617, -1.033922791481018]\n",
      "[0.6544596552848816, -0.7952332496643066]\n",
      "[-0.9114482402801514, -0.29612767696380615]\n",
      "[0.6156817674636841, -0.7619967460632324]\n",
      "[0.41592586040496826, -0.5907857418060303]\n",
      "[-0.8501620292663574, -1.3366559743881226]\n",
      "[0.6638949513435364, -0.8033202886581421]\n",
      "[-0.021281659603118896, -0.6088125705718994]\n",
      "[-1.1762490272521973, -0.5237466096878052]\n",
      "[-0.7671686410903931, -0.7961515188217163]\n",
      "[-0.5816773176193237, -0.3518138527870178]\n",
      "[-0.6768434047698975, -0.33881744742393494]\n",
      "[1.2032113075256348, -1.3821582794189453]\n",
      "[0.9763506054878235, -1.0711262226104736]\n",
      "[0.9981275200843811, -1.3694149255752563]\n",
      "[1.4997210502624512, -1.5197075605392456]\n",
      "[-1.2932767868041992, -0.6640655994415283]\n",
      "[0.08171731233596802, -0.9830049276351929]\n",
      "[-0.11983591318130493, -0.49682357907295227]\n",
      "[1.055903434753418, -1.7754169702529907]\n",
      "[1.5487380027770996, -1.903642177581787]\n",
      "[0.38096827268600464, -0.677354097366333]\n",
      "[-0.37092649936676025, -0.3521941900253296]\n",
      "[-0.6441566348075867, -0.3866179883480072]\n",
      "[-1.5594451427459717, -0.44791826605796814]\n",
      "[-1.4566891193389893, -0.4631482660770416]\n",
      "[-1.3404393196105957, -1.3161927461624146]\n",
      "[1.4412429332733154, -1.4695857763290405]\n",
      "[0.8561397194862366, -1.1538727283477783]\n",
      "[0.887995183467865, -1.1772830486297607]\n",
      "[-1.2368907928466797, -0.45606759190559387]\n",
      "[0.8512400984764099, -0.9638938903808594]\n",
      "[0.5261549949645996, -0.6852631568908691]\n",
      "[0.6287198662757874, -0.7731716632843018]\n",
      "[-2.0001132488250732, -1.6870925426483154]\n",
      "[-2.034418821334839, -0.40645724534988403]\n",
      "[0.6769581437110901, -0.8145166635513306]\n",
      "[-1.9420239925384521, -0.9824780225753784]\n",
      "[0.7658845782279968, -0.8907356262207031]\n",
      "[0.45933598279953003, -0.9054346084594727]\n",
      "[-0.26083847880363464, -0.3630799949169159]\n",
      "[0.9375742077827454, -1.037890911102295]\n",
      "[1.1157636642456055, -1.3514103889465332]\n",
      "[-1.8156511783599854, -1.2997772693634033]\n",
      "[0.6727098822593689, -0.8108755350112915]\n",
      "[0.9083653092384338, -1.5009818077087402]\n",
      "[1.0077383518218994, -1.0980287790298462]\n",
      "[0.6811143755912781, -0.8180789947509766]\n",
      "[0.647098958492279, -0.7889243364334106]\n",
      "[-0.11650258302688599, -0.6236114501953125]\n",
      "[-1.3944344520568848, -0.41223645210266113]\n",
      "[-1.0526013374328613, -0.5588572025299072]\n",
      "[1.0568501949310303, -1.1401225328445435]\n",
      "[-1.9385602474212646, -0.38525649905204773]\n",
      "[0.32094401121139526, -0.5093766450881958]\n",
      "[-0.44025251269340515, -0.5033100843429565]\n",
      "[0.7201955914497375, -0.8515756130218506]\n",
      "[0.7532843947410583, -1.0885441303253174]\n",
      "[-1.8006031513214111, -0.5070637464523315]\n",
      "[-0.10369545221328735, -0.387168288230896]\n",
      "[0.6559545397758484, -0.9990864992141724]\n",
      "[0.5414242744445801, -0.6983505487442017]\n",
      "[0.24453365802764893, -0.4909789264202118]\n",
      "[-0.8094878196716309, -0.9200421571731567]\n",
      "[0.6659848093986511, -0.8051115274429321]\n",
      "[-0.654323935508728, -0.30222246050834656]\n",
      "[-0.987436830997467, -0.4280373156070709]\n",
      "[0.8714630007743835, -0.981226921081543]\n",
      "[-1.8804197311401367, -0.6131829023361206]\n",
      "[1.5401389598846436, -1.5543497800827026]\n",
      "[-0.22942683100700378, -0.37647268176078796]\n",
      "[0.8282142281532288, -0.9441584348678589]\n",
      "[0.7378255724906921, -0.866686224937439]\n",
      "[-1.9508941173553467, -0.2735048234462738]\n",
      "[0.5048500445412254, 52.045918367346935]\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "data_show = datatorch[0]\n",
    "x_edges, x_edges_values, x_nodes, x_nodes_coord, _,  y_edges = data_show\n",
    "output, _ = model(x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges)\n",
    "pred = torch.tensor([[(1 if output[0][i][j][1] > output[0][i][j][0] else 0) for j in range(101)] for i in range(101)], dtype=torch.double)\n",
    "y = y_edges[0]\n",
    "diff = 0\n",
    "for j in range(len(pred)):\n",
    "    a_list = [1 if i > 0.05 else 0 for i in pred[j]]\n",
    "    diff += sum([(y[j][i].tolist() - a_list[i])**2 for i in range(len(a_list))])\n",
    "    print([1 if i*10 > 0.1 else 0 for i in pred[j]], sum(a_list))\n",
    "print(diff)\n",
    "print(sum([y[j][i].tolist() for i in range(101) for j in range(101)]))\n",
    "for i in range(101):\n",
    "    for j in range(101):\n",
    "        print(output[0][i][j].tolist())\n",
    "edge_labels = y_edges.cpu().numpy().flatten()\n",
    "edge_cw = compute_class_weight(\"balanced\", classes=np.unique(edge_labels), y=edge_labels).tolist()\n",
    "print(edge_cw)\n",
    "print(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
