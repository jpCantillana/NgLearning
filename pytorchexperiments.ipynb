{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric as tg\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GraphConv, global_add_pool\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([[0, 1, 1, 2],\n",
    "                           [1, 0, 2, 1]], dtype=torch.long)\n",
    "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = tg.utils.to_networkx(data1, to_undirected=True)\n",
    "nx.draw(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([[0, 1],\n",
    "                           [1, 0],\n",
    "                           [1, 2],\n",
    "                           [2, 1]], dtype=torch.long)\n",
    "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index.t().contiguous())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to add graphs with our need for Ng sets for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_graph_list = []\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if i != j:\n",
    "            complete_graph_list.append([i,j])\n",
    "edge_index = torch.tensor(complete_graph_list, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_to_check = [[2,6], [3,6], [5,6], [4,6]]\n",
    "dependent_arcs = []\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if i != j:\n",
    "            if [i,j] in edges_to_check:\n",
    "                dependent_arcs.append(1)\n",
    "            else:\n",
    "                dependent_arcs.append(0)\n",
    "y = torch.tensor(dependent_arcs, dtype=torch.long)\n",
    "x = torch.tensor([[0, 3, 2], [1, 1, 6], [2, 3, 6], [3, 5, 6], [4, 6, 5], [5, 4, 4], [6, 3, 4], [7, 1, 3]], dtype=torch.float)\n",
    "attr = [[i] for i in range(len(edge_index))]\n",
    "loc_list = [[0, 3, 2], [1, 1, 6], [2, 3, 6], [3, 5, 6], [4, 6, 5], [5, 4, 4], [6, 3, 4], [7, 1, 3]]\n",
    "loc_dict = {(i[0],j[0]): sqrt((i[1]-j[1])**2 + (i[2]-j[2])**2) if i != j else next for i in loc_list for j in loc_list}\n",
    "cnt = -1\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if i != j:\n",
    "            cnt += 1\n",
    "            attr[cnt].append(loc_dict[i,j])\n",
    "attr = torch.tensor(attr, dtype=torch.long)\n",
    "pos = []\n",
    "for i in loc_list:\n",
    "    pos.append([i[1], i[2]])\n",
    "pos = torch.tensor(pos)\n",
    "\n",
    "data1 = Data(x=x, y=y.t().contiguous(), edge_index=edge_index.t().contiguous(), pos=pos, edge_attr=attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  4],\n",
      "        [ 1,  4],\n",
      "        [ 2,  4],\n",
      "        [ 3,  4],\n",
      "        [ 4,  2],\n",
      "        [ 5,  2],\n",
      "        [ 6,  2],\n",
      "        [ 7,  4],\n",
      "        [ 8,  2],\n",
      "        [ 9,  4],\n",
      "        [10,  5],\n",
      "        [11,  3],\n",
      "        [12,  2],\n",
      "        [13,  3],\n",
      "        [14,  4],\n",
      "        [15,  2],\n",
      "        [16,  2],\n",
      "        [17,  3],\n",
      "        [18,  2],\n",
      "        [19,  2],\n",
      "        [20,  3],\n",
      "        [21,  4],\n",
      "        [22,  4],\n",
      "        [23,  2],\n",
      "        [24,  1],\n",
      "        [25,  2],\n",
      "        [26,  2],\n",
      "        [27,  5],\n",
      "        [28,  4],\n",
      "        [29,  5],\n",
      "        [30,  3],\n",
      "        [31,  1],\n",
      "        [32,  2],\n",
      "        [33,  3],\n",
      "        [34,  5],\n",
      "        [35,  2],\n",
      "        [36,  3],\n",
      "        [37,  2],\n",
      "        [38,  2],\n",
      "        [39,  2],\n",
      "        [40,  1],\n",
      "        [41,  3],\n",
      "        [42,  2],\n",
      "        [43,  2],\n",
      "        [44,  2],\n",
      "        [45,  2],\n",
      "        [46,  3],\n",
      "        [47,  1],\n",
      "        [48,  2],\n",
      "        [49,  2],\n",
      "        [50,  3],\n",
      "        [51,  3],\n",
      "        [52,  5],\n",
      "        [53,  5],\n",
      "        [54,  3],\n",
      "        [55,  2]])\n"
     ]
    }
   ],
   "source": [
    "print(data1.edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_to_check = [[3,1], [2,1]]\n",
    "dependent_arcs = []\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if i != j:\n",
    "            if [i,j] in edges_to_check:\n",
    "                dependent_arcs.append(1)\n",
    "            else:\n",
    "                dependent_arcs.append(0)\n",
    "y = torch.tensor(dependent_arcs, dtype=torch.long)\n",
    "x = torch.tensor([[0, 4, 4], [1, 2, 4], [2, 2, 5], [3, 2, 2], [4, 4, 1], [5, 6, 6], [6, 7, 3], [7, 3, 7]], dtype=torch.float)\n",
    "attr = [[i] for i in range(len(edge_index))]\n",
    "loc_list = [[0, 4, 4], [1, 2, 4], [2, 2, 5], [3, 2, 2], [4, 4, 1], [5, 6, 6], [6, 7, 3], [7, 3, 7]]\n",
    "loc_dict = {(i[0],j[0]): sqrt((i[1]-j[1])**2 + (i[2]-j[2])**2) if i != j else next for i in loc_list for j in loc_list}\n",
    "cnt = -1\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if i != j:\n",
    "            cnt += 1\n",
    "            attr[cnt].append(loc_dict[i,j])\n",
    "attr = torch.tensor(attr, dtype=torch.long)\n",
    "pos = []\n",
    "for i in loc_list:\n",
    "    pos.append([i[1], i[2]])\n",
    "pos = torch.tensor(pos)\n",
    "\n",
    "data2 = Data(x=x, y=y.t().contiguous(), edge_index=edge_index.t().contiguous(), pos=pos, edge_attr=attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_to_check = [[3,7], [5,7]]\n",
    "dependent_arcs = []\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if i != j:\n",
    "            if [i,j] in edges_to_check:\n",
    "                dependent_arcs.append(1)\n",
    "            else:\n",
    "                dependent_arcs.append(0)\n",
    "y = torch.tensor(dependent_arcs, dtype=torch.long)\n",
    "x = torch.tensor([[0, 6, 2], [1, 1, 3], [2, 3, 7], [3, 6, 6], [4, 1, 7], [5, 6, 4], [6, 4, 3], [7, 7, 5]], dtype=torch.float)\n",
    "attr = [[i] for i in range(len(edge_index))]\n",
    "loc_list = [[0, 6, 2], [1, 1, 3], [2, 3, 7], [3, 6, 6], [4, 1, 7], [5, 6, 4], [6, 4, 3], [7, 7, 5]]\n",
    "loc_dict = {(i[0],j[0]): sqrt((i[1]-j[1])**2 + (i[2]-j[2])**2) if i != j else next for i in loc_list for j in loc_list}\n",
    "cnt = -1\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if i != j:\n",
    "            cnt += 1\n",
    "            attr[cnt].append(loc_dict[i,j])\n",
    "attr = torch.tensor(attr, dtype=torch.long)\n",
    "pos = []\n",
    "for i in loc_list:\n",
    "    pos.append([i[1], i[2]])\n",
    "pos = torch.tensor(pos)\n",
    "\n",
    "data3 = Data(x=x, y=y.t().contiguous(), edge_index=edge_index.t().contiguous(), pos=pos, edge_attr=attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_to_check = [[1,7], [5,7], [2,3], [6,3]]\n",
    "dependent_arcs = []\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if i != j:\n",
    "            if [i,j] in edges_to_check:\n",
    "                dependent_arcs.append(1)\n",
    "            else:\n",
    "                dependent_arcs.append(0)\n",
    "y = torch.tensor(dependent_arcs, dtype=torch.long)\n",
    "x = torch.tensor([[0, 4, 3], [1, 1, 1], [2, 6, 6], [3, 7,5], [4, 2, 5], [5, 3, 1], [6, 7,3], [7, 2,2]], dtype=torch.float)\n",
    "attr = [[i] for i in range(len(edge_index))]\n",
    "loc_list = [[0, 4, 3], [1, 1, 1], [2, 6, 6], [3, 7,5], [4, 2, 5], [5, 3, 1], [6, 7,3], [7, 2,2]]\n",
    "loc_dict = {(i[0],j[0]): sqrt((i[1]-j[1])**2 + (i[2]-j[2])**2) if i != j else next for i in loc_list for j in loc_list}\n",
    "cnt = -1\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if i != j:\n",
    "            cnt += 1\n",
    "            attr[cnt].append(loc_dict[i,j])\n",
    "attr = torch.tensor(attr, dtype=torch.long)\n",
    "pos = []\n",
    "for i in loc_list:\n",
    "    pos.append([i[1], i[2]])\n",
    "pos = torch.tensor(pos)\n",
    "\n",
    "data4 = Data(x=x, y=y.t().contiguous(), edge_index=edge_index.t().contiguous(), pos=pos, edge_attr=attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # self.conv1 = GCNConv(3, 2)\n",
    "        # self.conv2 = SAGEConv(2,8)\n",
    "        num_features = 3\n",
    "        dim = 56\n",
    "        features = 56\n",
    "\n",
    "        self.conv1 = GraphConv(num_features, dim)\n",
    "        self.conv2 = GraphConv(dim, dim)\n",
    "        self.conv3 = GraphConv(dim, dim)\n",
    "        self.conv4 = GraphConv(dim, dim)\n",
    "        self.conv5 = GraphConv(dim, dim)\n",
    "\n",
    "        self.lin1 = Linear(dim, dim)\n",
    "        self.lin2 = Linear(dim, 56)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_weight=None):\n",
    "        # x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # x = self.conv1(x, edge_index)\n",
    "        # x = F.relu(x)\n",
    "        # x = F.sigmoid(self.conv2(x, edge_index))\n",
    "        # x = F.dropout(x, training=self.training)\n",
    "        # x = self.conv2(x, edge_index)\n",
    "\n",
    "        x = self.conv1(x, edge_index, edge_weight).relu()\n",
    "        x = self.conv2(x, edge_index, edge_weight).relu()\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "        # return x\n",
    "        # return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    if epoch == 51:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.5 * param_group['lr']\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in dataloader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        # print(output, data.y)\n",
    "        # print(output)\n",
    "        loss = F.l1_loss(output[0], data.y)\n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return loss_all / 4\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "    return correct / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [data1, data2, data3, data4]\n",
    "dataloader = DataLoader(data_list, batch_size=1)\n",
    "data_iter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 109.2643, Train Acc: 2.0000, Test Acc: 2.0000\n",
      "Epoch: 002, Loss: 85.5794, Train Acc: 3.0000, Test Acc: 3.0000\n",
      "Epoch: 003, Loss: 51.6510, Train Acc: 0.5000, Test Acc: 0.5000\n",
      "Epoch: 004, Loss: 36.3253, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 005, Loss: 39.6960, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 006, Loss: 28.4578, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 007, Loss: 15.0359, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 008, Loss: 15.1526, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 009, Loss: 7.6742, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 010, Loss: 5.1370, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 011, Loss: 4.5597, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 012, Loss: 4.8443, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 013, Loss: 4.0847, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 014, Loss: 4.0836, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 015, Loss: 4.0906, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 016, Loss: 4.0819, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 017, Loss: 4.0902, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 018, Loss: 4.0814, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 019, Loss: 4.0814, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 020, Loss: 4.0814, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 021, Loss: 4.0814, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 022, Loss: 4.0814, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 023, Loss: 4.0813, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 024, Loss: 4.0813, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 025, Loss: 4.0813, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 026, Loss: 4.0813, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 027, Loss: 4.0813, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 028, Loss: 4.0813, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 029, Loss: 4.0812, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 030, Loss: 4.0812, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 031, Loss: 4.0812, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 032, Loss: 4.0812, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 033, Loss: 4.0812, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 034, Loss: 4.0811, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 035, Loss: 4.0811, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 036, Loss: 4.0811, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 037, Loss: 4.0811, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 038, Loss: 4.0811, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 039, Loss: 4.0811, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 040, Loss: 4.0810, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 041, Loss: 4.0810, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 042, Loss: 4.0810, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 043, Loss: 4.0810, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 044, Loss: 4.0810, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 045, Loss: 4.0809, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 046, Loss: 4.0809, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 047, Loss: 4.0809, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 048, Loss: 4.0809, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 049, Loss: 4.0809, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 050, Loss: 4.0809, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 051, Loss: 4.0808, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 052, Loss: 4.0808, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 053, Loss: 4.0808, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 054, Loss: 4.0808, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 055, Loss: 4.0808, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 056, Loss: 4.0808, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 057, Loss: 4.0808, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 058, Loss: 4.0808, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 059, Loss: 4.0808, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 060, Loss: 4.0808, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 061, Loss: 4.0807, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 062, Loss: 4.0807, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 063, Loss: 4.0807, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 064, Loss: 4.0807, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 065, Loss: 4.0807, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 066, Loss: 4.0807, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 067, Loss: 4.0807, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 068, Loss: 4.0807, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 069, Loss: 4.0807, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 070, Loss: 4.0807, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 071, Loss: 4.0807, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 072, Loss: 4.0806, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 073, Loss: 4.0806, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 074, Loss: 4.0806, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 075, Loss: 4.0806, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 076, Loss: 4.0806, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 077, Loss: 4.0806, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 078, Loss: 4.0806, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 079, Loss: 4.0806, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 080, Loss: 4.0806, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 081, Loss: 4.0806, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 082, Loss: 4.0806, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 083, Loss: 4.0805, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 084, Loss: 4.0805, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 085, Loss: 4.0805, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 086, Loss: 4.0805, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 087, Loss: 4.0805, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 088, Loss: 4.0805, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 089, Loss: 4.0805, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 090, Loss: 4.0805, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 091, Loss: 4.0805, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 092, Loss: 4.0805, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 093, Loss: 4.0805, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 094, Loss: 4.0804, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 095, Loss: 4.0804, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 096, Loss: 4.0804, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 097, Loss: 4.0804, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 098, Loss: 4.0804, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 099, Loss: 4.0804, Train Acc: 0.0000, Test Acc: 0.0000\n",
      "Epoch: 100, Loss: 4.0804, Train Acc: 0.0000, Test Acc: 0.0000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train(epoch)\n",
    "    train_acc = test(dataloader)\n",
    "    test_acc = test(dataloader)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "          f'Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.1071, -4.1501, -4.0723, -4.0766, -4.0141, -4.0125, -4.0495, -4.0744,\n",
      "         -3.9675, -3.9538, -4.0993, -3.9662, -4.0108, -4.0182, -3.9789, -4.0058,\n",
      "         -4.0230, -4.0914, -3.9619, -4.0678, -4.0535, -3.9740, -3.9900, -4.0338,\n",
      "         -3.9822, -3.9320, -3.9537, -4.0625, -4.0228, -4.0420, -3.9686, -4.0979,\n",
      "         -4.0211, -3.9790, -4.1766, -4.0589, -4.0102, -3.9617, -4.0303, -4.0415,\n",
      "         -3.9674, -3.9588, -4.0272, -4.0858, -4.0073, -4.0519, -4.0711, -4.0162,\n",
      "         -4.0823, -3.9716, -3.9764, -3.9318, -4.0532, -4.0651, -4.0218, -4.1204]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model(data1.x, data1.edge_index, data1.batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([-3.8952], grad_fn=<MaxBackward0>),\n",
      "indices=tensor([43]))\n"
     ]
    }
   ],
   "source": [
    "print(model(data1.x, data1.edge_index, data1.batch).max(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017054583756193526 0 1\n",
      "0.018807433040471482 0 2\n",
      "0.01864622709283721 0 3\n",
      "0.0185163893971816 0 4\n",
      "0.018901960220738468 0 5\n",
      "0.01913803475351945 0 6\n",
      "0.0169980851558266 0 7\n",
      "0.019324264818562746 1 0\n",
      "0.0172632634093058 1 2\n",
      "0.01623241383072122 1 3\n",
      "0.019217034505268488 1 4\n",
      "0.018716506438001275 1 5\n",
      "0.019432408517537848 1 6\n",
      "0.017196665509857295 1 7\n",
      "0.018392704079795236 2 0\n",
      "0.01813460166125799 2 1\n",
      "0.019511106863446582 2 3\n",
      "0.015423137776570301 2 4\n",
      "0.018696017565700843 2 5\n",
      "0.01652251705407729 2 6\n",
      "0.01994147649637608 2 7\n",
      "0.019009712026023782 3 0\n",
      "0.01645280542877164 3 1\n",
      "0.017392912456847313 3 2\n",
      "0.016292868869727594 3 4\n",
      "0.01966279356453263 3 5\n",
      "0.01931744727108654 3 6\n",
      "0.017623689701140186 3 7\n",
      "0.016355358295294855 4 0\n",
      "0.01608023142455609 4 1\n",
      "0.017299966654668774 4 2\n",
      "0.01933289154131632 4 3\n",
      "0.016939075731398065 4 5\n",
      "0.01789201449024852 4 6\n",
      "0.01835694281012177 4 7\n",
      "0.018236770311277073 5 0\n",
      "0.016057865118093186 5 1\n",
      "0.018025354080497637 5 2\n",
      "0.017245080789824133 5 3\n",
      "0.015916940086974186 5 4\n",
      "0.019076357749319362 5 6\n",
      "0.019734749374627294 5 7\n",
      "0.01676300995663953 6 0\n",
      "0.015939300115182325 6 1\n",
      "0.015374939454773152 6 2\n",
      "0.02008194374401311 6 3\n",
      "0.016627127315132457 6 4\n",
      "0.016959458634823364 6 5\n",
      "0.018718835936375047 6 7\n",
      "0.0200573202752883 7 0\n",
      "0.016296544042591118 7 1\n",
      "0.017140824109182783 7 2\n",
      "0.019088769160430657 7 3\n",
      "0.017660729753981738 7 4\n",
      "0.017942969237189108 7 5\n",
      "0.016979485527736426 7 6\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    "\n",
    "sol1 = model(data1.x, data1.edge_index, data1.batch).tolist()[0]\n",
    "\n",
    "pos = -1\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        if i != j:\n",
    "            pos +=1\n",
    "            print(exp(sol1[pos]), i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "batches = {}\n",
    "for batch_idx in range(len(data_list)):\n",
    "    batches[batch_idx] = data_list[batch_idx].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    in_between_loss = 0\n",
    "    within_loss = 0\n",
    "    for batch_idx in range(len(data_list)):\n",
    "        batch = batches[batch_idx]\n",
    "        out = model(batch)\n",
    "        loss = F.mse_loss(out, batches[batch_idx].y)\n",
    "        within_loss += loss.data.item()\n",
    "    in_between_loss = [within_loss/len(data_list)]\n",
    "    loss.data = torch.tensor(in_between_loss, dtype=torch.float)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0027, 0.0695, 0.0690, 0.0038, 0.0034, 0.0640, 0.0821, 0.0026],\n",
      "        [0.0027, 0.0695, 0.0690, 0.0038, 0.0034, 0.0640, 0.0821, 0.0026],\n",
      "        [0.0027, 0.0695, 0.0690, 0.0038, 0.0034, 0.0640, 0.0821, 0.0026],\n",
      "        [0.0027, 0.0695, 0.0690, 0.0038, 0.0034, 0.0640, 0.0821, 0.0026],\n",
      "        [0.0027, 0.0695, 0.0690, 0.0038, 0.0034, 0.0640, 0.0821, 0.0026],\n",
      "        [0.0027, 0.0695, 0.0690, 0.0038, 0.0034, 0.0640, 0.0821, 0.0026],\n",
      "        [0.0027, 0.0695, 0.0690, 0.0038, 0.0034, 0.0640, 0.0821, 0.0026],\n",
      "        [0.0027, 0.0695, 0.0690, 0.0038, 0.0034, 0.0640, 0.0821, 0.0026]],\n",
      "       grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(data1)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred)\n\u001b[1;32m----> 4\u001b[0m correct \u001b[38;5;241m=\u001b[39m ([pred[i][j] \u001b[38;5;241m==\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[j] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m)])\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m      5\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(correct) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mint\u001b[39m(data\u001b[38;5;241m.\u001b[39mtest_mask\u001b[38;5;241m.\u001b[39msum())\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = model(data1)\n",
    "print(pred)\n",
    "correct = ([pred[i][j] == data.y[i][j] for i in range(8) for j in range(8)]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "\n",
    "path = '.'\n",
    "dataset = TUDataset(path, name='Mutagenicity').shuffle()\n",
    "test_dataset = dataset[:len(dataset) // 10]\n",
    "train_dataset = dataset[len(dataset) // 10:]\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: Mutagenicity(4337):\n",
      "======================\n",
      "Number of graphs: 4337\n",
      "Number of features: 14\n",
      "Number of classes: 2\n",
      "\n",
      "Data(edge_index=[2, 24], x=[13, 14], edge_attr=[24, 3], y=[1])\n",
      "===========================================================================================================\n",
      "Number of nodes: 13\n",
      "Number of edges: 24\n",
      "Average node degree: 1.85\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "# dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('===========================================================================================================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        num_features = dataset.num_features\n",
    "        self.dim = dim\n",
    "\n",
    "        self.conv1 = GraphConv(num_features, dim)\n",
    "        self.conv2 = GraphConv(dim, dim)\n",
    "        self.conv3 = GraphConv(dim, dim)\n",
    "        self.conv4 = GraphConv(dim, dim)\n",
    "        self.conv5 = GraphConv(dim, dim)\n",
    "\n",
    "        self.lin1 = Linear(dim, dim)\n",
    "        self.lin2 = Linear(dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_weight=None):\n",
    "        x = self.conv1(x, edge_index, edge_weight).relu()\n",
    "        x = self.conv2(x, edge_index, edge_weight).relu()\n",
    "        x = self.conv3(x, edge_index, edge_weight).relu()\n",
    "        x = self.conv4(x, edge_index, edge_weight).relu()\n",
    "        x = self.conv5(x, edge_index, edge_weight).relu()\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "    \n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    if epoch == 51:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.5 * param_group['lr']\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        print(output)\n",
    "        print(data.y)\n",
    "        exit()\n",
    "        loss = F.nll_loss(output, data.y)\n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-7.9544e-02, -2.5710e+00],\n",
      "        [ 0.0000e+00, -2.4521e+01],\n",
      "        [-3.2041e+00, -4.1441e-02],\n",
      "        [-1.3136e-01, -2.0948e+00],\n",
      "        [-3.5234e-01, -1.2142e+00],\n",
      "        [-2.9585e-01, -1.3622e+00],\n",
      "        [-1.0162e-02, -4.5942e+00],\n",
      "        [-1.0766e+00, -4.1666e-01],\n",
      "        [-4.9121e-01, -9.4645e-01],\n",
      "        [-1.9291e-01, -1.7405e+00],\n",
      "        [-2.3842e-07, -1.5343e+01],\n",
      "        [-6.2136e-04, -7.3840e+00],\n",
      "        [-4.8757e-01, -9.5223e-01],\n",
      "        [-6.1950e-01, -7.7265e-01],\n",
      "        [-3.7913e+00, -2.2825e-02],\n",
      "        [-5.1425e-01, -9.1117e-01],\n",
      "        [-3.0185e-01, -1.3450e+00],\n",
      "        [ 0.0000e+00, -1.8063e+01],\n",
      "        [-1.1951e-01, -2.1835e+00],\n",
      "        [-9.5585e-02, -2.3951e+00],\n",
      "        [-2.2438e+00, -1.1211e-01],\n",
      "        [-3.7700e-01, -1.1581e+00],\n",
      "        [-7.2500e-01, -6.6228e-01],\n",
      "        [-2.9350e+00, -5.4594e-02],\n",
      "        [-2.7798e-01, -1.4160e+00],\n",
      "        [-1.1085e+01, -1.5378e-05],\n",
      "        [-1.8863e-01, -1.7608e+00],\n",
      "        [-7.6554e-01, -6.2565e-01],\n",
      "        [-3.3738e-01, -1.2505e+00],\n",
      "        [-1.0064e+00, -4.5496e-01],\n",
      "        [-1.5749e-01, -1.9261e+00],\n",
      "        [-4.8489e-01, -9.5650e-01],\n",
      "        [-5.8681e-01, -8.1215e-01],\n",
      "        [-1.1889e-02, -4.4381e+00],\n",
      "        [-4.0961e+00, -1.6777e-02],\n",
      "        [-6.6611e-03, -5.0148e+00],\n",
      "        [-1.4611e+00, -2.6395e-01],\n",
      "        [-5.0349e-01, -9.2739e-01],\n",
      "        [-6.9202e-03, -4.9768e+00],\n",
      "        [-5.4231e-01, -8.7085e-01],\n",
      "        [-3.7147e-02, -3.3114e+00],\n",
      "        [-6.1296e-02, -2.8225e+00],\n",
      "        [-4.6378e-01, -9.9129e-01],\n",
      "        [-1.1039e-02, -4.5118e+00],\n",
      "        [-3.7526e+00, -2.3736e-02],\n",
      "        [-3.4239e-01, -1.2381e+00],\n",
      "        [-2.0326e-01, -1.6932e+00],\n",
      "        [-4.8975e-01, -9.4876e-01],\n",
      "        [-1.9882e+00, -1.4727e-01],\n",
      "        [-2.9366e-03, -5.8320e+00],\n",
      "        [-2.4221e-01, -1.5366e+00],\n",
      "        [-4.0934e-01, -1.0909e+00],\n",
      "        [-5.7619e-01, -8.2561e-01],\n",
      "        [-1.1512e+00, -3.8018e-01],\n",
      "        [-9.9203e-01, -4.6334e-01],\n",
      "        [-1.4387e+00, -2.7081e-01],\n",
      "        [-1.3349e+00, -3.0542e-01],\n",
      "        [-4.7328e-01, -9.7540e-01],\n",
      "        [-2.2583e+00, -1.1040e-01],\n",
      "        [-7.5238e-01, -6.3722e-01],\n",
      "        [-6.7272e-02, -2.7325e+00],\n",
      "        [-1.0290e+00, -4.4220e-01],\n",
      "        [-4.8089e-01, -9.6295e-01],\n",
      "        [-1.0252e+00, -4.4429e-01],\n",
      "        [-3.4554e-01, -1.2304e+00],\n",
      "        [-1.9990e-01, -1.7082e+00],\n",
      "        [-1.4366e+00, -2.7146e-01],\n",
      "        [-2.7343e+00, -6.7145e-02],\n",
      "        [-6.5302e-01, -7.3495e-01],\n",
      "        [-5.3319e-01, -8.8365e-01],\n",
      "        [-3.9275e-01, -1.1245e+00],\n",
      "        [-2.1319e+00, -1.2626e-01],\n",
      "        [-2.1998e+00, -1.1746e-01],\n",
      "        [-5.3998e-01, -8.7409e-01],\n",
      "        [-6.3521e-02, -2.7880e+00],\n",
      "        [-1.5518e+00, -2.3808e-01],\n",
      "        [-1.4557e-01, -1.9990e+00],\n",
      "        [-2.4965e-01, -1.5099e+00],\n",
      "        [-5.2599e-01, -8.9397e-01],\n",
      "        [-2.3481e-01, -1.5641e+00],\n",
      "        [-2.3205e-01, -1.5746e+00],\n",
      "        [-2.8009e-03, -5.8792e+00],\n",
      "        [-3.3049e+00, -3.7393e-02],\n",
      "        [-8.4933e-01, -5.5810e-01],\n",
      "        [-9.3309e-01, -4.9978e-01],\n",
      "        [-1.1199e+00, -3.9497e-01],\n",
      "        [-6.5687e-02, -2.7555e+00],\n",
      "        [-1.4456e-02, -4.2439e+00],\n",
      "        [-1.5996e+00, -2.2561e-01],\n",
      "        [-1.6226e-01, -1.8986e+00],\n",
      "        [-1.1358e+00, -3.8738e-01],\n",
      "        [-2.6977e-02, -3.6262e+00],\n",
      "        [-2.0997e-03, -6.1670e+00],\n",
      "        [-6.7898e-02, -2.7235e+00],\n",
      "        [-1.5693e-01, -1.9294e+00],\n",
      "        [-1.7614e+00, -1.8851e-01],\n",
      "        [-9.2801e-01, -5.0309e-01],\n",
      "        [-8.7003e-02, -2.4850e+00],\n",
      "        [-6.3584e-02, -2.7870e+00],\n",
      "        [-8.7442e-01, -5.3974e-01],\n",
      "        [-1.2566e+00, -3.3494e-01],\n",
      "        [-2.3817e-01, -1.5515e+00],\n",
      "        [-4.3562e-04, -7.7391e+00],\n",
      "        [-4.6158e-01, -9.9504e-01],\n",
      "        [-3.0749e-02, -3.4972e+00],\n",
      "        [-4.2408e-01, -1.0624e+00],\n",
      "        [-1.5569e-01, -1.9367e+00],\n",
      "        [-8.8639e-02, -2.4672e+00],\n",
      "        [-2.9350e+00, -5.4592e-02],\n",
      "        [-1.4021e+00, -2.8245e-01],\n",
      "        [-3.0468e-01, -1.3370e+00],\n",
      "        [-3.8851e-01, -1.1334e+00],\n",
      "        [-4.4636e-01, -1.0215e+00],\n",
      "        [-1.6928e+00, -2.0335e-01],\n",
      "        [-1.3150e+00, -3.1264e-01],\n",
      "        [-1.4777e+00, -2.5897e-01],\n",
      "        [-1.7947e+00, -1.8174e-01],\n",
      "        [-1.0467e-02, -4.5648e+00],\n",
      "        [-7.0578e-01, -6.8067e-01],\n",
      "        [-4.6622e-03, -5.3706e+00],\n",
      "        [-3.9557e-01, -1.1187e+00],\n",
      "        [-2.1998e-01, -1.6222e+00],\n",
      "        [-2.3756e+00, -9.7571e-02],\n",
      "        [-6.9182e-01, -6.9447e-01],\n",
      "        [-4.2672e-01, -1.0574e+00],\n",
      "        [-1.7677e-01, -1.8200e+00],\n",
      "        [-2.1119e-03, -6.1612e+00],\n",
      "        [-4.9815e-01, -9.3561e-01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
      "        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
      "        0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([[-1.8487e-01, -1.7791e+00],\n",
      "        [-2.2455e+00, -1.1191e-01],\n",
      "        [-5.4126e-02, -2.9434e+00],\n",
      "        [-6.0728e-01, -7.8709e-01],\n",
      "        [-9.9325e-01, -4.6262e-01],\n",
      "        [-8.5806e-01, -5.5162e-01],\n",
      "        [-9.0261e-01, -5.2005e-01],\n",
      "        [-2.5377e-01, -1.4955e+00],\n",
      "        [-6.7831e-01, -7.0821e-01],\n",
      "        [-3.8508e-01, -1.1407e+00],\n",
      "        [-1.2007e+00, -3.5810e-01],\n",
      "        [-9.9027e-02, -2.3615e+00],\n",
      "        [-8.3333e-01, -5.7022e-01],\n",
      "        [-1.1096e+00, -3.9999e-01],\n",
      "        [-4.4135e-01, -1.0305e+00],\n",
      "        [-7.8556e-05, -9.4519e+00],\n",
      "        [-1.2857e+00, -3.2358e-01],\n",
      "        [-1.1388e+00, -3.8598e-01],\n",
      "        [-4.6063e-01, -9.9666e-01],\n",
      "        [-4.1810e+00, -1.5401e-02],\n",
      "        [-7.4397e-01, -6.4478e-01],\n",
      "        [-1.0245e+00, -4.4472e-01],\n",
      "        [-2.0050e+00, -1.4464e-01],\n",
      "        [-3.7946e+00, -2.2748e-02],\n",
      "        [-5.8832e-01, -8.1026e-01],\n",
      "        [-3.1049e+00, -4.5867e-02],\n",
      "        [-2.8504e+00, -5.9559e-02],\n",
      "        [-1.5110e+00, -2.4935e-01],\n",
      "        [-5.9066e-01, -8.0735e-01],\n",
      "        [-2.3187e-01, -1.5753e+00],\n",
      "        [-5.1247e-01, -9.1383e-01],\n",
      "        [-1.3848e-01, -2.0455e+00],\n",
      "        [-6.6857e-01, -7.1834e-01],\n",
      "        [-1.0031e+00, -4.5687e-01],\n",
      "        [-1.7299e+00, -1.9516e-01],\n",
      "        [-8.0516e+00, -3.1860e-04],\n",
      "        [-6.7641e-01, -7.1017e-01],\n",
      "        [-2.1830e+00, -1.1958e-01],\n",
      "        [-3.1473e+00, -4.3920e-02],\n",
      "        [-5.9895e-01, -7.9714e-01],\n",
      "        [-1.0724e+00, -4.1884e-01],\n",
      "        [-6.8170e-01, -7.0473e-01],\n",
      "        [-9.0498e-01, -5.1844e-01],\n",
      "        [-1.4804e-01, -1.9834e+00],\n",
      "        [-1.0170e+00, -4.4893e-01],\n",
      "        [-1.7352e+00, -1.9403e-01],\n",
      "        [-4.6488e-01, -9.8942e-01],\n",
      "        [-7.2087e-01, -6.6617e-01],\n",
      "        [-1.0892e-01, -2.2711e+00],\n",
      "        [-2.0411e+00, -1.3913e-01],\n",
      "        [-1.7499e+00, -1.9092e-01],\n",
      "        [-9.2393e-01, -5.0577e-01],\n",
      "        [-9.8639e-01, -4.6668e-01],\n",
      "        [-7.8659e-01, -6.0770e-01],\n",
      "        [-9.4138e-01, -4.9444e-01],\n",
      "        [-5.5919e-01, -8.4786e-01],\n",
      "        [-7.2920e-01, -6.5835e-01],\n",
      "        [-1.8676e+00, -1.6782e-01],\n",
      "        [-1.2146e-02, -4.4168e+00],\n",
      "        [-2.8327e+00, -6.0658e-02],\n",
      "        [-8.7996e-01, -5.3580e-01],\n",
      "        [-2.8671e-01, -1.3892e+00],\n",
      "        [-7.5011e-01, -6.3926e-01],\n",
      "        [-1.4366e+00, -2.7147e-01],\n",
      "        [-9.1784e-01, -5.0979e-01],\n",
      "        [-7.1088e-01, -6.7572e-01],\n",
      "        [-9.0761e-01, -5.1666e-01],\n",
      "        [-2.2363e+00, -1.1301e-01],\n",
      "        [-1.2375e-03, -6.6954e+00],\n",
      "        [-1.6190e+00, -2.2076e-01],\n",
      "        [-9.1240e+00, -1.0907e-04],\n",
      "        [-3.0467e+00, -4.8679e-02],\n",
      "        [-1.8944e+00, -1.6301e-01],\n",
      "        [-1.9501e-01, -1.7306e+00],\n",
      "        [-3.4460e-01, -1.2327e+00],\n",
      "        [-1.4221e+00, -2.7604e-01],\n",
      "        [-1.5371e+00, -2.4207e-01],\n",
      "        [-6.3340e-01, -7.5669e-01],\n",
      "        [-2.5278e-01, -1.4990e+00],\n",
      "        [-3.0981e+00, -4.6187e-02],\n",
      "        [-3.0698e+00, -4.7544e-02],\n",
      "        [-7.0353e-01, -6.8287e-01],\n",
      "        [-2.6939e+00, -7.0015e-02],\n",
      "        [-1.8020e-01, -1.8024e+00],\n",
      "        [-1.7776e-01, -1.8149e+00],\n",
      "        [-1.7794e+00, -1.8481e-01],\n",
      "        [-2.7376e+00, -6.6913e-02],\n",
      "        [-1.3044e+00, -3.1653e-01],\n",
      "        [-3.4055e-01, -1.2426e+00],\n",
      "        [-1.1369e+00, -3.8684e-01],\n",
      "        [-7.4172e-01, -6.4683e-01],\n",
      "        [-1.7913e+00, -1.8241e-01],\n",
      "        [-1.1562e+00, -3.7785e-01],\n",
      "        [-1.5482e-01, -1.9419e+00],\n",
      "        [-5.0369e-02, -3.0135e+00],\n",
      "        [-4.3218e-01, -1.0472e+00],\n",
      "        [-1.0616e+00, -4.2450e-01],\n",
      "        [-1.1170e+00, -3.9640e-01],\n",
      "        [-8.9779e-02, -2.4550e+00],\n",
      "        [-2.7536e+00, -6.5816e-02],\n",
      "        [-8.2044e-02, -2.5412e+00],\n",
      "        [-1.2595e+00, -3.3380e-01],\n",
      "        [-9.8101e-01, -4.6990e-01],\n",
      "        [-1.5559e+00, -2.3698e-01],\n",
      "        [-1.1976e+00, -3.5943e-01],\n",
      "        [-1.8631e+00, -1.6865e-01],\n",
      "        [-1.3920e+00, -2.8579e-01],\n",
      "        [-2.2230e+00, -1.1461e-01],\n",
      "        [-2.2519e+00, -1.1115e-01],\n",
      "        [-6.0214e-01, -7.9327e-01],\n",
      "        [-9.6752e-01, -4.7808e-01],\n",
      "        [-1.1961e+00, -3.6005e-01],\n",
      "        [-1.1715e+00, -3.7091e-01],\n",
      "        [-1.3853e+00, -2.8802e-01],\n",
      "        [-4.3445e+00, -1.3063e-02],\n",
      "        [-3.1026e-01, -1.3215e+00],\n",
      "        [-1.8748e+00, -1.6650e-01],\n",
      "        [-3.0956e+00, -4.6302e-02],\n",
      "        [-2.3700e+00, -9.8141e-02],\n",
      "        [-1.0202e+00, -4.4713e-01],\n",
      "        [-6.6446e-01, -7.2268e-01],\n",
      "        [-1.3627e-01, -2.0605e+00],\n",
      "        [-6.4080e-01, -7.4839e-01],\n",
      "        [-1.9746e-01, -1.7193e+00],\n",
      "        [-6.9543e-01, -6.9086e-01],\n",
      "        [-7.4119e-01, -6.4731e-01],\n",
      "        [-6.4409e-01, -7.4474e-01],\n",
      "        [-3.8910e-01, -1.1322e+00]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 1])\n",
      "tensor([[-3.5960e+00, -2.7816e-02],\n",
      "        [-8.4777e-01, -5.5926e-01],\n",
      "        [-1.4045e+00, -2.8167e-01],\n",
      "        [-2.2460e+00, -1.1184e-01],\n",
      "        [-9.3654e-01, -4.9755e-01],\n",
      "        [-8.5066e-01, -5.5710e-01],\n",
      "        [-1.5011e+00, -2.5217e-01],\n",
      "        [-1.4769e+00, -2.5921e-01],\n",
      "        [-6.1160e-01, -7.8194e-01],\n",
      "        [-7.6508e-01, -6.2604e-01],\n",
      "        [-2.4498e+00, -9.0268e-02],\n",
      "        [-1.4506e+00, -2.6712e-01],\n",
      "        [-3.9596e+00, -1.9254e-02],\n",
      "        [-1.0681e+00, -4.2109e-01],\n",
      "        [-7.3936e-01, -6.4897e-01],\n",
      "        [-7.2755e-01, -6.5989e-01],\n",
      "        [-3.3108e+00, -3.7170e-02],\n",
      "        [-6.0573e-01, -7.8895e-01],\n",
      "        [-1.9398e+00, -1.5518e-01],\n",
      "        [-9.0231e-01, -5.2026e-01],\n",
      "        [-1.0485e+00, -4.3151e-01],\n",
      "        [-2.2790e+00, -1.0801e-01],\n",
      "        [-7.1152e-01, -6.7510e-01],\n",
      "        [-8.6015e-01, -5.5009e-01],\n",
      "        [-2.0037e+00, -1.4484e-01],\n",
      "        [-1.0477e+00, -4.3192e-01],\n",
      "        [-1.9268e+00, -1.5738e-01],\n",
      "        [-2.2188e-01, -1.6145e+00],\n",
      "        [-1.1159e+00, -3.9693e-01],\n",
      "        [-7.0396e-01, -6.8245e-01],\n",
      "        [-1.0639e+00, -4.2330e-01],\n",
      "        [-2.8707e+00, -5.8330e-02],\n",
      "        [-5.8663e-01, -8.1238e-01],\n",
      "        [-1.7127e+00, -1.9892e-01],\n",
      "        [-2.4951e+00, -8.6086e-02],\n",
      "        [-5.5683e-01, -8.5103e-01],\n",
      "        [-7.4692e-01, -6.4212e-01],\n",
      "        [-7.7967e-01, -6.1352e-01],\n",
      "        [-2.2379e+00, -1.1282e-01],\n",
      "        [-8.8427e-01, -5.3276e-01],\n",
      "        [-2.9853e-01, -1.3544e+00],\n",
      "        [-6.9043e-01, -6.9588e-01],\n",
      "        [-9.0427e-01, -5.1892e-01],\n",
      "        [-1.3977e+00, -2.8390e-01],\n",
      "        [-1.2489e+00, -3.3804e-01],\n",
      "        [-5.7653e-01, -8.2518e-01],\n",
      "        [-5.8854e-01, -8.0999e-01],\n",
      "        [-1.0167e+00, -4.4908e-01],\n",
      "        [-1.4429e+00, -2.6951e-01],\n",
      "        [-4.2208e+00, -1.4796e-02],\n",
      "        [-8.1231e-01, -5.8668e-01],\n",
      "        [-7.4591e+00, -5.7633e-04],\n",
      "        [-6.2536e-01, -7.6586e-01],\n",
      "        [-1.2475e+00, -3.3860e-01],\n",
      "        [-6.4717e-01, -7.4134e-01],\n",
      "        [-4.3161e-01, -1.0483e+00],\n",
      "        [-1.0190e+00, -4.4778e-01],\n",
      "        [-2.3377e-01, -1.5680e+00],\n",
      "        [-7.4506e-01, -6.4380e-01],\n",
      "        [-1.2813e+00, -3.2527e-01],\n",
      "        [-8.8719e-01, -5.3071e-01],\n",
      "        [-1.2926e+00, -3.2098e-01],\n",
      "        [-4.2478e+00, -1.4399e-02],\n",
      "        [-8.6497e-01, -5.4656e-01],\n",
      "        [-9.8198e-01, -4.6932e-01],\n",
      "        [-1.0771e+00, -4.1640e-01],\n",
      "        [-4.6419e-01, -9.9059e-01],\n",
      "        [-1.4311e+00, -2.7319e-01],\n",
      "        [-7.6905e-01, -6.2260e-01],\n",
      "        [-1.1768e+00, -3.6854e-01],\n",
      "        [-2.0599e+00, -1.3636e-01],\n",
      "        [-1.7215e+00, -1.9700e-01],\n",
      "        [-1.7648e+00, -1.8781e-01],\n",
      "        [-1.7190e+00, -1.9753e-01],\n",
      "        [-7.8079e-01, -6.1257e-01],\n",
      "        [-1.3859e+00, -2.8782e-01],\n",
      "        [-1.9807e+00, -1.4846e-01],\n",
      "        [-4.9696e-01, -9.3745e-01],\n",
      "        [-4.4779e-01, -1.0190e+00],\n",
      "        [-1.4238e+00, -2.7550e-01],\n",
      "        [-1.0392e+00, -4.3656e-01],\n",
      "        [-6.2005e-01, -7.7201e-01],\n",
      "        [-1.5177e+00, -2.4746e-01],\n",
      "        [-2.2279e-01, -1.6109e+00],\n",
      "        [-1.2887e+00, -3.2244e-01],\n",
      "        [-2.3879e+00, -9.6315e-02],\n",
      "        [-3.4989e+00, -3.0698e-02],\n",
      "        [-1.2867e+00, -3.2322e-01],\n",
      "        [-4.4185e-01, -1.0296e+00],\n",
      "        [-3.4877e-01, -1.2227e+00],\n",
      "        [-1.1329e+00, -3.8877e-01],\n",
      "        [-6.0015e-01, -7.9569e-01],\n",
      "        [-8.1154e-01, -5.8730e-01],\n",
      "        [-9.0126e+00, -1.2182e-04],\n",
      "        [-6.7337e-01, -7.1333e-01],\n",
      "        [-1.4921e+00, -2.5478e-01],\n",
      "        [-6.9590e-01, -6.9040e-01],\n",
      "        [-1.8068e+00, -1.7934e-01],\n",
      "        [-1.3535e+00, -2.9885e-01],\n",
      "        [-7.3403e-01, -6.5387e-01],\n",
      "        [-8.7080e-01, -5.4235e-01],\n",
      "        [-1.1219e+00, -3.9400e-01],\n",
      "        [-9.5975e-01, -4.8287e-01],\n",
      "        [-5.2605e-01, -8.9388e-01],\n",
      "        [-6.5064e-01, -7.3754e-01],\n",
      "        [-1.9253e+00, -1.5763e-01],\n",
      "        [-6.9784e-01, -6.8847e-01],\n",
      "        [-1.2785e+00, -3.2637e-01],\n",
      "        [-5.1509e-01, -9.0994e-01],\n",
      "        [-1.0470e+00, -4.3233e-01],\n",
      "        [-1.1243e+00, -3.9287e-01],\n",
      "        [-1.2684e+00, -3.3027e-01],\n",
      "        [-1.3384e+00, -3.0417e-01],\n",
      "        [-6.8476e+00, -1.0625e-03],\n",
      "        [-1.5079e+00, -2.5024e-01],\n",
      "        [-8.1382e-01, -5.8548e-01],\n",
      "        [-9.9810e-01, -4.5978e-01],\n",
      "        [-6.0862e-01, -7.8548e-01],\n",
      "        [-4.4185e-01, -1.0296e+00],\n",
      "        [-9.5217e-01, -4.8760e-01],\n",
      "        [-4.9539e-01, -9.3989e-01],\n",
      "        [-7.5243e-01, -6.3718e-01],\n",
      "        [-6.5353e-01, -7.3440e-01],\n",
      "        [-1.2584e+00, -3.3424e-01],\n",
      "        [-2.1448e+00, -1.2453e-01],\n",
      "        [-1.3015e+00, -3.1761e-01],\n",
      "        [-1.3119e+00, -3.1375e-01],\n",
      "        [-2.4412e+00, -9.1081e-02]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
      "        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 0, 1, 0])\n",
      "tensor([[-0.5924, -0.8052],\n",
      "        [-1.2003, -0.3583],\n",
      "        [-0.2377, -1.5531],\n",
      "        [-1.4050, -0.2815],\n",
      "        [-0.7211, -0.6659],\n",
      "        [-1.3874, -0.2873],\n",
      "        [-0.3819, -1.1474],\n",
      "        [-0.9469, -0.4909],\n",
      "        [-1.4487, -0.2677],\n",
      "        [-0.5000, -0.9327],\n",
      "        [-0.8139, -0.5854],\n",
      "        [-1.9566, -0.1524],\n",
      "        [-0.1905, -1.7517],\n",
      "        [-0.9533, -0.4869],\n",
      "        [-0.9125, -0.5133],\n",
      "        [-0.7809, -0.6124],\n",
      "        [-0.7180, -0.6689],\n",
      "        [-1.4922, -0.2547],\n",
      "        [-0.3406, -1.2424],\n",
      "        [-0.2140, -1.6470],\n",
      "        [-0.6988, -0.6875],\n",
      "        [-1.0617, -0.4245],\n",
      "        [-0.4675, -0.9851],\n",
      "        [-0.9344, -0.4989],\n",
      "        [-3.3693, -0.0350],\n",
      "        [-1.1417, -0.3846],\n",
      "        [-0.9849, -0.4676],\n",
      "        [-0.8569, -0.5524],\n",
      "        [-1.5795, -0.2308],\n",
      "        [-0.9155, -0.5113],\n",
      "        [-1.2345, -0.3439],\n",
      "        [-0.2724, -1.4335],\n",
      "        [-0.2719, -1.4352],\n",
      "        [-0.6783, -0.7082],\n",
      "        [-0.1389, -2.0428],\n",
      "        [-1.3800, -0.2898],\n",
      "        [-1.2101, -0.3540],\n",
      "        [-1.1132, -0.3982],\n",
      "        [-0.9189, -0.5091],\n",
      "        [-1.5645, -0.2347],\n",
      "        [-0.6687, -0.7182],\n",
      "        [-0.9656, -0.4793],\n",
      "        [-1.3400, -0.3036],\n",
      "        [-0.9785, -0.4714],\n",
      "        [-0.9984, -0.4596],\n",
      "        [-1.9488, -0.1537],\n",
      "        [-0.9677, -0.4779],\n",
      "        [-0.4565, -1.0038],\n",
      "        [-0.9637, -0.4805],\n",
      "        [-0.5841, -0.8155],\n",
      "        [-0.5771, -0.8245],\n",
      "        [-0.7606, -0.6300],\n",
      "        [-0.4698, -0.9812],\n",
      "        [-1.7350, -0.1941],\n",
      "        [-0.6545, -0.7334],\n",
      "        [-1.0027, -0.4571],\n",
      "        [-1.5159, -0.2480],\n",
      "        [-3.2556, -0.0393],\n",
      "        [-0.7835, -0.6103],\n",
      "        [-1.0573, -0.4268],\n",
      "        [-0.3718, -1.1696],\n",
      "        [-1.0789, -0.4155],\n",
      "        [-0.6734, -0.7133],\n",
      "        [-1.5082, -0.2501],\n",
      "        [-0.8096, -0.5889],\n",
      "        [-0.5659, -0.8389],\n",
      "        [-0.0752, -2.6245],\n",
      "        [-1.6234, -0.2197],\n",
      "        [-1.1078, -0.4009],\n",
      "        [-0.5418, -0.8716],\n",
      "        [-1.2062, -0.3557],\n",
      "        [-0.7260, -0.6613],\n",
      "        [-1.4367, -0.2714],\n",
      "        [-0.5367, -0.8787],\n",
      "        [-0.6173, -0.7752],\n",
      "        [-0.7618, -0.6289],\n",
      "        [-0.8973, -0.5237],\n",
      "        [-0.7502, -0.6392],\n",
      "        [-0.4285, -1.0541],\n",
      "        [-0.6220, -0.7698],\n",
      "        [-1.8573, -0.1697],\n",
      "        [-0.3044, -1.3377],\n",
      "        [-0.9444, -0.4925],\n",
      "        [-0.9704, -0.4763],\n",
      "        [-0.5144, -0.9110],\n",
      "        [-0.5698, -0.8339],\n",
      "        [-3.0965, -0.0463],\n",
      "        [-0.9170, -0.5104],\n",
      "        [-1.7351, -0.1941],\n",
      "        [-0.4224, -1.0656],\n",
      "        [-2.2059, -0.1167],\n",
      "        [-2.8312, -0.0608],\n",
      "        [-1.4470, -0.2682],\n",
      "        [-2.2576, -0.1105],\n",
      "        [-1.0939, -0.4078],\n",
      "        [-3.4799, -0.0313],\n",
      "        [-0.3937, -1.1226],\n",
      "        [-0.7135, -0.6732],\n",
      "        [-3.2204, -0.0408],\n",
      "        [-0.8076, -0.5905],\n",
      "        [-0.4569, -1.0030],\n",
      "        [-0.5628, -0.8431],\n",
      "        [-0.4798, -0.9647],\n",
      "        [-0.8529, -0.5554],\n",
      "        [-0.2507, -1.5062],\n",
      "        [-0.5971, -0.7994],\n",
      "        [-0.8009, -0.5959],\n",
      "        [-1.7785, -0.1850],\n",
      "        [-0.9269, -0.5038],\n",
      "        [-0.1859, -1.7741],\n",
      "        [-0.6580, -0.7296],\n",
      "        [-0.4369, -1.0386],\n",
      "        [-0.6126, -0.7808],\n",
      "        [-0.9501, -0.4889],\n",
      "        [-0.8239, -0.5775],\n",
      "        [-0.5528, -0.8565],\n",
      "        [-0.8177, -0.5824],\n",
      "        [-0.7224, -0.6647],\n",
      "        [-1.6111, -0.2227],\n",
      "        [-0.7402, -0.6482],\n",
      "        [-0.7114, -0.6753],\n",
      "        [-0.3544, -1.2093],\n",
      "        [-1.2948, -0.3201],\n",
      "        [-0.5822, -0.8179],\n",
      "        [-0.4697, -0.9814],\n",
      "        [-0.7747, -0.6178],\n",
      "        [-1.1874, -0.3639],\n",
      "        [-1.1679, -0.3725]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
      "        1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
      "        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
      "        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor([[-5.1888e-01, -9.0433e-01],\n",
      "        [-1.2163e+00, -3.5145e-01],\n",
      "        [-1.2515e+00, -3.3697e-01],\n",
      "        [-2.6140e+00, -7.6062e-02],\n",
      "        [-6.2480e-01, -7.6651e-01],\n",
      "        [-6.7038e-01, -7.1645e-01],\n",
      "        [-9.9044e-01, -4.6428e-01],\n",
      "        [-1.1335e+00, -3.8846e-01],\n",
      "        [-5.4040e-01, -8.7350e-01],\n",
      "        [-1.5800e+00, -2.3064e-01],\n",
      "        [-9.6134e-01, -4.8188e-01],\n",
      "        [-2.2215e+00, -1.1479e-01],\n",
      "        [-2.3726e-01, -1.5549e+00],\n",
      "        [-5.7013e-01, -8.3345e-01],\n",
      "        [-8.5253e-01, -5.5571e-01],\n",
      "        [-1.7753e+00, -1.8564e-01],\n",
      "        [-2.0868e-01, -1.6695e+00],\n",
      "        [-3.3739e-01, -1.2505e+00],\n",
      "        [-1.0703e+00, -4.1994e-01],\n",
      "        [-7.8838e-01, -6.0620e-01],\n",
      "        [-6.3975e-01, -7.4956e-01],\n",
      "        [-1.6040e-01, -1.9092e+00],\n",
      "        [-7.1539e-01, -6.7138e-01],\n",
      "        [-8.5791e-01, -5.5173e-01],\n",
      "        [-6.1053e-01, -7.8321e-01],\n",
      "        [-1.1174e+00, -3.9622e-01],\n",
      "        [-6.1286e-01, -7.8045e-01],\n",
      "        [-1.4163e-01, -2.0245e+00],\n",
      "        [-7.3707e-01, -6.5107e-01],\n",
      "        [-3.4878e-01, -1.2226e+00],\n",
      "        [-8.6983e-01, -5.4304e-01],\n",
      "        [-6.3328e-01, -7.5682e-01],\n",
      "        [-7.3913e-01, -6.4919e-01],\n",
      "        [-3.1559e-01, -1.3070e+00],\n",
      "        [-5.3242e-01, -8.8476e-01],\n",
      "        [-9.4551e-01, -4.9181e-01],\n",
      "        [-1.7390e-01, -1.8349e+00],\n",
      "        [-6.6860e-01, -7.1831e-01],\n",
      "        [-3.1044e-01, -1.3210e+00],\n",
      "        [-4.0494e-01, -1.0997e+00],\n",
      "        [-1.0170e+00, -4.4893e-01],\n",
      "        [-6.3277e-01, -7.5740e-01],\n",
      "        [-5.7117e-01, -8.3209e-01],\n",
      "        [-3.5836e-01, -1.2000e+00],\n",
      "        [-4.0963e-01, -1.0903e+00],\n",
      "        [-5.6969e-01, -8.3403e-01],\n",
      "        [-1.0875e+00, -4.1105e-01],\n",
      "        [-2.8829e-01, -1.3845e+00],\n",
      "        [-9.4592e-01, -4.9155e-01],\n",
      "        [-9.4558e-01, -4.9177e-01],\n",
      "        [-7.3994e-01, -6.4845e-01],\n",
      "        [-7.0833e-01, -6.7820e-01],\n",
      "        [-7.6096e-02, -2.6136e+00],\n",
      "        [-5.5486e-01, -8.5368e-01],\n",
      "        [-6.1267e-01, -7.8067e-01],\n",
      "        [-1.5778e+00, -2.3121e-01],\n",
      "        [-3.8684e-01, -1.1369e+00],\n",
      "        [-1.0557e+00, -4.2764e-01],\n",
      "        [-1.3753e+00, -2.9136e-01],\n",
      "        [-2.2842e+00, -1.0742e-01],\n",
      "        [-7.3510e-01, -6.5289e-01],\n",
      "        [-2.9810e-01, -1.3557e+00],\n",
      "        [-6.4345e-01, -7.4545e-01],\n",
      "        [-4.3696e-01, -1.0385e+00],\n",
      "        [-5.1074e-01, -9.1641e-01],\n",
      "        [-4.7953e-01, -9.6515e-01],\n",
      "        [-8.8540e-01, -5.3196e-01],\n",
      "        [-3.6896e-01, -1.1759e+00],\n",
      "        [-8.1246e-01, -5.8657e-01],\n",
      "        [-5.8630e-01, -8.1279e-01],\n",
      "        [-1.1673e+00, -3.7280e-01],\n",
      "        [-6.9899e-01, -6.8734e-01],\n",
      "        [-7.0066e-01, -6.8569e-01],\n",
      "        [-1.8702e-03, -6.2826e+00],\n",
      "        [-7.8598e-01, -6.0820e-01],\n",
      "        [-1.4400e+00, -2.7040e-01],\n",
      "        [-9.1895e-01, -5.0906e-01],\n",
      "        [-5.1718e-01, -9.0683e-01],\n",
      "        [-4.3123e-01, -1.0490e+00],\n",
      "        [-9.7649e-01, -4.7262e-01],\n",
      "        [-1.0700e+00, -4.2008e-01],\n",
      "        [-6.8187e-01, -7.0455e-01],\n",
      "        [-9.1840e-01, -5.0942e-01],\n",
      "        [-8.5936e-01, -5.5066e-01],\n",
      "        [-1.0482e+00, -4.3168e-01],\n",
      "        [-3.2422e-01, -1.2841e+00],\n",
      "        [-4.4107e-01, -1.0310e+00],\n",
      "        [-8.5134e-01, -5.5659e-01],\n",
      "        [-3.5895e-01, -1.1987e+00],\n",
      "        [-5.8480e-01, -8.1467e-01],\n",
      "        [-9.0129e-01, -5.2095e-01],\n",
      "        [-9.6192e-01, -4.8152e-01],\n",
      "        [-8.5615e-01, -5.5303e-01],\n",
      "        [-7.7986e-01, -6.1336e-01],\n",
      "        [-7.6367e-01, -6.2727e-01],\n",
      "        [-3.0867e-01, -1.3259e+00],\n",
      "        [-8.5968e-01, -5.5043e-01],\n",
      "        [-1.8263e+00, -1.7555e-01],\n",
      "        [-3.5307e-01, -1.2124e+00],\n",
      "        [-6.7357e-01, -7.1311e-01],\n",
      "        [-9.5975e-01, -4.8287e-01],\n",
      "        [-2.4875e-01, -1.5131e+00],\n",
      "        [-2.7388e-01, -1.4289e+00],\n",
      "        [-1.0390e+00, -4.3664e-01],\n",
      "        [-1.3303e+00, -3.0708e-01],\n",
      "        [-1.1524e+00, -3.7961e-01],\n",
      "        [-1.3508e+00, -2.9979e-01],\n",
      "        [-2.9609e-01, -1.3615e+00],\n",
      "        [-1.8125e-01, -1.7971e+00],\n",
      "        [-9.8498e-01, -4.6752e-01],\n",
      "        [-1.0510e+00, -4.3016e-01],\n",
      "        [-7.8779e-01, -6.0669e-01],\n",
      "        [-6.8677e-01, -6.9956e-01],\n",
      "        [-1.1246e+00, -3.9272e-01],\n",
      "        [-2.0890e+00, -1.3218e-01],\n",
      "        [-2.6239e-01, -1.4663e+00],\n",
      "        [-1.8438e+00, -1.7223e-01],\n",
      "        [-1.8639e+00, -1.6850e-01],\n",
      "        [-5.3989e-01, -8.7422e-01],\n",
      "        [-5.3055e-01, -8.8742e-01],\n",
      "        [-1.2004e+00, -3.5821e-01],\n",
      "        [-2.3086e-01, -1.5792e+00],\n",
      "        [-4.8576e-01, -9.5511e-01],\n",
      "        [-4.8088e-01, -9.6296e-01],\n",
      "        [-3.2042e-01, -1.2941e+00],\n",
      "        [-5.7680e-01, -8.2484e-01],\n",
      "        [-9.7695e-01, -4.7234e-01],\n",
      "        [-7.2793e-01, -6.5953e-01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
      "        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 1, 1, 0, 1, 0, 1])\n",
      "tensor([[-0.2038, -1.6909],\n",
      "        [-0.0388, -3.2682],\n",
      "        [-0.3951, -1.1196],\n",
      "        [-0.5041, -0.9264],\n",
      "        [-0.7057, -0.6807],\n",
      "        [-0.3377, -1.2496],\n",
      "        [-0.2815, -1.4050],\n",
      "        [-0.3394, -1.2455],\n",
      "        [-1.1801, -0.3671],\n",
      "        [-0.7254, -0.6619],\n",
      "        [-0.3107, -1.3204],\n",
      "        [-0.4231, -1.0643],\n",
      "        [-0.6782, -0.7083],\n",
      "        [-0.7493, -0.6400],\n",
      "        [-0.6752, -0.7115],\n",
      "        [-0.4086, -1.0923],\n",
      "        [-0.4743, -0.9737],\n",
      "        [-0.3019, -1.3448],\n",
      "        [-0.0164, -4.1200],\n",
      "        [-0.6935, -0.6928],\n",
      "        [-0.9474, -0.4906],\n",
      "        [-0.5469, -0.8645],\n",
      "        [-0.2905, -1.3778],\n",
      "        [-0.6073, -0.7871],\n",
      "        [-0.5192, -0.9039],\n",
      "        [-0.3310, -1.2665],\n",
      "        [-0.0665, -2.7432],\n",
      "        [-0.2581, -1.4807],\n",
      "        [-1.2761, -0.3273],\n",
      "        [-0.6038, -0.7913],\n",
      "        [-0.2659, -1.4545],\n",
      "        [-0.6488, -0.7395],\n",
      "        [-0.5151, -0.9099],\n",
      "        [-0.7333, -0.6546],\n",
      "        [-0.9301, -0.5017],\n",
      "        [-0.3997, -1.1102],\n",
      "        [-0.9957, -0.4612],\n",
      "        [-0.1183, -2.1933],\n",
      "        [-0.0538, -2.9484],\n",
      "        [-0.3987, -1.1123],\n",
      "        [-1.1236, -0.3932],\n",
      "        [-0.4494, -1.0162],\n",
      "        [-0.4542, -1.0077],\n",
      "        [-1.1766, -0.3687],\n",
      "        [-0.6260, -0.7651],\n",
      "        [-1.3206, -0.3106],\n",
      "        [-0.9602, -0.4826],\n",
      "        [-0.0791, -2.5765],\n",
      "        [-0.7436, -0.6451],\n",
      "        [-0.2726, -1.4330],\n",
      "        [-0.3692, -1.1754],\n",
      "        [-0.4500, -1.0151],\n",
      "        [-0.4938, -0.9424],\n",
      "        [-0.9592, -0.4832],\n",
      "        [-1.3764, -0.2910],\n",
      "        [-0.4364, -1.0394],\n",
      "        [-0.6362, -0.7536],\n",
      "        [-0.3076, -1.3289],\n",
      "        [-0.8784, -0.5369],\n",
      "        [-0.0951, -2.4002],\n",
      "        [-0.5223, -0.8993],\n",
      "        [-0.5422, -0.8710],\n",
      "        [-0.1294, -2.1086],\n",
      "        [-0.3145, -1.3098],\n",
      "        [-0.5059, -0.9237],\n",
      "        [-0.8325, -0.5708],\n",
      "        [-0.3339, -1.2593],\n",
      "        [-0.3348, -1.2570],\n",
      "        [-0.2319, -1.5751],\n",
      "        [-0.5296, -0.8888],\n",
      "        [-0.5766, -0.8251],\n",
      "        [-0.4082, -1.0931],\n",
      "        [-0.2834, -1.3993],\n",
      "        [-0.3433, -1.2360],\n",
      "        [-0.7599, -0.6306],\n",
      "        [-0.4851, -0.9561],\n",
      "        [-0.5637, -0.8419],\n",
      "        [-0.2573, -1.4836],\n",
      "        [-0.5289, -0.8897],\n",
      "        [-0.1483, -1.9817],\n",
      "        [-0.7313, -0.6564],\n",
      "        [-0.7833, -0.6105],\n",
      "        [-0.4024, -1.1047],\n",
      "        [-0.4161, -1.0776],\n",
      "        [-1.2420, -0.3408],\n",
      "        [-0.1466, -1.9928],\n",
      "        [-0.7993, -0.5972],\n",
      "        [-0.6090, -0.7851],\n",
      "        [-0.3843, -1.1424],\n",
      "        [-0.7942, -0.6014],\n",
      "        [-0.5919, -0.8059],\n",
      "        [-0.7914, -0.6037],\n",
      "        [-0.5846, -0.8149],\n",
      "        [-0.3286, -1.2727],\n",
      "        [-0.4450, -1.0239],\n",
      "        [-0.5198, -0.9030],\n",
      "        [-0.4478, -1.0190],\n",
      "        [-0.5869, -0.8120],\n",
      "        [-0.9250, -0.5051],\n",
      "        [-0.4774, -0.9687],\n",
      "        [-0.7137, -0.6730],\n",
      "        [-0.5139, -0.9117],\n",
      "        [-0.6571, -0.7306],\n",
      "        [-0.7097, -0.6769],\n",
      "        [-1.1504, -0.3806],\n",
      "        [-0.0414, -3.2059],\n",
      "        [-0.1814, -1.7964],\n",
      "        [-0.6305, -0.7599],\n",
      "        [-0.4619, -0.9945],\n",
      "        [-1.0131, -0.4511],\n",
      "        [-0.4685, -0.9833],\n",
      "        [-0.8150, -0.5845],\n",
      "        [-0.6555, -0.7323],\n",
      "        [-0.1086, -2.2740],\n",
      "        [-0.8681, -0.5443],\n",
      "        [-0.0067, -5.0075],\n",
      "        [-0.5567, -0.8511],\n",
      "        [-0.3877, -1.1352],\n",
      "        [-0.5318, -0.8856],\n",
      "        [-0.5864, -0.8127],\n",
      "        [-1.1846, -0.3651],\n",
      "        [-0.5903, -0.8078],\n",
      "        [-0.0635, -2.7880],\n",
      "        [-0.4190, -1.0722],\n",
      "        [-0.7963, -0.5997],\n",
      "        [-0.7361, -0.6520],\n",
      "        [-1.2333, -0.3444],\n",
      "        [-0.5699, -0.8337]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
      "        1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,\n",
      "        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
      "        0, 0, 1, 1, 1, 0, 1, 1])\n",
      "tensor([[-0.7034, -0.6830],\n",
      "        [-0.3813, -1.1488],\n",
      "        [-0.7915, -0.6036],\n",
      "        [-0.5418, -0.8715],\n",
      "        [-0.6506, -0.7376],\n",
      "        [-0.4011, -1.1074],\n",
      "        [-0.6423, -0.7468],\n",
      "        [-0.6125, -0.7809],\n",
      "        [-1.7379, -0.1935],\n",
      "        [-0.4702, -0.9804],\n",
      "        [-0.4142, -1.0814],\n",
      "        [-0.3673, -1.1797],\n",
      "        [-0.2678, -1.4483],\n",
      "        [-0.6223, -0.7694],\n",
      "        [-0.3247, -1.2827],\n",
      "        [-0.4422, -1.0289],\n",
      "        [-0.6124, -0.7810],\n",
      "        [-0.4758, -0.9713],\n",
      "        [-0.6887, -0.6976],\n",
      "        [-0.8706, -0.5425],\n",
      "        [-0.3136, -1.3122],\n",
      "        [-0.6217, -0.7701],\n",
      "        [-0.4947, -0.9409],\n",
      "        [-0.2497, -1.5097],\n",
      "        [-0.5119, -0.9146],\n",
      "        [-0.6378, -0.7518],\n",
      "        [-0.5982, -0.7981],\n",
      "        [-0.2045, -1.6878],\n",
      "        [-0.4743, -0.9738],\n",
      "        [-0.6639, -0.7233],\n",
      "        [-0.4149, -1.0799],\n",
      "        [-0.5894, -0.8089],\n",
      "        [-0.8329, -0.5706],\n",
      "        [-0.7270, -0.6604],\n",
      "        [-0.5140, -0.9116],\n",
      "        [-0.1097, -2.2645],\n",
      "        [-0.1009, -2.3441],\n",
      "        [-0.4596, -0.9985],\n",
      "        [-0.6575, -0.7301],\n",
      "        [-0.5709, -0.8325],\n",
      "        [-0.7403, -0.6481],\n",
      "        [-0.4169, -1.0762],\n",
      "        [-0.5807, -0.8199],\n",
      "        [-0.8030, -0.5942],\n",
      "        [-0.2726, -1.4330],\n",
      "        [-0.6077, -0.7865],\n",
      "        [-0.5679, -0.8363],\n",
      "        [-1.4964, -0.2535],\n",
      "        [-0.2779, -1.4164],\n",
      "        [-0.8530, -0.5553],\n",
      "        [-0.6456, -0.7430],\n",
      "        [-0.7173, -0.6696],\n",
      "        [-0.3057, -1.3340],\n",
      "        [-1.1684, -0.3723],\n",
      "        [-0.2928, -1.3710],\n",
      "        [-0.1568, -1.9304],\n",
      "        [-0.5328, -0.8841],\n",
      "        [-0.5320, -0.8854],\n",
      "        [-0.1791, -1.8080],\n",
      "        [-0.6128, -0.7805],\n",
      "        [-0.2574, -1.4831],\n",
      "        [-1.1131, -0.3983],\n",
      "        [-0.7192, -0.6677],\n",
      "        [-0.1859, -1.7739],\n",
      "        [-0.6880, -0.6984],\n",
      "        [-0.3255, -1.2807],\n",
      "        [-0.6777, -0.7088],\n",
      "        [-0.1804, -1.8013],\n",
      "        [-0.3477, -1.2254],\n",
      "        [-0.0913, -2.4384],\n",
      "        [-0.2925, -1.3719],\n",
      "        [-0.5607, -0.8458],\n",
      "        [-0.1203, -2.1776],\n",
      "        [-0.4130, -1.0838],\n",
      "        [-0.3388, -1.2469],\n",
      "        [-0.4873, -0.9527],\n",
      "        [-0.2938, -1.3682],\n",
      "        [-0.4222, -1.0659],\n",
      "        [-0.6542, -0.7337],\n",
      "        [-0.3125, -1.3152],\n",
      "        [-0.1146, -2.2228],\n",
      "        [-0.2755, -1.4238],\n",
      "        [-0.3378, -1.2495],\n",
      "        [-0.2216, -1.6157],\n",
      "        [-0.7123, -0.6743],\n",
      "        [-0.5469, -0.8644],\n",
      "        [-0.1125, -2.2409],\n",
      "        [-0.2189, -1.6265],\n",
      "        [-0.5288, -0.8899],\n",
      "        [-0.5975, -0.7990],\n",
      "        [-0.4812, -0.9624],\n",
      "        [-0.8066, -0.5913],\n",
      "        [-0.7040, -0.6824],\n",
      "        [-0.9046, -0.5187],\n",
      "        [-1.1953, -0.3604],\n",
      "        [-0.2974, -1.3578],\n",
      "        [-0.2354, -1.5620],\n",
      "        [-0.9593, -0.4831],\n",
      "        [-0.1017, -2.3366],\n",
      "        [-0.3547, -1.2086],\n",
      "        [-0.3375, -1.2503],\n",
      "        [-0.3012, -1.3467],\n",
      "        [-0.2163, -1.6372],\n",
      "        [-0.1651, -1.8824],\n",
      "        [-0.0486, -3.0474],\n",
      "        [-0.2124, -1.6538],\n",
      "        [-0.8425, -0.5632],\n",
      "        [-0.0445, -3.1350],\n",
      "        [-0.5087, -0.9194],\n",
      "        [-1.5288, -0.2444],\n",
      "        [-0.6996, -0.6868],\n",
      "        [-0.7707, -0.6212],\n",
      "        [-0.4543, -1.0076],\n",
      "        [-0.4622, -0.9939],\n",
      "        [-0.5042, -0.9263],\n",
      "        [-0.6256, -0.7656],\n",
      "        [-0.7108, -0.6758],\n",
      "        [-0.1340, -2.0764],\n",
      "        [-0.0871, -2.4839],\n",
      "        [-0.4375, -1.0375],\n",
      "        [-0.5954, -0.8015],\n",
      "        [-0.6040, -0.7910],\n",
      "        [-0.7578, -0.6324],\n",
      "        [-0.2358, -1.5603],\n",
      "        [-0.6983, -0.6880],\n",
      "        [-0.5664, -0.8384],\n",
      "        [-0.3151, -1.3083],\n",
      "        [-0.6999, -0.6864]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
      "        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 1, 0, 0])\n",
      "tensor([[-4.3191e-01, -1.0477e+00],\n",
      "        [-3.2087e-01, -1.2929e+00],\n",
      "        [-6.2010e-01, -7.7196e-01],\n",
      "        [-5.6475e-02, -2.9021e+00],\n",
      "        [-2.0589e+00, -1.3651e-01],\n",
      "        [-5.6241e-01, -8.4359e-01],\n",
      "        [-5.8476e-01, -8.1472e-01],\n",
      "        [-4.8083e-01, -9.6304e-01],\n",
      "        [-7.0635e-01, -6.8011e-01],\n",
      "        [-3.8451e-01, -1.1419e+00],\n",
      "        [-3.0076e-01, -1.3481e+00],\n",
      "        [-4.7732e-01, -9.6875e-01],\n",
      "        [-4.3730e-01, -1.0378e+00],\n",
      "        [-1.2807e+00, -3.2550e-01],\n",
      "        [-7.8065e-01, -6.1269e-01],\n",
      "        [-1.5325e-01, -1.9513e+00],\n",
      "        [-3.9596e-01, -1.1179e+00],\n",
      "        [-5.7970e-01, -8.2113e-01],\n",
      "        [-6.2509e-01, -7.6617e-01],\n",
      "        [-8.2866e-01, -5.7383e-01],\n",
      "        [-1.0912e-01, -2.2694e+00],\n",
      "        [-5.9397e-01, -8.0326e-01],\n",
      "        [-4.1500e-01, -1.0798e+00],\n",
      "        [-4.0107e-01, -1.1075e+00],\n",
      "        [-6.8111e-01, -7.0533e-01],\n",
      "        [-1.2009e+00, -3.5798e-01],\n",
      "        [-5.3088e-01, -8.8694e-01],\n",
      "        [-5.3408e-01, -8.8239e-01],\n",
      "        [-5.7266e-01, -8.3017e-01],\n",
      "        [-3.5517e-01, -1.2075e+00],\n",
      "        [-4.6001e-01, -9.9770e-01],\n",
      "        [-7.1303e-01, -6.7365e-01],\n",
      "        [-3.5170e-01, -1.2157e+00],\n",
      "        [-7.0321e-01, -6.8318e-01],\n",
      "        [-5.7199e-01, -8.3103e-01],\n",
      "        [-8.3601e-01, -5.6817e-01],\n",
      "        [-5.8534e-01, -8.1400e-01],\n",
      "        [-5.3786e-01, -8.7706e-01],\n",
      "        [-3.8409e-01, -1.1428e+00],\n",
      "        [-3.8666e-01, -1.1373e+00],\n",
      "        [-1.0530e+00, -4.2905e-01],\n",
      "        [-5.1701e-01, -9.0708e-01],\n",
      "        [-4.5216e-01, -1.0113e+00],\n",
      "        [-7.1938e-01, -6.6758e-01],\n",
      "        [-3.8570e-01, -1.1394e+00],\n",
      "        [-5.3391e-01, -8.8263e-01],\n",
      "        [-6.2827e-01, -7.6253e-01],\n",
      "        [-9.0666e-02, -2.4456e+00],\n",
      "        [-6.9912e-01, -6.8721e-01],\n",
      "        [-6.7577e-01, -7.1083e-01],\n",
      "        [-4.4762e-01, -1.0193e+00],\n",
      "        [-1.9417e-01, -1.7345e+00],\n",
      "        [-8.1807e-01, -5.8211e-01],\n",
      "        [-5.9717e-01, -7.9932e-01],\n",
      "        [-4.1104e-01, -1.0876e+00],\n",
      "        [-3.7334e-01, -1.1661e+00],\n",
      "        [-2.9444e-01, -1.3663e+00],\n",
      "        [-6.2673e-01, -7.6429e-01],\n",
      "        [-6.5958e-01, -7.2788e-01],\n",
      "        [-4.6021e-01, -9.9737e-01],\n",
      "        [-5.0793e-01, -9.2065e-01],\n",
      "        [-2.7523e-01, -1.4246e+00],\n",
      "        [-5.5829e-01, -8.4907e-01],\n",
      "        [-5.0971e-01, -9.1797e-01],\n",
      "        [-1.5049e+00, -2.5109e-01],\n",
      "        [-2.2432e-01, -1.6047e+00],\n",
      "        [-6.3104e-01, -7.5936e-01],\n",
      "        [-4.3311e-01, -1.0455e+00],\n",
      "        [-6.3133e-01, -7.5904e-01],\n",
      "        [-3.4625e-01, -1.2287e+00],\n",
      "        [-4.7460e-01, -9.7322e-01],\n",
      "        [-3.4149e-01, -1.2403e+00],\n",
      "        [-6.6630e-01, -7.2073e-01],\n",
      "        [-7.5578e-01, -6.3420e-01],\n",
      "        [-9.3696e-02, -2.4142e+00],\n",
      "        [-8.2945e-01, -5.7322e-01],\n",
      "        [-3.5576e-01, -1.2061e+00],\n",
      "        [-4.9892e-01, -9.3441e-01],\n",
      "        [-2.3026e-01, -1.5814e+00],\n",
      "        [-5.7051e-01, -8.3295e-01],\n",
      "        [-2.1179e-01, -1.6562e+00],\n",
      "        [-2.1165e-01, -1.6568e+00],\n",
      "        [-6.2261e-01, -7.6904e-01],\n",
      "        [-3.9927e-01, -1.1111e+00],\n",
      "        [-3.1988e-01, -1.2955e+00],\n",
      "        [-2.6933e-01, -1.4435e+00],\n",
      "        [-6.8382e-01, -7.0256e-01],\n",
      "        [-1.3131e+00, -3.1331e-01],\n",
      "        [-7.4784e-01, -6.4129e-01],\n",
      "        [-4.6111e-01, -9.9583e-01],\n",
      "        [-4.3845e-01, -1.0357e+00],\n",
      "        [-9.9237e-02, -2.3595e+00],\n",
      "        [-3.4584e-01, -1.2297e+00],\n",
      "        [-5.3391e-01, -8.8263e-01],\n",
      "        [-1.0251e+00, -4.4437e-01],\n",
      "        [-6.9750e-01, -6.8882e-01],\n",
      "        [-3.9748e-01, -1.1148e+00],\n",
      "        [-8.1415e-01, -5.8522e-01],\n",
      "        [-5.4580e-02, -2.9353e+00],\n",
      "        [-6.7096e-02, -2.7350e+00],\n",
      "        [-3.8238e-01, -1.1464e+00],\n",
      "        [-4.8922e-01, -9.4960e-01],\n",
      "        [-2.6954e-01, -1.4428e+00],\n",
      "        [-5.7976e-01, -8.2105e-01],\n",
      "        [-1.2171e-03, -6.7119e+00],\n",
      "        [-5.5657e-01, -8.5137e-01],\n",
      "        [-1.5701e-01, -1.9289e+00],\n",
      "        [-5.2940e-01, -8.8906e-01],\n",
      "        [-4.5576e-01, -1.0050e+00],\n",
      "        [-3.7434e-01, -1.1639e+00],\n",
      "        [-4.7661e-02, -3.0674e+00],\n",
      "        [-1.1180e-01, -2.2464e+00],\n",
      "        [-4.1964e-01, -1.0708e+00],\n",
      "        [-4.5393e-01, -1.0082e+00],\n",
      "        [-3.6297e-01, -1.1894e+00],\n",
      "        [-4.4824e-01, -1.0182e+00],\n",
      "        [-7.7485e-01, -6.1762e-01],\n",
      "        [-2.0088e-01, -1.7038e+00],\n",
      "        [-5.6734e-01, -8.3709e-01],\n",
      "        [-6.0190e-01, -7.9356e-01],\n",
      "        [-1.9250e-01, -1.7424e+00],\n",
      "        [-4.8844e-01, -9.5084e-01],\n",
      "        [-4.8682e-01, -9.5341e-01],\n",
      "        [-2.2072e-01, -1.6192e+00],\n",
      "        [-3.2946e-01, -1.2705e+00],\n",
      "        [-5.8913e-01, -8.0926e-01],\n",
      "        [-7.7675e-02, -2.5938e+00],\n",
      "        [-6.8933e-01, -6.9698e-01]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
      "        1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
      "        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
      "        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0, 0, 0])\n",
      "tensor([[-0.2489, -1.5127],\n",
      "        [-0.3517, -1.2156],\n",
      "        [-0.3271, -1.2766],\n",
      "        [-0.4743, -0.9737],\n",
      "        [-0.6763, -0.7103],\n",
      "        [-0.2776, -1.4170],\n",
      "        [-0.4378, -1.0370],\n",
      "        [-0.3396, -1.2449],\n",
      "        [-0.6999, -0.6864],\n",
      "        [-0.4959, -0.9390],\n",
      "        [-0.6385, -0.7510],\n",
      "        [-0.7221, -0.6650],\n",
      "        [-0.2419, -1.5376],\n",
      "        [-0.5793, -0.8217],\n",
      "        [-0.1474, -1.9873],\n",
      "        [-0.6667, -0.7203],\n",
      "        [-0.8052, -0.5924],\n",
      "        [-1.3316, -0.3066],\n",
      "        [-0.0600, -2.8431],\n",
      "        [-0.5874, -0.8114],\n",
      "        [-0.6256, -0.7656],\n",
      "        [-0.6275, -0.7634],\n",
      "        [-0.3131, -1.3137],\n",
      "        [-0.3745, -1.1636],\n",
      "        [-0.6683, -0.7186],\n",
      "        [-0.5717, -0.8314],\n",
      "        [-0.5838, -0.8160],\n",
      "        [-0.1619, -1.9009],\n",
      "        [-0.7130, -0.6736],\n",
      "        [-0.5207, -0.9016],\n",
      "        [-0.8924, -0.5271],\n",
      "        [-0.4000, -1.1096],\n",
      "        [-0.7277, -0.6598],\n",
      "        [-0.3112, -1.3188],\n",
      "        [-0.6057, -0.7890],\n",
      "        [-0.5762, -0.8257],\n",
      "        [-0.0515, -2.9912],\n",
      "        [-0.5032, -0.9278],\n",
      "        [-0.7575, -0.6327],\n",
      "        [-0.7089, -0.6776],\n",
      "        [-0.1181, -2.1946],\n",
      "        [-0.3329, -1.2617],\n",
      "        [-0.3790, -1.1537],\n",
      "        [-0.1622, -1.8989],\n",
      "        [-0.5393, -0.8751],\n",
      "        [-0.1684, -1.8647],\n",
      "        [-0.2372, -1.5550],\n",
      "        [-0.4495, -1.0159],\n",
      "        [-0.5533, -0.8558],\n",
      "        [-0.7567, -0.6334],\n",
      "        [-0.3618, -1.1922],\n",
      "        [-1.0622, -0.4242],\n",
      "        [-0.0440, -3.1457],\n",
      "        [-0.3147, -1.3094],\n",
      "        [-0.5327, -0.8843],\n",
      "        [-1.2853, -0.3237],\n",
      "        [-0.5480, -0.8630],\n",
      "        [-0.1330, -2.0834],\n",
      "        [-0.5453, -0.8667],\n",
      "        [-0.5801, -0.8206],\n",
      "        [-0.1594, -1.9147],\n",
      "        [-0.3239, -1.2849],\n",
      "        [-0.5761, -0.8257],\n",
      "        [-0.5224, -0.8992],\n",
      "        [-0.2383, -1.5511],\n",
      "        [-0.7645, -0.6266],\n",
      "        [-0.2826, -1.4016],\n",
      "        [-0.3565, -1.2045],\n",
      "        [-0.2966, -1.3601],\n",
      "        [-0.3982, -1.1133],\n",
      "        [-0.4176, -1.0748],\n",
      "        [-0.2246, -1.6037],\n",
      "        [-0.4653, -0.9887],\n",
      "        [-0.7522, -0.6374],\n",
      "        [-0.6144, -0.7786],\n",
      "        [-0.3340, -1.2591],\n",
      "        [-0.8228, -0.5784],\n",
      "        [-0.7030, -0.6834],\n",
      "        [-0.4905, -0.9475],\n",
      "        [-0.8280, -0.5743],\n",
      "        [-0.5762, -0.8256],\n",
      "        [-0.5597, -0.8471],\n",
      "        [-0.1653, -1.8813],\n",
      "        [-0.2056, -1.6828],\n",
      "        [-0.6363, -0.7534],\n",
      "        [-0.2525, -1.4999],\n",
      "        [-0.5695, -0.8342],\n",
      "        [-0.2814, -1.4053],\n",
      "        [-0.6564, -0.7313],\n",
      "        [-0.7102, -0.6764],\n",
      "        [-0.3948, -1.1203],\n",
      "        [-0.1533, -1.9509],\n",
      "        [-0.1182, -2.1940],\n",
      "        [-0.4549, -1.0065],\n",
      "        [-0.4927, -0.9440],\n",
      "        [-0.6399, -0.7494],\n",
      "        [-0.8375, -0.5671],\n",
      "        [-0.3164, -1.3048],\n",
      "        [-0.2835, -1.3990],\n",
      "        [-0.5376, -0.8775],\n",
      "        [-0.1494, -1.9750],\n",
      "        [-0.6429, -0.7460],\n",
      "        [-0.4165, -1.0769],\n",
      "        [-0.8320, -0.5712],\n",
      "        [-0.6240, -0.7675],\n",
      "        [-0.5870, -0.8119],\n",
      "        [-0.2519, -1.5019],\n",
      "        [-0.4944, -0.9415],\n",
      "        [-0.4985, -0.9351],\n",
      "        [-0.3156, -1.3070],\n",
      "        [-0.3679, -1.1783],\n",
      "        [-0.6503, -0.7379],\n",
      "        [-0.7758, -0.6168],\n",
      "        [-0.6813, -0.7051],\n",
      "        [-0.7158, -0.6710],\n",
      "        [-0.3176, -1.3017],\n",
      "        [-0.4892, -0.9497],\n",
      "        [-0.1918, -1.7454],\n",
      "        [-0.5910, -0.8069],\n",
      "        [-0.6670, -0.7200],\n",
      "        [-0.1044, -2.3113],\n",
      "        [-0.7152, -0.6716],\n",
      "        [-0.8816, -0.5347],\n",
      "        [-0.3553, -1.2072],\n",
      "        [-0.4097, -1.0902],\n",
      "        [-0.5209, -0.9013],\n",
      "        [-0.3749, -1.1627],\n",
      "        [-0.5874, -0.8114]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
      "        1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
      "        1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
      "        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 1])\n",
      "tensor([[-0.5893, -0.8090],\n",
      "        [-0.4763, -0.9704],\n",
      "        [-0.5573, -0.8504],\n",
      "        [-0.2291, -1.5861],\n",
      "        [-0.1725, -1.8425],\n",
      "        [-0.5935, -0.8039],\n",
      "        [-0.5793, -0.8216],\n",
      "        [-0.5826, -0.8175],\n",
      "        [-0.9516, -0.4880],\n",
      "        [-0.6285, -0.7623],\n",
      "        [-0.5778, -0.8235],\n",
      "        [-0.6037, -0.7914],\n",
      "        [-0.4303, -1.0507],\n",
      "        [-0.6997, -0.6866],\n",
      "        [-0.3193, -1.2970],\n",
      "        [-0.4575, -1.0021],\n",
      "        [-0.8723, -0.5413],\n",
      "        [-0.3664, -1.1816],\n",
      "        [-0.6487, -0.7396],\n",
      "        [-0.5078, -0.9209],\n",
      "        [-0.6928, -0.6935],\n",
      "        [-0.4259, -1.0589],\n",
      "        [-0.5710, -0.8323],\n",
      "        [-0.5429, -0.8700],\n",
      "        [-0.5058, -0.9239],\n",
      "        [-0.0076, -4.8785],\n",
      "        [-0.6570, -0.7307],\n",
      "        [-0.7667, -0.6247],\n",
      "        [-0.5622, -0.8439],\n",
      "        [-0.5420, -0.8713],\n",
      "        [-0.3709, -1.1716],\n",
      "        [-0.4434, -1.0268],\n",
      "        [-0.4613, -0.9955],\n",
      "        [-0.5951, -0.8019],\n",
      "        [-0.2817, -1.4043],\n",
      "        [-0.2568, -1.4851],\n",
      "        [-0.3746, -1.1633],\n",
      "        [-0.6765, -0.7101],\n",
      "        [-0.5828, -0.8173],\n",
      "        [-0.5318, -0.8856],\n",
      "        [-0.2226, -1.6117],\n",
      "        [-0.7123, -0.6744],\n",
      "        [-0.5742, -0.8282],\n",
      "        [-0.7042, -0.6822],\n",
      "        [-0.3576, -1.2019],\n",
      "        [-0.7503, -0.6390],\n",
      "        [-0.6169, -0.7757],\n",
      "        [-0.5870, -0.8120],\n",
      "        [-0.6273, -0.7636],\n",
      "        [-0.6542, -0.7336],\n",
      "        [-0.6581, -0.7294],\n",
      "        [-0.2100, -1.6640],\n",
      "        [-0.4993, -0.9338],\n",
      "        [-0.6937, -0.6926],\n",
      "        [-0.6983, -0.6880],\n",
      "        [-0.5677, -0.8366],\n",
      "        [-0.5897, -0.8086],\n",
      "        [-0.7080, -0.6785],\n",
      "        [-0.0830, -2.5305],\n",
      "        [-0.7279, -0.6596],\n",
      "        [-0.2675, -1.4496],\n",
      "        [-0.5921, -0.8055],\n",
      "        [-0.7280, -0.6595],\n",
      "        [-0.2177, -1.6317],\n",
      "        [-0.6414, -0.7477],\n",
      "        [-0.9891, -0.4651],\n",
      "        [-0.7061, -0.6803],\n",
      "        [-0.8985, -0.5228],\n",
      "        [-0.4624, -0.9937],\n",
      "        [-0.1038, -2.3172],\n",
      "        [-0.5631, -0.8426],\n",
      "        [-0.5598, -0.8470],\n",
      "        [-0.3481, -1.2244],\n",
      "        [-0.4646, -0.9899],\n",
      "        [-0.5316, -0.8859],\n",
      "        [-0.1296, -2.1074],\n",
      "        [-0.6500, -0.7383],\n",
      "        [-0.5970, -0.7995],\n",
      "        [-1.1047, -0.4024],\n",
      "        [-0.2462, -1.5221],\n",
      "        [-0.7701, -0.6217],\n",
      "        [-0.2532, -1.4975],\n",
      "        [-0.7262, -0.6612],\n",
      "        [-0.4202, -1.0699],\n",
      "        [-0.4058, -1.0979],\n",
      "        [-0.3172, -1.3025],\n",
      "        [-0.6062, -0.7884],\n",
      "        [-0.6191, -0.7731],\n",
      "        [-0.2657, -1.4553],\n",
      "        [-0.5183, -0.9052],\n",
      "        [-0.4647, -0.9898],\n",
      "        [-0.5859, -0.8133],\n",
      "        [-0.6644, -0.7227],\n",
      "        [-0.7437, -0.6450],\n",
      "        [-0.4651, -0.9891],\n",
      "        [-0.5604, -0.8462],\n",
      "        [-0.6899, -0.6964],\n",
      "        [-0.8038, -0.5935],\n",
      "        [-0.6941, -0.6922],\n",
      "        [-0.5436, -0.8691],\n",
      "        [-1.1337, -0.3884],\n",
      "        [-0.5324, -0.8848],\n",
      "        [-0.3610, -1.1939],\n",
      "        [-0.6006, -0.7952],\n",
      "        [-0.5647, -0.8405],\n",
      "        [-0.4089, -1.0917],\n",
      "        [-0.3267, -1.2777],\n",
      "        [-0.2297, -1.5835],\n",
      "        [-0.5221, -0.8996],\n",
      "        [-0.6227, -0.7690],\n",
      "        [-0.3997, -1.1102],\n",
      "        [-0.4340, -1.0439],\n",
      "        [-0.3634, -1.1885],\n",
      "        [-0.7221, -0.6650],\n",
      "        [-0.7159, -0.6709],\n",
      "        [-0.0883, -2.4704],\n",
      "        [-0.6630, -0.7242],\n",
      "        [-0.6354, -0.7544],\n",
      "        [-0.6960, -0.6903],\n",
      "        [-0.4383, -1.0360],\n",
      "        [-0.8936, -0.5263],\n",
      "        [-0.6007, -0.7950],\n",
      "        [-0.4886, -0.9506],\n",
      "        [-0.5143, -0.9110],\n",
      "        [-0.4543, -1.0075],\n",
      "        [-0.4570, -1.0029],\n",
      "        [-0.4932, -0.9434],\n",
      "        [-0.6871, -0.6993]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1,\n",
      "        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 0, 1, 1, 1, 0])\n",
      "tensor([[-0.6023, -0.7930],\n",
      "        [-0.5334, -0.8833],\n",
      "        [-0.5363, -0.8793],\n",
      "        [-0.6523, -0.7357],\n",
      "        [-0.6207, -0.7713],\n",
      "        [-0.5303, -0.8877],\n",
      "        [-0.9786, -0.4714],\n",
      "        [-0.7802, -0.6131],\n",
      "        [-0.6620, -0.7253],\n",
      "        [-0.1489, -1.9778],\n",
      "        [-0.6504, -0.7378],\n",
      "        [-0.4377, -1.0371],\n",
      "        [-0.5626, -0.8433],\n",
      "        [-0.4119, -1.0859],\n",
      "        [-0.3718, -1.1695],\n",
      "        [-0.6347, -0.7552],\n",
      "        [-0.6613, -0.7261],\n",
      "        [-0.1066, -2.2911],\n",
      "        [-0.4115, -1.0867],\n",
      "        [-0.6618, -0.7255],\n",
      "        [-0.6986, -0.6878],\n",
      "        [-0.2506, -1.5064],\n",
      "        [-0.8065, -0.5914],\n",
      "        [-0.2841, -1.3970],\n",
      "        [-0.7088, -0.6777],\n",
      "        [-0.6571, -0.7305],\n",
      "        [-0.7692, -0.6225],\n",
      "        [-0.3665, -1.1814],\n",
      "        [-0.5936, -0.8037],\n",
      "        [-0.6317, -0.7586],\n",
      "        [-0.5904, -0.8077],\n",
      "        [-0.5261, -0.8939],\n",
      "        [-0.6475, -0.7410],\n",
      "        [-0.4819, -0.9613],\n",
      "        [-0.6590, -0.7285],\n",
      "        [-0.8585, -0.5513],\n",
      "        [-0.5643, -0.8411],\n",
      "        [-0.3974, -1.1150],\n",
      "        [-0.9047, -0.5186],\n",
      "        [-0.4090, -1.0915],\n",
      "        [-0.5328, -0.8842],\n",
      "        [-0.3457, -1.2301],\n",
      "        [-0.2186, -1.6280],\n",
      "        [-0.7854, -0.6087],\n",
      "        [-0.5032, -0.9278],\n",
      "        [-0.3693, -1.1750],\n",
      "        [-0.8820, -0.5344],\n",
      "        [-0.4005, -1.1087],\n",
      "        [-0.7774, -0.6154],\n",
      "        [-0.4945, -0.9413],\n",
      "        [-0.6070, -0.7874],\n",
      "        [-0.5581, -0.8493],\n",
      "        [-0.6450, -0.7438],\n",
      "        [-0.6777, -0.7088],\n",
      "        [-0.4687, -0.9830],\n",
      "        [-0.5359, -0.8799],\n",
      "        [-0.7872, -0.6072],\n",
      "        [-0.5269, -0.8927],\n",
      "        [-0.4114, -1.0868],\n",
      "        [-0.6844, -0.7019],\n",
      "        [-0.3569, -1.2034],\n",
      "        [-0.6693, -0.7176],\n",
      "        [-0.5119, -0.9146],\n",
      "        [-0.5255, -0.8946],\n",
      "        [-0.7911, -0.6040],\n",
      "        [-0.5610, -0.8454],\n",
      "        [-0.6465, -0.7421],\n",
      "        [-0.4994, -0.9337],\n",
      "        [-0.4952, -0.9402],\n",
      "        [-0.3002, -1.3496],\n",
      "        [-0.5289, -0.8898],\n",
      "        [-0.6734, -0.7133],\n",
      "        [-0.4926, -0.9442],\n",
      "        [-0.9071, -0.5170],\n",
      "        [-0.4373, -1.0378],\n",
      "        [-1.0122, -0.4516],\n",
      "        [-0.3469, -1.2271],\n",
      "        [-0.4812, -0.9624],\n",
      "        [-0.5586, -0.8487],\n",
      "        [-0.5522, -0.8572],\n",
      "        [-0.6719, -0.7149],\n",
      "        [-0.5285, -0.8904],\n",
      "        [-0.2505, -1.5071],\n",
      "        [-0.7026, -0.6838],\n",
      "        [-0.6730, -0.7137],\n",
      "        [-0.7596, -0.6309],\n",
      "        [-0.5560, -0.8521],\n",
      "        [-0.5517, -0.8579],\n",
      "        [-0.6131, -0.7801],\n",
      "        [-0.4997, -0.9333],\n",
      "        [-0.4558, -1.0050],\n",
      "        [-0.5216, -0.9004],\n",
      "        [-0.5408, -0.8730],\n",
      "        [-0.3856, -1.1395],\n",
      "        [-0.5283, -0.8907],\n",
      "        [-0.6956, -0.6907],\n",
      "        [-0.5959, -0.8008],\n",
      "        [-0.6896, -0.6967],\n",
      "        [-0.2417, -1.5386],\n",
      "        [-0.5216, -0.9004],\n",
      "        [-0.7159, -0.6709],\n",
      "        [-0.2161, -1.6380],\n",
      "        [-0.6315, -0.7588],\n",
      "        [-0.3617, -1.1922],\n",
      "        [-0.5334, -0.8834],\n",
      "        [-0.6271, -0.7638],\n",
      "        [-0.6680, -0.7190],\n",
      "        [-0.5670, -0.8375],\n",
      "        [-0.7455, -0.6434],\n",
      "        [-0.7115, -0.6752],\n",
      "        [-0.6378, -0.7517],\n",
      "        [-0.9265, -0.5041],\n",
      "        [-0.6486, -0.7398],\n",
      "        [-0.5913, -0.8066],\n",
      "        [-0.5999, -0.7960],\n",
      "        [-0.2724, -1.4337],\n",
      "        [-1.1357, -0.3874],\n",
      "        [-0.2178, -1.6309],\n",
      "        [-0.3400, -1.2439],\n",
      "        [-0.8284, -0.5740],\n",
      "        [-0.6588, -0.7287],\n",
      "        [-0.6563, -0.7314],\n",
      "        [-0.4955, -0.9397],\n",
      "        [-0.6760, -0.7106],\n",
      "        [-0.6052, -0.7896],\n",
      "        [-0.5446, -0.8677],\n",
      "        [-0.0794, -2.5723],\n",
      "        [-0.2874, -1.3873]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
      "        0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0])\n",
      "tensor([[-0.7964, -0.5995],\n",
      "        [-0.6178, -0.7747],\n",
      "        [-0.7235, -0.6637],\n",
      "        [-0.7728, -0.6194],\n",
      "        [-0.9416, -0.4943],\n",
      "        [-0.3957, -1.1184],\n",
      "        [-0.4572, -1.0026],\n",
      "        [-0.7095, -0.6771],\n",
      "        [-0.5544, -0.8542],\n",
      "        [-0.6641, -0.7231],\n",
      "        [-0.4795, -0.9652],\n",
      "        [-0.5720, -0.8310],\n",
      "        [-0.7645, -0.6266],\n",
      "        [-0.6685, -0.7185],\n",
      "        [-0.3106, -1.3206],\n",
      "        [-0.2957, -1.3626],\n",
      "        [-0.6590, -0.7285],\n",
      "        [-0.8907, -0.5283],\n",
      "        [-0.6495, -0.7388],\n",
      "        [-0.8442, -0.5620],\n",
      "        [-0.4458, -1.0225],\n",
      "        [-0.5000, -0.9327],\n",
      "        [-0.6537, -0.7343],\n",
      "        [-0.4989, -0.9345],\n",
      "        [-0.8824, -0.5341],\n",
      "        [-0.8892, -0.5293],\n",
      "        [-0.4695, -0.9817],\n",
      "        [-0.3440, -1.2341],\n",
      "        [-0.7967, -0.5993],\n",
      "        [-0.4645, -0.9900],\n",
      "        [-0.5640, -0.8415],\n",
      "        [-0.5408, -0.8729],\n",
      "        [-0.9019, -0.5205],\n",
      "        [-0.5003, -0.9323],\n",
      "        [-0.7770, -0.6158],\n",
      "        [-0.9613, -0.4819],\n",
      "        [-0.7499, -0.6394],\n",
      "        [-0.4892, -0.9496],\n",
      "        [-0.7913, -0.6037],\n",
      "        [-0.2632, -1.4635],\n",
      "        [-0.6924, -0.6939],\n",
      "        [-0.6676, -0.7194],\n",
      "        [-0.5870, -0.8119],\n",
      "        [-0.7475, -0.6416],\n",
      "        [-0.3389, -1.2468],\n",
      "        [-0.6426, -0.7464],\n",
      "        [-0.4583, -1.0007],\n",
      "        [-0.1878, -1.7647],\n",
      "        [-0.1400, -2.0356],\n",
      "        [-0.7977, -0.5985],\n",
      "        [-0.7513, -0.6382],\n",
      "        [-0.6299, -0.7607],\n",
      "        [-0.5845, -0.8150],\n",
      "        [-0.4670, -0.9858],\n",
      "        [-0.4213, -1.0676],\n",
      "        [-0.4943, -0.9415],\n",
      "        [-0.5200, -0.9027],\n",
      "        [-0.5654, -0.8397],\n",
      "        [-0.5843, -0.8153],\n",
      "        [-1.0679, -0.4212],\n",
      "        [-0.5874, -0.8114],\n",
      "        [-0.6104, -0.7833],\n",
      "        [-0.5309, -0.8868],\n",
      "        [-0.6248, -0.7665],\n",
      "        [-0.7015, -0.6849],\n",
      "        [-0.2728, -1.4323],\n",
      "        [-0.6713, -0.7154],\n",
      "        [-0.6253, -0.7659],\n",
      "        [-0.5807, -0.8199],\n",
      "        [-0.7810, -0.6124],\n",
      "        [-0.6184, -0.7740],\n",
      "        [-0.5753, -0.8268],\n",
      "        [-0.4885, -0.9507],\n",
      "        [-0.3893, -1.1317],\n",
      "        [-0.6146, -0.7784],\n",
      "        [-0.7782, -0.6147],\n",
      "        [-0.5915, -0.8063],\n",
      "        [-0.5065, -0.9228],\n",
      "        [-0.6772, -0.7093],\n",
      "        [-0.6824, -0.7040],\n",
      "        [-0.6680, -0.7190],\n",
      "        [-0.3273, -1.2762],\n",
      "        [-0.1636, -1.8912],\n",
      "        [-0.4630, -0.9925],\n",
      "        [-0.6548, -0.7331],\n",
      "        [-0.0521, -2.9799],\n",
      "        [-0.7179, -0.6690],\n",
      "        [-0.6950, -0.6913],\n",
      "        [-0.6207, -0.7712],\n",
      "        [-0.2432, -1.5331],\n",
      "        [-0.5658, -0.8391],\n",
      "        [-0.3969, -1.1159],\n",
      "        [-0.4011, -1.1074],\n",
      "        [-0.8762, -0.5385],\n",
      "        [-0.9431, -0.4933],\n",
      "        [-0.7736, -0.6187],\n",
      "        [-0.5399, -0.8742],\n",
      "        [-1.1553, -0.3783],\n",
      "        [-0.2267, -1.5955],\n",
      "        [-0.4878, -0.9518],\n",
      "        [-0.5986, -0.7976],\n",
      "        [-0.5449, -0.8672],\n",
      "        [-0.8534, -0.5550],\n",
      "        [-0.6493, -0.7390],\n",
      "        [-0.7112, -0.6754],\n",
      "        [-0.3861, -1.1384],\n",
      "        [-0.5694, -0.8344],\n",
      "        [-0.2743, -1.4274],\n",
      "        [-0.6044, -0.7906],\n",
      "        [-0.7910, -0.6040],\n",
      "        [-0.7404, -0.6480],\n",
      "        [-0.6686, -0.7183],\n",
      "        [-0.7328, -0.6551],\n",
      "        [-0.7348, -0.6531],\n",
      "        [-0.3275, -1.2756],\n",
      "        [-0.7246, -0.6626],\n",
      "        [-0.8005, -0.5962],\n",
      "        [-0.7172, -0.6697],\n",
      "        [-1.1054, -0.4021],\n",
      "        [-0.6817, -0.7047],\n",
      "        [-0.5122, -0.9142],\n",
      "        [-0.8353, -0.5687],\n",
      "        [-0.5468, -0.8646],\n",
      "        [-0.5908, -0.8072],\n",
      "        [-0.6952, -0.6911],\n",
      "        [-0.3018, -1.3450],\n",
      "        [-0.7098, -0.6767],\n",
      "        [-0.6381, -0.7514]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
      "        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
      "        0, 0, 1, 1, 1, 0, 0, 1])\n",
      "tensor([[-0.7006, -0.6858],\n",
      "        [-0.6539, -0.7340],\n",
      "        [-0.5604, -0.8463],\n",
      "        [-0.4320, -1.0476],\n",
      "        [-0.9567, -0.4848],\n",
      "        [-0.6592, -0.7283],\n",
      "        [-0.6662, -0.7209],\n",
      "        [-0.8611, -0.5494],\n",
      "        [-0.6220, -0.7698],\n",
      "        [-0.5897, -0.8086],\n",
      "        [-0.7398, -0.6486],\n",
      "        [-0.6509, -0.7373],\n",
      "        [-0.6772, -0.7093],\n",
      "        [-0.7235, -0.6637],\n",
      "        [-0.5713, -0.8319],\n",
      "        [-1.4302, -0.2735],\n",
      "        [-0.3865, -1.1377],\n",
      "        [-0.6681, -0.7188],\n",
      "        [-0.3946, -1.1207],\n",
      "        [-0.8787, -0.5367],\n",
      "        [-0.1537, -1.9489],\n",
      "        [-0.6753, -0.7114],\n",
      "        [-0.4440, -1.0257],\n",
      "        [-0.6300, -0.7606],\n",
      "        [-0.5793, -0.8217],\n",
      "        [-0.7047, -0.6818],\n",
      "        [-0.7091, -0.6775],\n",
      "        [-0.6173, -0.7753],\n",
      "        [-0.7826, -0.6111],\n",
      "        [-0.7674, -0.6240],\n",
      "        [-0.7465, -0.6425],\n",
      "        [-0.4329, -1.0458],\n",
      "        [-0.7692, -0.6225],\n",
      "        [-0.7503, -0.6391],\n",
      "        [-0.6859, -0.7004],\n",
      "        [-0.6458, -0.7429],\n",
      "        [-0.5335, -0.8832],\n",
      "        [-0.5897, -0.8085],\n",
      "        [-0.6924, -0.6939],\n",
      "        [-0.6917, -0.6946],\n",
      "        [-0.6743, -0.7123],\n",
      "        [-0.4252, -1.0603],\n",
      "        [-0.8740, -0.5400],\n",
      "        [-0.2140, -1.6468],\n",
      "        [-0.3902, -1.1299],\n",
      "        [-0.5189, -0.9043],\n",
      "        [-0.6576, -0.7300],\n",
      "        [-0.7687, -0.6229],\n",
      "        [-0.5856, -0.8136],\n",
      "        [-0.9896, -0.4648],\n",
      "        [-0.6668, -0.7202],\n",
      "        [-0.4793, -0.9655],\n",
      "        [-0.8876, -0.5304],\n",
      "        [-0.8530, -0.5553],\n",
      "        [-0.6838, -0.7026],\n",
      "        [-0.6071, -0.7873],\n",
      "        [-0.2107, -1.6608],\n",
      "        [-0.7515, -0.6380],\n",
      "        [-0.5937, -0.8035],\n",
      "        [-0.7764, -0.6163],\n",
      "        [-0.5748, -0.8274],\n",
      "        [-0.6003, -0.7955],\n",
      "        [-0.4701, -0.9807],\n",
      "        [-0.8485, -0.5587],\n",
      "        [-0.5636, -0.8419],\n",
      "        [-0.6800, -0.7065],\n",
      "        [-1.0715, -0.4193],\n",
      "        [-0.7197, -0.6673],\n",
      "        [-0.3958, -1.1183],\n",
      "        [-0.8017, -0.5952],\n",
      "        [-0.6301, -0.7605],\n",
      "        [-0.7671, -0.6243],\n",
      "        [-0.8312, -0.5718],\n",
      "        [-0.6363, -0.7535],\n",
      "        [-0.3574, -1.2024],\n",
      "        [-0.5573, -0.8503],\n",
      "        [-0.7573, -0.6329],\n",
      "        [-0.5978, -0.7986],\n",
      "        [-0.5732, -0.8294],\n",
      "        [-0.8631, -0.5479],\n",
      "        [-0.6852, -0.7012],\n",
      "        [-1.1778, -0.3681],\n",
      "        [-0.7164, -0.6704],\n",
      "        [-0.7782, -0.6148],\n",
      "        [-0.5461, -0.8657],\n",
      "        [-0.6355, -0.7543],\n",
      "        [-0.8602, -0.5500],\n",
      "        [-0.5593, -0.8477],\n",
      "        [-0.8492, -0.5582],\n",
      "        [-0.6345, -0.7554],\n",
      "        [-0.6299, -0.7606],\n",
      "        [-0.5797, -0.8212],\n",
      "        [-0.8508, -0.5570],\n",
      "        [-0.5733, -0.8293],\n",
      "        [-0.6554, -0.7324],\n",
      "        [-0.6137, -0.7795],\n",
      "        [-0.7465, -0.6425],\n",
      "        [-0.8446, -0.5617],\n",
      "        [-0.4035, -1.1026],\n",
      "        [-1.0073, -0.4545],\n",
      "        [-0.7122, -0.6745],\n",
      "        [-0.7891, -0.6056],\n",
      "        [-0.5424, -0.8708],\n",
      "        [-0.5178, -0.9059],\n",
      "        [-0.9423, -0.4939],\n",
      "        [-0.5135, -0.9123],\n",
      "        [-0.6896, -0.6967],\n",
      "        [-0.8001, -0.5966],\n",
      "        [-0.6316, -0.7587],\n",
      "        [-0.6884, -0.6979],\n",
      "        [-0.6917, -0.6945],\n",
      "        [-0.8117, -0.5872],\n",
      "        [-0.6260, -0.7651],\n",
      "        [-0.7277, -0.6598],\n",
      "        [-0.6944, -0.6919],\n",
      "        [-0.9385, -0.4963],\n",
      "        [-0.6798, -0.7066],\n",
      "        [-0.6862, -0.7001],\n",
      "        [-0.8609, -0.5496],\n",
      "        [-0.4493, -1.0163],\n",
      "        [-1.0085, -0.4538],\n",
      "        [-0.5924, -0.8051],\n",
      "        [-0.5344, -0.8820],\n",
      "        [-0.7757, -0.6169],\n",
      "        [-0.7462, -0.6428],\n",
      "        [-0.5009, -0.9314],\n",
      "        [-0.7433, -0.6453],\n",
      "        [-0.4583, -1.0006]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
      "        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
      "        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
      "        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0])\n",
      "tensor([[-0.7003, -0.6860],\n",
      "        [-0.2748, -1.4259],\n",
      "        [-0.6244, -0.7670],\n",
      "        [-0.9440, -0.4927],\n",
      "        [-0.6941, -0.6922],\n",
      "        [-0.6817, -0.7047],\n",
      "        [-1.3410, -0.3032],\n",
      "        [-1.0317, -0.4407],\n",
      "        [-0.6515, -0.7366],\n",
      "        [-0.7178, -0.6690],\n",
      "        [-0.5931, -0.8043],\n",
      "        [-0.6186, -0.7737],\n",
      "        [-0.5806, -0.8200],\n",
      "        [-0.6668, -0.7202],\n",
      "        [-0.6308, -0.7596],\n",
      "        [-0.7043, -0.6821],\n",
      "        [-0.6225, -0.7691],\n",
      "        [-0.5366, -0.8789],\n",
      "        [-0.6763, -0.7102],\n",
      "        [-0.4936, -0.9428],\n",
      "        [-0.7298, -0.6578],\n",
      "        [-0.6522, -0.7358],\n",
      "        [-0.9032, -0.5196],\n",
      "        [-0.8937, -0.5261],\n",
      "        [-0.8367, -0.5676],\n",
      "        [-0.7711, -0.6209],\n",
      "        [-0.6341, -0.7559],\n",
      "        [-0.5627, -0.8431],\n",
      "        [-0.8651, -0.5465],\n",
      "        [-1.0107, -0.4525],\n",
      "        [-0.7075, -0.6790],\n",
      "        [-0.9671, -0.4783],\n",
      "        [-0.7116, -0.6750],\n",
      "        [-0.6732, -0.7135],\n",
      "        [-0.6971, -0.6892],\n",
      "        [-0.5015, -0.9304],\n",
      "        [-0.6054, -0.7893],\n",
      "        [-0.8647, -0.5468],\n",
      "        [-0.7558, -0.6342],\n",
      "        [-0.6934, -0.6929],\n",
      "        [-0.6029, -0.7924],\n",
      "        [-0.7962, -0.5998],\n",
      "        [-0.2150, -1.6427],\n",
      "        [-0.6592, -0.7283],\n",
      "        [-0.7285, -0.6590],\n",
      "        [-0.6947, -0.6916],\n",
      "        [-0.6509, -0.7372],\n",
      "        [-0.6160, -0.7768],\n",
      "        [-0.8068, -0.5911],\n",
      "        [-0.6872, -0.6992],\n",
      "        [-0.7371, -0.6511],\n",
      "        [-0.7062, -0.6803],\n",
      "        [-0.7111, -0.6756],\n",
      "        [-0.5611, -0.8453],\n",
      "        [-0.7130, -0.6737],\n",
      "        [-0.6050, -0.7899],\n",
      "        [-0.8411, -0.5643],\n",
      "        [-0.6878, -0.6985],\n",
      "        [-0.6741, -0.7125],\n",
      "        [-0.2900, -1.3794],\n",
      "        [-0.2276, -1.5916],\n",
      "        [-0.8115, -0.5873],\n",
      "        [-0.5027, -0.9287],\n",
      "        [-0.3759, -1.1606],\n",
      "        [-0.8645, -0.5469],\n",
      "        [-0.6410, -0.7482],\n",
      "        [-0.6900, -0.6963],\n",
      "        [-0.6372, -0.7524],\n",
      "        [-0.8181, -0.5821],\n",
      "        [-0.6773, -0.7092],\n",
      "        [-0.7730, -0.6192],\n",
      "        [-0.8043, -0.5931],\n",
      "        [-0.3442, -1.2336],\n",
      "        [-0.6807, -0.7058],\n",
      "        [-0.8623, -0.5485],\n",
      "        [-0.5311, -0.8867],\n",
      "        [-0.6808, -0.7057],\n",
      "        [-0.6239, -0.7675],\n",
      "        [-0.7454, -0.6435],\n",
      "        [-0.7111, -0.6755],\n",
      "        [-0.6152, -0.7776],\n",
      "        [-0.6249, -0.7664],\n",
      "        [-0.6433, -0.7456],\n",
      "        [-0.5150, -0.9101],\n",
      "        [-0.7503, -0.6391],\n",
      "        [-0.5819, -0.8184],\n",
      "        [-0.6334, -0.7567],\n",
      "        [-0.4247, -1.0613],\n",
      "        [-0.6651, -0.7220],\n",
      "        [-0.5293, -0.8892],\n",
      "        [-0.5851, -0.8143],\n",
      "        [-0.8671, -0.5450],\n",
      "        [-0.6688, -0.7181],\n",
      "        [-0.7076, -0.6789],\n",
      "        [-1.0549, -0.4281],\n",
      "        [-0.7662, -0.6251],\n",
      "        [-0.5500, -0.8602],\n",
      "        [-0.5851, -0.8143],\n",
      "        [-0.6705, -0.7163],\n",
      "        [-0.5127, -0.9135],\n",
      "        [-0.7288, -0.6587],\n",
      "        [-0.5023, -0.9291],\n",
      "        [-0.5808, -0.8197],\n",
      "        [-0.7443, -0.6445],\n",
      "        [-0.6483, -0.7401],\n",
      "        [-0.6848, -0.7016],\n",
      "        [-0.5741, -0.8284],\n",
      "        [-0.8942, -0.5258],\n",
      "        [-0.4665, -0.9867],\n",
      "        [-0.7621, -0.6286],\n",
      "        [-0.8466, -0.5601],\n",
      "        [-0.6375, -0.7521],\n",
      "        [-0.6892, -0.6971],\n",
      "        [-0.7246, -0.6627],\n",
      "        [-0.7899, -0.6050],\n",
      "        [-0.6686, -0.7183],\n",
      "        [-0.6520, -0.7361],\n",
      "        [-0.8567, -0.5526],\n",
      "        [-0.5817, -0.8185],\n",
      "        [-0.8030, -0.5941],\n",
      "        [-1.0447, -0.4336],\n",
      "        [-0.7232, -0.6640],\n",
      "        [-0.5702, -0.8333],\n",
      "        [-0.3226, -1.2884],\n",
      "        [-0.6589, -0.7286],\n",
      "        [-0.6943, -0.6920],\n",
      "        [-0.5943, -0.8029],\n",
      "        [-0.8028, -0.5943]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
      "        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        0, 1, 0, 0, 1, 1, 1, 1])\n",
      "tensor([[-0.5793, -0.8216],\n",
      "        [-0.6825, -0.7039],\n",
      "        [-0.6801, -0.7064],\n",
      "        [-0.5834, -0.8165],\n",
      "        [-0.6251, -0.7661],\n",
      "        [-0.8662, -0.5456],\n",
      "        [-0.4367, -1.0390],\n",
      "        [-0.9944, -0.4619],\n",
      "        [-0.6874, -0.6989],\n",
      "        [-0.5663, -0.8385],\n",
      "        [-0.9950, -0.4616],\n",
      "        [-0.6342, -0.7558],\n",
      "        [-0.8753, -0.5391],\n",
      "        [-0.5322, -0.8850],\n",
      "        [-0.6375, -0.7521],\n",
      "        [-0.6498, -0.7384],\n",
      "        [-0.6306, -0.7599],\n",
      "        [-0.3527, -1.2133],\n",
      "        [-0.5276, -0.8916],\n",
      "        [-0.9909, -0.4640],\n",
      "        [-0.6474, -0.7411],\n",
      "        [-0.6662, -0.7208],\n",
      "        [-0.6486, -0.7397],\n",
      "        [-0.6097, -0.7842],\n",
      "        [-0.6965, -0.6898],\n",
      "        [-0.6232, -0.7684],\n",
      "        [-0.6033, -0.7919],\n",
      "        [-0.6971, -0.6892],\n",
      "        [-0.8644, -0.5470],\n",
      "        [-0.7332, -0.6547],\n",
      "        [-0.7058, -0.6806],\n",
      "        [-0.6806, -0.7059],\n",
      "        [-0.6686, -0.7183],\n",
      "        [-0.5307, -0.8872],\n",
      "        [-0.7273, -0.6602],\n",
      "        [-1.2161, -0.3515],\n",
      "        [-0.7208, -0.6663],\n",
      "        [-0.3867, -1.1373],\n",
      "        [-0.7896, -0.6052],\n",
      "        [-0.6584, -0.7291],\n",
      "        [-0.8308, -0.5721],\n",
      "        [-0.7156, -0.6712],\n",
      "        [-0.8691, -0.5435],\n",
      "        [-0.8311, -0.5719],\n",
      "        [-0.6365, -0.7532],\n",
      "        [-0.5658, -0.8392],\n",
      "        [-0.6063, -0.7882],\n",
      "        [-0.7418, -0.6468],\n",
      "        [-0.4010, -1.1076],\n",
      "        [-0.7626, -0.6282],\n",
      "        [-0.6079, -0.7863],\n",
      "        [-0.5097, -0.9180],\n",
      "        [-0.6573, -0.7303],\n",
      "        [-0.9078, -0.5165],\n",
      "        [-0.7734, -0.6188],\n",
      "        [-0.6765, -0.7101],\n",
      "        [-0.6495, -0.7388],\n",
      "        [-0.5139, -0.9118],\n",
      "        [-0.6368, -0.7529],\n",
      "        [-0.7522, -0.6373],\n",
      "        [-0.6118, -0.7817],\n",
      "        [-0.8180, -0.5821],\n",
      "        [-0.6997, -0.6866],\n",
      "        [-0.8329, -0.5706],\n",
      "        [-0.8791, -0.5364],\n",
      "        [-0.7811, -0.6123],\n",
      "        [-0.5867, -0.8123],\n",
      "        [-0.8486, -0.5586],\n",
      "        [-0.5893, -0.8091],\n",
      "        [-0.8158, -0.5839],\n",
      "        [-0.5777, -0.8237],\n",
      "        [-0.7959, -0.6000],\n",
      "        [-0.8796, -0.5361],\n",
      "        [-0.8573, -0.5522],\n",
      "        [-0.6280, -0.7629],\n",
      "        [-0.8155, -0.5841],\n",
      "        [-0.6389, -0.7506],\n",
      "        [-0.5704, -0.8331],\n",
      "        [-0.6196, -0.7726],\n",
      "        [-0.3808, -1.1497],\n",
      "        [-0.7560, -0.6340],\n",
      "        [-0.7016, -0.6847],\n",
      "        [-0.6041, -0.7909],\n",
      "        [-0.6382, -0.7513],\n",
      "        [-0.5702, -0.8334],\n",
      "        [-0.5764, -0.8253],\n",
      "        [-0.6493, -0.7391],\n",
      "        [-0.6721, -0.7146],\n",
      "        [-0.6110, -0.7826],\n",
      "        [-0.7440, -0.6448],\n",
      "        [-0.4675, -0.9851],\n",
      "        [-0.3925, -1.1251],\n",
      "        [-0.7799, -0.6133],\n",
      "        [-0.8334, -0.5702],\n",
      "        [-0.4844, -0.9573],\n",
      "        [-0.4268, -1.0572],\n",
      "        [-0.7389, -0.6494],\n",
      "        [-0.6314, -0.7590],\n",
      "        [-0.2825, -1.4019],\n",
      "        [-0.8662, -0.5457],\n",
      "        [-0.7434, -0.6453],\n",
      "        [-0.7194, -0.6676],\n",
      "        [-0.8682, -0.5443],\n",
      "        [-0.7447, -0.6441],\n",
      "        [-0.6515, -0.7366],\n",
      "        [-0.9678, -0.4779],\n",
      "        [-0.6699, -0.7170],\n",
      "        [-0.4826, -0.9602],\n",
      "        [-0.6649, -0.7223],\n",
      "        [-0.8271, -0.5750],\n",
      "        [-0.6265, -0.7646],\n",
      "        [-0.8808, -0.5352],\n",
      "        [-0.6609, -0.7265],\n",
      "        [-0.9970, -0.4604],\n",
      "        [-0.7944, -0.6012],\n",
      "        [-0.4929, -0.9438],\n",
      "        [-0.4074, -1.0947],\n",
      "        [-0.9725, -0.4750],\n",
      "        [-0.6961, -0.6902],\n",
      "        [-0.9245, -0.5054],\n",
      "        [-0.6110, -0.7827],\n",
      "        [-0.5925, -0.8051],\n",
      "        [-0.6552, -0.7326],\n",
      "        [-0.6236, -0.7679],\n",
      "        [-0.8247, -0.5769],\n",
      "        [-0.6057, -0.7890],\n",
      "        [-0.8772, -0.5378],\n",
      "        [-0.7491, -0.6401]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
      "        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
      "        0, 0, 1, 1, 0, 0, 1, 1])\n",
      "tensor([[-0.9896, -0.4648],\n",
      "        [-1.0165, -0.4492],\n",
      "        [-0.7518, -0.6377],\n",
      "        [-0.6181, -0.7743],\n",
      "        [-0.6718, -0.7150],\n",
      "        [-0.6622, -0.7251],\n",
      "        [-0.6626, -0.7247],\n",
      "        [-0.5895, -0.8088],\n",
      "        [-0.7202, -0.6668],\n",
      "        [-1.0613, -0.4246],\n",
      "        [-0.8447, -0.5616],\n",
      "        [-0.8391, -0.5658],\n",
      "        [-0.5305, -0.8875],\n",
      "        [-0.6832, -0.7032],\n",
      "        [-0.6502, -0.7381],\n",
      "        [-1.3003, -0.3181],\n",
      "        [-0.7216, -0.6655],\n",
      "        [-0.7986, -0.5977],\n",
      "        [-0.6134, -0.7798],\n",
      "        [-0.8047, -0.5928],\n",
      "        [-0.8504, -0.5573],\n",
      "        [-0.5155, -0.9093],\n",
      "        [-0.6893, -0.6970],\n",
      "        [-0.6464, -0.7421],\n",
      "        [-0.6775, -0.7091],\n",
      "        [-0.6498, -0.7385],\n",
      "        [-0.7506, -0.6388],\n",
      "        [-0.6776, -0.7089],\n",
      "        [-0.6512, -0.7369],\n",
      "        [-0.6375, -0.7521],\n",
      "        [-0.7774, -0.6154],\n",
      "        [-0.6234, -0.7681],\n",
      "        [-0.8848, -0.5324],\n",
      "        [-0.6076, -0.7867],\n",
      "        [-0.6739, -0.7128],\n",
      "        [-0.8418, -0.5638],\n",
      "        [-0.9508, -0.4885],\n",
      "        [-0.5997, -0.7962],\n",
      "        [-0.6904, -0.6959],\n",
      "        [-0.6740, -0.7127],\n",
      "        [-0.5101, -0.9174],\n",
      "        [-0.7524, -0.6373],\n",
      "        [-1.0183, -0.4482],\n",
      "        [-0.7142, -0.6725],\n",
      "        [-0.6167, -0.7760],\n",
      "        [-0.7968, -0.5992],\n",
      "        [-0.7060, -0.6805],\n",
      "        [-1.1079, -0.4008],\n",
      "        [-1.2676, -0.3306],\n",
      "        [-0.9183, -0.5095],\n",
      "        [-0.9651, -0.4796],\n",
      "        [-0.6196, -0.7725],\n",
      "        [-1.0062, -0.4551],\n",
      "        [-0.8273, -0.5749],\n",
      "        [-0.7297, -0.6578],\n",
      "        [-0.6674, -0.7196],\n",
      "        [-0.6471, -0.7415],\n",
      "        [-0.7185, -0.6684],\n",
      "        [-0.6947, -0.6916],\n",
      "        [-0.7096, -0.6770],\n",
      "        [-0.7145, -0.6723],\n",
      "        [-0.8434, -0.5625],\n",
      "        [-0.6286, -0.7622],\n",
      "        [-0.8953, -0.5251],\n",
      "        [-0.5114, -0.9154],\n",
      "        [-0.6373, -0.7523],\n",
      "        [-0.7677, -0.6238],\n",
      "        [-0.5802, -0.8205],\n",
      "        [-0.6795, -0.7070],\n",
      "        [-0.6591, -0.7284],\n",
      "        [-0.7974, -0.5988],\n",
      "        [-1.2380, -0.3424],\n",
      "        [-0.5183, -0.9052],\n",
      "        [-0.7164, -0.6704],\n",
      "        [-0.8124, -0.5866],\n",
      "        [-0.7304, -0.6572],\n",
      "        [-0.6088, -0.7852],\n",
      "        [-0.5125, -0.9138],\n",
      "        [-0.4891, -0.9498],\n",
      "        [-0.7378, -0.6504],\n",
      "        [-0.7189, -0.6680],\n",
      "        [-0.7001, -0.6863],\n",
      "        [-0.7106, -0.6760],\n",
      "        [-0.6899, -0.6964],\n",
      "        [-1.1160, -0.3969],\n",
      "        [-0.5902, -0.8079],\n",
      "        [-0.6273, -0.7636],\n",
      "        [-1.4323, -0.2728],\n",
      "        [-0.6967, -0.6896],\n",
      "        [-0.8228, -0.5784],\n",
      "        [-0.7013, -0.6850],\n",
      "        [-1.3223, -0.3100],\n",
      "        [-0.6944, -0.6919],\n",
      "        [-0.6409, -0.7482],\n",
      "        [-0.8901, -0.5287],\n",
      "        [-0.7347, -0.6533],\n",
      "        [-0.6274, -0.7635],\n",
      "        [-0.8238, -0.5776],\n",
      "        [-0.7748, -0.6176],\n",
      "        [-0.7900, -0.6049],\n",
      "        [-0.8662, -0.5456],\n",
      "        [-0.5410, -0.8727],\n",
      "        [-0.6801, -0.7064],\n",
      "        [-0.7249, -0.6624],\n",
      "        [-0.4353, -1.0415],\n",
      "        [-0.6032, -0.7920],\n",
      "        [-0.7706, -0.6213],\n",
      "        [-1.0130, -0.4512],\n",
      "        [-0.6297, -0.7608],\n",
      "        [-0.4408, -1.0315],\n",
      "        [-0.5600, -0.8467],\n",
      "        [-0.7141, -0.6726],\n",
      "        [-0.6151, -0.7779],\n",
      "        [-0.6878, -0.6985],\n",
      "        [-0.6160, -0.7768],\n",
      "        [-0.7213, -0.6658],\n",
      "        [-0.6688, -0.7181],\n",
      "        [-0.6145, -0.7785],\n",
      "        [-0.6559, -0.7318],\n",
      "        [-0.5704, -0.8331],\n",
      "        [-0.7893, -0.6054],\n",
      "        [-0.4526, -1.0106],\n",
      "        [-0.7126, -0.6740],\n",
      "        [-0.6255, -0.7657],\n",
      "        [-0.9012, -0.5210],\n",
      "        [-0.8628, -0.5482],\n",
      "        [-0.6091, -0.7849],\n",
      "        [-0.8302, -0.5726]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,\n",
      "        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
      "        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 0, 0, 1])\n",
      "tensor([[-0.6158, -0.7769],\n",
      "        [-0.8886, -0.5298],\n",
      "        [-0.6092, -0.7848],\n",
      "        [-0.4646, -0.9900],\n",
      "        [-0.6220, -0.7697],\n",
      "        [-1.1115, -0.3991],\n",
      "        [-0.7983, -0.5980],\n",
      "        [-0.7358, -0.6522],\n",
      "        [-0.6775, -0.7091],\n",
      "        [-0.7015, -0.6848],\n",
      "        [-0.9235, -0.5061],\n",
      "        [-0.7337, -0.6542],\n",
      "        [-0.5900, -0.8082],\n",
      "        [-0.7754, -0.6172],\n",
      "        [-0.4123, -1.0851],\n",
      "        [-0.9088, -0.5159],\n",
      "        [-0.6816, -0.7048],\n",
      "        [-0.7141, -0.6726],\n",
      "        [-0.7155, -0.6713],\n",
      "        [-0.8907, -0.5283],\n",
      "        [-1.1563, -0.3778],\n",
      "        [-0.7348, -0.6531],\n",
      "        [-0.9728, -0.4748],\n",
      "        [-0.5822, -0.8179],\n",
      "        [-0.6082, -0.7860],\n",
      "        [-0.8035, -0.5937],\n",
      "        [-0.7045, -0.6819],\n",
      "        [-0.6068, -0.7876],\n",
      "        [-0.6585, -0.7290],\n",
      "        [-0.5269, -0.8926],\n",
      "        [-0.5154, -0.9094],\n",
      "        [-0.7553, -0.6347],\n",
      "        [-0.7268, -0.6606],\n",
      "        [-0.5298, -0.8884],\n",
      "        [-0.6204, -0.7716],\n",
      "        [-0.7381, -0.6502],\n",
      "        [-0.6894, -0.6969],\n",
      "        [-0.6012, -0.7944],\n",
      "        [-0.7060, -0.6804],\n",
      "        [-0.6132, -0.7801],\n",
      "        [-0.5684, -0.8357],\n",
      "        [-0.8426, -0.5631],\n",
      "        [-0.6792, -0.7073],\n",
      "        [-0.5285, -0.8903],\n",
      "        [-0.3835, -1.1441],\n",
      "        [-0.6956, -0.6907],\n",
      "        [-0.5382, -0.8766],\n",
      "        [-0.6256, -0.7656],\n",
      "        [-0.6567, -0.7310],\n",
      "        [-0.5321, -0.8852],\n",
      "        [-0.5594, -0.8475],\n",
      "        [-0.5273, -0.8921],\n",
      "        [-0.5269, -0.8926],\n",
      "        [-0.6634, -0.7238],\n",
      "        [-0.9272, -0.5036],\n",
      "        [-0.6975, -0.6889],\n",
      "        [-0.6595, -0.7279],\n",
      "        [-0.2828, -1.4010],\n",
      "        [-0.5496, -0.8608],\n",
      "        [-0.5826, -0.8174],\n",
      "        [-0.5514, -0.8584],\n",
      "        [-0.8165, -0.5834],\n",
      "        [-0.6282, -0.7626],\n",
      "        [-1.0490, -0.4312],\n",
      "        [-0.5076, -0.9212],\n",
      "        [-0.7386, -0.6497],\n",
      "        [-0.7284, -0.6591],\n",
      "        [-0.7218, -0.6653],\n",
      "        [-0.3755, -1.1613],\n",
      "        [-0.7360, -0.6521],\n",
      "        [-0.6908, -0.6955],\n",
      "        [-0.7899, -0.6049],\n",
      "        [-0.8582, -0.5515],\n",
      "        [-0.5586, -0.8486],\n",
      "        [-0.5399, -0.8741],\n",
      "        [-0.6871, -0.6992],\n",
      "        [-0.7892, -0.6055],\n",
      "        [-0.5338, -0.8828],\n",
      "        [-0.5693, -0.8346],\n",
      "        [-0.7335, -0.6543],\n",
      "        [-0.5515, -0.8582],\n",
      "        [-0.7172, -0.6696],\n",
      "        [-0.8430, -0.5629],\n",
      "        [-0.6664, -0.7206],\n",
      "        [-0.7018, -0.6846],\n",
      "        [-0.3032, -1.3411],\n",
      "        [-0.8313, -0.5718],\n",
      "        [-0.8147, -0.5848],\n",
      "        [-0.6842, -0.7021],\n",
      "        [-0.3625, -1.1905],\n",
      "        [-0.6605, -0.7269],\n",
      "        [-1.0506, -0.4303],\n",
      "        [-0.5911, -0.8068],\n",
      "        [-0.7064, -0.6801],\n",
      "        [-0.6975, -0.6888],\n",
      "        [-0.6193, -0.7729],\n",
      "        [-0.6845, -0.7019],\n",
      "        [-0.7939, -0.6016],\n",
      "        [-0.9234, -0.5061],\n",
      "        [-0.7665, -0.6248],\n",
      "        [-0.6846, -0.7017],\n",
      "        [-0.5660, -0.8388],\n",
      "        [-0.4955, -0.9398],\n",
      "        [-0.7670, -0.6244],\n",
      "        [-0.5770, -0.8246],\n",
      "        [-0.7087, -0.6779],\n",
      "        [-0.6116, -0.7820],\n",
      "        [-0.7620, -0.6288],\n",
      "        [-0.7867, -0.6076],\n",
      "        [-0.8550, -0.5539],\n",
      "        [-0.7382, -0.6500],\n",
      "        [-0.4694, -0.9818],\n",
      "        [-0.4963, -0.9385],\n",
      "        [-0.4950, -0.9404],\n",
      "        [-1.0100, -0.4529],\n",
      "        [-0.3858, -1.1392],\n",
      "        [-0.7198, -0.6671],\n",
      "        [-0.8676, -0.5447],\n",
      "        [-0.6740, -0.7127],\n",
      "        [-0.6428, -0.7462],\n",
      "        [-0.6081, -0.7861],\n",
      "        [-0.7569, -0.6332],\n",
      "        [-0.4665, -0.9868],\n",
      "        [-0.3255, -1.2807],\n",
      "        [-0.6671, -0.7199],\n",
      "        [-0.7329, -0.6549],\n",
      "        [-0.7394, -0.6489],\n",
      "        [-0.6733, -0.7134]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
      "        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
      "        1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
      "        1, 1, 0, 0, 1, 0, 1, 0])\n",
      "tensor([[-0.6597, -0.7278],\n",
      "        [-0.7379, -0.6503],\n",
      "        [-0.6892, -0.6971],\n",
      "        [-0.7068, -0.6796],\n",
      "        [-0.6492, -0.7391],\n",
      "        [-0.4914, -0.9462],\n",
      "        [-0.6608, -0.7266],\n",
      "        [-0.5728, -0.8299],\n",
      "        [-0.7984, -0.5979],\n",
      "        [-0.7365, -0.6516],\n",
      "        [-0.7024, -0.6840],\n",
      "        [-0.3887, -1.1330],\n",
      "        [-0.6786, -0.7079],\n",
      "        [-0.5990, -0.7971],\n",
      "        [-0.8981, -0.5231],\n",
      "        [-0.5746, -0.8276],\n",
      "        [-0.6305, -0.7600],\n",
      "        [-0.7071, -0.6794],\n",
      "        [-0.6581, -0.7295],\n",
      "        [-0.7726, -0.6195],\n",
      "        [-0.7359, -0.6521],\n",
      "        [-0.6819, -0.7045],\n",
      "        [-0.6153, -0.7775],\n",
      "        [-1.1330, -0.3887],\n",
      "        [-0.3347, -1.2572],\n",
      "        [-0.7037, -0.6827],\n",
      "        [-0.6941, -0.6922],\n",
      "        [-0.6643, -0.7229],\n",
      "        [-0.4959, -0.9391],\n",
      "        [-0.5655, -0.8395],\n",
      "        [-0.5901, -0.8080],\n",
      "        [-0.5707, -0.8327],\n",
      "        [-0.9446, -0.4924],\n",
      "        [-0.4126, -1.0845],\n",
      "        [-0.6860, -0.7004],\n",
      "        [-0.7149, -0.6719],\n",
      "        [-1.0397, -0.4363],\n",
      "        [-0.9297, -0.5020],\n",
      "        [-0.6071, -0.7874],\n",
      "        [-0.6196, -0.7725],\n",
      "        [-0.8540, -0.5546],\n",
      "        [-0.6782, -0.7084],\n",
      "        [-0.8629, -0.5481],\n",
      "        [-0.7821, -0.6115],\n",
      "        [-0.3555, -1.2066],\n",
      "        [-0.5537, -0.8553],\n",
      "        [-0.4662, -0.9872],\n",
      "        [-0.8738, -0.5402],\n",
      "        [-0.1826, -1.7902],\n",
      "        [-0.4862, -0.9545],\n",
      "        [-0.7630, -0.6279],\n",
      "        [-1.2423, -0.3407],\n",
      "        [-0.5418, -0.8715],\n",
      "        [-0.7289, -0.6586],\n",
      "        [-0.7848, -0.6092],\n",
      "        [-0.6809, -0.7056],\n",
      "        [-0.5692, -0.8347],\n",
      "        [-0.6608, -0.7265],\n",
      "        [-0.7181, -0.6688],\n",
      "        [-0.7491, -0.6402],\n",
      "        [-0.6878, -0.6986],\n",
      "        [-0.6832, -0.7032],\n",
      "        [-0.6008, -0.7949],\n",
      "        [-0.7565, -0.6336],\n",
      "        [-0.7742, -0.6181],\n",
      "        [-0.6045, -0.7904],\n",
      "        [-0.6043, -0.7907],\n",
      "        [-0.7851, -0.6089],\n",
      "        [-0.6896, -0.6967],\n",
      "        [-0.3627, -1.1900],\n",
      "        [-0.7258, -0.6615],\n",
      "        [-0.4841, -0.9578],\n",
      "        [-0.7336, -0.6542],\n",
      "        [-0.3908, -1.1286],\n",
      "        [-0.6499, -0.7384],\n",
      "        [-0.6231, -0.7685],\n",
      "        [-0.6949, -0.6914],\n",
      "        [-0.7041, -0.6823],\n",
      "        [-0.9451, -0.4921],\n",
      "        [-0.9495, -0.4893],\n",
      "        [-0.6812, -0.7053],\n",
      "        [-0.7044, -0.6821],\n",
      "        [-0.9813, -0.4697],\n",
      "        [-0.7142, -0.6725],\n",
      "        [-0.6717, -0.7150],\n",
      "        [-0.5595, -0.8474],\n",
      "        [-0.7498, -0.6396],\n",
      "        [-0.6738, -0.7129],\n",
      "        [-0.7451, -0.6437],\n",
      "        [-0.6849, -0.7014],\n",
      "        [-0.5465, -0.8650],\n",
      "        [-0.7637, -0.6273],\n",
      "        [-0.7085, -0.6780],\n",
      "        [-0.5229, -0.8985],\n",
      "        [-0.6369, -0.7527],\n",
      "        [-0.7302, -0.6574],\n",
      "        [-0.6912, -0.6951],\n",
      "        [-0.6871, -0.6993],\n",
      "        [-0.7715, -0.6205],\n",
      "        [-0.6699, -0.7169],\n",
      "        [-0.6543, -0.7335],\n",
      "        [-0.7080, -0.6785],\n",
      "        [-0.8193, -0.5812],\n",
      "        [-0.7278, -0.6596],\n",
      "        [-0.6340, -0.7561],\n",
      "        [-0.7645, -0.6265],\n",
      "        [-0.7736, -0.6187],\n",
      "        [-0.4901, -0.9482],\n",
      "        [-0.3305, -1.2678],\n",
      "        [-0.6446, -0.7442],\n",
      "        [-0.8102, -0.5884],\n",
      "        [-0.7416, -0.6470],\n",
      "        [-0.1912, -1.7487],\n",
      "        [-0.7163, -0.6705],\n",
      "        [-0.9366, -0.4975],\n",
      "        [-0.5922, -0.8055],\n",
      "        [-0.4446, -1.0246],\n",
      "        [-0.6399, -0.7494],\n",
      "        [-0.6342, -0.7558],\n",
      "        [-0.8380, -0.5666],\n",
      "        [-0.6854, -0.7009],\n",
      "        [-0.5657, -0.8393],\n",
      "        [-0.6652, -0.7219],\n",
      "        [-0.6526, -0.7354],\n",
      "        [-0.8519, -0.5562],\n",
      "        [-0.9609, -0.4822],\n",
      "        [-0.6583, -0.7293],\n",
      "        [-0.7221, -0.6650]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
      "        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1])\n",
      "tensor([[-0.5048, -0.9254],\n",
      "        [-0.3657, -1.1832],\n",
      "        [-0.7780, -0.6150],\n",
      "        [-0.5730, -0.8298],\n",
      "        [-0.6454, -0.7433],\n",
      "        [-0.5378, -0.8771],\n",
      "        [-0.7165, -0.6703],\n",
      "        [-0.7728, -0.6194],\n",
      "        [-0.5161, -0.9084],\n",
      "        [-0.6750, -0.7116],\n",
      "        [-0.9549, -0.4859],\n",
      "        [-0.6215, -0.7703],\n",
      "        [-0.7908, -0.6042],\n",
      "        [-0.6855, -0.7008],\n",
      "        [-0.5629, -0.8430],\n",
      "        [-0.4206, -1.0690],\n",
      "        [-0.7879, -0.6066],\n",
      "        [-1.1000, -0.4048],\n",
      "        [-0.6795, -0.7070],\n",
      "        [-0.7485, -0.6407],\n",
      "        [-0.6725, -0.7142],\n",
      "        [-0.5479, -0.8631],\n",
      "        [-0.5653, -0.8397],\n",
      "        [-0.6672, -0.7197],\n",
      "        [-0.7392, -0.6491],\n",
      "        [-0.6541, -0.7337],\n",
      "        [-0.4248, -1.0611],\n",
      "        [-0.9671, -0.4784],\n",
      "        [-0.5459, -0.8658],\n",
      "        [-0.6457, -0.7430],\n",
      "        [-0.5814, -0.8190],\n",
      "        [-0.7412, -0.6473],\n",
      "        [-0.6528, -0.7352],\n",
      "        [-0.8672, -0.5450],\n",
      "        [-0.6809, -0.7056],\n",
      "        [-0.7610, -0.6296],\n",
      "        [-0.6984, -0.6879],\n",
      "        [-0.7189, -0.6681],\n",
      "        [-0.8504, -0.5573],\n",
      "        [-0.6551, -0.7327],\n",
      "        [-0.9577, -0.4842],\n",
      "        [-0.6775, -0.7090],\n",
      "        [-0.6978, -0.6885],\n",
      "        [-0.5005, -0.9320],\n",
      "        [-0.5173, -0.9067],\n",
      "        [-0.7653, -0.6258],\n",
      "        [-0.6378, -0.7517],\n",
      "        [-0.7079, -0.6786],\n",
      "        [-0.9379, -0.4967],\n",
      "        [-0.7143, -0.6724],\n",
      "        [-0.7345, -0.6535],\n",
      "        [-0.5373, -0.8778],\n",
      "        [-0.8611, -0.5494],\n",
      "        [-0.8229, -0.5783],\n",
      "        [-0.6967, -0.6896],\n",
      "        [-0.7221, -0.6650],\n",
      "        [-0.6859, -0.7004],\n",
      "        [-0.5675, -0.8369],\n",
      "        [-0.6778, -0.7088],\n",
      "        [-0.5389, -0.8757],\n",
      "        [-0.7293, -0.6582],\n",
      "        [-0.6680, -0.7189],\n",
      "        [-0.5112, -0.9157],\n",
      "        [-0.5172, -0.9068],\n",
      "        [-0.6597, -0.7278],\n",
      "        [-0.7636, -0.6273],\n",
      "        [-0.4770, -0.9693],\n",
      "        [-0.6394, -0.7499],\n",
      "        [-0.6237, -0.7678],\n",
      "        [-0.6720, -0.7147],\n",
      "        [-0.8192, -0.5813],\n",
      "        [-0.6589, -0.7286],\n",
      "        [-0.8406, -0.5647],\n",
      "        [-1.0429, -0.4345],\n",
      "        [-0.7291, -0.6585],\n",
      "        [-0.5503, -0.8599],\n",
      "        [-0.8252, -0.5765],\n",
      "        [-0.6320, -0.7583],\n",
      "        [-0.5649, -0.8403],\n",
      "        [-0.7471, -0.6419],\n",
      "        [-0.6492, -0.7391],\n",
      "        [-0.7624, -0.6284],\n",
      "        [-0.7278, -0.6596],\n",
      "        [-0.6926, -0.6937],\n",
      "        [-0.6840, -0.7024],\n",
      "        [-0.7133, -0.6734],\n",
      "        [-0.9745, -0.4738],\n",
      "        [-0.6652, -0.7218],\n",
      "        [-0.7655, -0.6257],\n",
      "        [-0.4863, -0.9543],\n",
      "        [-1.0635, -0.4235],\n",
      "        [-0.4822, -0.9609],\n",
      "        [-0.6564, -0.7313],\n",
      "        [-0.7672, -0.6242],\n",
      "        [-0.8406, -0.5647],\n",
      "        [-0.4219, -1.0666],\n",
      "        [-0.7788, -0.6143],\n",
      "        [-0.7912, -0.6038],\n",
      "        [-0.3266, -1.2780],\n",
      "        [-0.8312, -0.5719],\n",
      "        [-0.8722, -0.5413],\n",
      "        [-0.7874, -0.6070],\n",
      "        [-0.7063, -0.6802],\n",
      "        [-0.7033, -0.6831],\n",
      "        [-0.6814, -0.7050],\n",
      "        [-0.7253, -0.6620],\n",
      "        [-0.7217, -0.6654],\n",
      "        [-0.6953, -0.6910],\n",
      "        [-0.6615, -0.7258],\n",
      "        [-0.7023, -0.6841],\n",
      "        [-0.6430, -0.7459],\n",
      "        [-0.6661, -0.7209],\n",
      "        [-0.5946, -0.8024],\n",
      "        [-0.7140, -0.6727],\n",
      "        [-1.4315, -0.2731],\n",
      "        [-0.6439, -0.7450],\n",
      "        [-0.6730, -0.7137],\n",
      "        [-0.7526, -0.6370],\n",
      "        [-0.6747, -0.7119],\n",
      "        [-0.8775, -0.5375],\n",
      "        [-0.8020, -0.5950],\n",
      "        [-0.7730, -0.6192],\n",
      "        [-0.6713, -0.7155],\n",
      "        [-0.6972, -0.6891],\n",
      "        [-0.8864, -0.5312],\n",
      "        [-0.6304, -0.7601],\n",
      "        [-0.9284, -0.5028],\n",
      "        [-0.6306, -0.7599]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
      "        0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
      "        1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
      "        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0,\n",
      "        1, 0, 1, 0, 1, 1, 1, 1])\n",
      "tensor([[-0.4752, -0.9723],\n",
      "        [-0.5331, -0.8838],\n",
      "        [-0.6550, -0.7328],\n",
      "        [-0.4371, -1.0382],\n",
      "        [-0.7054, -0.6810],\n",
      "        [-0.4549, -1.0066],\n",
      "        [-0.6987, -0.6877],\n",
      "        [-0.7920, -0.6032],\n",
      "        [-0.7303, -0.6573],\n",
      "        [-0.7081, -0.6784],\n",
      "        [-0.9722, -0.4752],\n",
      "        [-0.4343, -1.0433],\n",
      "        [-0.7117, -0.6749],\n",
      "        [-0.4425, -1.0284],\n",
      "        [-0.7305, -0.6572],\n",
      "        [-0.7980, -0.5983],\n",
      "        [-0.6951, -0.6912],\n",
      "        [-0.7597, -0.6308],\n",
      "        [-0.6704, -0.7164],\n",
      "        [-1.2954, -0.3199],\n",
      "        [-0.6114, -0.7822],\n",
      "        [-0.6244, -0.7670],\n",
      "        [-0.8166, -0.5833],\n",
      "        [-0.8301, -0.5727],\n",
      "        [-0.5701, -0.8335],\n",
      "        [-0.6091, -0.7849],\n",
      "        [-0.6681, -0.7189],\n",
      "        [-0.7942, -0.6013],\n",
      "        [-2.1507, -0.1237],\n",
      "        [-0.7310, -0.6567],\n",
      "        [-1.0113, -0.4521],\n",
      "        [-0.4858, -0.9550],\n",
      "        [-0.5966, -0.8000],\n",
      "        [-0.3730, -1.1668],\n",
      "        [-0.6561, -0.7316],\n",
      "        [-0.4256, -1.0596],\n",
      "        [-0.7592, -0.6312],\n",
      "        [-0.7855, -0.6086],\n",
      "        [-0.6823, -0.7041],\n",
      "        [-0.7116, -0.6750],\n",
      "        [-0.6629, -0.7243],\n",
      "        [-0.6386, -0.7508],\n",
      "        [-0.4155, -1.0789],\n",
      "        [-0.6170, -0.7756],\n",
      "        [-0.6111, -0.7825],\n",
      "        [-0.2166, -1.6359],\n",
      "        [-0.6538, -0.7341],\n",
      "        [-0.7786, -0.6145],\n",
      "        [-0.5426, -0.8704],\n",
      "        [-0.8072, -0.5908],\n",
      "        [-0.8772, -0.5377],\n",
      "        [-0.6884, -0.6979],\n",
      "        [-0.5658, -0.8391],\n",
      "        [-0.7574, -0.6327],\n",
      "        [-0.8245, -0.5771],\n",
      "        [-0.9273, -0.5035],\n",
      "        [-0.5947, -0.8024],\n",
      "        [-0.7330, -0.6549],\n",
      "        [-0.7764, -0.6163],\n",
      "        [-0.6812, -0.7053],\n",
      "        [-0.7569, -0.6332],\n",
      "        [-0.7423, -0.6463],\n",
      "        [-0.7185, -0.6684],\n",
      "        [-0.6565, -0.7312],\n",
      "        [-0.7613, -0.6294],\n",
      "        [-0.5611, -0.8453],\n",
      "        [-0.6110, -0.7827],\n",
      "        [-0.5949, -0.8021],\n",
      "        [-0.6517, -0.7363],\n",
      "        [-0.7267, -0.6607],\n",
      "        [-0.6604, -0.7270],\n",
      "        [-0.8997, -0.5221],\n",
      "        [-0.5570, -0.8507],\n",
      "        [-0.6991, -0.6872],\n",
      "        [-0.6283, -0.7625],\n",
      "        [-0.4654, -0.9885],\n",
      "        [-0.5990, -0.7970],\n",
      "        [-0.6161, -0.7766],\n",
      "        [-0.6447, -0.7441],\n",
      "        [-0.6598, -0.7276],\n",
      "        [-0.4663, -0.9870],\n",
      "        [-0.6614, -0.7260],\n",
      "        [-0.4788, -0.9664],\n",
      "        [-0.1042, -2.3130],\n",
      "        [-0.6468, -0.7418],\n",
      "        [-0.6762, -0.7104],\n",
      "        [-0.7241, -0.6631],\n",
      "        [-0.7069, -0.6796],\n",
      "        [-0.4465, -1.0214],\n",
      "        [-0.4725, -0.9767],\n",
      "        [-1.0041, -0.4563],\n",
      "        [-0.7201, -0.6669],\n",
      "        [-0.6198, -0.7723],\n",
      "        [-0.6956, -0.6907],\n",
      "        [-0.5413, -0.8722],\n",
      "        [-0.6874, -0.6990],\n",
      "        [-0.9727, -0.4749],\n",
      "        [-0.8056, -0.5921],\n",
      "        [-0.8163, -0.5835],\n",
      "        [-0.3895, -1.1313],\n",
      "        [-0.6752, -0.7115],\n",
      "        [-0.6358, -0.7540],\n",
      "        [-0.5292, -0.8894],\n",
      "        [-0.9702, -0.4764],\n",
      "        [-0.6547, -0.7331],\n",
      "        [-0.5913, -0.8065],\n",
      "        [-0.5873, -0.8115],\n",
      "        [-0.6212, -0.7707],\n",
      "        [-0.4732, -0.9755],\n",
      "        [-0.6763, -0.7103],\n",
      "        [-0.7984, -0.5979],\n",
      "        [-0.6329, -0.7573],\n",
      "        [-0.7229, -0.6643],\n",
      "        [-0.6964, -0.6900],\n",
      "        [-0.6713, -0.7154],\n",
      "        [-0.6797, -0.7068],\n",
      "        [-0.6269, -0.7641],\n",
      "        [-0.6985, -0.6878],\n",
      "        [-0.4596, -0.9984],\n",
      "        [-0.4530, -1.0099],\n",
      "        [-0.7432, -0.6455],\n",
      "        [-0.6893, -0.6970],\n",
      "        [-0.6738, -0.7129],\n",
      "        [-0.6181, -0.7743],\n",
      "        [-0.6701, -0.7168],\n",
      "        [-0.6177, -0.7748],\n",
      "        [-0.6846, -0.7018],\n",
      "        [-0.7961, -0.5998]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
      "        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 1, 0, 0, 0, 1])\n",
      "tensor([[-0.9873, -0.4661],\n",
      "        [-0.3625, -1.1904],\n",
      "        [-0.7012, -0.6852],\n",
      "        [-0.6828, -0.7036],\n",
      "        [-0.7045, -0.6819],\n",
      "        [-0.3204, -1.2941],\n",
      "        [-0.3372, -1.2508],\n",
      "        [-0.7124, -0.6742],\n",
      "        [-0.6429, -0.7460],\n",
      "        [-0.8407, -0.5646],\n",
      "        [-0.8652, -0.5464],\n",
      "        [-0.7782, -0.6148],\n",
      "        [-0.7474, -0.6416],\n",
      "        [-0.4415, -1.0303],\n",
      "        [-0.6365, -0.7532],\n",
      "        [-0.4740, -0.9742],\n",
      "        [-0.7789, -0.6142],\n",
      "        [-0.4534, -1.0092],\n",
      "        [-0.6798, -0.7067],\n",
      "        [-0.3041, -1.3386],\n",
      "        [-0.8497, -0.5578],\n",
      "        [-0.7960, -0.5999],\n",
      "        [-0.6877, -0.6987],\n",
      "        [-0.7133, -0.6734],\n",
      "        [-0.6185, -0.7738],\n",
      "        [-0.6263, -0.7648],\n",
      "        [-0.6671, -0.7199],\n",
      "        [-0.9551, -0.4857],\n",
      "        [-0.7196, -0.6674],\n",
      "        [-0.5076, -0.9212],\n",
      "        [-0.6981, -0.6883],\n",
      "        [-0.7713, -0.6207],\n",
      "        [-0.4168, -1.0764],\n",
      "        [-0.4298, -1.0516],\n",
      "        [-0.7308, -0.6569],\n",
      "        [-0.8042, -0.5932],\n",
      "        [-0.6870, -0.6993],\n",
      "        [-0.4918, -0.9456],\n",
      "        [-0.6521, -0.7360],\n",
      "        [-1.0412, -0.4355],\n",
      "        [-0.6741, -0.7125],\n",
      "        [-0.6669, -0.7201],\n",
      "        [-0.5607, -0.8459],\n",
      "        [-0.6571, -0.7306],\n",
      "        [-0.6463, -0.7423],\n",
      "        [-0.5841, -0.8156],\n",
      "        [-0.5926, -0.8049],\n",
      "        [-0.5177, -0.9061],\n",
      "        [-0.5251, -0.8953],\n",
      "        [-0.8004, -0.5963],\n",
      "        [-0.5726, -0.8302],\n",
      "        [-0.4380, -1.0365],\n",
      "        [-0.5971, -0.7994],\n",
      "        [-0.4402, -1.0326],\n",
      "        [-0.6789, -0.7076],\n",
      "        [-0.9257, -0.5046],\n",
      "        [-0.7762, -0.6165],\n",
      "        [-0.4429, -1.0277],\n",
      "        [-0.6376, -0.7520],\n",
      "        [-0.8008, -0.5959],\n",
      "        [-0.3105, -1.3208],\n",
      "        [-0.6142, -0.7789],\n",
      "        [-0.5456, -0.8663],\n",
      "        [-0.4888, -0.9503],\n",
      "        [-0.7452, -0.6436],\n",
      "        [-0.7356, -0.6524],\n",
      "        [-0.6235, -0.7680],\n",
      "        [-0.7464, -0.6426],\n",
      "        [-0.5152, -0.9098],\n",
      "        [-0.6611, -0.7262],\n",
      "        [-0.7257, -0.6616],\n",
      "        [-0.4790, -0.9659],\n",
      "        [-0.7264, -0.6609],\n",
      "        [-0.6953, -0.6910],\n",
      "        [-0.8290, -0.5736],\n",
      "        [-0.5743, -0.8281],\n",
      "        [-0.3127, -1.3149],\n",
      "        [-0.7398, -0.6486],\n",
      "        [-0.7012, -0.6851],\n",
      "        [-0.6570, -0.7307],\n",
      "        [-0.5980, -0.7983],\n",
      "        [-0.7960, -0.5999],\n",
      "        [-0.4047, -1.1002],\n",
      "        [-0.6000, -0.7958],\n",
      "        [-0.6991, -0.6872],\n",
      "        [-0.6598, -0.7276],\n",
      "        [-0.7784, -0.6146],\n",
      "        [-0.7460, -0.6430],\n",
      "        [-0.3683, -1.1775],\n",
      "        [-0.5018, -0.9300],\n",
      "        [-0.6716, -0.7152],\n",
      "        [-0.4835, -0.9587],\n",
      "        [-0.5516, -0.8581],\n",
      "        [-0.6655, -0.7216],\n",
      "        [-0.3720, -1.1691],\n",
      "        [-0.2791, -1.4126],\n",
      "        [-0.4500, -1.0151],\n",
      "        [-0.7323, -0.6555],\n",
      "        [-0.5503, -0.8598],\n",
      "        [-0.5804, -0.8202],\n",
      "        [-0.4242, -1.0621],\n",
      "        [-0.6910, -0.6953],\n",
      "        [-0.7118, -0.6749],\n",
      "        [-0.5826, -0.8175],\n",
      "        [-0.7048, -0.6817],\n",
      "        [-0.7423, -0.6463],\n",
      "        [-0.8310, -0.5720],\n",
      "        [-0.7072, -0.6793],\n",
      "        [-0.5321, -0.8853],\n",
      "        [-0.6947, -0.6916],\n",
      "        [-0.6762, -0.7104],\n",
      "        [-0.1881, -1.7632],\n",
      "        [-0.6878, -0.6985],\n",
      "        [-0.6274, -0.7635],\n",
      "        [-0.4641, -0.9907],\n",
      "        [-0.6783, -0.7082],\n",
      "        [-0.8274, -0.5748],\n",
      "        [-0.5133, -0.9125],\n",
      "        [-0.7767, -0.6160],\n",
      "        [-0.3315, -1.2653],\n",
      "        [-0.5151, -0.9099],\n",
      "        [-0.5684, -0.8358],\n",
      "        [-0.6689, -0.7180],\n",
      "        [-0.6540, -0.7338],\n",
      "        [-0.9247, -0.5052],\n",
      "        [-0.7144, -0.6723],\n",
      "        [-0.6250, -0.7663],\n",
      "        [-0.7460, -0.6429]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
      "        1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
      "        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 1, 0, 0])\n",
      "tensor([[-0.5871, -0.8117],\n",
      "        [-0.7883, -0.6063],\n",
      "        [-0.4260, -1.0589],\n",
      "        [-0.7156, -0.6712],\n",
      "        [-0.5756, -0.8264],\n",
      "        [-0.2034, -1.6924],\n",
      "        [-0.6320, -0.7583],\n",
      "        [-0.7471, -0.6420],\n",
      "        [-0.6411, -0.7480],\n",
      "        [-0.6460, -0.7426],\n",
      "        [-0.5230, -0.8983],\n",
      "        [-0.6669, -0.7201],\n",
      "        [-0.6655, -0.7216],\n",
      "        [-0.6738, -0.7129],\n",
      "        [-0.5187, -0.9046],\n",
      "        [-0.6148, -0.7782],\n",
      "        [-0.3323, -1.2633],\n",
      "        [-0.7072, -0.6792],\n",
      "        [-0.5723, -0.8307],\n",
      "        [-0.2737, -1.4294],\n",
      "        [-0.7561, -0.6339],\n",
      "        [-0.8819, -0.5344],\n",
      "        [-0.6272, -0.7638],\n",
      "        [-0.3034, -1.3405],\n",
      "        [-0.4204, -1.0694],\n",
      "        [-0.7344, -0.6535],\n",
      "        [-0.5486, -0.8622],\n",
      "        [-0.3889, -1.1326],\n",
      "        [-0.4688, -0.9829],\n",
      "        [-0.7290, -0.6586],\n",
      "        [-0.6699, -0.7170],\n",
      "        [-0.6874, -0.6989],\n",
      "        [-0.4109, -1.0878],\n",
      "        [-0.6950, -0.6913],\n",
      "        [-0.5234, -0.8978],\n",
      "        [-0.5818, -0.8184],\n",
      "        [-0.6879, -0.6984],\n",
      "        [-0.2180, -1.6304],\n",
      "        [-0.5178, -0.9059],\n",
      "        [-0.6471, -0.7414],\n",
      "        [-0.7319, -0.6559],\n",
      "        [-0.6665, -0.7206],\n",
      "        [-0.8768, -0.5380],\n",
      "        [-0.8205, -0.5802],\n",
      "        [-0.4075, -1.0945],\n",
      "        [-0.7137, -0.6730],\n",
      "        [-1.2819, -0.3251],\n",
      "        [-0.7173, -0.6696],\n",
      "        [-0.6788, -0.7077],\n",
      "        [-0.2372, -1.5551],\n",
      "        [-0.5653, -0.8398],\n",
      "        [-0.5377, -0.8774],\n",
      "        [-0.3007, -1.3483],\n",
      "        [-0.6605, -0.7269],\n",
      "        [-0.6216, -0.7702],\n",
      "        [-0.7456, -0.6433],\n",
      "        [-0.4673, -0.9854],\n",
      "        [-0.7449, -0.6439],\n",
      "        [-0.6741, -0.7125],\n",
      "        [-0.7237, -0.6635],\n",
      "        [-0.7532, -0.6365],\n",
      "        [-0.6429, -0.7461],\n",
      "        [-0.5486, -0.8622],\n",
      "        [-0.4720, -0.9776],\n",
      "        [-0.3853, -1.1401],\n",
      "        [-0.6377, -0.7519],\n",
      "        [-0.7801, -0.6132],\n",
      "        [-0.7455, -0.6434],\n",
      "        [-0.1498, -1.9723],\n",
      "        [-0.5160, -0.9085],\n",
      "        [-0.7126, -0.6741],\n",
      "        [-0.3360, -1.2539],\n",
      "        [-0.6275, -0.7635],\n",
      "        [-0.7545, -0.6353],\n",
      "        [-0.7510, -0.6384],\n",
      "        [-0.7977, -0.5985],\n",
      "        [-0.7046, -0.6818],\n",
      "        [-0.4248, -1.0610],\n",
      "        [-0.6664, -0.7207],\n",
      "        [-0.7590, -0.6314],\n",
      "        [-0.6007, -0.7950],\n",
      "        [-0.1019, -2.3340],\n",
      "        [-0.4074, -1.0948],\n",
      "        [-0.6130, -0.7803],\n",
      "        [-0.3387, -1.2473],\n",
      "        [-0.5881, -0.8106],\n",
      "        [-0.2525, -1.5000],\n",
      "        [-0.2905, -1.3778],\n",
      "        [-0.3878, -1.1350],\n",
      "        [-0.4688, -0.9829],\n",
      "        [-0.7886, -0.6060],\n",
      "        [-0.6685, -0.7185],\n",
      "        [-0.5899, -0.8083],\n",
      "        [-0.7040, -0.6825],\n",
      "        [-0.7205, -0.6665],\n",
      "        [-0.9474, -0.4906],\n",
      "        [-0.6000, -0.7959],\n",
      "        [-1.0883, -0.4106],\n",
      "        [-0.6773, -0.7093],\n",
      "        [-0.3346, -1.2575],\n",
      "        [-0.8098, -0.5887],\n",
      "        [-0.6421, -0.7470],\n",
      "        [-0.5596, -0.8473],\n",
      "        [-0.6609, -0.7265],\n",
      "        [-0.5351, -0.8810],\n",
      "        [-0.7064, -0.6800],\n",
      "        [-0.7051, -0.6813],\n",
      "        [-0.2387, -1.5494],\n",
      "        [-0.8873, -0.5307],\n",
      "        [-0.4202, -1.0698],\n",
      "        [-0.4079, -1.0937],\n",
      "        [-0.3810, -1.1493],\n",
      "        [-0.3533, -1.2119],\n",
      "        [-0.6184, -0.7740],\n",
      "        [-0.3065, -1.3318],\n",
      "        [-0.8324, -0.5709],\n",
      "        [-0.6739, -0.7128],\n",
      "        [-0.7428, -0.6458],\n",
      "        [-0.8018, -0.5951],\n",
      "        [-0.3845, -1.1418],\n",
      "        [-0.6507, -0.7375],\n",
      "        [-0.7124, -0.6743],\n",
      "        [-0.6659, -0.7212],\n",
      "        [-0.9388, -0.4961],\n",
      "        [-0.5053, -0.9247],\n",
      "        [-0.4399, -1.0331],\n",
      "        [-0.5210, -0.9012],\n",
      "        [-0.6609, -0.7265]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
      "        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
      "        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
      "        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 1, 1])\n",
      "tensor([[-0.3098, -1.3227],\n",
      "        [-0.6834, -0.7030],\n",
      "        [-0.7954, -0.6004],\n",
      "        [-0.5736, -0.8289],\n",
      "        [-0.1392, -2.0407],\n",
      "        [-0.6427, -0.7462],\n",
      "        [-0.4749, -0.9727],\n",
      "        [-0.5141, -0.9114],\n",
      "        [-0.3365, -1.2527],\n",
      "        [-0.4189, -1.0723],\n",
      "        [-0.3348, -1.2568],\n",
      "        [-0.6062, -0.7884],\n",
      "        [-0.4570, -1.0029],\n",
      "        [-0.7384, -0.6499],\n",
      "        [-0.5111, -0.9158],\n",
      "        [-0.5650, -0.8402],\n",
      "        [-0.8303, -0.5726],\n",
      "        [-0.6002, -0.7956],\n",
      "        [-0.2405, -1.5430],\n",
      "        [-0.5718, -0.8313],\n",
      "        [-0.5877, -0.8110],\n",
      "        [-0.7286, -0.6589],\n",
      "        [-0.3098, -1.3228],\n",
      "        [-0.3434, -1.2357],\n",
      "        [-0.8855, -0.5319],\n",
      "        [-0.1800, -1.8035],\n",
      "        [-1.0253, -0.4443],\n",
      "        [-0.7266, -0.6608],\n",
      "        [-0.6054, -0.7893],\n",
      "        [-0.4697, -0.9813],\n",
      "        [-0.5522, -0.8573],\n",
      "        [-0.6818, -0.7046],\n",
      "        [-0.5834, -0.8164],\n",
      "        [-0.8854, -0.5319],\n",
      "        [-0.7359, -0.6521],\n",
      "        [-0.6267, -0.7643],\n",
      "        [-0.3474, -1.2259],\n",
      "        [-0.2995, -1.3516],\n",
      "        [-0.4559, -1.0049],\n",
      "        [-0.7636, -0.6273],\n",
      "        [-0.5763, -0.8254],\n",
      "        [-0.5448, -0.8674],\n",
      "        [-0.6991, -0.6872],\n",
      "        [-0.7667, -0.6247],\n",
      "        [-0.3888, -1.1329],\n",
      "        [-0.7740, -0.6184],\n",
      "        [-0.7737, -0.6186],\n",
      "        [-0.7482, -0.6410],\n",
      "        [-0.7202, -0.6668],\n",
      "        [-0.4736, -0.9748],\n",
      "        [-0.8105, -0.5882],\n",
      "        [-0.5791, -0.8219],\n",
      "        [-0.6607, -0.7267],\n",
      "        [-0.2891, -1.3821],\n",
      "        [-0.7519, -0.6377],\n",
      "        [-0.4271, -1.0566],\n",
      "        [-0.6398, -0.7495],\n",
      "        [-0.7576, -0.6326],\n",
      "        [-0.7055, -0.6809],\n",
      "        [-0.5071, -0.9219],\n",
      "        [-0.8138, -0.5855],\n",
      "        [-0.3171, -1.3029],\n",
      "        [-0.3323, -1.2632],\n",
      "        [-0.7061, -0.6803],\n",
      "        [-0.6932, -0.6931],\n",
      "        [-0.5849, -0.8145],\n",
      "        [-0.4215, -1.0674],\n",
      "        [-0.7491, -0.6402],\n",
      "        [-0.5837, -0.8160],\n",
      "        [-0.6607, -0.7267],\n",
      "        [-0.7489, -0.6404],\n",
      "        [-0.6827, -0.7037],\n",
      "        [-0.6658, -0.7213],\n",
      "        [-0.1831, -1.7881],\n",
      "        [-0.5629, -0.8429],\n",
      "        [-0.4486, -1.0176],\n",
      "        [-0.9855, -0.4672],\n",
      "        [-0.6754, -0.7113],\n",
      "        [-0.5673, -0.8371],\n",
      "        [-0.8260, -0.5759],\n",
      "        [-0.3619, -1.1919],\n",
      "        [-0.7522, -0.6373],\n",
      "        [-0.6996, -0.6867],\n",
      "        [-0.7376, -0.6505],\n",
      "        [-0.7875, -0.6069],\n",
      "        [-0.6910, -0.6953],\n",
      "        [-0.2522, -1.5011],\n",
      "        [-0.7655, -0.6256],\n",
      "        [-0.5853, -0.8141],\n",
      "        [-1.1560, -0.3780],\n",
      "        [-0.8982, -0.5231],\n",
      "        [-0.6618, -0.7255],\n",
      "        [-0.5694, -0.8344],\n",
      "        [-0.6607, -0.7267],\n",
      "        [-0.7266, -0.6608],\n",
      "        [-0.6142, -0.7789],\n",
      "        [-0.2463, -1.5217],\n",
      "        [-0.6191, -0.7731],\n",
      "        [-0.6418, -0.7473],\n",
      "        [-0.8489, -0.5584],\n",
      "        [-0.6375, -0.7520],\n",
      "        [-0.5956, -0.8013],\n",
      "        [-0.4823, -0.9606],\n",
      "        [-0.9068, -0.5172],\n",
      "        [-0.6609, -0.7265],\n",
      "        [-0.5962, -0.8005],\n",
      "        [-0.5904, -0.8076],\n",
      "        [-0.1600, -1.9117],\n",
      "        [-0.4385, -1.0356],\n",
      "        [-0.8672, -0.5450],\n",
      "        [-0.4850, -0.9564],\n",
      "        [-0.6545, -0.7333],\n",
      "        [-0.4184, -1.0732],\n",
      "        [-0.9202, -0.5082],\n",
      "        [-0.5762, -0.8256],\n",
      "        [-1.6256, -0.2191],\n",
      "        [-0.6009, -0.7948],\n",
      "        [-0.7372, -0.6509],\n",
      "        [-0.6999, -0.6864],\n",
      "        [-0.6854, -0.7009],\n",
      "        [-0.2573, -1.4833],\n",
      "        [-0.7145, -0.6722],\n",
      "        [-0.6877, -0.6986],\n",
      "        [-0.2027, -1.6957],\n",
      "        [-0.6260, -0.7651],\n",
      "        [-0.7411, -0.6474],\n",
      "        [-0.2533, -1.4973],\n",
      "        [-0.7333, -0.6545]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
      "        1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0])\n",
      "tensor([[-0.2077, -1.6738],\n",
      "        [-0.6300, -0.7605],\n",
      "        [-0.1474, -1.9871],\n",
      "        [-0.7213, -0.6658],\n",
      "        [-0.5045, -0.9259],\n",
      "        [-0.2942, -1.3669],\n",
      "        [-0.4254, -1.0599],\n",
      "        [-0.7508, -0.6386],\n",
      "        [-0.6329, -0.7573],\n",
      "        [-0.6582, -0.7293],\n",
      "        [-0.3393, -1.2457],\n",
      "        [-0.6605, -0.7269],\n",
      "        [-0.5211, -0.9011],\n",
      "        [-0.2918, -1.3740],\n",
      "        [-0.2386, -1.5498],\n",
      "        [-0.8453, -0.5611],\n",
      "        [-0.4695, -0.9817],\n",
      "        [-0.3443, -1.2335],\n",
      "        [-0.7048, -0.6816],\n",
      "        [-0.6492, -0.7391],\n",
      "        [-0.5198, -0.9030],\n",
      "        [-0.3283, -1.2734],\n",
      "        [-0.6878, -0.6985],\n",
      "        [-0.5586, -0.8487],\n",
      "        [-0.6574, -0.7303],\n",
      "        [-0.5258, -0.8943],\n",
      "        [-0.2259, -1.5985],\n",
      "        [-0.1144, -2.2244],\n",
      "        [-0.7162, -0.6706],\n",
      "        [-0.9085, -0.5160],\n",
      "        [-0.5839, -0.8158],\n",
      "        [-0.6563, -0.7314],\n",
      "        [-0.3480, -1.2246],\n",
      "        [-0.6524, -0.7356],\n",
      "        [-0.7389, -0.6494],\n",
      "        [-0.3963, -1.1173],\n",
      "        [-0.4462, -1.0217],\n",
      "        [-0.6667, -0.7203],\n",
      "        [-0.6826, -0.7038],\n",
      "        [-0.3422, -1.2385],\n",
      "        [-0.6558, -0.7319],\n",
      "        [-0.6759, -0.7107],\n",
      "        [-0.7130, -0.6737],\n",
      "        [-0.7331, -0.6547],\n",
      "        [-0.4180, -1.0740],\n",
      "        [-0.4279, -1.0552],\n",
      "        [-0.7169, -0.6700],\n",
      "        [-0.3456, -1.2302],\n",
      "        [-0.6063, -0.7882],\n",
      "        [-0.6944, -0.6919],\n",
      "        [-0.4259, -1.0589],\n",
      "        [-0.7263, -0.6610],\n",
      "        [-0.6491, -0.7392],\n",
      "        [-0.4511, -1.0132],\n",
      "        [-0.7538, -0.6359],\n",
      "        [-0.6772, -0.7094],\n",
      "        [-0.6563, -0.7314],\n",
      "        [-0.2582, -1.4804],\n",
      "        [-0.5924, -0.8052],\n",
      "        [-0.3809, -1.1496],\n",
      "        [-0.2885, -1.3840],\n",
      "        [-0.5677, -0.8366],\n",
      "        [-0.7048, -0.6816],\n",
      "        [-0.5275, -0.8917],\n",
      "        [-0.5196, -0.9032],\n",
      "        [-0.6538, -0.7341],\n",
      "        [-0.3024, -1.3435],\n",
      "        [-0.7051, -0.6813],\n",
      "        [-0.6262, -0.7649],\n",
      "        [-0.4166, -1.0767],\n",
      "        [-0.5498, -0.8605],\n",
      "        [-0.3993, -1.1110],\n",
      "        [-1.0889, -0.4104],\n",
      "        [-0.4853, -0.9558],\n",
      "        [-0.6631, -0.7242],\n",
      "        [-0.4513, -1.0129],\n",
      "        [-0.7491, -0.6402],\n",
      "        [-0.5456, -0.8663],\n",
      "        [-0.7310, -0.6567],\n",
      "        [-0.5063, -0.9231],\n",
      "        [-0.7445, -0.6443],\n",
      "        [-0.3514, -1.2165],\n",
      "        [-0.5554, -0.8530],\n",
      "        [-0.6485, -0.7399],\n",
      "        [-0.6528, -0.7352],\n",
      "        [-0.6074, -0.7869],\n",
      "        [-0.3161, -1.3056],\n",
      "        [-1.0192, -0.4477],\n",
      "        [-0.7107, -0.6759],\n",
      "        [-0.6526, -0.7354],\n",
      "        [-0.7367, -0.6514],\n",
      "        [-0.5831, -0.8168],\n",
      "        [-0.8272, -0.5750],\n",
      "        [-0.5012, -0.9308],\n",
      "        [-0.5259, -0.8941],\n",
      "        [-0.6638, -0.7234],\n",
      "        [-0.6854, -0.7009],\n",
      "        [-0.7850, -0.6090],\n",
      "        [-0.7491, -0.6401],\n",
      "        [-0.4265, -1.0578],\n",
      "        [-0.3762, -1.1597],\n",
      "        [-0.7668, -0.6245],\n",
      "        [-0.2492, -1.5114],\n",
      "        [-0.6326, -0.7576],\n",
      "        [-0.7975, -0.5987],\n",
      "        [-0.8462, -0.5604],\n",
      "        [-0.6399, -0.7494],\n",
      "        [-0.3481, -1.2242],\n",
      "        [-0.8776, -0.5375],\n",
      "        [-0.4971, -0.9372],\n",
      "        [-0.6829, -0.7036],\n",
      "        [-0.7291, -0.6584],\n",
      "        [-0.6946, -0.6917],\n",
      "        [-0.6857, -0.7007],\n",
      "        [-0.6557, -0.7321],\n",
      "        [-0.5206, -0.9019],\n",
      "        [-0.6321, -0.7582],\n",
      "        [-0.7248, -0.6625],\n",
      "        [-0.7675, -0.6239],\n",
      "        [-0.5112, -0.9158],\n",
      "        [-0.7510, -0.6385],\n",
      "        [-0.6291, -0.7616],\n",
      "        [-0.6952, -0.6911],\n",
      "        [-0.1031, -2.3229],\n",
      "        [-0.2452, -1.5257],\n",
      "        [-0.2280, -1.5901],\n",
      "        [-0.6237, -0.7678],\n",
      "        [-0.6981, -0.6882]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0, 1, 1])\n",
      "tensor([[-0.6794, -0.7070],\n",
      "        [-0.6785, -0.7080],\n",
      "        [-0.6645, -0.7227],\n",
      "        [-0.6258, -0.7654],\n",
      "        [-0.6963, -0.6900],\n",
      "        [-0.5015, -0.9305],\n",
      "        [-0.7102, -0.6764],\n",
      "        [-0.7098, -0.6768],\n",
      "        [-0.7345, -0.6535],\n",
      "        [-0.1156, -2.2150],\n",
      "        [-0.4905, -0.9476],\n",
      "        [-0.6746, -0.7120],\n",
      "        [-0.7455, -0.6434],\n",
      "        [-0.0748, -2.6296],\n",
      "        [-0.1251, -2.1406],\n",
      "        [-0.4533, -1.0093],\n",
      "        [-1.1313, -0.3895],\n",
      "        [-0.4131, -1.0835],\n",
      "        [-0.5953, -0.8016],\n",
      "        [-0.2974, -1.3576],\n",
      "        [-0.8003, -0.5964],\n",
      "        [-0.2690, -1.4445],\n",
      "        [-0.6507, -0.7375],\n",
      "        [-0.7274, -0.6600],\n",
      "        [-0.6022, -0.7932],\n",
      "        [-0.6214, -0.7705],\n",
      "        [-0.6127, -0.7806],\n",
      "        [-0.7967, -0.5994],\n",
      "        [-0.5081, -0.9204],\n",
      "        [-0.5253, -0.8949],\n",
      "        [-0.6477, -0.7408],\n",
      "        [-0.7561, -0.6339],\n",
      "        [-0.6749, -0.7118],\n",
      "        [-0.3830, -1.1451],\n",
      "        [-0.2303, -1.5814],\n",
      "        [-0.6591, -0.7284],\n",
      "        [-0.4841, -0.9578],\n",
      "        [-0.5658, -0.8392],\n",
      "        [-0.4413, -1.0307],\n",
      "        [-0.7425, -0.6461],\n",
      "        [-0.5555, -0.8528],\n",
      "        [-0.7122, -0.6744],\n",
      "        [-0.6201, -0.7719],\n",
      "        [-0.8305, -0.5724],\n",
      "        [-0.3291, -1.2715],\n",
      "        [-0.3622, -1.1912],\n",
      "        [-0.1831, -1.7876],\n",
      "        [-0.6381, -0.7514],\n",
      "        [-0.3074, -1.3293],\n",
      "        [-0.4893, -0.9495],\n",
      "        [-0.6172, -0.7754],\n",
      "        [-0.6069, -0.7875],\n",
      "        [-0.6610, -0.7264],\n",
      "        [-0.8477, -0.5593],\n",
      "        [-0.7026, -0.6837],\n",
      "        [-0.8178, -0.5823],\n",
      "        [-0.4545, -1.0072],\n",
      "        [-0.7092, -0.6773],\n",
      "        [-0.4971, -0.9372],\n",
      "        [-0.6975, -0.6888],\n",
      "        [-0.4071, -1.0954],\n",
      "        [-0.5830, -0.8169],\n",
      "        [-0.4873, -0.9526],\n",
      "        [-0.7005, -0.6859],\n",
      "        [-0.5918, -0.8059],\n",
      "        [-0.3738, -1.1652],\n",
      "        [-0.6271, -0.7638],\n",
      "        [-0.6752, -0.7114],\n",
      "        [-0.6287, -0.7620],\n",
      "        [-0.5084, -0.9199],\n",
      "        [-0.5684, -0.8357],\n",
      "        [-0.5908, -0.8071],\n",
      "        [-0.6835, -0.7029],\n",
      "        [-0.7126, -0.6741],\n",
      "        [-0.2408, -1.5416],\n",
      "        [-0.4980, -0.9359],\n",
      "        [-0.5325, -0.8846],\n",
      "        [-0.3781, -1.1558],\n",
      "        [-0.7228, -0.6643],\n",
      "        [-0.4979, -0.9360],\n",
      "        [-0.5813, -0.8192],\n",
      "        [-0.4965, -0.9382],\n",
      "        [-0.2269, -1.5944],\n",
      "        [-0.3114, -1.3182],\n",
      "        [-0.4601, -0.9975],\n",
      "        [-0.9439, -0.4928],\n",
      "        [-0.3557, -1.2062],\n",
      "        [-0.4905, -0.9476],\n",
      "        [-0.5254, -0.8948],\n",
      "        [-0.5325, -0.8847],\n",
      "        [-0.0710, -2.6799],\n",
      "        [-0.6162, -0.7765],\n",
      "        [-0.7461, -0.6429],\n",
      "        [-0.7800, -0.6132],\n",
      "        [-0.5922, -0.8054],\n",
      "        [-0.6359, -0.7539],\n",
      "        [-0.6496, -0.7387],\n",
      "        [-0.7229, -0.6642],\n",
      "        [-0.3961, -1.1177],\n",
      "        [-0.6925, -0.6938],\n",
      "        [-1.0022, -0.4574],\n",
      "        [-0.5923, -0.8053],\n",
      "        [-0.7025, -0.6838],\n",
      "        [-0.5640, -0.8415],\n",
      "        [-0.7592, -0.6311],\n",
      "        [-0.5718, -0.8313],\n",
      "        [-0.4497, -1.0156],\n",
      "        [-0.6689, -0.7180],\n",
      "        [-0.2003, -1.7065],\n",
      "        [-0.4920, -0.9452],\n",
      "        [-0.5940, -0.8032],\n",
      "        [-0.0952, -2.3985],\n",
      "        [-0.6177, -0.7747],\n",
      "        [-0.8691, -0.5436],\n",
      "        [-0.8560, -0.5531],\n",
      "        [-0.7141, -0.6726],\n",
      "        [-0.8542, -0.5545],\n",
      "        [-1.0123, -0.4516],\n",
      "        [-0.4571, -1.0027],\n",
      "        [-0.5882, -0.8104],\n",
      "        [-2.0936, -0.1315],\n",
      "        [-0.6942, -0.6921],\n",
      "        [-0.6366, -0.7531],\n",
      "        [-0.5737, -0.8288],\n",
      "        [-0.5761, -0.8258],\n",
      "        [-0.9479, -0.4903],\n",
      "        [-0.2777, -1.4168],\n",
      "        [-0.1808, -1.7997]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
      "        1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
      "        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
      "        0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,\n",
      "        1, 0, 1, 0, 0, 1, 0, 0])\n",
      "tensor([[-0.6072, -0.7871],\n",
      "        [-0.5710, -0.8324],\n",
      "        [-0.7036, -0.6828],\n",
      "        [-0.6942, -0.6921],\n",
      "        [-0.4530, -1.0098],\n",
      "        [-0.6366, -0.7531],\n",
      "        [-0.6951, -0.6912],\n",
      "        [-0.5462, -0.8655],\n",
      "        [-0.7755, -0.6171],\n",
      "        [-0.8723, -0.5413],\n",
      "        [-0.6268, -0.7642],\n",
      "        [-0.2809, -1.4070],\n",
      "        [-0.6271, -0.7639],\n",
      "        [-0.5755, -0.8264],\n",
      "        [-0.7121, -0.6746],\n",
      "        [-0.3870, -1.1366],\n",
      "        [-0.4185, -1.0731],\n",
      "        [-0.5140, -0.9116],\n",
      "        [-0.5371, -0.8782],\n",
      "        [-0.8294, -0.5733],\n",
      "        [-0.1816, -1.7954],\n",
      "        [-0.5746, -0.8277],\n",
      "        [-0.6614, -0.7259],\n",
      "        [-0.8826, -0.5339],\n",
      "        [-0.7659, -0.6253],\n",
      "        [-0.7404, -0.6480],\n",
      "        [-0.4991, -0.9342],\n",
      "        [-0.2963, -1.3607],\n",
      "        [-0.6600, -0.7274],\n",
      "        [-0.7223, -0.6648],\n",
      "        [-0.6803, -0.7062],\n",
      "        [-0.6980, -0.6883],\n",
      "        [-0.5550, -0.8535],\n",
      "        [-0.7682, -0.6233],\n",
      "        [-0.5634, -0.8422],\n",
      "        [-0.7080, -0.6785],\n",
      "        [-0.6751, -0.7115],\n",
      "        [-0.3420, -1.2390],\n",
      "        [-0.6707, -0.7161],\n",
      "        [-0.6885, -0.6978],\n",
      "        [-0.5409, -0.8728],\n",
      "        [-0.5165, -0.9079],\n",
      "        [-0.5253, -0.8949],\n",
      "        [-0.1864, -1.7717],\n",
      "        [-0.8687, -0.5438],\n",
      "        [-0.3378, -1.2495],\n",
      "        [-0.8153, -0.5843],\n",
      "        [-0.4707, -0.9797],\n",
      "        [-0.8625, -0.5484],\n",
      "        [-0.6271, -0.7638],\n",
      "        [-0.3149, -1.3089],\n",
      "        [-0.6644, -0.7227],\n",
      "        [-0.5113, -0.9155],\n",
      "        [-0.0736, -2.6451],\n",
      "        [-0.6788, -0.7078],\n",
      "        [-0.8207, -0.5801],\n",
      "        [-0.7531, -0.6366],\n",
      "        [-0.8027, -0.5944],\n",
      "        [-0.7431, -0.6456],\n",
      "        [-0.6616, -0.7258],\n",
      "        [-0.6774, -0.7091],\n",
      "        [-0.6147, -0.7783],\n",
      "        [-0.5144, -0.9110],\n",
      "        [-0.6775, -0.7090],\n",
      "        [-0.7134, -0.6733],\n",
      "        [-0.1874, -1.7666],\n",
      "        [-0.8084, -0.5898],\n",
      "        [-0.7924, -0.6029],\n",
      "        [-0.6204, -0.7717],\n",
      "        [-0.7997, -0.5969],\n",
      "        [-0.4982, -0.9356],\n",
      "        [-0.4001, -1.1094],\n",
      "        [-0.6096, -0.7843],\n",
      "        [-0.2623, -1.4665],\n",
      "        [-0.2408, -1.5416],\n",
      "        [-0.5448, -0.8673],\n",
      "        [-0.6563, -0.7314],\n",
      "        [-0.4044, -1.1007],\n",
      "        [-0.7339, -0.6540],\n",
      "        [-0.5003, -0.9323],\n",
      "        [-0.5245, -0.8962],\n",
      "        [-0.6201, -0.7719],\n",
      "        [-0.7071, -0.6794],\n",
      "        [-0.5586, -0.8487],\n",
      "        [-0.4713, -0.9786],\n",
      "        [-0.5218, -0.9001],\n",
      "        [-0.5803, -0.8204],\n",
      "        [-0.4254, -1.0598],\n",
      "        [-0.8196, -0.5809],\n",
      "        [-0.6495, -0.7388],\n",
      "        [-0.7859, -0.6083],\n",
      "        [-0.6191, -0.7732],\n",
      "        [-0.1493, -1.9758],\n",
      "        [-0.6566, -0.7311],\n",
      "        [-0.6342, -0.7558],\n",
      "        [-0.0979, -2.3719],\n",
      "        [-0.4099, -1.0898],\n",
      "        [-0.6478, -0.7407],\n",
      "        [-0.8171, -0.5828],\n",
      "        [-0.6016, -0.7939],\n",
      "        [-0.3677, -1.1787],\n",
      "        [-0.4530, -1.0098],\n",
      "        [-0.4774, -0.9686],\n",
      "        [-0.5999, -0.7960],\n",
      "        [-0.5938, -0.8035],\n",
      "        [-0.4625, -0.9934],\n",
      "        [-0.7460, -0.6430],\n",
      "        [-0.7099, -0.6766],\n",
      "        [-0.2905, -1.3780],\n",
      "        [-0.6839, -0.7025],\n",
      "        [-0.6444, -0.7444],\n",
      "        [-0.5094, -0.9184],\n",
      "        [-0.8023, -0.5947],\n",
      "        [-0.6380, -0.7515],\n",
      "        [-0.7111, -0.6755],\n",
      "        [-0.1004, -2.3483],\n",
      "        [-0.8428, -0.5630],\n",
      "        [-0.5533, -0.8558],\n",
      "        [-0.6688, -0.7181],\n",
      "        [-0.8418, -0.5638],\n",
      "        [-0.4302, -1.0509],\n",
      "        [-0.5725, -0.8303],\n",
      "        [-0.6298, -0.7608],\n",
      "        [-0.4786, -0.9666],\n",
      "        [-0.5684, -0.8357],\n",
      "        [-0.3782, -1.1555],\n",
      "        [-0.4956, -0.9396],\n",
      "        [-0.8563, -0.5529]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
      "        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "        1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
      "        0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
      "        1, 1, 1, 0, 1, 1, 0, 1])\n",
      "tensor([[-0.7756, -0.6169],\n",
      "        [-0.6308, -0.7597],\n",
      "        [-0.7316, -0.6562],\n",
      "        [-0.7459, -0.6431],\n",
      "        [-0.6753, -0.7113],\n",
      "        [-0.6428, -0.7461],\n",
      "        [-0.6131, -0.7801],\n",
      "        [-0.3153, -1.3076],\n",
      "        [-0.5061, -0.9235],\n",
      "        [-0.6518, -0.7363],\n",
      "        [-0.5314, -0.8862],\n",
      "        [-0.5796, -0.8212],\n",
      "        [-0.6777, -0.7088],\n",
      "        [-0.7538, -0.6359],\n",
      "        [-0.6018, -0.7937],\n",
      "        [-0.7966, -0.5994],\n",
      "        [-0.2783, -1.4149],\n",
      "        [-0.4256, -1.0596],\n",
      "        [-0.4192, -1.0718],\n",
      "        [-0.6048, -0.7901],\n",
      "        [-0.6113, -0.7823],\n",
      "        [-0.8265, -0.5755],\n",
      "        [-0.6651, -0.7220],\n",
      "        [-0.4473, -1.0199],\n",
      "        [-0.5107, -0.9165],\n",
      "        [-0.6694, -0.7174],\n",
      "        [-0.8544, -0.5543],\n",
      "        [-0.7609, -0.6297],\n",
      "        [-0.6666, -0.7204],\n",
      "        [-0.7115, -0.6751],\n",
      "        [-0.6396, -0.7498],\n",
      "        [-0.3040, -1.3389],\n",
      "        [-0.7907, -0.6042],\n",
      "        [-0.6498, -0.7385],\n",
      "        [-0.7882, -0.6063],\n",
      "        [-0.6833, -0.7031],\n",
      "        [-0.4163, -1.0773],\n",
      "        [-0.6095, -0.7844],\n",
      "        [-0.7478, -0.6413],\n",
      "        [-0.6382, -0.7513],\n",
      "        [-0.6795, -0.7070],\n",
      "        [-0.5163, -0.9081],\n",
      "        [-0.6572, -0.7304],\n",
      "        [-0.5394, -0.8748],\n",
      "        [-0.6547, -0.7331],\n",
      "        [-0.2870, -1.3884],\n",
      "        [-0.2735, -1.4300],\n",
      "        [-0.6284, -0.7623],\n",
      "        [-0.7514, -0.6381],\n",
      "        [-0.5395, -0.8748],\n",
      "        [-0.7151, -0.6716],\n",
      "        [-0.5311, -0.8867],\n",
      "        [-0.7057, -0.6808],\n",
      "        [-0.2995, -1.3517],\n",
      "        [-0.1170, -2.2034],\n",
      "        [-0.1534, -1.9507],\n",
      "        [-0.5160, -0.9085],\n",
      "        [-0.8345, -0.5693],\n",
      "        [-0.6054, -0.7893],\n",
      "        [-0.3724, -1.1681],\n",
      "        [-0.3876, -1.1354],\n",
      "        [-0.5975, -0.7989],\n",
      "        [-0.6608, -0.7266],\n",
      "        [-0.7491, -0.6402],\n",
      "        [-0.6791, -0.7074],\n",
      "        [-0.7111, -0.6755],\n",
      "        [-0.6961, -0.6902],\n",
      "        [-0.5902, -0.8079],\n",
      "        [-0.0552, -2.9234],\n",
      "        [-0.6887, -0.6976],\n",
      "        [-0.0989, -2.3626],\n",
      "        [-0.6167, -0.7760],\n",
      "        [-0.7448, -0.6440],\n",
      "        [-0.9138, -0.5125],\n",
      "        [-0.8440, -0.5621],\n",
      "        [-0.6114, -0.7822],\n",
      "        [-0.8102, -0.5884],\n",
      "        [-0.6594, -0.7281],\n",
      "        [-0.7389, -0.6494],\n",
      "        [-0.3599, -1.1965],\n",
      "        [-0.5512, -0.8586],\n",
      "        [-0.6490, -0.7393],\n",
      "        [-0.6475, -0.7410],\n",
      "        [-0.2632, -1.4635],\n",
      "        [-0.8460, -0.5606],\n",
      "        [-0.7176, -0.6693],\n",
      "        [-0.4637, -0.9914],\n",
      "        [-0.5458, -0.8660],\n",
      "        [-0.7578, -0.6324],\n",
      "        [-0.9327, -0.5001],\n",
      "        [-0.5366, -0.8789],\n",
      "        [-0.6573, -0.7304],\n",
      "        [-0.6096, -0.7843],\n",
      "        [-0.6264, -0.7647],\n",
      "        [-0.6417, -0.7474],\n",
      "        [-0.5634, -0.8423],\n",
      "        [-0.7409, -0.6476],\n",
      "        [-1.0014, -0.4579],\n",
      "        [-0.4911, -0.9466],\n",
      "        [-0.6847, -0.7017],\n",
      "        [-0.8101, -0.5885],\n",
      "        [-0.8397, -0.5654],\n",
      "        [-0.5938, -0.8035],\n",
      "        [-0.2961, -1.3616],\n",
      "        [-0.3595, -1.1973],\n",
      "        [-0.6388, -0.7506],\n",
      "        [-0.5304, -0.8876],\n",
      "        [-0.6736, -0.7131],\n",
      "        [-0.5936, -0.8037],\n",
      "        [-0.4556, -1.0053],\n",
      "        [-1.0476, -0.4320],\n",
      "        [-0.3738, -1.1651],\n",
      "        [-0.2366, -1.5575],\n",
      "        [-0.6754, -0.7112],\n",
      "        [-0.4828, -0.9599],\n",
      "        [-0.7356, -0.6524],\n",
      "        [-0.6153, -0.7776],\n",
      "        [-0.3983, -1.1131],\n",
      "        [-0.5308, -0.8870],\n",
      "        [-0.1408, -2.0302],\n",
      "        [-0.4249, -1.0609],\n",
      "        [-0.6987, -0.6876],\n",
      "        [-0.6400, -0.7493],\n",
      "        [-0.2816, -1.4048],\n",
      "        [-0.2992, -1.3526],\n",
      "        [-0.6432, -0.7457],\n",
      "        [-0.7726, -0.6195],\n",
      "        [-0.2020, -1.6990]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
      "        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "        1, 1, 0, 0, 1, 0, 0, 0])\n",
      "tensor([[-0.4623, -0.9938],\n",
      "        [-0.6476, -0.7409],\n",
      "        [-0.4926, -0.9442],\n",
      "        [-0.7941, -0.6014],\n",
      "        [-0.7349, -0.6531],\n",
      "        [-0.4655, -0.9884],\n",
      "        [-0.5558, -0.8524],\n",
      "        [-0.3956, -1.1186],\n",
      "        [-0.5771, -0.8244],\n",
      "        [-0.1008, -2.3448],\n",
      "        [-0.3529, -1.2127],\n",
      "        [-0.4834, -0.9589],\n",
      "        [-0.5390, -0.8755],\n",
      "        [-0.8113, -0.5875],\n",
      "        [-0.5374, -0.8777],\n",
      "        [-0.6824, -0.7040],\n",
      "        [-0.6544, -0.7335],\n",
      "        [-0.6170, -0.7756],\n",
      "        [-0.2316, -1.5761],\n",
      "        [-0.5973, -0.7992],\n",
      "        [-0.5718, -0.8313],\n",
      "        [-0.2102, -1.6630],\n",
      "        [-0.6982, -0.6881],\n",
      "        [-0.5806, -0.8200],\n",
      "        [-0.2093, -1.6670],\n",
      "        [-0.2706, -1.4393],\n",
      "        [-0.6470, -0.7415],\n",
      "        [-0.1962, -1.7252],\n",
      "        [-0.9716, -0.4756],\n",
      "        [-0.2323, -1.5735],\n",
      "        [-0.6523, -0.7357],\n",
      "        [-0.7639, -0.6271],\n",
      "        [-0.0281, -3.5875],\n",
      "        [-0.7290, -0.6585],\n",
      "        [-0.2127, -1.6524],\n",
      "        [-0.9367, -0.4974],\n",
      "        [-0.7288, -0.6587],\n",
      "        [-0.7259, -0.6614],\n",
      "        [-0.7554, -0.6345],\n",
      "        [-0.5820, -0.8182],\n",
      "        [-0.0920, -2.4311],\n",
      "        [-0.4881, -0.9514],\n",
      "        [-0.8514, -0.5566],\n",
      "        [-0.6765, -0.7101],\n",
      "        [-0.3009, -1.3477],\n",
      "        [-0.4923, -0.9447],\n",
      "        [-0.6828, -0.7036],\n",
      "        [-0.5447, -0.8675],\n",
      "        [-0.7027, -0.6837],\n",
      "        [-0.5923, -0.8053],\n",
      "        [-0.5826, -0.8174],\n",
      "        [-0.9727, -0.4749],\n",
      "        [-0.8799, -0.5359],\n",
      "        [-0.2406, -1.5424],\n",
      "        [-0.6572, -0.7304],\n",
      "        [-0.2446, -1.5280],\n",
      "        [-0.4944, -0.9414],\n",
      "        [-0.7583, -0.6320],\n",
      "        [-0.4315, -1.0485],\n",
      "        [-0.6521, -0.7359],\n",
      "        [-0.5158, -0.9089],\n",
      "        [-0.6765, -0.7101],\n",
      "        [-0.5011, -0.9310],\n",
      "        [-0.5148, -0.9104],\n",
      "        [-0.6369, -0.7528],\n",
      "        [-0.2312, -1.5780],\n",
      "        [-0.4850, -0.9563],\n",
      "        [-0.5631, -0.8427],\n",
      "        [-0.5050, -0.9250],\n",
      "        [-0.1589, -1.9177],\n",
      "        [-0.6707, -0.7162],\n",
      "        [-0.7031, -0.6833],\n",
      "        [-0.2097, -1.6650],\n",
      "        [-0.5530, -0.8562],\n",
      "        [-0.6965, -0.6898],\n",
      "        [-0.7198, -0.6672],\n",
      "        [-0.4729, -0.9760],\n",
      "        [-0.6854, -0.7009],\n",
      "        [-0.6678, -0.7191],\n",
      "        [-0.7309, -0.6567],\n",
      "        [-0.6468, -0.7417],\n",
      "        [-0.6453, -0.7434],\n",
      "        [-0.5845, -0.8151],\n",
      "        [-0.7210, -0.6660],\n",
      "        [-0.8206, -0.5801],\n",
      "        [-0.5367, -0.8787],\n",
      "        [-0.4975, -0.9367],\n",
      "        [-0.1829, -1.7889],\n",
      "        [-0.3503, -1.2189],\n",
      "        [-0.3800, -1.1516],\n",
      "        [-0.7850, -0.6090],\n",
      "        [-0.2725, -1.4334],\n",
      "        [-0.6903, -0.6960],\n",
      "        [-0.6517, -0.7364],\n",
      "        [-0.3884, -1.1337],\n",
      "        [-0.2043, -1.6887],\n",
      "        [-0.8671, -0.5450],\n",
      "        [-0.7176, -0.6693],\n",
      "        [-0.6865, -0.6999],\n",
      "        [-0.5178, -0.9059],\n",
      "        [-0.7161, -0.6707],\n",
      "        [-0.5307, -0.8873],\n",
      "        [-0.2053, -1.6844],\n",
      "        [-0.7811, -0.6123],\n",
      "        [-0.6257, -0.7655],\n",
      "        [-0.3464, -1.2283],\n",
      "        [-0.5923, -0.8053],\n",
      "        [-0.8188, -0.5815],\n",
      "        [-0.7045, -0.6820],\n",
      "        [-0.1280, -2.1193],\n",
      "        [-0.7301, -0.6575],\n",
      "        [-0.5177, -0.9061],\n",
      "        [-0.6002, -0.7956],\n",
      "        [-0.6297, -0.7609],\n",
      "        [-0.3914, -1.1274],\n",
      "        [-0.5100, -0.9175],\n",
      "        [-0.7035, -0.6829],\n",
      "        [-0.6800, -0.7065],\n",
      "        [-0.6232, -0.7684],\n",
      "        [-0.2435, -1.5319],\n",
      "        [-0.8639, -0.5474],\n",
      "        [-0.2066, -1.6785],\n",
      "        [-0.3174, -1.3021],\n",
      "        [-0.4239, -1.0627],\n",
      "        [-0.5906, -0.8074],\n",
      "        [-0.7158, -0.6710],\n",
      "        [-0.9050, -0.5184],\n",
      "        [-0.3177, -1.3013]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
      "        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 1, 1, 0])\n",
      "tensor([[-0.7036, -0.6828],\n",
      "        [-0.6743, -0.7123],\n",
      "        [-0.6598, -0.7277],\n",
      "        [-0.1563, -1.9332],\n",
      "        [-0.6865, -0.6999],\n",
      "        [-0.6995, -0.6869],\n",
      "        [-0.3152, -1.3081],\n",
      "        [-0.2242, -1.6054],\n",
      "        [-0.6173, -0.7752],\n",
      "        [-0.2294, -1.5850],\n",
      "        [-0.5531, -0.8561],\n",
      "        [-0.6140, -0.7791],\n",
      "        [-0.5046, -0.9257],\n",
      "        [-0.5319, -0.8855],\n",
      "        [-0.4601, -0.9976],\n",
      "        [-0.1268, -2.1279],\n",
      "        [-0.1842, -1.7824],\n",
      "        [-0.6868, -0.6996],\n",
      "        [-0.2989, -1.3534],\n",
      "        [-0.0264, -3.6490],\n",
      "        [-0.1015, -2.3380],\n",
      "        [-0.2898, -1.3799],\n",
      "        [-0.3502, -1.2191],\n",
      "        [-0.7624, -0.6283],\n",
      "        [-0.7493, -0.6400],\n",
      "        [-0.8271, -0.5751],\n",
      "        [-0.3335, -1.2603],\n",
      "        [-0.5003, -0.9323],\n",
      "        [-0.6498, -0.7385],\n",
      "        [-0.6273, -0.7637],\n",
      "        [-0.5974, -0.7990],\n",
      "        [-0.6918, -0.6945],\n",
      "        [-0.2913, -1.3756],\n",
      "        [-0.7372, -0.6509],\n",
      "        [-0.5493, -0.8612],\n",
      "        [-0.4953, -0.9400],\n",
      "        [-2.0695, -0.1350],\n",
      "        [-0.2490, -1.5122],\n",
      "        [-0.5975, -0.7989],\n",
      "        [-0.5001, -0.9326],\n",
      "        [-0.3427, -1.2373],\n",
      "        [-0.4193, -1.0716],\n",
      "        [-0.6130, -0.7803],\n",
      "        [-0.7028, -0.6836],\n",
      "        [-0.4855, -0.9556],\n",
      "        [-0.6016, -0.7939],\n",
      "        [-0.6530, -0.7349],\n",
      "        [-0.3931, -1.1238],\n",
      "        [-0.6636, -0.7236],\n",
      "        [-0.5263, -0.8935],\n",
      "        [-0.7708, -0.6211],\n",
      "        [-0.6078, -0.7865],\n",
      "        [-0.5780, -0.8233],\n",
      "        [-0.7465, -0.6425],\n",
      "        [-0.7190, -0.6680],\n",
      "        [-0.3198, -1.2957],\n",
      "        [-0.1629, -1.8951],\n",
      "        [-0.5010, -0.9312],\n",
      "        [-0.2609, -1.4714],\n",
      "        [-0.4956, -0.9396],\n",
      "        [-0.3004, -1.3491],\n",
      "        [-0.5102, -0.9172],\n",
      "        [-0.5038, -0.9269],\n",
      "        [-0.7414, -0.6471],\n",
      "        [-0.4813, -0.9622],\n",
      "        [-0.5554, -0.8530],\n",
      "        [-0.3400, -1.2440],\n",
      "        [-0.6207, -0.7712],\n",
      "        [-0.6461, -0.7426],\n",
      "        [-0.2307, -1.5797],\n",
      "        [-1.0302, -0.4415],\n",
      "        [-0.4786, -0.9667],\n",
      "        [-0.6978, -0.6885],\n",
      "        [-0.6401, -0.7492],\n",
      "        [-0.1297, -2.1064],\n",
      "        [-0.8021, -0.5949],\n",
      "        [-0.7976, -0.5986],\n",
      "        [-0.5842, -0.8154],\n",
      "        [-0.6102, -0.7836],\n",
      "        [-0.4852, -0.9559],\n",
      "        [-1.0003, -0.4585],\n",
      "        [-0.5110, -0.9160],\n",
      "        [-0.6567, -0.7310],\n",
      "        [-0.3306, -1.2677],\n",
      "        [-0.9177, -0.5099],\n",
      "        [-0.5634, -0.8422],\n",
      "        [-0.2272, -1.5933],\n",
      "        [-0.7167, -0.6702],\n",
      "        [-0.3473, -1.2261],\n",
      "        [-0.6707, -0.7161],\n",
      "        [-0.7415, -0.6470],\n",
      "        [-0.7211, -0.6660],\n",
      "        [-0.2984, -1.3549],\n",
      "        [-0.8033, -0.5939],\n",
      "        [-0.4153, -1.0793],\n",
      "        [-0.4847, -0.9568],\n",
      "        [-0.5589, -0.8482],\n",
      "        [-0.6933, -0.6930],\n",
      "        [-0.6971, -0.6892],\n",
      "        [-0.3138, -1.3117],\n",
      "        [-0.5117, -0.9150],\n",
      "        [-0.6388, -0.7506],\n",
      "        [-0.7205, -0.6665],\n",
      "        [-0.5634, -0.8423],\n",
      "        [-0.7132, -0.6735],\n",
      "        [-0.6726, -0.7141],\n",
      "        [-0.6632, -0.7240],\n",
      "        [-1.6526, -0.2126],\n",
      "        [-0.7497, -0.6397],\n",
      "        [-0.1594, -1.9152],\n",
      "        [-0.6499, -0.7384],\n",
      "        [-0.5894, -0.8089],\n",
      "        [-0.3873, -1.1359],\n",
      "        [-0.5569, -0.8509],\n",
      "        [-0.6208, -0.7711],\n",
      "        [-0.3376, -1.2500],\n",
      "        [-0.3041, -1.3385],\n",
      "        [-0.4974, -0.9367],\n",
      "        [-0.5624, -0.8437],\n",
      "        [-0.7764, -0.6163],\n",
      "        [-0.5511, -0.8588],\n",
      "        [-0.7474, -0.6416],\n",
      "        [-0.4879, -0.9517],\n",
      "        [-0.6569, -0.7307],\n",
      "        [-0.6658, -0.7213],\n",
      "        [-0.7404, -0.6480],\n",
      "        [-0.6614, -0.7260],\n",
      "        [-0.6868, -0.6995]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0, 1, 1])\n",
      "tensor([[-0.5757, -0.8263],\n",
      "        [-0.6791, -0.7074],\n",
      "        [-0.6622, -0.7251],\n",
      "        [-0.4976, -0.9364],\n",
      "        [-0.3494, -1.2211],\n",
      "        [-0.6773, -0.7093],\n",
      "        [-0.6816, -0.7049],\n",
      "        [-0.6968, -0.6895],\n",
      "        [-0.6169, -0.7757],\n",
      "        [-0.6755, -0.7111],\n",
      "        [-0.7382, -0.6501],\n",
      "        [-0.3032, -1.3411],\n",
      "        [-0.6544, -0.7335],\n",
      "        [-0.5607, -0.8459],\n",
      "        [-0.7334, -0.6545],\n",
      "        [-0.4446, -1.0247],\n",
      "        [-0.5405, -0.8734],\n",
      "        [-0.1381, -2.0477],\n",
      "        [-0.3628, -1.1898],\n",
      "        [-0.4036, -1.1024],\n",
      "        [-0.6879, -0.6984],\n",
      "        [-0.5910, -0.8069],\n",
      "        [-0.6887, -0.6976],\n",
      "        [-0.7515, -0.6380],\n",
      "        [-0.7446, -0.6442],\n",
      "        [-0.5820, -0.8182],\n",
      "        [-0.7526, -0.6370],\n",
      "        [-0.2396, -1.5461],\n",
      "        [-0.3388, -1.2470],\n",
      "        [-0.8556, -0.5534],\n",
      "        [-0.8469, -0.5599],\n",
      "        [-0.6616, -0.7257],\n",
      "        [-0.6006, -0.7951],\n",
      "        [-1.0947, -0.4074],\n",
      "        [-0.2612, -1.4701],\n",
      "        [-0.7338, -0.6541],\n",
      "        [-0.6308, -0.7596],\n",
      "        [-0.4905, -0.9475],\n",
      "        [-0.7583, -0.6320],\n",
      "        [-0.6954, -0.6909],\n",
      "        [-0.7841, -0.6097],\n",
      "        [-0.4339, -1.0441],\n",
      "        [-0.2741, -1.4282],\n",
      "        [-0.5312, -0.8866],\n",
      "        [-0.6800, -0.7065],\n",
      "        [-0.6138, -0.7794],\n",
      "        [-0.5391, -0.8754],\n",
      "        [-0.5800, -0.8208],\n",
      "        [-0.6341, -0.7559],\n",
      "        [-0.3678, -1.1784],\n",
      "        [-0.6301, -0.7604],\n",
      "        [-0.8081, -0.5901],\n",
      "        [-0.8159, -0.5838],\n",
      "        [-0.6828, -0.7036],\n",
      "        [-0.7661, -0.6251],\n",
      "        [-0.6853, -0.7010],\n",
      "        [-0.3432, -1.2362],\n",
      "        [-0.4527, -1.0104],\n",
      "        [-0.3027, -1.3424],\n",
      "        [-0.5871, -0.8118],\n",
      "        [-0.6051, -0.7897],\n",
      "        [-0.6960, -0.6903],\n",
      "        [-0.1486, -1.9801],\n",
      "        [-0.8066, -0.5913],\n",
      "        [-0.5460, -0.8657],\n",
      "        [-0.6397, -0.7496],\n",
      "        [-0.6225, -0.7692],\n",
      "        [-0.6298, -0.7608],\n",
      "        [-0.3946, -1.1207],\n",
      "        [-0.7419, -0.6467],\n",
      "        [-0.6567, -0.7310],\n",
      "        [-0.7443, -0.6445],\n",
      "        [-0.5810, -0.8195],\n",
      "        [-0.7043, -0.6821],\n",
      "        [-0.4426, -1.0282],\n",
      "        [-0.9751, -0.4735],\n",
      "        [-0.3982, -1.1134],\n",
      "        [-0.7012, -0.6852],\n",
      "        [-0.9420, -0.4940],\n",
      "        [-0.6225, -0.7691],\n",
      "        [-0.6596, -0.7279],\n",
      "        [-0.6646, -0.7225],\n",
      "        [-0.7043, -0.6821],\n",
      "        [-0.8237, -0.5777],\n",
      "        [-0.1358, -2.0638],\n",
      "        [-0.3859, -1.1390],\n",
      "        [-0.7228, -0.6643],\n",
      "        [-0.0983, -2.3683],\n",
      "        [-0.7855, -0.6086],\n",
      "        [-0.7105, -0.6761],\n",
      "        [-0.3691, -1.1757],\n",
      "        [-0.6023, -0.7930],\n",
      "        [-0.8669, -0.5452],\n",
      "        [-0.6633, -0.7239],\n",
      "        [-0.6237, -0.7678],\n",
      "        [-0.6788, -0.7077],\n",
      "        [-0.5243, -0.8964],\n",
      "        [-0.1336, -2.0787],\n",
      "        [-0.6111, -0.7825],\n",
      "        [-0.7206, -0.6665],\n",
      "        [-0.7629, -0.6279],\n",
      "        [-0.4655, -0.9884],\n",
      "        [-0.6273, -0.7636],\n",
      "        [-0.1030, -2.3237],\n",
      "        [-0.1963, -1.7248],\n",
      "        [-0.3452, -1.2314],\n",
      "        [-0.6458, -0.7428],\n",
      "        [-0.2979, -1.3562],\n",
      "        [-0.4257, -1.0592],\n",
      "        [-0.4677, -0.9846],\n",
      "        [-0.7091, -0.6775],\n",
      "        [-0.8205, -0.5802],\n",
      "        [-0.7429, -0.6457],\n",
      "        [-0.3793, -1.1532],\n",
      "        [-0.3656, -1.1836],\n",
      "        [-0.5648, -0.8404],\n",
      "        [-0.5500, -0.8602],\n",
      "        [-0.7333, -0.6545],\n",
      "        [-0.2050, -1.6854],\n",
      "        [-0.8363, -0.5680],\n",
      "        [-0.4061, -1.0974],\n",
      "        [-0.7171, -0.6697],\n",
      "        [-0.6711, -0.7156],\n",
      "        [-0.6303, -0.7603],\n",
      "        [-0.8002, -0.5965],\n",
      "        [-0.4433, -1.0270],\n",
      "        [-0.5995, -0.7965],\n",
      "        [-0.7467, -0.6423]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
      "        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "        0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
      "        0, 1, 0, 1, 0, 0, 0, 1])\n",
      "tensor([[-0.7614, -0.6293],\n",
      "        [-0.4483, -1.0182],\n",
      "        [-0.5913, -0.8066],\n",
      "        [-0.3287, -1.2725],\n",
      "        [-0.2638, -1.4614],\n",
      "        [-0.6520, -0.7361],\n",
      "        [-0.6297, -0.7609],\n",
      "        [-0.6582, -0.7294],\n",
      "        [-0.3212, -1.2921],\n",
      "        [-0.3405, -1.2428],\n",
      "        [-0.7340, -0.6539],\n",
      "        [-0.6342, -0.7558],\n",
      "        [-0.4796, -0.9650],\n",
      "        [-0.3308, -1.2672],\n",
      "        [-0.7416, -0.6469],\n",
      "        [-0.6666, -0.7204],\n",
      "        [-0.6528, -0.7352],\n",
      "        [-0.4214, -1.0675],\n",
      "        [-0.4304, -1.0505],\n",
      "        [-1.0764, -0.4168],\n",
      "        [-0.5255, -0.8947],\n",
      "        [-0.5882, -0.8104],\n",
      "        [-0.8352, -0.5688],\n",
      "        [-0.8571, -0.5524],\n",
      "        [-0.3518, -1.2155],\n",
      "        [-0.4240, -1.0625],\n",
      "        [-0.5002, -0.9325],\n",
      "        [-0.6275, -0.7634],\n",
      "        [-0.7876, -0.6069],\n",
      "        [-0.7311, -0.6566],\n",
      "        [-0.6553, -0.7325],\n",
      "        [-0.6233, -0.7682],\n",
      "        [-0.5026, -0.9288],\n",
      "        [-0.5992, -0.7969],\n",
      "        [-0.6016, -0.7940],\n",
      "        [-0.5980, -0.7983],\n",
      "        [-0.6139, -0.7792],\n",
      "        [-0.8907, -0.5282],\n",
      "        [-0.2914, -1.3752],\n",
      "        [-0.7390, -0.6493],\n",
      "        [-0.4850, -0.9563],\n",
      "        [-0.7492, -0.6400],\n",
      "        [-0.5928, -0.8047],\n",
      "        [-0.5311, -0.8867],\n",
      "        [-0.6486, -0.7397],\n",
      "        [-0.8997, -0.5220],\n",
      "        [-0.1911, -1.7488],\n",
      "        [-1.0121, -0.4517],\n",
      "        [-0.7313, -0.6564],\n",
      "        [-0.6025, -0.7928],\n",
      "        [-0.7732, -0.6190],\n",
      "        [-1.1786, -0.3678],\n",
      "        [-0.6561, -0.7316],\n",
      "        [-0.6238, -0.7676],\n",
      "        [-0.5780, -0.8233],\n",
      "        [-0.2341, -1.5667],\n",
      "        [-0.9045, -0.5188],\n",
      "        [-0.3866, -1.1375],\n",
      "        [-0.7047, -0.6817],\n",
      "        [-0.3864, -1.1379],\n",
      "        [-0.2312, -1.5778],\n",
      "        [-0.7064, -0.6800],\n",
      "        [-0.5096, -0.9182],\n",
      "        [-0.4574, -1.0022]], grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
      "        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0])\n",
      "Epoch: 001, Loss: 0.7444, Train Acc: 0.6329, Test Acc: 0.6443\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(dim=32).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1, 2):\n",
    "    loss = train(epoch)\n",
    "    train_acc = test(train_loader)\n",
    "    exit()\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "          f'Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "play with actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_files_list = [\"./export/\"+f for f in os.listdir(\"./export\") ]\n",
    "instance_dict = {}\n",
    "for dir_str in data_files_list:\n",
    "    with open(dir_str, 'r') as text_file:\n",
    "        cnt = 0\n",
    "        instance = \"\"\n",
    "        for line in text_file:\n",
    "            if cnt < 9:\n",
    "                if cnt == 0:\n",
    "                    instance = line.split()[0]\n",
    "                    instance_dict[instance] = []\n",
    "                cnt += 1\n",
    "                continue\n",
    "            split_line = line.split()\n",
    "            instance_dict[instance].append([int(i) for i in split_line])\n",
    "        text_file.close()\n",
    "\n",
    "ng_dict = {}\n",
    "cnt = -1\n",
    "with open(\"ng_outs.csv\", 'r') as text_file:\n",
    "    for line in text_file:\n",
    "        if cnt < 2:\n",
    "            cnt += 1\n",
    "            continue\n",
    "        raw_line = line.strip()\n",
    "        split_line_list = raw_line.split(sep=\";\")\n",
    "        instance = split_line_list[3]\n",
    "        if instance not in ng_dict:\n",
    "            ng_dict[instance] = [[0 for i in range(101)]]\n",
    "        ng_dict[instance].append([0] + [int(i) for i in split_line_list[5:-1]])\n",
    "        if len(split_line_list[5:-1]) != 100:\n",
    "            print(\"case found for instance \"+instance)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print((ng_dict['J000000'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import torch\n",
    "import torch_geometric as tg\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GraphConv, global_add_pool\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_graph_list = []\n",
    "for i in range(101):\n",
    "    for j in range(101):\n",
    "        if i != j:\n",
    "            complete_graph_list.append([i,j])\n",
    "edge_index = torch.tensor(complete_graph_list, dtype=torch.long).t().contiguous()\n",
    "n_edges = len(complete_graph_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for instance_name in ng_dict:\n",
    "    for i in range(101):\n",
    "        for j in range(101):\n",
    "            if i == j:\n",
    "                ng_dict[instance_name][i][j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for instance_name in ng_dict:\n",
    "    y = torch.tensor(ng_dict[instance_name], dtype=torch.double)\n",
    "    x = torch.tensor(instance_dict[instance_name], dtype=torch.double)\n",
    "    attr = [[i] for i in range(n_edges)]\n",
    "    loc_dict = {(i[0],j[0]): sqrt((i[1]-j[1])**2 + (i[2]-j[2])**2) for i in instance_dict[instance_name] for j in instance_dict[instance_name]}\n",
    "    cnt = -1\n",
    "    for i in range(101):\n",
    "        for j in range(101):\n",
    "            if i != j:\n",
    "                cnt += 1\n",
    "                attr[cnt].append(loc_dict[i,j])\n",
    "    attr = torch.tensor(attr, dtype=torch.double)\n",
    "    pos = []\n",
    "    for i in instance_dict[instance_name]:\n",
    "        pos.append([i[1], i[2]])\n",
    "    pos = torch.tensor(pos, dtype=torch.double)\n",
    "    data_list.append(Data(x=x, y=y, edge_index=edge_index, pos=pos, edge_attr=attr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataloader = DataLoader(data_list[428:458], batch_size=1)\n",
    "data_iter = iter(dataloader)\n",
    "data_test = DataLoader(data_list[458:468], batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to produce edges as an adjacency matrix\n",
    "complete_adj_matrix_list = [[0 for i in range(101)] for i in range(101)]\n",
    "for edge in complete_graph_list:\n",
    "    i, j = edge\n",
    "    complete_adj_matrix_list[i][j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Instances:\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "    \n",
    "    def to_torch_geometric(self, start=0, end=-1, batch_size=1):\n",
    "        return DataLoader(data_list[start:end], batch_size=batch_size)\n",
    "    \n",
    "    def to_conv_nets(self, start=0, end=-1, batch_size=1):\n",
    "        final_data = []\n",
    "        nodes = []\n",
    "        nodes_coor = []\n",
    "        nodes_timew = []\n",
    "        x_edges = []\n",
    "        x_edges_values = []\n",
    "        y_edges = []\n",
    "        cnt = 0\n",
    "        current_batch = 0\n",
    "        for graph in self.data_list[start:end]:\n",
    "            if cnt >= batch_size:\n",
    "                cnt = 0\n",
    "                current_batch += 1\n",
    "                nodes = torch.tensor(nodes, dtype=torch.long)\n",
    "                nodes_coor = torch.tensor(nodes_coor, dtype=torch.float)\n",
    "                nodes_timew = torch.tensor(nodes_timew, dtype=torch.long)\n",
    "                x_edges = torch.tensor(x_edges, dtype=torch.long)\n",
    "                x_edges_values = torch.tensor(x_edges_values, dtype=torch.float)\n",
    "                y_edges = torch.tensor(y_edges, dtype=torch.long)\n",
    "                final_data.append((x_edges, x_edges_values, nodes, nodes_coor, nodes_timew, y_edges))\n",
    "                nodes = []\n",
    "                nodes_coor = []\n",
    "                nodes_timew = []\n",
    "                x_edges = []\n",
    "                x_edges_values = []\n",
    "                y_edges = []\n",
    "            nodes.append([i for i in range(101)]) \n",
    "            nodes_coor.append(graph.pos.tolist())\n",
    "            tw = []\n",
    "            x_raw = graph.x.tolist()\n",
    "            for i in range(101):\n",
    "                tw.append([x_raw[i][4], x_raw[i][5]])\n",
    "            nodes_timew.append(tw)\n",
    "            x_edges.append(complete_adj_matrix_list)\n",
    "            dist_matrix = [[0 for _ in range(101)] for _ in range(101)]\n",
    "            dist_list = [i for _, i in graph.edge_attr.tolist()]\n",
    "            pos_dist = 0\n",
    "            for i in range(101):\n",
    "                for j in range(101):\n",
    "                    if i != j:\n",
    "                        dist_matrix[i][j] = dist_list[pos_dist]\n",
    "                        pos_dist += 1\n",
    "            x_edges_values.append(dist_matrix)\n",
    "            y_edges.append(graph.y.tolist()) #TODO: remove the transpose and also the contiguous when generating y\n",
    "            cnt += 1\n",
    "        return final_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = Instances(data_list)\n",
    "dataloader = data_source.to_torch_geometric(start=428, end=458, batch_size=1)\n",
    "datatorch = data_source.to_conv_nets(start=428, end=458, batch_size=1)\n",
    "torchtest = data_source.to_conv_nets(start=458, end=468, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        num_features = 7\n",
    "        dim = 101*101\n",
    "\n",
    "        self.conv1 = GraphConv(num_features, dim)\n",
    "        self.conv2 = GraphConv(dim, dim)\n",
    "\n",
    "        self.lin1 = Linear(dim, dim)\n",
    "\n",
    "        self.double()\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_weight=None):\n",
    "        # x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # x = self.conv1(x, edge_index)\n",
    "        # x = F.relu(x)\n",
    "        # x = F.sigmoid(self.conv2(x, edge_index))\n",
    "        # x = F.dropout(x, training=self.training)\n",
    "        # x = self.conv2(x, edge_index)\n",
    "\n",
    "        x = self.conv1(x, edge_index, edge_weight).relu()\n",
    "        x = self.conv2(x, edge_index, edge_weight).relu()\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # x = self.lin2(x)\n",
    "        x = torch.reshape(x, (101, 101))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    if epoch == 51:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.5 * param_group['lr']\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in dataloader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # print(data.x, data.edge_index, data.batch)\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        # print(output, data.y)\n",
    "        # print(output)\n",
    "        loss = F.l1_loss(output, data.y)\n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "        test_size = 10\n",
    "    return loss_all / test_size\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "        test_size = 10\n",
    "    return correct / test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 12316329.8885, Train Acc: 27914.9000, Test Acc: 9321.4000\n",
      "Epoch: 002, Loss: 15050.8765, Train Acc: 27375.4000, Test Acc: 9180.6000\n",
      "Epoch: 003, Loss: 6520.1387, Train Acc: 29867.4000, Test Acc: 9922.2000\n",
      "Epoch: 004, Loss: 3516.2888, Train Acc: 29887.2000, Test Acc: 9941.8000\n",
      "Epoch: 005, Loss: 3118.8949, Train Acc: 29746.9000, Test Acc: 9921.7000\n",
      "Epoch: 006, Loss: 2790.8632, Train Acc: 29253.6000, Test Acc: 9770.8000\n",
      "Epoch: 007, Loss: 6193.2795, Train Acc: 28825.8000, Test Acc: 9601.0000\n",
      "Epoch: 008, Loss: 10322.0821, Train Acc: 29437.1000, Test Acc: 9821.8000\n",
      "Epoch: 009, Loss: 8848.0151, Train Acc: 29436.2000, Test Acc: 9821.8000\n",
      "Epoch: 010, Loss: 3819.2164, Train Acc: 29756.7000, Test Acc: 9922.1000\n",
      "Epoch: 011, Loss: 2152.1094, Train Acc: 30037.2000, Test Acc: 10022.2000\n",
      "Epoch: 012, Loss: 4402.4668, Train Acc: 29436.8000, Test Acc: 9821.6000\n",
      "Epoch: 013, Loss: 4791.5913, Train Acc: 30337.9000, Test Acc: 10122.4000\n",
      "Epoch: 014, Loss: 5397.9118, Train Acc: 30037.9000, Test Acc: 10022.3000\n",
      "Epoch: 015, Loss: 1904.8505, Train Acc: 29847.7000, Test Acc: 10022.2000\n",
      "Epoch: 016, Loss: 2320.9467, Train Acc: 30337.9000, Test Acc: 10122.4000\n",
      "Epoch: 017, Loss: 1745.1811, Train Acc: 30097.4000, Test Acc: 10022.4000\n",
      "Epoch: 018, Loss: 1567.5528, Train Acc: 30037.4000, Test Acc: 10022.4000\n",
      "Epoch: 019, Loss: 4198.0330, Train Acc: 30037.8000, Test Acc: 10022.2000\n",
      "Epoch: 020, Loss: 3712.2741, Train Acc: 29436.9000, Test Acc: 9822.0000\n",
      "Epoch: 021, Loss: 1016.5476, Train Acc: 29797.5000, Test Acc: 9922.2000\n",
      "Epoch: 022, Loss: 3924.2838, Train Acc: 30037.5000, Test Acc: 10022.0000\n",
      "Epoch: 023, Loss: 3081.6943, Train Acc: 30037.4000, Test Acc: 10022.0000\n",
      "Epoch: 024, Loss: 1626.2530, Train Acc: 30327.9000, Test Acc: 10122.4000\n",
      "Epoch: 025, Loss: 3968.9558, Train Acc: 30318.0000, Test Acc: 10122.4000\n",
      "Epoch: 026, Loss: 690.5695, Train Acc: 30337.9000, Test Acc: 10122.4000\n",
      "Epoch: 027, Loss: 1707.2010, Train Acc: 29767.7000, Test Acc: 9922.2000\n",
      "Epoch: 028, Loss: 3747.1343, Train Acc: 30337.9000, Test Acc: 10122.4000\n",
      "Epoch: 029, Loss: 2478.9858, Train Acc: 30337.9000, Test Acc: 10122.4000\n",
      "Epoch: 030, Loss: 1096.4461, Train Acc: 30337.9000, Test Acc: 10122.4000\n",
      "Epoch: 031, Loss: 210.4835, Train Acc: 30337.9000, Test Acc: 10122.4000\n",
      "Epoch: 032, Loss: 558.1692, Train Acc: 30337.9000, Test Acc: 10122.4000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1, 33):\n",
    "    loss = train(epoch)\n",
    "    train_acc = test(dataloader)\n",
    "    test_acc = test(data_test)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "          f'Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): GraphConv(7, 10201)\n",
      "  (conv2): GraphConv(10201, 10201)\n",
      "  (conv3): GraphConv(10201, 10201)\n",
      "  (conv4): GraphConv(10201, 10201)\n",
      "  (conv5): GraphConv(10201, 10201)\n",
      "  (lin1): Linear(in_features=10201, out_features=10201, bias=True)\n",
      "  (lin2): Linear(in_features=10201, out_features=10201, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0] 1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] 1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 1\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] 0\n",
      "73.0\n",
      "65.0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "data_show = data_list[430]\n",
    "pred = model(data_show.x, data_show.edge_index, data_show.batch).tolist()\n",
    "y = data_show.y\n",
    "diff = 0\n",
    "for j in range(len(pred)):\n",
    "    a_list = [1 if i*100 > 1 else 0 for i in pred[j]]\n",
    "    diff += sum([(y[j][i].tolist() - a_list[i])**2 for i in range(len(a_list))])\n",
    "    print([1 if i*10 > 0.1 else 0 for i in pred[j]], sum(a_list))\n",
    "print(diff)\n",
    "print(sum([y[j][i].tolist() for i in range(101) for j in range(101)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_scatter import scatter\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormNode(nn.Module):\n",
    "    \"\"\"Batch normalization for node features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(BatchNormNode, self).__init__()\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim, track_running_stats=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features (batch_size, num_nodes, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            x_bn: Node features after batch normalization (batch_size, num_nodes, hidden_dim)\n",
    "        \"\"\"\n",
    "        # The batch norm normalizes the hidden dim over batch and node dimensions\n",
    "        if x.dim() == 2:\n",
    "            # If we have sparse version we have only one batch dimension\n",
    "            # simply perform batch norm over this (so this normalizes over batch and node dimension)\n",
    "            return self.batch_norm(x)\n",
    "        x_trans = x.transpose(1, 2).contiguous()  # Reshape input: (batch_size, hidden_dim, num_nodes)\n",
    "        x_trans_bn = self.batch_norm(x_trans)\n",
    "        x_bn = x_trans_bn.transpose(1, 2).contiguous()  # Reshape to original shape\n",
    "        # x_bn2 = self.batch_norm(x.view(-1, x.size(-1))).view_as(x)\n",
    "        # assert torch.allclose(x_bn, x_bn2, atol=1e-5)\n",
    "        return x_bn\n",
    "\n",
    "\n",
    "class BatchNormEdge(nn.Module):\n",
    "    \"\"\"Batch normalization for edge features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(BatchNormEdge, self).__init__()\n",
    "        self.batch_norm = nn.BatchNorm2d(hidden_dim, track_running_stats=False)\n",
    "\n",
    "    def forward(self, e):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            e: Edge features (batch_size, num_nodes, num_nodes, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            e_bn: Edge features after batch normalization (batch_size, num_nodes, num_nodes, hidden_dim)\n",
    "        \"\"\"\n",
    "        # The batch norm normalizes the hidden dim over batch and edge dimensions\n",
    "        if e.dim() == 2:\n",
    "            # If we have sparse version we have only one batch dimension\n",
    "            # simply perform batch norm over this (so this normalizes over batch and node dimension)\n",
    "            # We can use the BatchNorm2d module by inserting dummy dimensions\n",
    "            return self.batch_norm(e[:, :, None, None]).view_as(e)\n",
    "        e_trans = e.transpose(1, 3).contiguous()  # Reshape input: (batch_size, hidden_dim, num_nodes, num_nodes)\n",
    "        e_trans_bn = self.batch_norm(e_trans)\n",
    "        e_bn = e_trans_bn.transpose(1, 3).contiguous()  # Reshape to original\n",
    "        return e_bn\n",
    "\n",
    "class NodeFeatures(nn.Module):\n",
    "    \"\"\"Convnet features for nodes.\n",
    "    \n",
    "    Using `sum` aggregation:\n",
    "        x_i = U*x_i +  sum_j [ gate_ij * (V*x_j) ]\n",
    "    \n",
    "    Using `mean` aggregation:\n",
    "        x_i = U*x_i + ( sum_j [ gate_ij * (V*x_j) ] / sum_j [ gate_ij] )\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim, aggregation=\"mean\"):\n",
    "        super(NodeFeatures, self).__init__() # We must always sum, since mean means 'weighted mean' so sum weighted messages\n",
    "        self.aggregation = aggregation\n",
    "        self.U = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "        self.V = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "\n",
    "    def forward(self, x, edge_gate, edge_index=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features (batch_size, num_nodes, hidden_dim)\n",
    "            edge_gate: Edge gate values (batch_size, num_nodes, num_nodes, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            x_new: Convolved node features (batch_size, num_nodes, hidden_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        Ux = self.U(x)  # B x V x H\n",
    "        Vx = self.V(x)  # B x V x H\n",
    "\n",
    "        if edge_index is not None:\n",
    "            # Sparse version\n",
    "            return self.propagate(edge_index, Ux=Ux, Vx=Vx, edge_gate=edge_gate)\n",
    "\n",
    "        from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "        # The rest is a relatively cheap operation that uses a lot of memory\n",
    "        # No it does not use a lot of memory\n",
    "        use_checkpoint = False\n",
    "        if use_checkpoint:\n",
    "            x_new = checkpoint(self._inner, edge_gate, Ux, Vx)\n",
    "        else:\n",
    "\n",
    "            x_new = self._inner(edge_gate, Ux, Vx)\n",
    "            # print(\"Dense x\", edge_gate.size(), Ux.size(), Vx.size())\n",
    "            # print(x_new.flatten()[-10:])\n",
    "        return x_new\n",
    "\n",
    "    def _inner(self, edge_gate, Ux, Vx):\n",
    "        use_einsum = False\n",
    "        use_matmul = False\n",
    "\n",
    "        if use_einsum:  # Seems to use more memory\n",
    "            x_add = torch.einsum('bijd,bjd->bid', edge_gate, Vx)\n",
    "        elif use_matmul:  # Seems to use same memory as einsum, not much faster\n",
    "            x_add = torch.matmul(\n",
    "                edge_gate.unsqueeze(1).transpose(1, 4).squeeze(-1),\n",
    "                Vx.unsqueeze(1).transpose(1, 3)\n",
    "            ).transpose(1, 3).squeeze(1)\n",
    "        else:\n",
    "            Vx = Vx.unsqueeze(1)  # extend Vx from \"B x V x H\" to \"B x 1 x V x H\"\n",
    "            gateVx = edge_gate * Vx  # B x V x V x H\n",
    "            x_add = torch.sum(gateVx, dim=-2)\n",
    "        if self.aggregation==\"mean\":\n",
    "            x_new = Ux + x_add / (1e-20 + torch.sum(edge_gate, dim=-2))  # B x V x H\n",
    "        elif self.aggregation==\"sum\":\n",
    "            x_new = Ux + x_add  # B x V x H\n",
    "        return x_new\n",
    "\n",
    "    def message(self, edge_gate, Vx_j):\n",
    "        return edge_gate * Vx_j\n",
    "\n",
    "    def update(self, agg, Ux, edge_gate, edge_index):\n",
    "        src, tgt = edge_index\n",
    "        # Aggregate here exactly as in _inner. Normalizing here is more efficient than normalizing the messages.\n",
    "        if self.aggregation == \"mean\":\n",
    "            gate_sum = scatter(edge_gate, tgt, dim=0, dim_size=Ux.size(0), reduce='sum')\n",
    "            return Ux + agg / (1e-20 + gate_sum)\n",
    "        assert self.aggregation == \"sum\"\n",
    "        return Ux + agg  # Skip connection\n",
    "\n",
    "\n",
    "class EdgeFeatures(nn.Module):\n",
    "    \"\"\"Convnet features for edges.\n",
    "\n",
    "    e_ij = U*e_ij + V*(x_i + x_j)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, directed=False):\n",
    "        super(EdgeFeatures, self).__init__()\n",
    "        self.U = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "        self.V = nn.Linear(hidden_dim, hidden_dim, True)\n",
    "        self.W = nn.Linear(hidden_dim, hidden_dim, True) if directed else None\n",
    "        \n",
    "    def forward(self, x, e, edge_index=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features (batch_size, num_nodes, hidden_dim)\n",
    "            e: Edge features (batch_size, num_nodes, num_nodes, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            e_new: Convolved edge features (batch_size, num_nodes, num_nodes, hidden_dim)\n",
    "        \"\"\"\n",
    "        Ue = self.U(e)\n",
    "        Vx = self.V(x)\n",
    "        Wx = Vx if self.W is None else self.W(x)  # If self.W is none, graph is undirected\n",
    "        if edge_index is not None:\n",
    "            # Sparse version\n",
    "            src, dst = edge_index\n",
    "            Wx = Wx[dst]  # = to\n",
    "            Vx = Vx[src]  # = from\n",
    "        else:\n",
    "            Wx = Wx.unsqueeze(1)  # Extend Wx from \"B x V x H\" to \"B x 1 x V x H\" = to\n",
    "            Vx = Vx.unsqueeze(2)  # extend Vx from \"B x V x H\" to \"B x V x 1 x H\" = from\n",
    "\n",
    "        e_new = Ue + Vx + Wx\n",
    "        return e_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualGatedGCNLayer(nn.Module):\n",
    "    \"\"\"Convnet layer with gating and residual connection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, aggregation=\"sum\", directed=False):\n",
    "        super(ResidualGatedGCNLayer, self).__init__()\n",
    "        self.node_feat = NodeFeatures(hidden_dim, aggregation)\n",
    "        self.edge_feat = EdgeFeatures(hidden_dim, directed)\n",
    "        self.bn_node = BatchNormNode(hidden_dim)\n",
    "        self.bn_edge = BatchNormEdge(hidden_dim)\n",
    "\n",
    "    def forward(self, x, e, edge_index=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Node features (batch_size, num_nodes, hidden_dim)\n",
    "            e: Edge features (batch_size, num_nodes, num_nodes, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            x_new: Convolved node features (batch_size, num_nodes, hidden_dim)\n",
    "            e_new: Convolved edge features (batch_size, num_nodes, num_nodes, hidden_dim)\n",
    "        \"\"\"\n",
    "        e_in = e\n",
    "        x_in = x\n",
    "        # Edge convolution\n",
    "        e_tmp = self.edge_feat(x_in, e_in, edge_index)  # B x V x V x H\n",
    "        # Compute edge gates\n",
    "        edge_gate = F.sigmoid(e_tmp)\n",
    "        # Node convolution\n",
    "        x_tmp = self.node_feat(x_in, edge_gate, edge_index)\n",
    "        # Batch normalization\n",
    "        e_tmp = self.bn_edge(e_tmp)\n",
    "        x_tmp = self.bn_node(x_tmp)\n",
    "        # ReLU Activation\n",
    "        e = F.relu(e_tmp)\n",
    "        x = F.relu(x_tmp)\n",
    "        # Residual connection\n",
    "        x_new = x_in + x\n",
    "        e_new = e_in + e\n",
    "        return x_new, e_new\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Multi-layer Perceptron for output prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, output_dim, L=2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.L = L\n",
    "        U = []\n",
    "        for layer in range(self.L - 1):\n",
    "            U.append(nn.Linear(hidden_dim, hidden_dim, True))\n",
    "        self.U = nn.ModuleList(U)\n",
    "        self.V = nn.Linear(hidden_dim, output_dim, True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input features (batch_size, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            y: Output predictions (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        Ux = x\n",
    "        for U_i in self.U:\n",
    "            Ux = U_i(Ux)  # B x H\n",
    "            Ux = F.relu(Ux)  # B x H\n",
    "        y = self.V(Ux)  # B x O\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualGatedGCNModelVRP(nn.Module):\n",
    "    \"\"\"Residual Gated GCN Model for outputting predictions as edge adjacency matrices.\n",
    "\n",
    "    References:\n",
    "        Paper: https://arxiv.org/pdf/1711.07553v2.pdf\n",
    "        Code: https://github.com/xbresson/spatial_graph_convnets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ResidualGatedGCNModelVRP, self).__init__()\n",
    "        self.dtypeFloat = torch.FloatTensor\n",
    "        self.dtypeLong = torch.LongTensor\n",
    "        # Define net parameters\n",
    "        # self.num_nodes = config.num_nodes\n",
    "        # self.node_dim = config.node_dim\n",
    "        # self.voc_nodes_in = config['voc_nodes_in']\n",
    "        # self.voc_nodes_out = config['num_nodes']  # config['voc_nodes_out']\n",
    "        # self.voc_edges_in = config['voc_edges_in']\n",
    "        # self.voc_edges_out = config['voc_edges_out']\n",
    "        # self.hidden_dim = config['hidden_dim']\n",
    "        # self.num_layers = config['num_layers']\n",
    "        # self.mlp_layers = config['mlp_layers']\n",
    "        # self.aggregation = config['aggregation']\n",
    "        # self.num_segments_checkpoint = config.get('num_segments_checkpoint', 0)\n",
    "        self.num_nodes = 100\n",
    "        self.node_dim = 2\n",
    "        self.voc_nodes_in = 200\n",
    "        self.voc_nodes_out = 2\n",
    "        self.voc_edges_in = 3\n",
    "        self.voc_edges_out = 1\n",
    "        self.hidden_dim = 6\n",
    "        self.num_layers = 30\n",
    "        self.mlp_layers = 3\n",
    "        self.aggregation = \"mean\"\n",
    "        self.num_segments_checkpoint = 5\n",
    "\n",
    "        # Node and edge embedding layers/lookups\n",
    "        self.nodes_coord_embedding = nn.Linear(self.node_dim, self.hidden_dim // 2, bias=False)\n",
    "        # self.nodes_coord_embedding2 = nn.Linear(self.node_dim, self.hidden_dim, bias=False)\n",
    "        self.edges_values_embedding = nn.Linear(1, self.hidden_dim // 2, bias=False)\n",
    "        # self.edges_values_embedding2 = nn.Linear(1, self.hidden_dim, bias=False)\n",
    "        self.edges_embedding = nn.Embedding(self.voc_edges_in, self.hidden_dim // 2)\n",
    "        # self.edges_embedding2 = nn.Embedding(self.voc_edges_in, self.hidden_dim)\n",
    "        self.nodes_embedding = nn.Embedding(self.voc_nodes_in, self.hidden_dim // 2)\n",
    "        # self.nodes_embedding2 = nn.Embedding(self.voc_nodes_in, self.hidden_dim)\n",
    "        # Define GCN Layers\n",
    "        gcn_layers = []\n",
    "        for layer in range(self.num_layers):\n",
    "            gcn_layers.append(ResidualGatedGCNLayer(self.hidden_dim, self.aggregation))\n",
    "        self.gcn_layers = nn.ModuleList(gcn_layers)\n",
    "        # Define MLP classifiers\n",
    "        self.mlp_edges = MLP(self.hidden_dim, self.voc_edges_out, self.mlp_layers)\n",
    "        # self.mlp_nodes = MLP(self.hidden_dim, self.voc_nodes_out, self.mlp_layers)\n",
    "\n",
    "    def forward(self, x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges=None, edge_cw=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_edges: Input edge adjacency matrix (batch_size, num_nodes, num_nodes)\n",
    "            x_edges_values: Input edge distance matrix (batch_size, num_nodes, num_nodes)\n",
    "            x_nodes: Input nodes (batch_size, num_nodes)\n",
    "            x_nodes_coord: Input node coordinates (batch_size, num_nodes, node_dim)\n",
    "            y_edges: Targets for edges (batch_size, num_nodes, num_nodes)\n",
    "            edge_cw: Class weights for edges loss\n",
    "            # y_nodes: Targets for nodes (batch_size, num_nodes, num_nodes)\n",
    "            # node_cw: Class weights for nodes loss\n",
    "\n",
    "        Returns:\n",
    "            y_pred_edges: Predictions for edges (batch_size, num_nodes, num_nodes)\n",
    "            # y_pred_nodes: Predictions for nodes (batch_size, num_nodes)\n",
    "            loss: Value of loss function\n",
    "        \"\"\"\n",
    "        # Node and edge embedding\n",
    "        ## Todo: fix this but gives bugs for now\n",
    "        x_vals = self.nodes_coord_embedding(x_nodes_coord)  # B x V x H\n",
    "        x_tags = self.nodes_embedding(x_nodes)\n",
    "        x = torch.cat((x_vals, x_tags), -1)\n",
    "        # x = self.nodes_embedding2(x_nodes)\n",
    "        e_vals = self.edges_values_embedding(x_edges_values.unsqueeze(3))  # B x V x V x H\n",
    "        e_tags = self.edges_embedding(x_edges)  # B x V x V x H\n",
    "        e = torch.cat((e_vals, e_tags), -1)\n",
    "        #e = self.edges_values_embedding2(x_edges_values.unsqueeze(3))\n",
    "        # GCN layers\n",
    "        if self.num_segments_checkpoint != 0:\n",
    "            layer_functions = [lambda args: layer(*args) for layer in self.gcn_layers]\n",
    "            x, e = torch.utils.checkpoint.checkpoint_sequential(layer_functions, self.num_segments_checkpoint, (x, e))\n",
    "        else:\n",
    "            for layer in range(self.num_layers):\n",
    "                # B x V x H, B x V x V x H\n",
    "                x, e = self.gcn_layers[layer](x, e)\n",
    "        # MLP classifier\n",
    "        y_pred_edges = self.mlp_edges(e)  # B x V x V x voc_edges_out\n",
    "        # y_pred_nodes = self.mlp_nodes(x)  # B x V x voc_nodes_out\n",
    "\n",
    "        #loss = loss_edges(y_pred_edges, y_edges, edge_cw)\n",
    "        # Edge loss\n",
    "        # y = F.log_softmax(y_pred_edges, dim=3)  # B x V x V x voc_edges\n",
    "        # For some reason we must make things contiguous to prevent errors during backward\n",
    "        # y_perm = y.permute(0, 3, 1, 2).contiguous()  # B x voc_edges x V x V\n",
    "        # y_perm = y.permute(0, 3, 1, 2).contiguous()\n",
    "        # if edge_cw == None:\n",
    "        #     edge_cw = [1 for i in range(101)]\n",
    "        # if type(edge_cw) != torch.Tensor:\n",
    "        #     edge_labels = y_edges.cpu().numpy().flatten()\n",
    "        #     edge_cw = compute_class_weight(\"balanced\", classes=np.unique(edge_labels), y=edge_labels).tolist()\n",
    "        # print(edge_cw)\n",
    "        # if y_edges is not None:\n",
    "        #     # Compute loss\n",
    "        #     print(\"forwarding1.1\")\n",
    "        #     if edge_cw != None:\n",
    "        #         edge_cw = torch.Tensor(edge_cw)  # Convert to tensors\n",
    "        #         print(\"forwarding1.1.1\")\n",
    "        #         edge_cw = edge_cw.type(self.dtypeFloat)\n",
    "        #     print(\"forwarding1.2\")\n",
    "        #     loss = nn.NLLLoss(edge_cw)\n",
    "        #     print(\"forwarding1.3\")\n",
    "        #     print(y_perm)\n",
    "        #     print(y_edges)\n",
    "        #     loss = loss(y_perm, y_edges)\n",
    "        #     print(\"forwarding1.4\")\n",
    "        # else:\n",
    "        #     print(\"forwarding2.1\")\n",
    "        #     loss = None\n",
    "        #     print(\"forwarding2.2\")\n",
    "        \n",
    "        return y_pred_edges.permute(0, 3, 1, 2)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    if epoch == 51:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.5 * param_group['lr']\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in datatorch:\n",
    "        # data = data.to(device)\n",
    "        x_edges, x_edges_values, x_nodes, x_nodes_coord, _,  y_edges = data\n",
    "        optimizer.zero_grad()\n",
    "        # print(data.x, data.edge_index, data.batch)\n",
    "        output = model(x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges)\n",
    "        # print(output, data.y)\n",
    "        # print(output)\n",
    "        loss = F.l1_loss(output, y_edges)\n",
    "        loss.backward()\n",
    "        loss_all += loss.item()\n",
    "        optimizer.step()\n",
    "        test_size = 30\n",
    "    return loss_all / test_size\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in datatorch:\n",
    "        # data = data.to(device)\n",
    "        x_edges, x_edges_values, x_nodes, x_nodes_coord, _,  y_edges = data\n",
    "        output = model(x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        correct += pred.eq(y_edges).sum().item()\n",
    "        test_size = 30\n",
    "    return correct / test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.4054, Train Acc: 225.3333, Test Acc: 225.3333\n",
      "Epoch: 002, Loss: 0.1792, Train Acc: 568.7000, Test Acc: 568.7000\n",
      "Epoch: 003, Loss: 0.1121, Train Acc: 532.9667, Test Acc: 532.9667\n",
      "Epoch: 004, Loss: 0.0888, Train Acc: 484.5333, Test Acc: 484.5333\n",
      "Epoch: 005, Loss: 0.0775, Train Acc: 608.0667, Test Acc: 608.0667\n",
      "Epoch: 006, Loss: 0.0674, Train Acc: 768.6000, Test Acc: 768.6000\n",
      "Epoch: 007, Loss: 0.0566, Train Acc: 975.3000, Test Acc: 975.3000\n",
      "Epoch: 008, Loss: 0.0499, Train Acc: 1006.8333, Test Acc: 1006.8333\n",
      "Epoch: 009, Loss: 0.0465, Train Acc: 894.2000, Test Acc: 894.2000\n",
      "Epoch: 010, Loss: 0.0432, Train Acc: 700.9667, Test Acc: 700.9667\n",
      "Epoch: 011, Loss: 0.0408, Train Acc: 607.8000, Test Acc: 607.8000\n",
      "Epoch: 012, Loss: 0.0390, Train Acc: 504.3333, Test Acc: 504.3333\n",
      "Epoch: 013, Loss: 0.0376, Train Acc: 460.8667, Test Acc: 460.8667\n",
      "Epoch: 014, Loss: 0.0362, Train Acc: 444.3667, Test Acc: 444.3667\n",
      "Epoch: 015, Loss: 0.0349, Train Acc: 420.9333, Test Acc: 420.9333\n",
      "Epoch: 016, Loss: 0.0336, Train Acc: 370.8333, Test Acc: 370.8333\n",
      "Epoch: 017, Loss: 0.0322, Train Acc: 350.4333, Test Acc: 350.4333\n",
      "Epoch: 018, Loss: 0.0310, Train Acc: 390.3000, Test Acc: 390.3000\n",
      "Epoch: 019, Loss: 0.0300, Train Acc: 477.2667, Test Acc: 477.2667\n",
      "Epoch: 020, Loss: 0.0291, Train Acc: 600.5000, Test Acc: 600.5000\n",
      "Epoch: 021, Loss: 0.0282, Train Acc: 677.0333, Test Acc: 677.0333\n",
      "Epoch: 022, Loss: 0.0274, Train Acc: 764.2000, Test Acc: 764.2000\n",
      "Epoch: 023, Loss: 0.0268, Train Acc: 811.1000, Test Acc: 811.1000\n",
      "Epoch: 024, Loss: 0.0261, Train Acc: 871.1667, Test Acc: 871.1667\n",
      "Epoch: 025, Loss: 0.0255, Train Acc: 891.3333, Test Acc: 891.3333\n",
      "Epoch: 026, Loss: 0.0248, Train Acc: 901.4667, Test Acc: 901.4667\n",
      "Epoch: 027, Loss: 0.0240, Train Acc: 928.5000, Test Acc: 928.5000\n",
      "Epoch: 028, Loss: 0.0230, Train Acc: 928.5000, Test Acc: 928.5000\n",
      "Epoch: 029, Loss: 0.0220, Train Acc: 918.5333, Test Acc: 918.5333\n",
      "Epoch: 030, Loss: 0.0216, Train Acc: 928.7333, Test Acc: 928.7333\n",
      "Epoch: 031, Loss: 0.0215, Train Acc: 928.0667, Test Acc: 928.0667\n",
      "Epoch: 032, Loss: 0.0213, Train Acc: 877.8000, Test Acc: 877.8000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = ResidualGatedGCNModelVRP().to(device)\n",
    "model = ResidualGatedGCNModelVRP().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1, 33):\n",
    "    loss = train(epoch)\n",
    "    train_acc = test(datatorch)\n",
    "    test_acc = test(torchtest)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "          f'Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04061722755432129\n",
      "0.026139676570892334\n",
      "0.03298273682594299\n",
      "0.03773820400238037\n",
      "0.038486093282699585\n",
      "0.04939129948616028\n",
      "0.05397091805934906\n",
      "0.03338390588760376\n",
      "0.06312549114227295\n",
      "0.02569076418876648\n",
      "0.019716009497642517\n",
      "0.033094197511672974\n",
      "0.06704597175121307\n",
      "0.0335821807384491\n",
      "0.01994246244430542\n",
      "0.016477003693580627\n",
      "0.023413091897964478\n",
      "0.023982614278793335\n",
      "0.030505478382110596\n",
      "0.026444211602211\n",
      "0.0350906103849411\n",
      "0.02687370777130127\n",
      "0.021697252988815308\n",
      "0.02322867512702942\n",
      "0.03857545554637909\n",
      "0.02047106623649597\n",
      "0.03502276539802551\n",
      "0.02367711067199707\n",
      "0.04663431644439697\n",
      "0.03362300992012024\n",
      "0.04196174442768097\n",
      "0.0077669620513916016\n",
      "0.05777248740196228\n",
      "0.021643802523612976\n",
      "0.031182914972305298\n",
      "0.03520798683166504\n",
      "0.04534919559955597\n",
      "0.03540125489234924\n",
      "0.024228215217590332\n",
      "0.031848981976509094\n",
      "0.020850330591201782\n",
      "0.044520437717437744\n",
      "0.033465832471847534\n",
      "0.03340800106525421\n",
      "0.04108119010925293\n",
      "0.021565943956375122\n",
      "-0.007046744227409363\n",
      "0.02462117373943329\n",
      "0.03521956503391266\n",
      "0.026212617754936218\n",
      "0.02376416325569153\n",
      "0.012140139937400818\n",
      "0.03535380959510803\n",
      "0.03540125489234924\n",
      "0.06704597175121307\n",
      "0.0347270667552948\n",
      "0.028480738401412964\n",
      "0.02699241042137146\n",
      "0.037346065044403076\n",
      "0.017221689224243164\n",
      "0.035442277789115906\n",
      "0.04361790418624878\n",
      "0.0338381826877594\n",
      "0.02721448242664337\n",
      "0.022765904664993286\n",
      "0.024699658155441284\n",
      "0.038486093282699585\n",
      "0.020930930972099304\n",
      "0.026577427983283997\n",
      "0.02806985378265381\n",
      "0.030385568737983704\n",
      "0.01524488627910614\n",
      "0.03150680661201477\n",
      "0.03307430446147919\n",
      "0.04939126968383789\n",
      "0.013563096523284912\n",
      "0.024253249168395996\n",
      "0.020177900791168213\n",
      "0.036861419677734375\n",
      "0.04269440472126007\n",
      "0.03105345368385315\n",
      "0.030717909336090088\n",
      "0.029605895280838013\n",
      "0.02224019169807434\n",
      "0.0347270667552948\n",
      "0.012849509716033936\n",
      "0.026444226503372192\n",
      "0.026352912187576294\n",
      "0.04093889892101288\n",
      "0.020226597785949707\n",
      "0.049348145723342896\n",
      "0.05777271091938019\n",
      "0.02401888370513916\n",
      "0.03307430446147919\n",
      "0.04082697629928589\n",
      "0.04721006751060486\n",
      "0.037346065044403076\n",
      "0.01932552456855774\n",
      "0.04441100358963013\n",
      "0.04611116647720337\n",
      "0.04052357375621796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:551: UserWarning: torch.utils.checkpoint.checkpoint_sequential: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "data_show = datatorch[0]\n",
    "x_edges, x_edges_values, x_nodes, x_nodes_coord, _,  y_edges = data_show\n",
    "pred = model(x_edges, x_edges_values, x_nodes, x_nodes_coord, y_edges).tolist()[0]\n",
    "for p in pred:\n",
    "    print(max(p))\n",
    "# y = y_edges[0]\n",
    "# diff = 0\n",
    "# for j in range(len(pred)):\n",
    "#     a_list = [1 if i > 0.5 else 0 for i in pred[j]]\n",
    "#     diff += sum([(y[j][i].tolist() - a_list[i])**2 for i in range(len(a_list))])\n",
    "#     print([1 if i*10 > 0.1 else 0 for i in pred[j]], sum(a_list))\n",
    "# print(diff)\n",
    "# print(sum([y[j][i].tolist() for i in range(101) for j in range(101)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
