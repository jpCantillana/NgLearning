{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Data, HeteroData\n",
    "from torch_geometric.nn import knn_graph, HeteroConv, GCNConv, Linear, SAGEConv, to_hetero\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "# from torch_geometric.nn import GCNConv, Linear\n",
    "from torch_geometric.utils import negative_sampling, dropout_edge\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from random import sample\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import copy\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_files_list = [\"./export/\"+f for f in os.listdir(\"./export\") ]\n",
    "instance_dict = {}\n",
    "instance_additionals = {}\n",
    "set_sizes_dict = {}\n",
    "for dir_str in data_files_list:\n",
    "    with open(dir_str, 'r') as text_file:\n",
    "        cnt = 0\n",
    "        instance = \"\"\n",
    "        for line in text_file:\n",
    "            if cnt < 9:\n",
    "                if cnt == 0:\n",
    "                    instance = line.split()[0]\n",
    "                    instance_dict[instance] = []\n",
    "                if cnt == 4:\n",
    "                    instance_additionals[instance] = []\n",
    "                    split_line = line.split()\n",
    "                    instance_additionals[instance].append([int(i) for i in split_line])\n",
    "                cnt += 1\n",
    "                continue\n",
    "            split_line = line.split()\n",
    "            instance_dict[instance].append([int(i) for i in split_line])\n",
    "        text_file.close()\n",
    "\n",
    "ng_dict = {}\n",
    "cnt = -1\n",
    "with open(\"ng_outs.csv\", 'r') as text_file:\n",
    "    for line in text_file:\n",
    "        if cnt < 2:\n",
    "            cnt += 1\n",
    "            continue\n",
    "        raw_line = line.strip()\n",
    "        split_line_list = raw_line.split(sep=\";\")\n",
    "        instance = split_line_list[3]\n",
    "        if instance not in ng_dict:\n",
    "            # ng_dict[instance] = [[0 for i in range(101)]]\n",
    "            ng_dict[instance] = []\n",
    "        ng_dict[instance].append([int(i) for i in split_line_list[5:-1]])\n",
    "        set_sizes_dict[instance] = sum(sum(i) for i in ng_dict[instance])\n",
    "        if len(split_line_list[5:-1]) != 100:\n",
    "            print(\"case found for instance \"+instance)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_difficulty = [(k,v) for k,v in set_sizes_dict.items()]\n",
    "ordered_difficulty = sorted(ordered_difficulty, key=lambda tup: tup[1], reverse=True)\n",
    "instances_to_add = [ordered_difficulty[i][0] for i in range(0,4000)]\n",
    "# instances_remaining = [ordered_difficulty[i][0] for i in range(1000,len(ordered_difficulty))]\n",
    "# easy_instances = sample(instances_remaining, 2000)\n",
    "instance_list = instances_to_add #+ easy_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files_list = [\"./export2/\"+f for f in os.listdir(\"./export2\") ]\n",
    "\n",
    "for dir_str in data_files_list:\n",
    "    with open(dir_str, 'r') as text_file:\n",
    "        cnt = 0\n",
    "        instance = \"\"\n",
    "        for line in text_file:\n",
    "            if cnt < 9:\n",
    "                if cnt == 0:\n",
    "                    instance = line.split()[0]\n",
    "                    instance_dict[instance] = []\n",
    "                if cnt == 4:\n",
    "                    instance_additionals[instance] = []\n",
    "                    split_line = line.split()\n",
    "                    instance_additionals[instance].append([int(i) for i in split_line])\n",
    "                cnt += 1\n",
    "                continue\n",
    "            split_line = line.split()\n",
    "            instance_dict[instance].append([int(i) for i in split_line])\n",
    "        text_file.close()\n",
    "\n",
    "cnt = -1\n",
    "with open(\"ng_outs_JLA.csv\", 'r') as text_file:\n",
    "    for line in text_file:\n",
    "        if cnt < 1:\n",
    "            cnt += 1\n",
    "            continue\n",
    "        raw_line = line.strip()\n",
    "        split_line_list = raw_line.split(sep=\";\")\n",
    "        instance = split_line_list[3]\n",
    "        if instance not in ng_dict:\n",
    "            # ng_dict[instance] = [[0 for i in range(101)]]\n",
    "            ng_dict[instance] = []\n",
    "        ng_dict[instance].append([int(i) for i in split_line_list[5:]])\n",
    "        set_sizes_dict[instance] = sum(sum(i) for i in ng_dict[instance])\n",
    "        # add new instances\n",
    "        instance_list.append(instance)\n",
    "        if len(split_line_list[5:]) != 100:\n",
    "            print(\"case found for instance \"+instance)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_files_list = [\"./export8/\"+f for f in os.listdir(\"./export8\") ]\n",
    "for dir_str in data_files_list:\n",
    "    with open(dir_str, 'r') as text_file:\n",
    "        cnt = 0\n",
    "        instance = \"\"\n",
    "        for line in text_file:\n",
    "            if cnt < 9:\n",
    "                if cnt == 0:\n",
    "                    instance = line.split()[0]\n",
    "                    instance_dict[instance] = []\n",
    "                if cnt == 4:\n",
    "                    instance_additionals[instance] = []\n",
    "                    split_line = line.split()\n",
    "                    instance_additionals[instance].append([int(i) for i in split_line])\n",
    "                cnt += 1\n",
    "                continue\n",
    "            split_line = line.split()\n",
    "            instance_dict[instance].append([int(i) for i in split_line])\n",
    "        text_file.close()\n",
    "\n",
    "cnt = -1\n",
    "with open(\"ng_outs_29-11.csv\", 'r') as text_file:\n",
    "    for line in text_file:\n",
    "        if cnt < 2:\n",
    "            cnt += 1\n",
    "            continue\n",
    "        raw_line = line.strip()\n",
    "        split_line_list = raw_line.split(sep=\";\")\n",
    "        instance = split_line_list[3]\n",
    "        if instance not in ng_dict:\n",
    "            # ng_dict[instance] = [[0 for i in range(101)]]\n",
    "            ng_dict[instance] = []\n",
    "        ng_dict[instance].append([int(i) for i in split_line_list[5:-1]])\n",
    "        set_sizes_dict[instance] = sum(sum(i) for i in ng_dict[instance])\n",
    "        if instance in instance_dict.keys():\n",
    "            instance_list.append(instance)\n",
    "        if len(split_line_list[5:-1]) != 100:\n",
    "            print(\"case found for instance \"+instance)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "# aux_data_list = []\n",
    "\n",
    "\n",
    "for instance_name in instance_list:\n",
    "    instance = instance_dict[instance_name]\n",
    "    attr = []\n",
    "    max_demand = -1\n",
    "    capacity = instance_additionals[instance_name][0][1]\n",
    "    for i in instance[1:]:\n",
    "        demand = i[3]\n",
    "        if demand > max_demand:\n",
    "            max_demand = demand\n",
    "        attr.append(i[1:] + instance_additionals[instance_name][0])\n",
    "    \n",
    "    y = torch.tensor(ng_dict[instance_name], dtype=torch.float)\n",
    "    # x = nn.functional.normalize( torch.tensor(attr, dtype=torch.float) )\n",
    "    x_stops = torch.tensor(attr, dtype=torch.float)\n",
    "    x_depot = torch.tensor([instance[0][1:] + instance_additionals[instance_name][0]], dtype=torch.float)\n",
    "    pos_depot = [[instance[0][1], instance[0][2]]]\n",
    "    pos_stops = []\n",
    "    tw_sets_dict = {}\n",
    "    for i in instance[1:]:\n",
    "        pos_stops.append([i[1], i[2]])\n",
    "    pos_stops = torch.tensor(pos_stops, dtype=torch.double)\n",
    "    pos_depot = torch.tensor(pos_depot, dtype=torch.double)\n",
    "    route_neighbors = min(max(capacity//max_demand, 15), 25)\n",
    "    edge_index = knn_graph(pos_stops[1:], route_neighbors)\n",
    "    new_graph = HeteroData()\n",
    "    new_graph[\"depot\"].x = x_depot\n",
    "    new_graph[\"stops\"].x = x_stops\n",
    "    new_graph[\"stops\", \"route\", \"stops\"].edge_index = edge_index\n",
    "    new_graph[\"depot\", \"departs\", \"stops\"].edge_index = torch.tensor([\n",
    "        [0 for _ in range(len(attr))],\n",
    "        [i for i in range(len(attr))]\n",
    "    ], dtype=torch.long)\n",
    "    new_graph[\"stops\", \"return\", \"depot\"].edge_index = torch.tensor([\n",
    "        [i for i in range(len(attr))],\n",
    "        [0 for _ in range(len(attr))]\n",
    "    ], dtype=torch.long)\n",
    "    new_graph[\"stops\", \"remember\", \"stops\"].edge_index = y.nonzero().t().contiguous()\n",
    "    new_graph[\"depot\"].pos = pos_depot\n",
    "    new_graph[\"stops\"].pos = pos_stops\n",
    "    data_list.append(new_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  depot={\n",
      "    x=[1, 8],\n",
      "    pos=[1, 2],\n",
      "  },\n",
      "  stops={\n",
      "    x=[100, 8],\n",
      "    pos=[100, 2],\n",
      "  },\n",
      "  (stops, route, stops)={ edge_index=[2, 495] },\n",
      "  (depot, departs, stops)={ edge_index=[2, 100] },\n",
      "  (stops, return, depot)={ edge_index=[2, 100] },\n",
      "  (stops, remember, stops)={ edge_index=[2, 649] }\n",
      ")\n",
      "HeteroData(\n",
      "  depot={\n",
      "    x=[1, 8],\n",
      "    pos=[1, 2],\n",
      "  },\n",
      "  stops={\n",
      "    x=[100, 8],\n",
      "    pos=[100, 2],\n",
      "  },\n",
      "  (stops, route, stops)={ edge_index=[2, 495] },\n",
      "  (depot, departs, stops)={ edge_index=[2, 100] },\n",
      "  (stops, return, depot)={ edge_index=[2, 100] },\n",
      "  (stops, remember, stops)={ edge_index=[2, 628] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(data_list[0])\n",
    "print(data_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 8])\n"
     ]
    }
   ],
   "source": [
    "print(data_list[0][\"stops\"].x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_edge_labels(graph):\n",
    "    transform = T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False, edge_types=[\"route\", \"remember\"])\n",
    "    return transform(graph)\n",
    "\n",
    "labeled_graphs = [add_edge_labels(graph) for graph in data_list]\n",
    "shuffle(labeled_graphs)\n",
    "\n",
    "train_size = [g[0] for g in labeled_graphs]\n",
    "val_size = [g[1] for g in labeled_graphs]\n",
    "test_size = [g[2] for g in labeled_graphs]\n",
    "\n",
    "train_loader = DataLoader(train_size, batch_size=100, shuffle=True)\n",
    "val_loader = DataLoader(val_size, batch_size=100, shuffle=False)\n",
    "test_loader = DataLoader(test_size, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroDataBatch(\n",
      "  depot={\n",
      "    x=[100, 8],\n",
      "    pos=[100, 2],\n",
      "    batch=[100],\n",
      "    ptr=[101],\n",
      "  },\n",
      "  stops={\n",
      "    x=[10000, 8],\n",
      "    pos=[10000, 2],\n",
      "    batch=[10000],\n",
      "    ptr=[101],\n",
      "  },\n",
      "  (stops, route, stops)={\n",
      "    edge_index=[2, 94450],\n",
      "    edge_label=[10388],\n",
      "    edge_label_index=[2, 10388],\n",
      "  },\n",
      "  (depot, departs, stops)={ edge_index=[2, 10000] },\n",
      "  (stops, return, depot)={ edge_index=[2, 10000] },\n",
      "  (stops, remember, stops)={\n",
      "    edge_index=[2, 51522],\n",
      "    edge_label=[5618],\n",
      "    edge_label_index=[2, 5618],\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in test_loader:\n",
    "    if cnt == 0:\n",
    "        print(i)\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers):\n",
    "        super().__init__()\n",
    "        # self.convs = torch.nn.ModuleList()\n",
    "        # for _ in range(num_layers):\n",
    "        #     conv = HeteroConv({\n",
    "        #         (\"stops\", \"route\", \"stops\") : GCNConv(-1, hidden_channels),\n",
    "        #         (\"depot\", \"departs\", \"stops\") : SAGEConv((-1, -1), hidden_channels),\n",
    "        #         (\"stops\", \"return\", \"depot\"): SAGEConv((-1, -1), hidden_channels),\n",
    "        #         (\"stops\", \"remember\", \"stops\") : GCNConv(-1, hidden_channels)\n",
    "        #     })\n",
    "        #     self.convs.append(conv)\n",
    "        self.linear_input_s = Linear(in_channels, 2 * in_channels)\n",
    "        self.linear_input_d = Linear(in_channels, 2 * in_channels)\n",
    "        self.lin_s = Linear(hidden_channels, out_channels)\n",
    "        self.lin_d = Linear(hidden_channels, out_channels)\n",
    "        self.lin_combo = Linear(3 * hidden_channels, hidden_channels)\n",
    "        self.lin_combo2 = Linear(3 * hidden_channels, hidden_channels)\n",
    "        self.conv_route1 = GCNConv(2 * in_channels, hidden_channels)\n",
    "        self.conv_route2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv_neighbor1 = GCNConv(2 * in_channels, hidden_channels)\n",
    "        self.conv_neighbor2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv_depotout1 = SAGEConv((2 * in_channels, 2 * in_channels), hidden_channels)\n",
    "        self.conv_depotout2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv_depotin1 = SAGEConv((2 * in_channels, 2 * in_channels), hidden_channels)\n",
    "        self.conv_depotin2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.activation = nn.RReLU()\n",
    "\n",
    "\n",
    "    def encode(self, x_dict, edge_index_dict):\n",
    "        # for conv in self.convs:\n",
    "        #     x_dict = conv(x_dict, edge_index_dict)\n",
    "        #     x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "        #     x_dict[\"stops\"] = self.lin(x_dict[\"stops\"])\n",
    "        #     x_dict[\"depot\"] = self.lin(x_dict[\"depot\"])\n",
    "        x_r0 = self.linear_input_s(x_dict[\"stops\"])\n",
    "        x_d0 = self.linear_input_d(x_dict[\"depot\"])\n",
    "\n",
    "        # STAGE 1\n",
    "        x_r = self.activation(self.conv_route1(x_r0, edge_index_dict[\"stops\", \"route\", \"stops\"]))\n",
    "        x_n = self.activation(self.conv_neighbor1(x_r0, edge_index_dict[\"stops\", \"remember\", \"stops\"]))\n",
    "        x_d1= self.conv_depotout1((x_d0, x_r0), edge_index_dict[\"depot\", \"departs\", \"stops\"])\n",
    "        x_d2 = self.conv_depotin1((x_r0, x_d0), edge_index_dict[\"stops\", \"return\", \"depot\"])\n",
    "        x_stops = self.lin_combo(torch.hstack([x_r, x_n, x_d1]))\n",
    "        x_depot = x_d2\n",
    "\n",
    "        # STAGE 2\n",
    "        # x_r = self.conv_route2(x_stops, edge_index_dict[\"stops\", \"route\", \"stops\"])\n",
    "        # x_n = self.conv_neighbor2(x_stops, edge_index_dict[\"stops\", \"remember\", \"stops\"])\n",
    "        # x_d1= self.conv_depotout2((x_depot,x_stops), edge_index_dict[\"depot\", \"departs\", \"stops\"])\n",
    "        # x_d2 = self.conv_depotin2((x_stops,x_depot), edge_index_dict[\"stops\", \"return\", \"depot\"])\n",
    "        # x_stops = self.lin_combo2(torch.hstack([x_r, x_n, x_d1]))\n",
    "        # x_depot = x_d2\n",
    "\n",
    "        # Final stage\n",
    "        x_stops = self.activation(self.lin_s(x_stops))\n",
    "        x_depot = self.activation(self.lin_d(x_depot))\n",
    "\n",
    "        x_dict[\"stops\"] = x_stops\n",
    "        x_dict[\"depot\"] = x_depot\n",
    "        return x_dict\n",
    "\n",
    "    def decode(self, z_dict, edge_label_index, edge_type=(\"stops\", \"remember\", \"stops\")):\n",
    "        # from ChatGPT\n",
    "        # Decode edges of the specific type (\"stops\", \"remember\", \"stops\") by computing dot products\n",
    "        src, dst = edge_label_index[0], edge_label_index[1]\n",
    "        z_src = z_dict[src]\n",
    "        z_dst = z_dict[dst]\n",
    "        # Compute the dot product as a prediction score for each edge\n",
    "        return (z_src * z_dst).sum(dim=-1)\n",
    "    \n",
    "    def forward(self, x_dict, edge_index_dict, edge_label_index, edge_type=(\"stops\", \"remember\", \"stops\")):\n",
    "        # Encode node embeddings\n",
    "        z_dict = self.encode(x_dict, edge_index_dict)\n",
    "        # Decode the embeddings for the given edge type\n",
    "        return self.decode(z_dict[\"stops\"], edge_label_index, edge_type)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['depot', 'stops'],\n",
       " [('stops', 'route', 'stops'),\n",
       "  ('depot', 'departs', 'stops'),\n",
       "  ('stops', 'return', 'depot'),\n",
       "  ('stops', 'remember', 'stops')])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[0].metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ed = Net(data_list[0][\"stops\"].num_features, 60, 80, 2).to(device)\n",
    "optimizer = torch.optim.Adam(params=model_ed.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# criterion = torch.nn.BCEWithLogitsLoss(weight=torch.tensor([10]))\n",
    "# model_ed = to_hetero(model_ed, data_list[0].metadata(), aggr='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "    x_dict = {\"stops\": labeled_graphs[0][0][\"stops\"].x, \"depot\": labeled_graphs[0][0][\"depot\"].x}\n",
    "    edge_index_dict = {(\"stops\", \"route\", \"stops\"): labeled_graphs[0][0][\"stops\", \"route\", \"stops\"].edge_index, \n",
    "                           (\"depot\", \"departs\", \"stops\"): labeled_graphs[0][0][\"depot\", \"departs\", \"stops\"].edge_index, \n",
    "                           (\"stops\", \"return\", \"depot\"): labeled_graphs[0][0][\"stops\", \"return\", \"depot\"].edge_index, \n",
    "                           (\"stops\", \"remember\", \"stops\"): labeled_graphs[0][0][\"stops\", \"remember\", \"stops\"].edge_index}\n",
    "    edge_label_index = labeled_graphs[0][0][\"stops\", \"remember\", \"stops\"].edge_label_index\n",
    "    out = model_ed(x_dict, edge_index_dict, edge_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader, epoch):\n",
    "    model_ed.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    if epoch == 8:\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = 0.001\n",
    "\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        x_dict = {\"stops\": batch[\"stops\"].x, \"depot\": batch[\"depot\"].x}\n",
    "        edge_index_dict = {(\"stops\", \"route\", \"stops\"): batch[\"stops\", \"route\", \"stops\"].edge_index, \n",
    "                           (\"depot\", \"departs\", \"stops\"): batch[\"depot\", \"departs\", \"stops\"].edge_index, \n",
    "                           (\"stops\", \"return\", \"depot\"): batch[\"stops\", \"return\", \"depot\"].edge_index, \n",
    "                           (\"stops\", \"remember\", \"stops\"): batch[\"stops\", \"remember\", \"stops\"].edge_index}\n",
    "        z = model_ed.encode(x_dict, edge_index_dict)\n",
    "\n",
    "        # We perform a new round of negative sampling for every training epoch:\n",
    "        neg_edge_index_remember = negative_sampling(\n",
    "            edge_index=batch[\"stops\", \"remember\", \"stops\"].edge_label_index, num_nodes=batch[\"stops\"].num_nodes,\n",
    "            num_neg_samples=batch[\"stops\", \"remember\", \"stops\"].edge_label_index.size(1), method='sparse')\n",
    "\n",
    "        # Concat positive and negative edge indices.\n",
    "        edge_label_index_remember = torch.cat(\n",
    "            [batch[\"stops\", \"remember\", \"stops\"].edge_label_index, neg_edge_index_remember],\n",
    "            dim=-1,\n",
    "        )\n",
    "        # Label for positive edges: 1, for negative edges: 0.\n",
    "        edge_label_remember = torch.cat([\n",
    "            batch[\"stops\", \"remember\", \"stops\"].edge_label,\n",
    "            batch[\"stops\", \"remember\", \"stops\"].edge_label.new_zeros(neg_edge_index_remember.size(1))\n",
    "        ], dim=0)\n",
    "\n",
    "        # We perform a new round of negative sampling for every training epoch:\n",
    "        neg_edge_index_route = negative_sampling(\n",
    "            edge_index=batch[\"stops\", \"route\", \"stops\"].edge_label_index, num_nodes=batch[\"stops\"].num_nodes,\n",
    "            num_neg_samples=batch[\"stops\", \"route\", \"stops\"].edge_label_index.size(1), method='sparse')\n",
    "\n",
    "        # Concat positive and negative edge indices.\n",
    "        edge_label_index_route = torch.cat(\n",
    "            [batch[\"stops\", \"route\", \"stops\"].edge_label_index, neg_edge_index_route],\n",
    "            dim=-1,\n",
    "        )\n",
    "        # Label for positive edges: 1, for negative edges: 0.\n",
    "        edge_label_route = torch.cat([\n",
    "            batch[\"stops\", \"route\", \"stops\"].edge_label,\n",
    "            batch[\"stops\", \"route\", \"stops\"].edge_label.new_zeros(neg_edge_index_route.size(1))\n",
    "        ], dim=0)\n",
    "\n",
    "        # Note: The model is trained in a supervised manner using the given\n",
    "        # `edge_label_index` and `edge_label` targets.\n",
    "        # print(z)\n",
    "        # print(z.size())\n",
    "        out_remember = model_ed.decode(z[\"stops\"], edge_label_index_remember).view(-1)\n",
    "        out_route = model_ed.decode(z[\"stops\"], edge_label_index_route).view(-1)\n",
    "        route_loss = criterion(out_route, edge_label_route)\n",
    "        remember_loss = criterion(out_remember, edge_label_remember)\n",
    "        sub_loss = route_loss + remember_loss\n",
    "        sub_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += sub_loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model_ed.eval()\n",
    "    all_out = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in loader:\n",
    "        x_dict = {\"stops\": batch[\"stops\"].x, \"depot\": batch[\"depot\"].x}\n",
    "        edge_index_dict = {(\"stops\", \"route\", \"stops\"): batch[\"stops\", \"route\", \"stops\"].edge_index, \n",
    "                           (\"depot\", \"departs\", \"stops\"): batch[\"depot\", \"departs\", \"stops\"].edge_index, \n",
    "                           (\"stops\", \"return\", \"depot\"): batch[\"stops\", \"return\", \"depot\"].edge_index, \n",
    "                           (\"stops\", \"remember\", \"stops\"): batch[\"stops\", \"remember\", \"stops\"].edge_index}\n",
    "        z = model_ed.encode(x_dict, edge_index_dict)\n",
    "        out = model_ed.decode(z[\"stops\"], batch[\"stops\", \"remember\", \"stops\"].edge_label_index).view(-1).sigmoid()\n",
    "        all_out.append(out.cpu().numpy())\n",
    "        all_labels.append(batch[\"stops\", \"remember\", \"stops\"].edge_label.cpu().numpy())\n",
    "\n",
    "    all_out = np.concatenate(all_out)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return roc_auc_score(all_labels, all_out), [all_labels], [all_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1813356.1019, Val: 0.5282, Test: 0.5270\n",
      "Epoch: 002, Loss: 24678.3794, Val: 0.5198, Test: 0.5193\n",
      "Epoch: 003, Loss: 3965.6048, Val: 0.5128, Test: 0.5122\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m             plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m val_auc, _, _ \u001b[38;5;241m=\u001b[39m test(val_loader)\n\u001b[0;32m     31\u001b[0m test_auc, all_labels, all_out \u001b[38;5;241m=\u001b[39m test(test_loader)\n",
      "Cell \u001b[1;32mIn[19], line 35\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(loader, epoch)\u001b[0m\n\u001b[0;32m     29\u001b[0m edge_label_remember \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\n\u001b[0;32m     30\u001b[0m     batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstops\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mremember\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstops\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39medge_label,\n\u001b[0;32m     31\u001b[0m     batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstops\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mremember\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstops\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39medge_label\u001b[38;5;241m.\u001b[39mnew_zeros(neg_edge_index_remember\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     32\u001b[0m ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# We perform a new round of negative sampling for every training epoch:\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m neg_edge_index_route \u001b[38;5;241m=\u001b[39m \u001b[43mnegative_sampling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstops\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroute\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstops\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_label_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstops\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_neg_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstops\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroute\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstops\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_label_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msparse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Concat positive and negative edge indices.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m edge_label_index_route \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[0;32m     41\u001b[0m     [batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstops\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroute\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstops\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39medge_label_index, neg_edge_index_route],\n\u001b[0;32m     42\u001b[0m     dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     43\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch_geometric\\utils\\_negative_sampling.py:103\u001b[0m, in \u001b[0;36mnegative_sampling\u001b[1;34m(edge_index, num_nodes, num_neg_samples, method, force_undirected)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):  \u001b[38;5;66;03m# Number of tries to sample negative indices.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     rnd \u001b[38;5;241m=\u001b[39m sample(population, sample_size, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 103\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m neg_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m         mask \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misin(rnd, neg_idx\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\arraysetops.py:890\u001b[0m, in \u001b[0;36misin\u001b[1;34m(element, test_elements, assume_unique, invert, kind)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    770\u001b[0m \u001b[38;5;124;03mCalculates ``element in test_elements``, broadcasting over `element` only.\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;124;03mReturns a boolean array of the same shape as `element` that is True\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;124;03m       [ True, False]])\u001b[39;00m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    889\u001b[0m element \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(element)\n\u001b[1;32m--> 890\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43min1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_elements\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massume_unique\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massume_unique\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m            \u001b[49m\u001b[43minvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minvert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(element\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\arraysetops.py:738\u001b[0m, in \u001b[0;36min1d\u001b[1;34m(ar1, ar2, assume_unique, invert, kind)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;66;03m# Otherwise use sorting\u001b[39;00m\n\u001b[0;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m assume_unique:\n\u001b[1;32m--> 738\u001b[0m     ar1, rev_idx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    739\u001b[0m     ar2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(ar2)\n\u001b[0;32m    741\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((ar1, ar2))\n",
      "File \u001b[1;32mc:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[0;32m    272\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jotape\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\arraysetops.py:333\u001b[0m, in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[0;32m    330\u001b[0m optional_indices \u001b[38;5;241m=\u001b[39m return_index \u001b[38;5;129;01mor\u001b[39;00m return_inverse\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optional_indices:\n\u001b[1;32m--> 333\u001b[0m     perm \u001b[38;5;241m=\u001b[39m \u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmergesort\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquicksort\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar[perm]\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train/Test Loop\n",
    "best_val_auc = final_test_auc = 0\n",
    "loss_collection = []\n",
    "val_auc_collection = []\n",
    "last_epoch = 30\n",
    "for epoch in range(1, last_epoch + 1):\n",
    "    if epoch > 5:\n",
    "        rate_of_change_in_loss_e = [loss_collection[-i]-loss_collection[-i-1] for i in range(1,3)]\n",
    "        rate_of_change_in_val_e = [val_auc_collection[-i]-val_auc_collection[-i-1] for i in range(1,3)]\n",
    "        rate_of_change_in_loss = sum(rate_of_change_in_loss_e)/len(rate_of_change_in_loss_e)\n",
    "        rate_of_change_in_val = sum(rate_of_change_in_val_e)/len(rate_of_change_in_val_e)\n",
    "        if rate_of_change_in_loss * -1 > 0:\n",
    "            if rate_of_change_in_val * -1 > 0:\n",
    "                y_true = all_labels[0]\n",
    "                y_pred = all_out[0]\n",
    "\n",
    "                fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "                roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "                plt.figure(1)\n",
    "                plt.plot([0, 1], [0, 1], 'k-')\n",
    "                plt.plot(fpr, tpr, label='CNN(area = {:.3f})'.format(roc_auc))\n",
    "                plt.xlabel('False positive rate')\n",
    "                plt.ylabel('True positive rate')\n",
    "                plt.title('ROC curve GCN network\\n2 layers, Hidden width: 60, Output: 80')\n",
    "                plt.legend(loc='best')\n",
    "                plt.show()\n",
    "                break\n",
    "    loss = train(train_loader, epoch)\n",
    "    val_auc, _, _ = test(val_loader)\n",
    "    test_auc, all_labels, all_out = test(test_loader)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "    loss_collection.append(loss)\n",
    "    val_auc_collection.append(val_auc)\n",
    "    if epoch == last_epoch:\n",
    "        y_true = all_labels[0]\n",
    "        y_pred = all_out[0]\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "        roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "        plt.figure(1)\n",
    "        plt.plot([0, 1], [0, 1], 'k-')\n",
    "        plt.plot(fpr, tpr, label='CNN(area = {:.3f})'.format(roc_auc))\n",
    "        plt.xlabel('False positive rate')\n",
    "        plt.ylabel('True positive rate')\n",
    "        plt.title('ROC curve GCN network\\n2 layers, Hidden width: 60, Output: 80')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convs.0.convs.<stops___route___stops>.bias True\n",
      "convs.0.convs.<stops___route___stops>.lin.weight True\n",
      "convs.0.convs.<depot___departs___stops>.lin_l.weight True\n",
      "convs.0.convs.<depot___departs___stops>.lin_l.bias True\n",
      "convs.0.convs.<depot___departs___stops>.lin_r.weight True\n",
      "convs.0.convs.<stops___return___depot>.lin_l.weight True\n",
      "convs.0.convs.<stops___return___depot>.lin_l.bias True\n",
      "convs.0.convs.<stops___return___depot>.lin_r.weight True\n",
      "convs.0.convs.<stops___remember___stops>.bias True\n",
      "convs.0.convs.<stops___remember___stops>.lin.weight True\n",
      "convs.1.convs.<stops___route___stops>.bias True\n",
      "convs.1.convs.<stops___route___stops>.lin.weight True\n",
      "convs.1.convs.<depot___departs___stops>.lin_l.weight True\n",
      "convs.1.convs.<depot___departs___stops>.lin_l.bias True\n",
      "convs.1.convs.<depot___departs___stops>.lin_r.weight True\n",
      "convs.1.convs.<stops___return___depot>.lin_l.weight True\n",
      "convs.1.convs.<stops___return___depot>.lin_l.bias True\n",
      "convs.1.convs.<stops___return___depot>.lin_r.weight True\n",
      "convs.1.convs.<stops___remember___stops>.bias True\n",
      "convs.1.convs.<stops___remember___stops>.lin.weight True\n",
      "lin.weight True\n",
      "lin.bias True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_ed.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
