{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Data, HeteroData\n",
    "from torch_geometric.nn import knn_graph, HeteroConv, GCNConv, Linear, SAGEConv, to_hetero\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "# from torch_geometric.nn import GCNConv, Linear\n",
    "from torch_geometric.utils import negative_sampling, dropout_edge\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from random import sample\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import copy\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_files_list = [\"./export/\"+f for f in os.listdir(\"./export\") ]\n",
    "instance_dict = {}\n",
    "instance_additionals = {}\n",
    "set_sizes_dict = {}\n",
    "for dir_str in data_files_list:\n",
    "    with open(dir_str, 'r') as text_file:\n",
    "        cnt = 0\n",
    "        instance = \"\"\n",
    "        for line in text_file:\n",
    "            if cnt < 9:\n",
    "                if cnt == 0:\n",
    "                    instance = line.split()[0]\n",
    "                    instance_dict[instance] = []\n",
    "                if cnt == 4:\n",
    "                    instance_additionals[instance] = []\n",
    "                    split_line = line.split()\n",
    "                    instance_additionals[instance].append([int(i) for i in split_line])\n",
    "                cnt += 1\n",
    "                continue\n",
    "            split_line = line.split()\n",
    "            instance_dict[instance].append([int(i) for i in split_line])\n",
    "        text_file.close()\n",
    "\n",
    "ng_dict = {}\n",
    "cnt = -1\n",
    "with open(\"ng_outs.csv\", 'r') as text_file:\n",
    "    for line in text_file:\n",
    "        if cnt < 2:\n",
    "            cnt += 1\n",
    "            continue\n",
    "        raw_line = line.strip()\n",
    "        split_line_list = raw_line.split(sep=\";\")\n",
    "        instance = split_line_list[3]\n",
    "        if instance not in ng_dict:\n",
    "            # ng_dict[instance] = [[0 for i in range(101)]]\n",
    "            ng_dict[instance] = []\n",
    "        ng_dict[instance].append([int(i) for i in split_line_list[5:-1]])\n",
    "        set_sizes_dict[instance] = sum(sum(i) for i in ng_dict[instance])\n",
    "        if len(split_line_list[5:-1]) != 100:\n",
    "            print(\"case found for instance \"+instance)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_difficulty = [(k,v) for k,v in set_sizes_dict.items()]\n",
    "ordered_difficulty = sorted(ordered_difficulty, key=lambda tup: tup[1], reverse=True)\n",
    "instances_to_add = [ordered_difficulty[i][0] for i in range(0,1000)]\n",
    "# instances_remaining = [ordered_difficulty[i][0] for i in range(1000,len(ordered_difficulty))]\n",
    "# easy_instances = sample(instances_remaining, 2000)\n",
    "instance_list = instances_to_add #+ easy_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files_list = [\"./export2/\"+f for f in os.listdir(\"./export2\") ]\n",
    "\n",
    "for dir_str in data_files_list:\n",
    "    with open(dir_str, 'r') as text_file:\n",
    "        cnt = 0\n",
    "        instance = \"\"\n",
    "        for line in text_file:\n",
    "            if cnt < 9:\n",
    "                if cnt == 0:\n",
    "                    instance = line.split()[0]\n",
    "                    instance_dict[instance] = []\n",
    "                if cnt == 4:\n",
    "                    instance_additionals[instance] = []\n",
    "                    split_line = line.split()\n",
    "                    instance_additionals[instance].append([int(i) for i in split_line])\n",
    "                cnt += 1\n",
    "                continue\n",
    "            split_line = line.split()\n",
    "            instance_dict[instance].append([int(i) for i in split_line])\n",
    "        text_file.close()\n",
    "\n",
    "cnt = -1\n",
    "with open(\"ng_outs_JLA.csv\", 'r') as text_file:\n",
    "    for line in text_file:\n",
    "        if cnt < 1:\n",
    "            cnt += 1\n",
    "            continue\n",
    "        raw_line = line.strip()\n",
    "        split_line_list = raw_line.split(sep=\";\")\n",
    "        instance = split_line_list[3]\n",
    "        if instance not in ng_dict:\n",
    "            # ng_dict[instance] = [[0 for i in range(101)]]\n",
    "            ng_dict[instance] = []\n",
    "        ng_dict[instance].append([int(i) for i in split_line_list[5:]])\n",
    "        set_sizes_dict[instance] = sum(sum(i) for i in ng_dict[instance])\n",
    "        # add new instances\n",
    "        instance_list.append(instance)\n",
    "        if len(split_line_list[5:]) != 100:\n",
    "            print(\"case found for instance \"+instance)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "# aux_data_list = []\n",
    "\n",
    "\n",
    "for instance_name in instance_list:\n",
    "    instance = instance_dict[instance_name]\n",
    "    attr = []\n",
    "    max_demand = -1\n",
    "    capacity = instance_additionals[instance_name][0][1]\n",
    "    for i in instance[1:]:\n",
    "        demand = i[3]\n",
    "        if demand > max_demand:\n",
    "            max_demand = demand\n",
    "        attr.append(i[1:] + instance_additionals[instance_name][0])\n",
    "    \n",
    "    y = torch.tensor(ng_dict[instance_name], dtype=torch.float)\n",
    "    # x = nn.functional.normalize( torch.tensor(attr, dtype=torch.float) )\n",
    "    x_stops = torch.tensor(attr, dtype=torch.float)\n",
    "    x_depot = torch.tensor([instance[0][1:] + instance_additionals[instance_name][0]], dtype=torch.float)\n",
    "    pos_depot = [[instance[0][1], instance[0][2]]]\n",
    "    pos_stops = []\n",
    "    tw_sets_dict = {}\n",
    "    for i in instance[1:]:\n",
    "        pos_stops.append([i[1], i[2]])\n",
    "    pos_stops = torch.tensor(pos_stops, dtype=torch.double)\n",
    "    pos_depot = torch.tensor(pos_depot, dtype=torch.double)\n",
    "    route_neighbors = min(max(capacity//max_demand, 15), 25)\n",
    "    edge_index = knn_graph(pos_stops[1:], route_neighbors)\n",
    "    new_graph = HeteroData()\n",
    "    new_graph[\"depot\"].x = x_depot\n",
    "    new_graph[\"stops\"].x = x_stops\n",
    "    new_graph[\"stops\", \"route\", \"stops\"].edge_index = edge_index\n",
    "    new_graph[\"depot\", \"departs\", \"stops\"].edge_index = torch.tensor([\n",
    "        [0 for _ in range(len(attr))],\n",
    "        [i for i in range(len(attr))]\n",
    "    ], dtype=torch.long)\n",
    "    new_graph[\"stops\", \"return\", \"depot\"].edge_index = torch.tensor([\n",
    "        [i for i in range(len(attr))],\n",
    "        [0 for _ in range(len(attr))]\n",
    "    ], dtype=torch.long)\n",
    "    new_graph[\"stops\", \"remember\", \"stops\"].edge_index = y.nonzero().t().contiguous()\n",
    "    new_graph[\"depot\"].pos = pos_depot\n",
    "    new_graph[\"stops\"].pos = pos_stops\n",
    "    data_list.append(new_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  depot={\n",
      "    x=[1, 8],\n",
      "    pos=[1, 2],\n",
      "  },\n",
      "  stops={\n",
      "    x=[100, 8],\n",
      "    pos=[100, 2],\n",
      "  },\n",
      "  (stops, route, stops)={ edge_index=[2, 495] },\n",
      "  (depot, departs, stops)={ edge_index=[2, 100] },\n",
      "  (stops, return, depot)={ edge_index=[2, 100] },\n",
      "  (stops, remember, stops)={ edge_index=[2, 649] }\n",
      ")\n",
      "HeteroData(\n",
      "  depot={\n",
      "    x=[1, 8],\n",
      "    pos=[1, 2],\n",
      "  },\n",
      "  stops={\n",
      "    x=[100, 8],\n",
      "    pos=[100, 2],\n",
      "  },\n",
      "  (stops, route, stops)={ edge_index=[2, 495] },\n",
      "  (depot, departs, stops)={ edge_index=[2, 100] },\n",
      "  (stops, return, depot)={ edge_index=[2, 100] },\n",
      "  (stops, remember, stops)={ edge_index=[2, 628] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(data_list[0])\n",
    "print(data_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 8])\n"
     ]
    }
   ],
   "source": [
    "print(data_list[0][\"stops\"].x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_edge_labels(graph):\n",
    "    transform = T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False, edge_types=[\"route\", \"remember\"])\n",
    "    return transform(graph)\n",
    "\n",
    "labeled_graphs = [add_edge_labels(graph) for graph in data_list]\n",
    "shuffle(labeled_graphs)\n",
    "\n",
    "train_size = [g[0] for g in labeled_graphs]\n",
    "val_size = [g[1] for g in labeled_graphs]\n",
    "test_size = [g[2] for g in labeled_graphs]\n",
    "\n",
    "train_loader = DataLoader(train_size, batch_size=100, shuffle=True)\n",
    "val_loader = DataLoader(val_size, batch_size=100, shuffle=False)\n",
    "test_loader = DataLoader(test_size, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroDataBatch(\n",
      "  depot={\n",
      "    x=[100, 8],\n",
      "    pos=[100, 2],\n",
      "    batch=[100],\n",
      "    ptr=[101],\n",
      "  },\n",
      "  stops={\n",
      "    x=[10000, 8],\n",
      "    pos=[10000, 2],\n",
      "    batch=[10000],\n",
      "    ptr=[101],\n",
      "  },\n",
      "  (stops, route, stops)={\n",
      "    edge_index=[2, 94450],\n",
      "    edge_label=[10388],\n",
      "    edge_label_index=[2, 10388],\n",
      "  },\n",
      "  (depot, departs, stops)={ edge_index=[2, 10000] },\n",
      "  (stops, return, depot)={ edge_index=[2, 10000] },\n",
      "  (stops, remember, stops)={\n",
      "    edge_index=[2, 51522],\n",
      "    edge_label=[5618],\n",
      "    edge_label_index=[2, 5618],\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in test_loader:\n",
    "    if cnt == 0:\n",
    "        print(i)\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers):\n",
    "        super().__init__()\n",
    "        # self.convs = torch.nn.ModuleList()\n",
    "        # for _ in range(num_layers):\n",
    "        #     conv = HeteroConv({\n",
    "        #         (\"stops\", \"route\", \"stops\") : GCNConv(-1, hidden_channels),\n",
    "        #         (\"depot\", \"departs\", \"stops\") : SAGEConv((-1, -1), hidden_channels),\n",
    "        #         (\"stops\", \"return\", \"depot\"): SAGEConv((-1, -1), hidden_channels),\n",
    "        #         (\"stops\", \"remember\", \"stops\") : GCNConv(-1, hidden_channels)\n",
    "        #     })\n",
    "        #     self.convs.append(conv)\n",
    "        self.lin_s = Linear(hidden_channels, out_channels)\n",
    "        self.lin_d = Linear(hidden_channels, out_channels)\n",
    "        self.lin_combo = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin_combo2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.conv_route1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv_route2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv_neighbor1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv_neighbor2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv_depotout1 = SAGEConv((in_channels, in_channels), hidden_channels)\n",
    "        self.conv_depotout2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv_depotin1 = SAGEConv((in_channels, in_channels), hidden_channels)\n",
    "        self.conv_depotin2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "\n",
    "\n",
    "    def encode(self, x_dict, edge_index_dict):\n",
    "        # for conv in self.convs:\n",
    "        #     x_dict = conv(x_dict, edge_index_dict)\n",
    "        #     x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "        #     x_dict[\"stops\"] = self.lin(x_dict[\"stops\"])\n",
    "        #     x_dict[\"depot\"] = self.lin(x_dict[\"depot\"])\n",
    "        # STAGE 1\n",
    "        x_r = self.conv_route1(x_dict[\"stops\"], edge_index_dict[\"stops\", \"route\", \"stops\"]).relu()\n",
    "        x_n = self.conv_neighbor1(x_dict[\"stops\"], edge_index_dict[\"stops\", \"remember\", \"stops\"]).relu()\n",
    "        x_d1= self.conv_depotout1((x_dict[\"depot\"], x_dict[\"stops\"]), edge_index_dict[\"depot\", \"departs\", \"stops\"]).relu()\n",
    "        x_d2 = self.conv_depotin1((x_dict[\"stops\"], x_dict[\"depot\"]), edge_index_dict[\"stops\", \"return\", \"depot\"]).relu()\n",
    "        x_stops = self.lin_combo(torch.cat([x_r, x_n, x_d1]))\n",
    "        x_depot = x_d2\n",
    "\n",
    "        # STAGE 2\n",
    "        # x_r = self.conv_route2(x_stops, edge_index_dict[\"stops\", \"route\", \"stops\"]).relu()\n",
    "        # x_n = self.conv_neighbor2(x_stops, edge_index_dict[\"stops\", \"remember\", \"stops\"]).relu()\n",
    "        # x_d1= self.conv_depotout2((x_depot,x_stops), edge_index_dict[\"depot\", \"departs\", \"stops\"]).relu()\n",
    "        # x_d2 = self.conv_depotin2((x_stops,x_depot), edge_index_dict[\"stops\", \"return\", \"depot\"]).relu()\n",
    "        # x_stops = self.lin_combo2(torch.cat([x_r, x_n, x_d1]))\n",
    "        # x_depot = x_d2\n",
    "\n",
    "        # Final stage\n",
    "        # x_stops = self.lin_s(x_stops)\n",
    "        # x_depot = self.lin_d(x_depot)\n",
    "\n",
    "        x_dict[\"stops\"] = x_stops\n",
    "        x_dict[\"depot\"] = x_depot\n",
    "        return x_dict\n",
    "\n",
    "    def decode(self, z_dict, edge_label_index, edge_type=(\"stops\", \"remember\", \"stops\")):\n",
    "        # from ChatGPT\n",
    "        # Decode edges of the specific type (\"stops\", \"remember\", \"stops\") by computing dot products\n",
    "        src, dst = edge_label_index[0], edge_label_index[1]\n",
    "        z_src = z_dict[src]\n",
    "        z_dst = z_dict[dst]\n",
    "        # Compute the dot product as a prediction score for each edge\n",
    "        return (z_src * z_dst).sum(dim=-1)\n",
    "    \n",
    "    def forward(self, x_dict, edge_index_dict, edge_label_index, edge_type=(\"stops\", \"remember\", \"stops\")):\n",
    "        # Encode node embeddings\n",
    "        z_dict = self.encode(x_dict, edge_index_dict)\n",
    "        # Decode the embeddings for the given edge type\n",
    "        return self.decode(z_dict[\"stops\"], edge_label_index, edge_type)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['depot', 'stops'],\n",
       " [('stops', 'route', 'stops'),\n",
       "  ('depot', 'departs', 'stops'),\n",
       "  ('stops', 'return', 'depot'),\n",
       "  ('stops', 'remember', 'stops')])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[0].metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ed = Net(data_list[0][\"stops\"].num_features, 64, 32, 2).to(device)\n",
    "optimizer = torch.optim.Adam(params=model_ed.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# criterion = torch.nn.BCEWithLogitsLoss(weight=torch.tensor([10]))\n",
    "# model_ed = to_hetero(model_ed, data_list[0].metadata(), aggr='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "    x_dict = {\"stops\": labeled_graphs[0][0][\"stops\"].x, \"depot\": labeled_graphs[0][0][\"depot\"].x}\n",
    "    edge_index_dict = {(\"stops\", \"route\", \"stops\"): labeled_graphs[0][0][\"stops\", \"route\", \"stops\"].edge_index, \n",
    "                           (\"depot\", \"departs\", \"stops\"): labeled_graphs[0][0][\"depot\", \"departs\", \"stops\"].edge_index, \n",
    "                           (\"stops\", \"return\", \"depot\"): labeled_graphs[0][0][\"stops\", \"return\", \"depot\"].edge_index, \n",
    "                           (\"stops\", \"remember\", \"stops\"): labeled_graphs[0][0][\"stops\", \"remember\", \"stops\"].edge_index}\n",
    "    edge_label_index = labeled_graphs[0][0][\"stops\", \"remember\", \"stops\"].edge_label_index\n",
    "    out = model_ed(x_dict, edge_index_dict, edge_label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader, epoch):\n",
    "    model_ed.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    if epoch == 8:\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = 0.001\n",
    "\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        x_dict = {\"stops\": batch[\"stops\"].x, \"depot\": batch[\"depot\"].x}\n",
    "        edge_index_dict = {(\"stops\", \"route\", \"stops\"): batch[\"stops\", \"route\", \"stops\"].edge_index, \n",
    "                           (\"depot\", \"departs\", \"stops\"): batch[\"depot\", \"departs\", \"stops\"].edge_index, \n",
    "                           (\"stops\", \"return\", \"depot\"): batch[\"stops\", \"return\", \"depot\"].edge_index, \n",
    "                           (\"stops\", \"remember\", \"stops\"): batch[\"stops\", \"remember\", \"stops\"].edge_index}\n",
    "        z = model_ed.encode(x_dict, edge_index_dict)\n",
    "\n",
    "        # We perform a new round of negative sampling for every training epoch:\n",
    "        neg_edge_index_remember = negative_sampling(\n",
    "            edge_index=batch[\"stops\", \"remember\", \"stops\"].edge_label_index, num_nodes=batch[\"stops\"].num_nodes,\n",
    "            num_neg_samples=batch[\"stops\", \"remember\", \"stops\"].edge_label_index.size(1), method='sparse')\n",
    "\n",
    "        # Concat positive and negative edge indices.\n",
    "        edge_label_index_remember = torch.cat(\n",
    "            [batch[\"stops\", \"remember\", \"stops\"].edge_label_index, neg_edge_index_remember],\n",
    "            dim=-1,\n",
    "        )\n",
    "        # Label for positive edges: 1, for negative edges: 0.\n",
    "        edge_label_remember = torch.cat([\n",
    "            batch[\"stops\", \"remember\", \"stops\"].edge_label,\n",
    "            batch[\"stops\", \"remember\", \"stops\"].edge_label.new_zeros(neg_edge_index_remember.size(1))\n",
    "        ], dim=0)\n",
    "\n",
    "        # We perform a new round of negative sampling for every training epoch:\n",
    "        neg_edge_index_route = negative_sampling(\n",
    "            edge_index=batch[\"stops\", \"route\", \"stops\"].edge_label_index, num_nodes=batch[\"stops\"].num_nodes,\n",
    "            num_neg_samples=batch[\"stops\", \"route\", \"stops\"].edge_label_index.size(1), method='sparse')\n",
    "\n",
    "        # Concat positive and negative edge indices.\n",
    "        edge_label_index_route = torch.cat(\n",
    "            [batch[\"stops\", \"route\", \"stops\"].edge_label_index, neg_edge_index_route],\n",
    "            dim=-1,\n",
    "        )\n",
    "        # Label for positive edges: 1, for negative edges: 0.\n",
    "        edge_label_route = torch.cat([\n",
    "            batch[\"stops\", \"route\", \"stops\"].edge_label,\n",
    "            batch[\"stops\", \"route\", \"stops\"].edge_label.new_zeros(neg_edge_index_route.size(1))\n",
    "        ], dim=0)\n",
    "\n",
    "        # Note: The model is trained in a supervised manner using the given\n",
    "        # `edge_label_index` and `edge_label` targets.\n",
    "        # print(z)\n",
    "        # print(z.size())\n",
    "        out_remember = model_ed.decode(z[\"stops\"], edge_label_index_remember).view(-1)\n",
    "        out_route = model_ed.decode(z[\"stops\"], edge_label_index_route).view(-1)\n",
    "        route_loss = criterion(out_route, edge_label_route)\n",
    "        remember_loss = criterion(out_remember, edge_label_remember)\n",
    "        sub_loss = route_loss + remember_loss\n",
    "        sub_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += sub_loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model_ed.eval()\n",
    "    all_out = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in loader:\n",
    "        x_dict = {\"stops\": batch[\"stops\"].x, \"depot\": batch[\"depot\"].x}\n",
    "        edge_index_dict = {(\"stops\", \"route\", \"stops\"): batch[\"stops\", \"route\", \"stops\"].edge_index, \n",
    "                           (\"depot\", \"departs\", \"stops\"): batch[\"depot\", \"departs\", \"stops\"].edge_index, \n",
    "                           (\"stops\", \"return\", \"depot\"): batch[\"stops\", \"return\", \"depot\"].edge_index, \n",
    "                           (\"stops\", \"remember\", \"stops\"): batch[\"stops\", \"remember\", \"stops\"].edge_index}\n",
    "        z = model_ed.encode(x_dict, edge_index_dict)\n",
    "        out = model_ed.decode(z[\"stops\"], batch[\"stops\", \"remember\", \"stops\"].edge_label_index).view(-1).sigmoid()\n",
    "        all_out.append(out.cpu().numpy())\n",
    "        all_labels.append(batch[\"stops\", \"remember\", \"stops\"].edge_label.cpu().numpy())\n",
    "\n",
    "    all_out = np.concatenate(all_out)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return roc_auc_score(all_labels, all_out), [all_labels], [all_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1003.5204, Val: 0.6162, Test: 0.6142\n",
      "Epoch: 002, Loss: 2.7945, Val: 0.6173, Test: 0.6149\n",
      "Epoch: 003, Loss: 2.0637, Val: 0.5985, Test: 0.5959\n",
      "Epoch: 004, Loss: 1.6007, Val: 0.6077, Test: 0.6045\n",
      "Epoch: 005, Loss: 1.3069, Val: 0.6214, Test: 0.6190\n",
      "Epoch: 006, Loss: 6.9535, Val: 0.5016, Test: 0.5016\n",
      "Epoch: 007, Loss: 1.3881, Val: 0.5010, Test: 0.5010\n",
      "Epoch: 008, Loss: 1.3830, Val: 0.5023, Test: 0.5023\n",
      "Epoch: 009, Loss: 1.3825, Val: 0.5025, Test: 0.5025\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m             plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m val_auc, _, _ \u001b[38;5;241m=\u001b[39m test(val_loader)\n\u001b[0;32m     31\u001b[0m test_auc, all_labels, all_out \u001b[38;5;241m=\u001b[39m test(test_loader)\n",
      "Cell \u001b[1;32mIn[24], line 54\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(loader, epoch)\u001b[0m\n\u001b[0;32m     45\u001b[0m edge_label_route \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\n\u001b[0;32m     46\u001b[0m     batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstops\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroute\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstops\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39medge_label,\n\u001b[0;32m     47\u001b[0m     batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstops\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroute\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstops\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39medge_label\u001b[38;5;241m.\u001b[39mnew_zeros(neg_edge_index_route\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     48\u001b[0m ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Note: The model is trained in a supervised manner using the given\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# `edge_label_index` and `edge_label` targets.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# print(z)\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# print(z.size())\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m out_remember \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_ed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstops\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_label_index_remember\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     55\u001b[0m out_route \u001b[38;5;241m=\u001b[39m model_ed\u001b[38;5;241m.\u001b[39mdecode(z[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstops\u001b[39m\u001b[38;5;124m\"\u001b[39m], edge_label_index_route)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     56\u001b[0m route_loss \u001b[38;5;241m=\u001b[39m criterion(out_route, edge_label_route)\n",
      "Cell \u001b[1;32mIn[21], line 66\u001b[0m, in \u001b[0;36mNet.decode\u001b[1;34m(self, z_dict, edge_label_index, edge_type)\u001b[0m\n\u001b[0;32m     64\u001b[0m z_dst \u001b[38;5;241m=\u001b[39m z_dict[dst]\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Compute the dot product as a prediction score for each edge\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mz_src\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mz_dst\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train/Test Loop\n",
    "best_val_auc = final_test_auc = 0\n",
    "loss_collection = []\n",
    "val_auc_collection = []\n",
    "last_epoch = 30\n",
    "for epoch in range(1, last_epoch + 1):\n",
    "    if epoch > 5:\n",
    "        rate_of_change_in_loss_e = [loss_collection[-i]-loss_collection[-i-1] for i in range(1,3)]\n",
    "        rate_of_change_in_val_e = [val_auc_collection[-i]-val_auc_collection[-i-1] for i in range(1,3)]\n",
    "        rate_of_change_in_loss = sum(rate_of_change_in_loss_e)/len(rate_of_change_in_loss_e)\n",
    "        rate_of_change_in_val = sum(rate_of_change_in_val_e)/len(rate_of_change_in_val_e)\n",
    "        if rate_of_change_in_loss * -1 > 0:\n",
    "            if rate_of_change_in_val * -1 > 0:\n",
    "                y_true = all_labels[0]\n",
    "                y_pred = all_out[0]\n",
    "\n",
    "                fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "                roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "                plt.figure(1)\n",
    "                plt.plot([0, 1], [0, 1], 'k-')\n",
    "                plt.plot(fpr, tpr, label='CNN(area = {:.3f})'.format(roc_auc))\n",
    "                plt.xlabel('False positive rate')\n",
    "                plt.ylabel('True positive rate')\n",
    "                plt.title('ROC curve GCN network\\n2 layers, Hidden width: 64, Output: 32')\n",
    "                plt.legend(loc='best')\n",
    "                plt.show()\n",
    "                break\n",
    "    loss = train(train_loader, epoch)\n",
    "    val_auc, _, _ = test(val_loader)\n",
    "    test_auc, all_labels, all_out = test(test_loader)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "    loss_collection.append(loss)\n",
    "    val_auc_collection.append(val_auc)\n",
    "    if epoch == last_epoch:\n",
    "        y_true = all_labels[0]\n",
    "        y_pred = all_out[0]\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "        roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "        plt.figure(1)\n",
    "        plt.plot([0, 1], [0, 1], 'k-')\n",
    "        plt.plot(fpr, tpr, label='CNN(area = {:.3f})'.format(roc_auc))\n",
    "        plt.xlabel('False positive rate')\n",
    "        plt.ylabel('True positive rate')\n",
    "        plt.title('ROC curve GCN network\\n2 layers, Hidden width: 64, Output: 32')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convs.0.convs.<stops___route___stops>.bias True\n",
      "convs.0.convs.<stops___route___stops>.lin.weight True\n",
      "convs.0.convs.<depot___departs___stops>.lin_l.weight True\n",
      "convs.0.convs.<depot___departs___stops>.lin_l.bias True\n",
      "convs.0.convs.<depot___departs___stops>.lin_r.weight True\n",
      "convs.0.convs.<stops___return___depot>.lin_l.weight True\n",
      "convs.0.convs.<stops___return___depot>.lin_l.bias True\n",
      "convs.0.convs.<stops___return___depot>.lin_r.weight True\n",
      "convs.0.convs.<stops___remember___stops>.bias True\n",
      "convs.0.convs.<stops___remember___stops>.lin.weight True\n",
      "convs.1.convs.<stops___route___stops>.bias True\n",
      "convs.1.convs.<stops___route___stops>.lin.weight True\n",
      "convs.1.convs.<depot___departs___stops>.lin_l.weight True\n",
      "convs.1.convs.<depot___departs___stops>.lin_l.bias True\n",
      "convs.1.convs.<depot___departs___stops>.lin_r.weight True\n",
      "convs.1.convs.<stops___return___depot>.lin_l.weight True\n",
      "convs.1.convs.<stops___return___depot>.lin_l.bias True\n",
      "convs.1.convs.<stops___return___depot>.lin_r.weight True\n",
      "convs.1.convs.<stops___remember___stops>.bias True\n",
      "convs.1.convs.<stops___remember___stops>.lin.weight True\n",
      "lin.weight True\n",
      "lin.bias True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_ed.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
