{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>First prototype: encode-decode-predict the immediate nearest neighbor</h1>\n",
    "In order to check if it's possible to learn a metric minimizer.\n",
    "Based on https://medium.com/the-modern-scientist/graph-neural-networks-series-part-3-node-embedding-36613cc967d5\n",
    "and https://machinelearningmastery.com/building-a-binary-classification-model-in-pytorch/ and suggestions of Florian Racoussier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1-NN data</h2>\n",
    "Create a simple dataset for testing the prototype. Each node is connected to at most 15 neighbors in order to provide structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from random import randint\n",
    "from sys import float_info\n",
    "from torch_geometric.nn import knn_graph\n",
    "\n",
    "instances = {}\n",
    "for k in range(0, 1000):\n",
    "    nodes = {}\n",
    "    for i in range(0, 50):\n",
    "        lat_i = randint(0, 100)\n",
    "        lon_i = randint(0, 100)\n",
    "        node_i = (lat_i, lon_i)\n",
    "        lat_j = randint(0, 100)\n",
    "        lon_j = randint(0, 100)\n",
    "        node_j = (lat_j, lon_j)\n",
    "        nodes[i + 1] = node_i\n",
    "        nodes[i + 51] = node_j\n",
    "\n",
    "    dist = {}\n",
    "    pairs = {}\n",
    "    for i in range(1, 101):\n",
    "        for j in range(1, 101):\n",
    "            if i != j:\n",
    "                dist[i,j] = sqrt( (nodes[i][0] - nodes[j][0])**2 + (nodes[i][1] - nodes[j][1])**2 )\n",
    "            else:\n",
    "                dist[i,j] = float_info.max\n",
    "    for i in range(1, 101):\n",
    "        for j in range(1, 101):\n",
    "            if i not in pairs:\n",
    "                pairs[i] = j\n",
    "            if i != j:\n",
    "                if dist[i,j] < dist[i,pairs[i]]:\n",
    "                    pairs[i] = j\n",
    "\n",
    "    nodes[0] = (0,0)\n",
    "    for i in range(1,101):\n",
    "        dist[0,i] = sqrt( (nodes[0][0] - nodes[i][0])**2 + (nodes[0][1] - nodes[i][1])**2 )\n",
    "        dist[i,0] = dist[0,i]\n",
    "    y = [[0 for _ in range(101)] for _ in range(101)]\n",
    "    for i in range(101):\n",
    "        if i > 0:\n",
    "            y[i][pairs[i]] = 1\n",
    "                \n",
    "    instances[k] = {\"nodes\": nodes, \"dist\": dist, \"y\": y}\n",
    "data_list = []\n",
    "for instance_name in instances:\n",
    "    y = torch.tensor(instances[instance_name][\"y\"], dtype=torch.float)\n",
    "    x = torch.tensor([instances[instance_name][\"nodes\"][i] for i in range(0, 101)], dtype=torch.float)\n",
    "    pos = []\n",
    "    for i in range(101):\n",
    "        pos.append(instances[instance_name][\"nodes\"][i])\n",
    "    pos = torch.tensor(pos, dtype=torch.double)\n",
    "    data_list.append(Data(x=x, y=y, edge_index = knn_graph(x, 15), pos=pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1-NN Batching and dividing data into train-test-validation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "def add_edge_labels(graph):\n",
    "    transform = T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False)\n",
    "    return transform(graph)\n",
    "\n",
    "labeled_graphs = [add_edge_labels(graph) for graph in data_list]\n",
    "\n",
    "train_size = [g[0] for g in labeled_graphs]\n",
    "val_size = [g[1] for g in labeled_graphs]\n",
    "test_size = [g[2] for g in labeled_graphs]\n",
    "\n",
    "train_loader = DataLoader(train_size, batch_size=20, shuffle=True)\n",
    "val_loader = DataLoader(val_size, batch_size=20, shuffle=False)\n",
    "test_loader = DataLoader(test_size, batch_size=20, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Encoder-Decoder architecture definition</h2>\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "model = Net(data_list[0].num_features, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Encoder-Decoder train-test-validate routine</h2>\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        z = model.encode(batch.x, batch.edge_index)\n",
    "\n",
    "        # We perform a new round of negative sampling for every training epoch:\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=batch.edge_index, num_nodes=batch.num_nodes,\n",
    "            num_neg_samples=batch.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "        # Concat positive and negative edge indices.\n",
    "        edge_label_index = torch.cat(\n",
    "            [batch.edge_label_index, neg_edge_index],\n",
    "            dim=-1,\n",
    "        )\n",
    "        # Label for positive edges: 1, for negative edges: 0.\n",
    "        edge_label = torch.cat([\n",
    "            batch.edge_label,\n",
    "            batch.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "        ], dim=0)\n",
    "\n",
    "        # Note: The model is trained in a supervised manner using the given\n",
    "        # `edge_label_index` and `edge_label` targets.\n",
    "        out = model.decode(z, edge_label_index).view(-1)\n",
    "        loss = criterion(out, edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    all_out = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in loader:\n",
    "        z = model.encode(batch.x, batch.edge_index)\n",
    "        out = model.decode(z, batch.edge_label_index).view(-1).sigmoid()\n",
    "        all_out.append(out.cpu().numpy())\n",
    "        all_labels.append(batch.edge_label.cpu().numpy())\n",
    "\n",
    "    all_out = np.concatenate(all_out)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return roc_auc_score(all_labels, all_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 122.2200, Val: 0.8519, Test: 0.8531\n",
      "Epoch: 002, Loss: 1.4640, Val: 0.8638, Test: 0.8646\n",
      "Epoch: 003, Loss: 0.7274, Val: 0.8689, Test: 0.8694\n",
      "Epoch: 004, Loss: 0.5976, Val: 0.8711, Test: 0.8717\n",
      "Epoch: 005, Loss: 0.5664, Val: 0.8718, Test: 0.8725\n",
      "Epoch: 006, Loss: 0.5568, Val: 0.8731, Test: 0.8737\n",
      "Epoch: 007, Loss: 0.5530, Val: 0.8743, Test: 0.8747\n",
      "Epoch: 008, Loss: 0.5514, Val: 0.8757, Test: 0.8763\n",
      "Epoch: 009, Loss: 0.5502, Val: 0.8769, Test: 0.8774\n",
      "Epoch: 010, Loss: 0.5489, Val: 0.8782, Test: 0.8787\n",
      "Epoch: 011, Loss: 0.5481, Val: 0.8803, Test: 0.8808\n",
      "Epoch: 012, Loss: 0.5472, Val: 0.8815, Test: 0.8821\n",
      "Epoch: 013, Loss: 0.5464, Val: 0.8841, Test: 0.8847\n",
      "Epoch: 014, Loss: 0.5448, Val: 0.8862, Test: 0.8868\n",
      "Epoch: 015, Loss: 0.5435, Val: 0.8886, Test: 0.8890\n",
      "Epoch: 016, Loss: 0.5421, Val: 0.8917, Test: 0.8921\n",
      "Epoch: 017, Loss: 0.5410, Val: 0.8945, Test: 0.8950\n",
      "Epoch: 018, Loss: 0.5394, Val: 0.8979, Test: 0.8983\n",
      "Epoch: 019, Loss: 0.5358, Val: 0.9014, Test: 0.9020\n",
      "Epoch: 020, Loss: 0.5338, Val: 0.9051, Test: 0.9057\n",
      "Epoch: 021, Loss: 0.5310, Val: 0.9096, Test: 0.9100\n",
      "Epoch: 022, Loss: 0.5281, Val: 0.9136, Test: 0.9140\n",
      "Epoch: 023, Loss: 0.5245, Val: 0.9169, Test: 0.9173\n",
      "Epoch: 024, Loss: 0.5218, Val: 0.9216, Test: 0.9218\n",
      "Epoch: 025, Loss: 0.5199, Val: 0.9234, Test: 0.9239\n",
      "Epoch: 026, Loss: 0.5170, Val: 0.9275, Test: 0.9276\n",
      "Epoch: 027, Loss: 0.5146, Val: 0.9280, Test: 0.9284\n",
      "Epoch: 028, Loss: 0.5120, Val: 0.9302, Test: 0.9305\n",
      "Epoch: 029, Loss: 0.5118, Val: 0.9313, Test: 0.9317\n",
      "Epoch: 030, Loss: 0.5106, Val: 0.9322, Test: 0.9326\n",
      "Epoch: 031, Loss: 0.5097, Val: 0.9327, Test: 0.9332\n",
      "Epoch: 032, Loss: 0.5092, Val: 0.9331, Test: 0.9336\n",
      "Epoch: 033, Loss: 0.5089, Val: 0.9336, Test: 0.9341\n",
      "Epoch: 034, Loss: 0.5089, Val: 0.9321, Test: 0.9328\n",
      "Epoch: 035, Loss: 0.5095, Val: 0.9337, Test: 0.9344\n",
      "Epoch: 036, Loss: 0.5071, Val: 0.9343, Test: 0.9350\n",
      "Epoch: 037, Loss: 0.5074, Val: 0.9337, Test: 0.9344\n",
      "Epoch: 038, Loss: 0.5069, Val: 0.9335, Test: 0.9343\n",
      "Epoch: 039, Loss: 0.5067, Val: 0.9350, Test: 0.9357\n",
      "Epoch: 040, Loss: 0.5065, Val: 0.9350, Test: 0.9355\n",
      "Epoch: 041, Loss: 0.5075, Val: 0.9353, Test: 0.9360\n",
      "Epoch: 042, Loss: 0.5064, Val: 0.9355, Test: 0.9361\n",
      "Epoch: 043, Loss: 0.5061, Val: 0.9352, Test: 0.9359\n",
      "Epoch: 044, Loss: 0.5054, Val: 0.9356, Test: 0.9363\n",
      "Epoch: 045, Loss: 0.5063, Val: 0.9359, Test: 0.9363\n",
      "Epoch: 046, Loss: 0.5058, Val: 0.9362, Test: 0.9367\n",
      "Epoch: 047, Loss: 0.5051, Val: 0.9350, Test: 0.9357\n",
      "Epoch: 048, Loss: 0.5059, Val: 0.9341, Test: 0.9350\n",
      "Epoch: 049, Loss: 0.5052, Val: 0.9352, Test: 0.9360\n",
      "Epoch: 050, Loss: 0.5043, Val: 0.9366, Test: 0.9371\n",
      "Epoch: 051, Loss: 0.5042, Val: 0.9349, Test: 0.9357\n",
      "Epoch: 052, Loss: 0.5053, Val: 0.9367, Test: 0.9374\n",
      "Epoch: 053, Loss: 0.5052, Val: 0.9370, Test: 0.9376\n",
      "Epoch: 054, Loss: 0.5034, Val: 0.9374, Test: 0.9381\n",
      "Epoch: 055, Loss: 0.5033, Val: 0.9376, Test: 0.9381\n",
      "Epoch: 056, Loss: 0.5031, Val: 0.9375, Test: 0.9381\n",
      "Epoch: 057, Loss: 0.5034, Val: 0.9363, Test: 0.9371\n",
      "Epoch: 058, Loss: 0.5030, Val: 0.9378, Test: 0.9383\n",
      "Epoch: 059, Loss: 0.5033, Val: 0.9377, Test: 0.9383\n",
      "Epoch: 060, Loss: 0.5035, Val: 0.9379, Test: 0.9384\n",
      "Epoch: 061, Loss: 0.5029, Val: 0.9369, Test: 0.9377\n",
      "Epoch: 062, Loss: 0.5021, Val: 0.9383, Test: 0.9390\n",
      "Epoch: 063, Loss: 0.5017, Val: 0.9371, Test: 0.9377\n",
      "Epoch: 064, Loss: 0.5021, Val: 0.9386, Test: 0.9391\n",
      "Epoch: 065, Loss: 0.5031, Val: 0.9388, Test: 0.9394\n",
      "Epoch: 066, Loss: 0.5016, Val: 0.9389, Test: 0.9391\n",
      "Epoch: 067, Loss: 0.5007, Val: 0.9386, Test: 0.9390\n",
      "Epoch: 068, Loss: 0.5019, Val: 0.9379, Test: 0.9386\n",
      "Epoch: 069, Loss: 0.5000, Val: 0.9390, Test: 0.9392\n",
      "Epoch: 070, Loss: 0.5012, Val: 0.9397, Test: 0.9402\n",
      "Epoch: 071, Loss: 0.5029, Val: 0.9383, Test: 0.9388\n",
      "Epoch: 072, Loss: 0.5006, Val: 0.9405, Test: 0.9408\n",
      "Epoch: 073, Loss: 0.5008, Val: 0.9402, Test: 0.9406\n",
      "Epoch: 074, Loss: 0.5018, Val: 0.9391, Test: 0.9395\n",
      "Epoch: 075, Loss: 0.5004, Val: 0.9405, Test: 0.9407\n",
      "Epoch: 076, Loss: 0.5009, Val: 0.9399, Test: 0.9403\n",
      "Epoch: 077, Loss: 0.5005, Val: 0.9402, Test: 0.9404\n",
      "Epoch: 078, Loss: 0.5002, Val: 0.9407, Test: 0.9410\n",
      "Epoch: 079, Loss: 0.4995, Val: 0.9409, Test: 0.9413\n",
      "Epoch: 080, Loss: 0.5002, Val: 0.9405, Test: 0.9405\n",
      "Epoch: 081, Loss: 0.5011, Val: 0.9414, Test: 0.9417\n",
      "Epoch: 082, Loss: 0.5000, Val: 0.9411, Test: 0.9415\n",
      "Epoch: 083, Loss: 0.5014, Val: 0.9402, Test: 0.9404\n",
      "Epoch: 084, Loss: 0.4979, Val: 0.9414, Test: 0.9415\n",
      "Epoch: 085, Loss: 0.5026, Val: 0.9417, Test: 0.9420\n",
      "Epoch: 086, Loss: 0.4987, Val: 0.9422, Test: 0.9422\n",
      "Epoch: 087, Loss: 0.5015, Val: 0.9414, Test: 0.9416\n",
      "Epoch: 088, Loss: 0.5001, Val: 0.9428, Test: 0.9428\n",
      "Epoch: 089, Loss: 0.5008, Val: 0.9421, Test: 0.9422\n",
      "Epoch: 090, Loss: 0.4997, Val: 0.9421, Test: 0.9420\n",
      "Epoch: 091, Loss: 0.4993, Val: 0.9431, Test: 0.9432\n",
      "Epoch: 092, Loss: 0.4991, Val: 0.9377, Test: 0.9374\n",
      "Epoch: 093, Loss: 0.5023, Val: 0.9426, Test: 0.9425\n",
      "Epoch: 094, Loss: 0.4983, Val: 0.9434, Test: 0.9435\n",
      "Epoch: 095, Loss: 0.4980, Val: 0.9443, Test: 0.9442\n",
      "Epoch: 096, Loss: 0.4999, Val: 0.9433, Test: 0.9434\n",
      "Epoch: 097, Loss: 0.5011, Val: 0.9437, Test: 0.9436\n",
      "Epoch: 098, Loss: 0.4969, Val: 0.9427, Test: 0.9428\n",
      "Epoch: 099, Loss: 0.4967, Val: 0.9421, Test: 0.9419\n",
      "Epoch: 100, Loss: 0.4985, Val: 0.9385, Test: 0.9389\n",
      "Final Test: 0.9442\n"
     ]
    }
   ],
   "source": [
    "# Train/Test Loop\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train(train_loader)\n",
    "    val_auc = test(val_loader)\n",
    "    test_auc = test(test_loader)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1-NN additional statistics</h2>\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict = {}\n",
    "cnt = 0\n",
    "for graph in data_list:\n",
    "    z_raw = model.encode(graph.x, graph.edge_index)\n",
    "    final_edge_index = model.decode_all(z_raw)\n",
    "    fei = final_edge_index.tolist()\n",
    "    edges_pred = {k:[] for k in range(101)}\n",
    "    edges_pred_inv = {k:[] for k in range(101)}\n",
    "    for i in range(len(fei[0])):\n",
    "        edges_pred[fei[0][i]].append(fei[1][i])\n",
    "        edges_pred_inv[fei[1][i]].append(fei[0][i])\n",
    "    ts0 = graph.edge_index.tolist()\n",
    "    edges = {k:[] for k in range(101)}\n",
    "    edges_inv = {k:[] for k in range(101)}\n",
    "    for i in range(len(ts0[0])):\n",
    "        edges[ts0[0][i]].append(ts0[1][i])\n",
    "        edges_inv[ts0[1][i]].append(ts0[0][i])\n",
    "    originals = {}\n",
    "    predictions = {}\n",
    "    for i in range(101):\n",
    "        originals[i] = set(edges[i] + edges_inv[i])\n",
    "        predictions[i] = set(edges_pred[i] + edges_pred_inv[i])\n",
    "    graph_dict[cnt] = {\"real\": originals, \"preds\": predictions}\n",
    "    cnt += 1\n",
    "\n",
    "confusion_dict = {}\n",
    "true_positives = []\n",
    "false_positives = []\n",
    "true_negatives = []\n",
    "false_negatives = []\n",
    "for key in graph_dict:\n",
    "    real = graph_dict[key][\"real\"]\n",
    "    pred = graph_dict[key][\"preds\"]\n",
    "    node_matrix = {}\n",
    "    for i in range(101):\n",
    "        tp = len(pred[i].intersection(real[i]))\n",
    "        fp = len(pred[i] - real[i])\n",
    "        real_neg = set([j for j in range(101)]) - {i} - real[i]\n",
    "        pred_neg = set([j for j in range(101)]) - {i} - pred[i]\n",
    "        tn = len(pred_neg.intersection(real_neg))\n",
    "        fn = len(pred_neg - real_neg)\n",
    "        total = tp + fp + tn + fn\n",
    "        node_matrix[i] = {\"tp\": tp, \"fp\": fp, \"tn\": tn, \"fn\": fn, \"total\": total}\n",
    "        true_positives.append(tp/total)\n",
    "        false_positives.append(fp/total)\n",
    "        true_negatives.append(tn/total)\n",
    "        false_negatives.append(fn/total)\n",
    "    confusion_dict[key] = node_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positives mean=0.17, stdev=0.03\n",
      "false positives mean=0.28, stdev=0.10\n",
      "true negatives mean=0.54, stdev=0.10\n",
      "false negatives mean=0.00, stdev=0.00\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, stdev\n",
    "print(\"true positives mean={0:.2f}, stdev={1:.2f}\".format(mean(true_positives), stdev(true_positives)))\n",
    "print(\"false positives mean={0:.2f}, stdev={1:.2f}\".format(mean(false_positives), stdev(false_positives)))\n",
    "print(\"true negatives mean={0:.2f}, stdev={1:.2f}\".format(mean(true_negatives), stdev(true_negatives)))\n",
    "print(\"false negatives mean={0:.2f}, stdev={1:.2f}\".format(mean(false_negatives), stdev(false_negatives)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some notion of closeness. It over-links, but rarely avoids a close node. Also, a middle stage as this is not always a requirement. We want to encode the data in embeddings that can be used for the next stage. The quality will be measured then over the ability to learn closeness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1-NN data preparation for second stage</h2>\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "data_raw_dict = {\n",
    "    \"positives\": [],\n",
    "    \"negatives\": []\n",
    "}\n",
    "cnt = 0\n",
    "pos_cnt = 0\n",
    "neg_cnt = 0\n",
    "for graph in data_list:\n",
    "    neg_cnt = 0\n",
    "    ng_matrix = graph.y.tolist()\n",
    "    encoding_matrix = model.encode(graph.x, graph.edge_index).tolist()\n",
    "    for i in range(101):\n",
    "        for j in sample(range(101), 30):\n",
    "            if i == j:\n",
    "                # if pos_cnt < 80000:\n",
    "                #     data_raw_dict[\"positives\"].append(encoding_matrix[i]+encoding_matrix[i])\n",
    "                #     pos_cnt += 1\n",
    "                continue\n",
    "            else:\n",
    "                if ng_matrix[i][j] > 0.5:\n",
    "                    if pos_cnt < 80000:\n",
    "                        data_raw_dict[\"positives\"].append(encoding_matrix[i]+encoding_matrix[j]+[1])\n",
    "                        pos_cnt += 1\n",
    "                else:\n",
    "                    if pos_cnt < 80000 and neg_cnt < 1010:\n",
    "                        data_raw_dict[\"negatives\"].append(encoding_matrix[i]+encoding_matrix[j]+[0])\n",
    "                        neg_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tensor = data_raw_dict[\"positives\"][:5000] + data_raw_dict[\"negatives\"][:5000]\n",
    "main_tensor = torch.tensor(pre_tensor, dtype=torch.float32)\n",
    "\n",
    "main_tensor = main_tensor[torch.randperm(main_tensor.size()[0])]\n",
    "labels = main_tensor[:,-1:]\n",
    "embeddings = main_tensor[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1-NN Wide or Deep network</h2>\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Wide(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(128, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "class Deep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(128, 128)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(128, 128)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    " \n",
    "def model_train(model, X_train, y_train, X_val, y_val):\n",
    "    # loss function and optimizer\n",
    "    loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    " \n",
    "    n_epochs = 20   # number of epochs to run\n",
    "    batch_size = 10  # size of each batch\n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
    " \n",
    "    # Hold the best model\n",
    "    best_acc = - np.inf   # init to negative infinity\n",
    "    best_weights = None\n",
    " \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "                # take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "                # forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "                # print progress\n",
    "                acc = (y_pred.round() == y_batch).float().mean()\n",
    "                bar.set_postfix(\n",
    "                    loss=float(loss),\n",
    "                    acc=float(acc)\n",
    "                )\n",
    "        # evaluate accuracy at end of each epoch\n",
    "        model.eval()\n",
    "        y_pred = model(X_val)\n",
    "        acc = (y_pred.round() == y_val).float().mean()\n",
    "        acc = float(acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "    # restore model and return best accuracy\n",
    "    model.load_state_dict(best_weights)\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (wide): 0.93\n",
      "Accuracy (wide): 0.94\n",
      "Accuracy (wide): 0.92\n",
      "Accuracy (wide): 0.94\n",
      "Accuracy (wide): 0.93\n",
      "Accuracy (deep): 0.95\n",
      "Accuracy (deep): 0.94\n",
      "Accuracy (deep): 0.95\n",
      "Accuracy (deep): 0.95\n",
      "Accuracy (deep): 0.95\n",
      "Wide: 93.19% (+/- 0.53%)\n",
      "Deep: 94.86% (+/- 0.42%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "# train-test split: Hold out the test set for final model evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, train_size=0.7, shuffle=True)\n",
    " \n",
    "# define 5-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "cv_scores_wide = []\n",
    "for train, test in kfold.split(X_train, y_train):\n",
    "    # create model, train, and get accuracy\n",
    "    model = Wide()\n",
    "    acc = model_train(model, X_train[train], y_train[train], X_train[test], y_train[test])\n",
    "    print(\"Accuracy (wide): %.2f\" % acc)\n",
    "    cv_scores_wide.append(acc)\n",
    "cv_scores_deep = []\n",
    "for train, test in kfold.split(X_train, y_train):\n",
    "    # create model, train, and get accuracy\n",
    "    model = Deep()\n",
    "    acc = model_train(model, X_train[train], y_train[train], X_train[test], y_train[test])\n",
    "    print(\"Accuracy (deep): %.2f\" % acc)\n",
    "    cv_scores_deep.append(acc)\n",
    " \n",
    "# evaluate the model\n",
    "wide_acc = np.mean(cv_scores_wide)\n",
    "wide_std = np.std(cv_scores_wide)\n",
    "deep_acc = np.mean(cv_scores_deep)\n",
    "deep_std = np.std(cv_scores_deep)\n",
    "print(\"Wide: %.2f%% (+/- %.2f%%)\" % (wide_acc*100, wide_std*100))\n",
    "print(\"Deep: %.2f%% (+/- %.2f%%)\" % (deep_acc*100, deep_std*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrain a deep model\n",
      "Final model accuracy: 95.13%\n"
     ]
    }
   ],
   "source": [
    "# rebuild model with full set of training data\n",
    "if wide_acc > deep_acc:\n",
    "    print(\"Retrain a wide model\")\n",
    "    model = Wide()\n",
    "else:\n",
    "    print(\"Retrain a deep model\")\n",
    "    model = Deep()\n",
    "acc = model_train(model, X_train, y_train, X_test, y_test)\n",
    "print(f\"Final model accuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.1717339e-01 -5.7865076e-02 -1.4151484e-02  1.5557876e-02\n",
      " -4.2768332e-01 -4.4370964e-01  3.3552490e-02  4.1792460e-02\n",
      " -4.4831714e-01 -7.0630580e-02  2.1677202e-01  2.2268556e-01\n",
      " -6.3116723e-01  8.4077612e-02  5.1827902e-01  3.1405866e-01\n",
      " -2.0323183e-01  8.0681220e-02  1.3163482e-01 -1.4057782e-01\n",
      " -1.0440167e-02  9.3202174e-02  1.8332930e-01  2.2968042e-01\n",
      "  1.0498860e-01 -2.6336920e-01  5.9151962e-02  5.4599382e-02\n",
      " -5.2246571e-02 -2.9523142e-02  2.7236146e-01 -5.9649184e-02\n",
      " -2.9510462e-01 -2.3909754e-01 -2.0827483e-01 -4.0251181e-01\n",
      " -1.6330114e-01 -2.0440927e-01 -7.0687167e-02  2.9918218e-01\n",
      "  9.8987930e-03  9.5853359e-02 -2.3753516e-01 -1.8214691e-01\n",
      " -4.3035455e-02 -1.5556519e-02 -1.2626988e-01  1.3818063e-01\n",
      " -2.8056109e-01  5.3613499e-02 -2.5029457e-01  3.6895238e-02\n",
      " -1.3619283e-01  5.3167921e-01 -3.1269984e-03 -3.6275232e-01\n",
      "  2.5250453e-01  1.8157607e-01 -2.1313316e-01 -8.1577316e-02\n",
      "  1.5478041e-02  2.6419458e-01  5.4498559e-01  2.4573167e-01\n",
      "  4.3307829e-01 -5.6470633e-02 -1.1331800e-02  2.2720667e-02\n",
      " -4.2329457e-01 -4.5480761e-01  4.4342883e-02  4.5427322e-02\n",
      " -4.4606289e-01 -8.0577120e-02  2.1183130e-01  2.3732670e-01\n",
      " -6.3463920e-01  9.8108679e-02  5.1491594e-01  3.1170848e-01\n",
      " -2.1752605e-01  8.0698565e-02  1.3647492e-01 -1.4649908e-01\n",
      " -1.1975139e-02  8.7222464e-02  1.8109830e-01  2.2941652e-01\n",
      "  9.2107445e-02 -2.6699829e-01  5.0050810e-02  4.7848269e-02\n",
      " -3.7784070e-02 -1.9218609e-02  2.7938482e-01 -5.8992282e-02\n",
      " -2.8603178e-01 -2.3820066e-01 -1.9802438e-01 -4.0632907e-01\n",
      " -1.5602684e-01 -2.1411142e-01 -7.1299389e-02  3.2000360e-01\n",
      "  1.4883975e-02  9.6994124e-02 -2.4576062e-01 -1.7677087e-01\n",
      " -3.5438828e-02 -1.8663231e-02 -1.2395805e-01  1.4290737e-01\n",
      " -2.8109637e-01  5.1804379e-02 -2.5030202e-01  2.8354084e-02\n",
      " -1.3504303e-01  5.2837777e-01  5.8637280e-04 -3.6035749e-01\n",
      "  2.5716203e-01  1.8610978e-01 -2.0425469e-01 -7.5680345e-02\n",
      "  1.3318401e-02  2.5793216e-01  5.3815591e-01  2.5184375e-01] -> [0.8791382] (expected [1.])\n",
      "[-0.330864    0.06145173 -0.19024104 -0.0187828   0.02340415 -0.02353234\n",
      " -0.09699653  0.01063286  0.07676873  0.10265465 -0.20202741 -0.15060855\n",
      "  0.16400571 -0.16511227 -0.13620533  0.17994827  0.12047814 -0.13892125\n",
      " -0.04614743  0.10489418  0.10139035  0.10708459  0.02570695 -0.2025975\n",
      "  0.27189144 -0.02998438  0.10394385  0.05386116 -0.0839919  -0.16725492\n",
      " -0.08409136 -0.05255683  0.03403759  0.01767641 -0.01208991  0.09068033\n",
      "  0.07768191  0.12292603  0.10131624 -0.34487686 -0.05155137  0.00810724\n",
      " -0.01497776 -0.16132963 -0.01401632  0.15828408  0.09045064  0.03769962\n",
      "  0.14451867 -0.05035711  0.15338582  0.08894943  0.18451214 -0.09988489\n",
      "  0.1258642  -0.01964192 -0.12790039 -0.1538146  -0.08576083 -0.08980085\n",
      " -0.06369558  0.13308588  0.15275443 -0.06599046  0.32791537 -0.05607147\n",
      " -0.01604865 -0.00805374 -0.3239973  -0.42272744  0.09292679  0.04010247\n",
      " -0.43265417 -0.13727242  0.17752124  0.2289034  -0.580992    0.06468205\n",
      "  0.36844456  0.31403068 -0.14854711  0.19902556  0.08086179 -0.1680052\n",
      "  0.00577518  0.02677535  0.1272771   0.15678254  0.05438989 -0.19849831\n",
      "  0.09766823 -0.03170805 -0.04216717  0.01377597  0.23065558  0.00603591\n",
      " -0.25511116 -0.19574961 -0.10372823 -0.37275997 -0.10334317 -0.1962428\n",
      "  0.01648377  0.26799214 -0.0055501   0.06452463 -0.14870533 -0.14268668\n",
      "  0.05730914  0.03938692 -0.08882365  0.06507913 -0.26012477  0.08325417\n",
      " -0.21879144 -0.02418476 -0.19352373  0.45169723 -0.04488386 -0.27481258\n",
      "  0.14917243  0.18355168 -0.14959304 -0.11858585 -0.02265492  0.17981535\n",
      "  0.5236769   0.20937355] -> [9.802447e-09] (expected [0.])\n",
      "[ 0.22271447 -0.15952486  0.18382713 -0.0736852  -0.1735313  -0.2958808\n",
      "  0.1651783  -0.12247935 -0.1891824  -0.0376753  -0.01021766  0.17950147\n",
      " -0.16975714  0.05774587  0.1254158   0.14762676 -0.12215778  0.63846254\n",
      " -0.07816781 -0.16154514 -0.07402876 -0.26636425 -0.13094182  0.13025358\n",
      " -0.24119826 -0.06175733  0.1358243  -0.1056514  -0.02328973  0.08909103\n",
      " -0.04106475  0.0768939  -0.27535206 -0.2432819   0.17938295 -0.48485598\n",
      " -0.21559832 -0.19126189  0.14253952  0.26128456 -0.05623893  0.05568039\n",
      "  0.02272103  0.03955377  0.23339663 -0.10984882  0.06858939  0.02321001\n",
      " -0.0886617   0.37852514 -0.4213081  -0.00954623 -0.43551075  0.03039259\n",
      " -0.12275901  0.00415562 -0.09740531  0.0846632  -0.02359264 -0.10913993\n",
      "  0.04740994 -0.1211182   0.21601045 -0.07385829 -0.32684976  0.22578472\n",
      " -0.18895131 -0.007252   -0.09391057 -0.12486035  0.06219388 -0.28197473\n",
      "  0.01795186  0.4119055  -0.104416   -0.26223752  0.21181701 -0.01028562\n",
      " -0.0881796   0.13924631  0.12256099 -0.01716547 -0.02672764  0.29523718\n",
      "  0.06995692  0.08960645  0.01676747 -0.22883645  0.30053788  0.06775854\n",
      "  0.2258673   0.07662316 -0.17618199 -0.37895942 -0.03361698 -0.16629015\n",
      " -0.07676308 -0.48291153  0.01749101 -0.43210956 -0.13394362  0.07412936\n",
      "  0.18788399 -0.2770224  -0.09938193  0.07316856  0.10176948 -0.49174565\n",
      " -0.20396474 -0.07086708  0.00149342  0.16793291  0.11178198 -0.08262042\n",
      " -0.22674532  0.3133783   0.3294002  -0.1406103   0.1196633  -0.16833246\n",
      " -0.23056692 -0.27149156 -0.15944612  0.02952655  0.125426    0.14204529\n",
      "  0.28754896 -0.09822766] -> [5.1421516e-06] (expected [0.])\n",
      "[-0.0837351  -0.0448166  -0.02411401 -0.04934224 -0.17384487  0.0436961\n",
      " -0.16253164  0.00429083 -0.23038313  0.04281615  0.10458341 -0.14767751\n",
      " -0.16443859 -0.25273436  0.2536457   0.28187922  0.18969768 -0.07971613\n",
      " -0.00124664  0.0167587   0.12776399  0.10312244  0.17823721  0.26184237\n",
      "  0.33748722 -0.12687102  0.15838829  0.15304278 -0.34170038 -0.28474018\n",
      "  0.14931548 -0.0870682  -0.40944332 -0.18892667 -0.45725405  0.03313363\n",
      " -0.05818337  0.18113264 -0.11131335 -0.33085167  0.00981047  0.03956431\n",
      " -0.05151433 -0.0908279  -0.1923779   0.13452251 -0.22167386  0.07144933\n",
      " -0.078137    0.03095455 -0.01341315  0.17364275  0.12554172  0.33699083\n",
      "  0.09001357 -0.22338557 -0.01216271  0.06234008 -0.27405703 -0.10425153\n",
      "  0.01439177  0.30483744  0.48207515 -0.03447701  0.14754231 -0.1325877\n",
      " -0.01699865 -0.05028243 -0.28408775 -0.13301158 -0.1358625   0.08784046\n",
      " -0.34732074 -0.06451582  0.14791946  0.02272195 -0.34264398 -0.17731088\n",
      "  0.36250448  0.30614182  0.03638357 -0.02239242  0.0565416  -0.06525221\n",
      "  0.06708034  0.10107493  0.1651579   0.23970264  0.22873928 -0.21504852\n",
      "  0.07301868  0.10800594 -0.21460019 -0.1220111   0.21358922 -0.04224831\n",
      " -0.36934036 -0.09301305 -0.40087014 -0.03471315 -0.08040024  0.02077419\n",
      " -0.12134337 -0.07245152  0.0294169   0.04603503 -0.15192299 -0.05395331\n",
      " -0.0933306   0.14605504 -0.19364022  0.02699062 -0.18397355  0.04634377\n",
      " -0.03619428  0.0591231  -0.0431584   0.46648848  0.03742462 -0.2539232\n",
      "  0.12606579  0.12370135 -0.2251651  -0.11487918 -0.01822015  0.28772992\n",
      "  0.49116087  0.09274285] -> [0.41050267] (expected [1.])\n",
      "[-7.63157979e-02 -2.40801387e-02  6.28724545e-02 -3.44757959e-02\n",
      "  1.64169565e-01  1.27958730e-02  9.96308103e-02  1.40649468e-01\n",
      "  8.38868394e-02 -2.85503089e-01 -1.41207337e-01  9.28785130e-02\n",
      " -4.95303869e-02 -7.77546614e-02 -1.27960384e-01  2.22919405e-01\n",
      "  8.11097696e-02  9.67583209e-02 -4.66409475e-02 -2.14993894e-01\n",
      "  3.77927944e-02 -6.59181550e-02 -6.07011244e-02  4.76374179e-02\n",
      " -6.41053170e-02 -2.46628150e-02 -8.66867602e-03 -4.12586555e-02\n",
      "  3.04690301e-02  1.43760383e-01  7.06293434e-03  1.76219314e-01\n",
      "  5.16500324e-02  2.35103041e-01  1.01228997e-01  2.44096160e-01\n",
      "  1.96702570e-01  9.34800804e-02 -5.77658042e-03 -1.25255555e-01\n",
      "  7.33259320e-03 -7.32862055e-02 -3.66118737e-03  1.56098679e-01\n",
      "  1.33527920e-01  1.36991665e-01  1.06185719e-01 -1.03929698e-01\n",
      "  3.40567641e-02  2.78462470e-02  2.24395454e-01 -2.43084431e-01\n",
      " -5.09101376e-02 -1.23238772e-01  3.37802768e-02  1.80506930e-01\n",
      " -7.67727867e-02  3.44040580e-02  2.73749754e-02 -1.14939764e-01\n",
      " -1.28552318e-01 -7.54444301e-02 -2.40921378e-02 -1.61090046e-02\n",
      "  1.32202223e-01 -3.02148402e-01  2.04013363e-01 -3.61703224e-02\n",
      "  9.46377963e-03  1.68025225e-01 -1.47889435e-01 -1.23928487e-01\n",
      "  1.78889155e-01 -4.99179959e-03  3.28678973e-02 -6.48027211e-02\n",
      "  1.92209333e-01 -2.33848020e-03 -2.74872854e-02 -2.61941344e-01\n",
      " -1.05844392e-02  5.35906404e-02  8.15910175e-02 -1.90383457e-02\n",
      " -1.10120445e-01 -1.31365150e-01 -1.44217923e-01  1.37729719e-01\n",
      " -3.24740946e-01  9.45773721e-03 -1.66778952e-01  1.16029754e-02\n",
      "  2.19870359e-02  1.53416544e-01  4.92712855e-03  4.93074730e-02\n",
      " -4.22657281e-02  1.38083979e-01 -1.36388168e-01  2.16364086e-01\n",
      " -1.41709000e-02 -2.08452344e-04 -1.71694905e-01  2.16244325e-01\n",
      "  2.47190781e-02 -5.79462349e-02  1.28445730e-01  2.93491840e-01\n",
      "  3.04497294e-02 -9.92563367e-02 -1.09400377e-01 -7.20842406e-02\n",
      " -4.58106101e-02  6.93200529e-02 -8.91467780e-02 -1.01222351e-01\n",
      " -8.20192769e-02 -2.83215642e-01 -6.67031705e-02  2.84511328e-01\n",
      "  5.32905385e-02 -2.10873820e-02  1.52256384e-01  1.98680595e-01\n",
      "  2.67397948e-02 -2.40933120e-01 -3.86802971e-01 -1.57303333e-01] -> [0.00160045] (expected [0.])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Test out inference with 5 samples\n",
    "    for i in range(5):\n",
    "        y_pred = model(X_test[i:i+1])\n",
    "        print(f\"{X_test[i].numpy()} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one above is an example for visual validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>NgSets prototype</h1>\n",
    "Once validated the simple prototype, let's continue to the actual problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load the data</h2>\n",
    "Here an \"export\" folder with .txt instance files used on the CTWVRP project, and a .csv file with the neighborhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_files_list = [\"./export/\"+f for f in os.listdir(\"./export\") ]\n",
    "instance_dict = {}\n",
    "for dir_str in data_files_list:\n",
    "    with open(dir_str, 'r') as text_file:\n",
    "        cnt = 0\n",
    "        instance = \"\"\n",
    "        for line in text_file:\n",
    "            if cnt < 9:\n",
    "                if cnt == 0:\n",
    "                    instance = line.split()[0]\n",
    "                    instance_dict[instance] = []\n",
    "                cnt += 1\n",
    "                continue\n",
    "            split_line = line.split()\n",
    "            instance_dict[instance].append([int(i) for i in split_line])\n",
    "        text_file.close()\n",
    "\n",
    "ng_dict = {}\n",
    "cnt = -1\n",
    "with open(\"ng_outs.csv\", 'r') as text_file:\n",
    "    for line in text_file:\n",
    "        if cnt < 2:\n",
    "            cnt += 1\n",
    "            continue\n",
    "        raw_line = line.strip()\n",
    "        split_line_list = raw_line.split(sep=\";\")\n",
    "        instance = split_line_list[3]\n",
    "        if instance not in ng_dict:\n",
    "            ng_dict[instance] = [[0 for i in range(101)]]\n",
    "        ng_dict[instance].append([0] + [int(i) for i in split_line_list[5:-1]])\n",
    "        if len(split_line_list[5:-1]) != 100:\n",
    "            print(\"case found for instance \"+instance)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import torch\n",
    "import torch_geometric as tg\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GraphConv, global_add_pool\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_list = []\n",
    "# for instance_name in ng_dict:\n",
    "#     y = torch.tensor(ng_dict[instance_name], dtype=torch.double)\n",
    "#     x = torch.tensor(instance_dict[instance_name], dtype=torch.double)\n",
    "#     attr = [[i] for i in range(n_edges)]\n",
    "#     loc_dict = {(i[0],j[0]): sqrt((i[1]-j[1])**2 + (i[2]-j[2])**2) for i in instance_dict[instance_name] for j in instance_dict[instance_name]}\n",
    "#     cnt = -1\n",
    "#     for i in range(101):\n",
    "#         for j in range(101):\n",
    "#             if i != j:\n",
    "#                 cnt += 1\n",
    "#                 attr[cnt].append(loc_dict[i,j])\n",
    "#     attr = torch.tensor(attr, dtype=torch.double)\n",
    "#     pos = []\n",
    "#     for i in instance_dict[instance_name]:\n",
    "#         pos.append([i[1], i[2]])\n",
    "#     pos = torch.tensor(pos, dtype=torch.double)\n",
    "#     data_list.append(Data(x=x, y=y, edge_index=edge_index, pos=pos, edge_attr=attr))\n",
    "\n",
    "data_list = []\n",
    "for instance_name in ng_dict:\n",
    "    y = torch.tensor(ng_dict[instance_name], dtype=torch.float)\n",
    "    x = torch.tensor(instance_dict[instance_name], dtype=torch.float)\n",
    "    pos = []\n",
    "    for i in instance_dict[instance_name]:\n",
    "        pos.append([i[1], i[2]])\n",
    "    pos = torch.tensor(pos, dtype=torch.double)\n",
    "    data_list.append(Data(x=x, y=y, edge_index = knn_graph(x, 15), pos=pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Prepare data as in prototype</h2>\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "def add_edge_labels(graph):\n",
    "    transform = T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False)\n",
    "    return transform(graph)\n",
    "\n",
    "labeled_graphs = [add_edge_labels(graph) for graph in data_list]\n",
    "\n",
    "train_size = [g[0] for g in labeled_graphs]\n",
    "val_size = [g[1] for g in labeled_graphs]\n",
    "test_size = [g[2] for g in labeled_graphs]\n",
    "\n",
    "train_loader = DataLoader(train_size, batch_size=500, shuffle=True)\n",
    "val_loader = DataLoader(val_size, batch_size=500, shuffle=False)\n",
    "test_loader = DataLoader(test_size, batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Encode-decode routine</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(data_list[0].num_features, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        z = model.encode(batch.x, batch.edge_index)\n",
    "\n",
    "        # We perform a new round of negative sampling for every training epoch:\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=batch.edge_index, num_nodes=batch.num_nodes,\n",
    "            num_neg_samples=batch.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "        # Concat positive and negative edge indices.\n",
    "        edge_label_index = torch.cat(\n",
    "            [batch.edge_label_index, neg_edge_index],\n",
    "            dim=-1,\n",
    "        )\n",
    "        # Label for positive edges: 1, for negative edges: 0.\n",
    "        edge_label = torch.cat([\n",
    "            batch.edge_label,\n",
    "            batch.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "        ], dim=0)\n",
    "\n",
    "        # Note: The model is trained in a supervised manner using the given\n",
    "        # `edge_label_index` and `edge_label` targets.\n",
    "        out = model.decode(z, edge_label_index).view(-1)\n",
    "        loss = criterion(out, edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    all_out = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in loader:\n",
    "        z = model.encode(batch.x, batch.edge_index)\n",
    "        out = model.decode(z, batch.edge_label_index).view(-1).sigmoid()\n",
    "        all_out.append(out.cpu().numpy())\n",
    "        all_labels.append(batch.edge_label.cpu().numpy())\n",
    "\n",
    "    all_out = np.concatenate(all_out)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return roc_auc_score(all_labels, all_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 9388.4564, Val: 0.5595, Test: 0.5594\n",
      "Epoch: 002, Loss: 78.0280, Val: 0.5884, Test: 0.5875\n",
      "Epoch: 003, Loss: 34.6380, Val: 0.6310, Test: 0.6301\n",
      "Epoch: 004, Loss: 21.8286, Val: 0.6735, Test: 0.6724\n",
      "Epoch: 005, Loss: 13.6175, Val: 0.6989, Test: 0.6978\n",
      "Epoch: 006, Loss: 10.3493, Val: 0.7115, Test: 0.7107\n",
      "Epoch: 007, Loss: 8.6041, Val: 0.7164, Test: 0.7158\n",
      "Epoch: 008, Loss: 7.2924, Val: 0.7272, Test: 0.7268\n",
      "Epoch: 009, Loss: 6.3743, Val: 0.7345, Test: 0.7344\n",
      "Epoch: 010, Loss: 5.5349, Val: 0.7324, Test: 0.7323\n",
      "Epoch: 011, Loss: 5.0403, Val: 0.7509, Test: 0.7512\n",
      "Epoch: 012, Loss: 4.3876, Val: 0.7545, Test: 0.7550\n",
      "Epoch: 013, Loss: 3.9929, Val: 0.7503, Test: 0.7508\n",
      "Epoch: 014, Loss: 3.7372, Val: 0.7604, Test: 0.7617\n",
      "Epoch: 015, Loss: 3.2096, Val: 0.7571, Test: 0.7582\n",
      "Epoch: 016, Loss: 2.8555, Val: 0.7551, Test: 0.7562\n",
      "Epoch: 017, Loss: 2.6036, Val: 0.7494, Test: 0.7504\n",
      "Epoch: 018, Loss: 2.4274, Val: 0.7503, Test: 0.7513\n",
      "Epoch: 019, Loss: 2.2488, Val: 0.7518, Test: 0.7529\n",
      "Epoch: 020, Loss: 2.0665, Val: 0.7472, Test: 0.7483\n",
      "Final Test: 0.7617\n"
     ]
    }
   ],
   "source": [
    "# Train/Test Loop\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 21):\n",
    "    loss = train(train_loader)\n",
    "    val_auc = test(val_loader)\n",
    "    test_auc = test(test_loader)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Check on values</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict = {}\n",
    "cnt = 0\n",
    "for graph in data_list:\n",
    "    z_raw = model.encode(graph.x, graph.edge_index)\n",
    "    final_edge_index = model.decode_all(z_raw)\n",
    "    fei = final_edge_index.tolist()\n",
    "    edges_pred = {k:[] for k in range(101)}\n",
    "    edges_pred_inv = {k:[] for k in range(101)}\n",
    "    for i in range(len(fei[0])):\n",
    "        edges_pred[fei[0][i]].append(fei[1][i])\n",
    "        edges_pred_inv[fei[1][i]].append(fei[0][i])\n",
    "    ts0 = graph.edge_index.tolist()\n",
    "    edges = {k:[] for k in range(101)}\n",
    "    edges_inv = {k:[] for k in range(101)}\n",
    "    for i in range(len(ts0[0])):\n",
    "        edges[ts0[0][i]].append(ts0[1][i])\n",
    "        edges_inv[ts0[1][i]].append(ts0[0][i])\n",
    "    originals = {}\n",
    "    predictions = {}\n",
    "    for i in range(101):\n",
    "        originals[i] = set(edges[i] + edges_inv[i])\n",
    "        predictions[i] = set(edges_pred[i] + edges_pred_inv[i])\n",
    "    graph_dict[cnt] = {\"real\": originals, \"preds\": predictions}\n",
    "    cnt += 1\n",
    "\n",
    "confusion_dict = {}\n",
    "true_positives = []\n",
    "false_positives = []\n",
    "true_negatives = []\n",
    "false_negatives = []\n",
    "for key in graph_dict:\n",
    "    real = graph_dict[key][\"real\"]\n",
    "    pred = graph_dict[key][\"preds\"]\n",
    "    node_matrix = {}\n",
    "    for i in range(101):\n",
    "        tp = len(pred[i].intersection(real[i]))\n",
    "        fp = len(pred[i] - real[i])\n",
    "        real_neg = set([j for j in range(101)]) - {i} - real[i]\n",
    "        pred_neg = set([j for j in range(101)]) - {i} - pred[i]\n",
    "        tn = len(pred_neg.intersection(real_neg))\n",
    "        fn = len(pred_neg - real_neg)\n",
    "        total = tp + fp + tn + fn\n",
    "        node_matrix[i] = {\"tp\": tp, \"fp\": fp, \"tn\": tn, \"fn\": fn, \"total\": total}\n",
    "        true_positives.append(tp/total)\n",
    "        false_positives.append(fp/total)\n",
    "        true_negatives.append(tn/total)\n",
    "        false_negatives.append(fn/total)\n",
    "    confusion_dict[key] = node_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positives mean=0.17, stdev=0.03\n",
      "false positives mean=0.65, stdev=0.18\n",
      "true negatives mean=0.18, stdev=0.18\n",
      "false negatives mean=0.00, stdev=0.00\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, stdev\n",
    "print(\"true positives mean={0:.2f}, stdev={1:.2f}\".format(mean(true_positives), stdev(true_positives)))\n",
    "print(\"false positives mean={0:.2f}, stdev={1:.2f}\".format(mean(false_positives), stdev(false_positives)))\n",
    "print(\"true negatives mean={0:.2f}, stdev={1:.2f}\".format(mean(true_negatives), stdev(true_negatives)))\n",
    "print(\"false negatives mean={0:.2f}, stdev={1:.2f}\".format(mean(false_negatives), stdev(false_negatives)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>NgLearning</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "data_raw_dict = {\n",
    "    \"positives\": [],\n",
    "    \"negatives\": []\n",
    "}\n",
    "cnt = 0\n",
    "pos_cnt = 0\n",
    "neg_cnt = 0\n",
    "for graph in data_list:\n",
    "    neg_cnt = 0\n",
    "    ng_matrix = graph.y.tolist()\n",
    "    encoding_matrix = model.encode(graph.x, graph.edge_index).tolist()\n",
    "    for i in range(101):\n",
    "        for j in sample(range(101), 30):\n",
    "            if i == j:\n",
    "                # if pos_cnt < 80000:\n",
    "                #     data_raw_dict[\"positives\"].append(encoding_matrix[i]+encoding_matrix[i])\n",
    "                #     pos_cnt += 1\n",
    "                continue\n",
    "            else:\n",
    "                if ng_matrix[i][j] > 0.5:\n",
    "                    if pos_cnt < 80000:\n",
    "                        data_raw_dict[\"positives\"].append(encoding_matrix[i]+encoding_matrix[j]+[1])\n",
    "                        pos_cnt += 1\n",
    "                else:\n",
    "                    if pos_cnt < 80000 and neg_cnt < 1010:\n",
    "                        data_raw_dict[\"negatives\"].append(encoding_matrix[i]+encoding_matrix[j]+[0])\n",
    "                        neg_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tensor = data_raw_dict[\"positives\"][:5000] + data_raw_dict[\"negatives\"][:5000]\n",
    "main_tensor = torch.tensor(pre_tensor, dtype=torch.float32)\n",
    "\n",
    "main_tensor = main_tensor[torch.randperm(main_tensor.size()[0])]\n",
    "labels = main_tensor[:,-1:]\n",
    "embeddings = main_tensor[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Learning stage</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(128, 128)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(128, 128)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, X_train, y_train, X_val, y_val):\n",
    "    # loss function and optimizer\n",
    "    loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    " \n",
    "    n_epochs = 20   # number of epochs to run\n",
    "    batch_size = 10  # size of each batch\n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
    " \n",
    "    # Hold the best model\n",
    "    best_acc = - np.inf   # init to negative infinity\n",
    "    best_weights = None\n",
    " \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "                # take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "                # forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "                # print progress\n",
    "                acc = (y_pred.round() == y_batch).float().mean()\n",
    "                bar.set_postfix(\n",
    "                    loss=float(loss),\n",
    "                    acc=float(acc)\n",
    "                )\n",
    "        # evaluate accuracy at end of each epoch\n",
    "        model.eval()\n",
    "        y_pred = model(X_val)\n",
    "        acc = (y_pred.round() == y_val).float().mean()\n",
    "        acc = float(acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "    # restore model and return best accuracy\n",
    "    model.load_state_dict(best_weights)\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (deep): 0.98\n",
      "Accuracy (deep): 0.98\n",
      "Accuracy (deep): 0.98\n",
      "Accuracy (deep): 0.98\n",
      "Accuracy (deep): 0.98\n",
      "Deep: 97.81% (+/- 0.25%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "# train-test split: Hold out the test set for final model evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, train_size=0.7, shuffle=True)\n",
    " \n",
    "# define 5-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "cv_scores_deep = []\n",
    "for train, test in kfold.split(X_train, y_train):\n",
    "    # create model, train, and get accuracy\n",
    "    model = Deep()\n",
    "    acc = model_train(model, X_train[train], y_train[train], X_train[test], y_train[test])\n",
    "    print(\"Accuracy (deep): %.2f\" % acc)\n",
    "    cv_scores_deep.append(acc)\n",
    " \n",
    "# evaluate the model\n",
    "deep_acc = np.mean(cv_scores_deep)\n",
    "deep_std = np.std(cv_scores_deep)\n",
    "print(\"Deep: %.2f%% (+/- %.2f%%)\" % (deep_acc*100, deep_std*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model accuracy: 98.23%\n"
     ]
    }
   ],
   "source": [
    "model = Deep()\n",
    "acc = model_train(model, X_train, y_train, X_test, y_test)\n",
    "print(f\"Final model accuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.17957088  0.3947396  -0.45261618 -0.03496477  0.42535105  0.6743047\n",
      " -0.23696764  0.6806395   0.5064205  -0.20391022 -0.8723331  -1.0085477\n",
      " -0.0831044   0.4129596   0.9198127   0.40857166  0.8815779  -0.06776576\n",
      "  1.217555   -0.29595935 -0.46431434 -0.72139764 -0.8256631   0.55083925\n",
      "  0.06886022  0.12763548 -0.6121382   0.29246357 -0.24496074 -0.24584423\n",
      "  1.0456184  -0.8436493  -0.00195669 -0.18024534  0.2314874   0.7783538\n",
      "  0.5932052   0.83533305  0.17551929 -0.06691554  0.90162    -0.3561267\n",
      "  0.7206132  -0.2649808  -0.10306723 -0.78351456  0.04361273  1.2637863\n",
      " -1.336325   -0.08951127  0.25899857 -0.66821    -0.17850304  0.3873586\n",
      " -0.2762001   0.49018297 -0.36008942 -1.4735388   0.4910735   0.0239816\n",
      " -0.75887245 -0.12177173  0.7138214  -0.1459892  -0.07668056  0.38986385\n",
      " -0.4528227  -0.01117831  0.5086343   0.6966045  -0.30085772  0.724807\n",
      "  0.52181864 -0.16619268 -0.88070065 -1.0551943  -0.15159656  0.4608794\n",
      "  0.95752     0.5011891   0.96247804 -0.11569326  1.2360055  -0.31479362\n",
      " -0.49703938 -0.734835   -0.7984426   0.5707672   0.08818905 -0.01953378\n",
      " -0.6623492   0.36130068 -0.26354703 -0.20781824  1.0241708  -0.9236652\n",
      "  0.01640448 -0.07491356  0.19462207  0.8755336   0.6202771   0.8970685\n",
      "  0.12314579  0.00309587  0.84985304 -0.3350716   0.7208285  -0.27453062\n",
      "  0.00306394 -0.7633149   0.03056637  1.288601   -1.4083673  -0.1229938\n",
      "  0.251011   -0.6661657  -0.22510654  0.37819153 -0.20872355  0.4964711\n",
      " -0.3566271  -1.4139353   0.51886845  0.00502904 -0.72533053 -0.16076261\n",
      "  0.69846773 -0.0945951 ] -> [1.] (expected [1.])\n",
      "[-0.58723783  0.07406888 -0.14435256 -0.18556541 -0.36347413  0.17443338\n",
      "  0.22520766 -0.19172482 -0.26204443 -0.14320414 -0.34863377 -0.18930602\n",
      "  0.24435426  0.05546898  0.08983618 -0.6266397  -0.28897452  0.19681418\n",
      "  0.22311135  0.03332406 -0.08227306 -0.06395513 -0.5887501   0.03888775\n",
      " -0.21260238  1.1476709  -0.16717571 -0.3112184   0.29664606 -0.46273467\n",
      "  0.21357663  0.20243101 -0.15959294 -0.8171569   0.19480816 -0.0338943\n",
      "  0.20705268  0.21718627  0.30637664 -0.35077018  0.6129114  -0.39228493\n",
      " -0.01333411 -0.43619677 -0.7874602  -0.59874284 -0.23635593  0.06572068\n",
      " -0.00640799  0.18158838  0.03006864 -0.4066329   0.41447425  0.6831809\n",
      " -0.6491287   0.1564875  -0.17477815 -1.2531868   0.398785    0.09779811\n",
      " -0.3184323   0.19901064  0.32784066 -0.48667902 -0.6700487   0.11485779\n",
      " -0.16953355 -0.18507534 -0.38579398  0.19883859  0.2589996  -0.20790513\n",
      " -0.21951152 -0.17330295 -0.38676488 -0.19940701  0.24731575  0.0060283\n",
      "  0.11464098 -0.65905005 -0.30001482  0.21605888  0.26448467  0.0183721\n",
      " -0.09361877 -0.11711763 -0.636782    0.04471933 -0.21016248  1.2267264\n",
      " -0.17693904 -0.32292095  0.3278793  -0.48161826  0.2535352   0.22798975\n",
      " -0.14052473 -0.87670505  0.24913871 -0.05765932  0.22629479  0.215859\n",
      "  0.32382455 -0.37595716  0.6953268  -0.4269648   0.04097755 -0.41922295\n",
      " -0.8371263  -0.5936988  -0.17028344  0.12456751 -0.02330389  0.1932404\n",
      "  0.05704693 -0.45629042  0.43951866  0.6671973  -0.69776076  0.20234971\n",
      " -0.19599737 -1.3312857   0.40476087  0.09499289 -0.33871597  0.19474947\n",
      "  0.36681205 -0.5342207 ] -> [1.] (expected [1.])\n",
      "[ 0.30986255 -0.13593245  0.08517111 -0.29129842  0.81671107  0.19152392\n",
      " -0.43140852  0.16452686  0.05199771 -0.05574082 -0.23395362 -0.00216458\n",
      " -0.72110766  0.32736167 -0.03216384 -0.02886206 -0.07165012 -0.3861451\n",
      " -0.02635414  0.00920812 -0.19238748 -0.04508208 -0.16852888  0.51197296\n",
      "  0.0170331  -1.3455712  -0.5156187   0.39098617  0.37612033  0.40443647\n",
      "  0.21230075 -0.20608501  0.26295233  0.01294853 -0.27078238  0.7314271\n",
      "  0.43677938  0.357358    0.00669107  0.39123377 -0.35064468 -0.4665521\n",
      "  0.01911276 -0.22166371  0.16901687 -0.32554218  0.30414787 -0.27400973\n",
      " -0.77272856 -0.342113   -0.00876341  0.2133539  -0.2707116  -0.24503613\n",
      "  0.37592298  0.08911213  0.3742594  -0.1440562   0.26057482 -0.14078416\n",
      "  0.29804203 -0.19916396 -0.09437048 -0.05751908  0.87253463 -0.32286397\n",
      "  0.18931064 -0.22345096  1.0210702   0.10871081 -0.691258    0.31861174\n",
      " -0.07981932  0.10492922 -0.02775218 -0.00761773 -0.82162833  0.6072052\n",
      " -0.12150557  0.25927594  0.1423471  -0.52847385 -0.17356583  0.00503048\n",
      " -0.16052349  0.19884445  0.12789102  0.48924878  0.03073003 -1.9583184\n",
      " -0.56906176  0.5438477   0.15579222  0.57530606 -0.0240574  -0.49405816\n",
      "  0.21201488  0.4951203  -0.59244066  0.96003896  0.3479817   0.42539483\n",
      " -0.16243033  0.6142553  -0.85644424 -0.21095511 -0.19517802 -0.26494843\n",
      "  0.6169298  -0.2572534  -0.00875808 -0.479823   -0.7710739  -0.4634493\n",
      " -0.15633945  0.45233408 -0.44761285 -0.23084168  0.7444346  -0.10926334\n",
      "  0.50067204  0.38482165  0.26089925 -0.15890327  0.4357404  -0.23379219\n",
      " -0.36522433  0.28098926] -> [0.9949734] (expected [1.])\n",
      "[-0.61132777  0.03314763  0.27708244 -0.5758449   0.45574793  0.4070059\n",
      " -0.00105265 -0.0189862   0.38892266 -0.42640224 -0.52045786  0.27402592\n",
      " -0.5824168  -0.35747927  0.00733348 -0.715356   -0.46942058 -0.11898275\n",
      " -0.24691963 -0.14012092  0.12774855 -0.3085037  -0.4138225   0.39554304\n",
      " -0.00138005 -0.44040582 -0.30292886 -0.14490548  0.540085    0.284514\n",
      "  0.38302007  0.473599    0.27393103 -0.8895737  -0.07236741  0.12354539\n",
      "  0.45491546 -0.08897993  0.43087927 -0.03116459  0.11103515 -0.97598666\n",
      "  0.29547524  0.18997437 -0.5485177  -0.12954229  0.88657343 -0.20025203\n",
      " -0.43931735 -0.0541939   0.04106761  0.38882577 -0.01813193 -0.5202127\n",
      "  0.11436233  0.5505953   0.05553341 -0.6367907   0.00723016 -0.10732624\n",
      "  0.17609394 -0.11981909  0.2780672  -0.46481216 -0.10401307  0.23157832\n",
      " -0.05218895 -0.00708493 -0.14859587 -0.04832904 -0.17508957  0.19307064\n",
      "  0.1878757  -0.23411906 -0.07519427  0.0786358  -0.06597175 -0.02974249\n",
      " -0.13341835  0.19294554 -0.00328162  0.23825076  0.20971939 -0.14742768\n",
      "  0.12092926 -0.11259657  0.1383676  -0.02019817 -0.08724105  0.06132079\n",
      "  0.01751148  0.02791934  0.05559357  0.11605461  0.18467359 -0.06911317\n",
      "  0.04303991 -0.0546532   0.01960001 -0.10803672 -0.13610613 -0.27918825\n",
      "  0.06582926 -0.00185362  0.01166589 -0.04178747  0.27241087  0.18035275\n",
      " -0.09654304  0.26679575  0.23879537  0.09737465  0.09753478  0.12994578\n",
      "  0.03263732  0.27736273 -0.09321114 -0.16343968 -0.049328    0.11882763\n",
      " -0.09840526  0.00392292 -0.1993548  -0.09167833 -0.1996811   0.02781735\n",
      " -0.1630954  -0.10199885] -> [0.00019451] (expected [0.])\n",
      "[-0.35618815  0.37302372  0.02459212 -0.1753456   0.00435849 -0.14159311\n",
      " -0.37891692  0.54595757  0.48023716 -0.5771786  -0.38461     0.21674553\n",
      " -0.39848256 -0.15487264 -0.23990905  0.33973348 -0.21267255  0.265742\n",
      "  0.2799843  -0.1833182   0.2185262  -0.28893283  0.3393271   0.09786198\n",
      " -0.14601025 -0.23053354  0.13562155  0.14363381  0.22255316  0.2967094\n",
      "  0.3354401   0.18486969  0.2362595  -0.35545233  0.01665642 -0.06007616\n",
      " -0.02566627 -0.55377984  0.2658158   0.29685572  0.18249434 -0.5237048\n",
      "  0.4918226   0.43878052 -0.28447515  0.5650869   0.5997405  -0.01147423\n",
      "  0.1625727   0.23787044  0.02002735  0.56356996 -0.3097706  -0.45247146\n",
      "  0.07286291  0.39317858 -0.06548803  0.13282485 -0.48237118 -0.275914\n",
      " -0.3176422  -0.08098175 -0.07627261 -0.19764118  0.31139687 -0.06704577\n",
      " -0.05349165 -0.1835638   0.57997954  0.23151785 -0.3750747   0.4110713\n",
      "  0.02512127 -0.05411569 -0.33978683 -0.36504513 -0.34391317  0.51494\n",
      "  0.22455412  0.19106852  0.26855847 -0.23137797  0.3140365  -0.0366741\n",
      " -0.21811281 -0.09670573 -0.2589256   0.4475023  -0.00405242 -0.7709954\n",
      " -0.43409035  0.3113964   0.01278976  0.10370804  0.41709596 -0.44946492\n",
      "  0.02836381  0.01469867 -0.22366175  0.7087502   0.3660488   0.4787111\n",
      "  0.06270351  0.21953437 -0.04560578 -0.29982466  0.09370324 -0.33023185\n",
      "  0.10407975 -0.5728367  -0.10558537  0.09848362 -0.8014447  -0.21522518\n",
      "  0.00731916  0.00541202 -0.24585885  0.16638704  0.16298626  0.04233607\n",
      "  0.14557157 -0.4801707   0.2978877  -0.06207211 -0.14224018 -0.10268471\n",
      "  0.10915214  0.03203639] -> [2.769023e-07] (expected [0.])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Test out inference with 5 samples\n",
    "    for i in range(5):\n",
    "        y_pred = model(X_test[i:i+1])\n",
    "        print(f\"{X_test[i].numpy()} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
