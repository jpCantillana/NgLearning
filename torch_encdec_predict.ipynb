{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>First prototype: encode-decode-predict the immediate nearest neighbor</h1>\n",
    "In order to check if it's possible to learn a metric minimizer.\n",
    "Based on https://medium.com/the-modern-scientist/graph-neural-networks-series-part-3-node-embedding-36613cc967d5\n",
    "and https://machinelearningmastery.com/building-a-binary-classification-model-in-pytorch/ and suggestions of Florian Racoussier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1-NN data</h2>\n",
    "Create a simple dataset for testing the prototype. Each node is connected to at most 15 neighbors in order to provide structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from random import randint\n",
    "from sys import float_info\n",
    "from torch_geometric.nn import knn_graph\n",
    "\n",
    "instances = {}\n",
    "for k in range(0, 1000):\n",
    "    nodes = {}\n",
    "    for i in range(0, 50):\n",
    "        lat_i = randint(0, 100)\n",
    "        lon_i = randint(0, 100)\n",
    "        node_i = (lat_i, lon_i)\n",
    "        lat_j = randint(0, 100)\n",
    "        lon_j = randint(0, 100)\n",
    "        node_j = (lat_j, lon_j)\n",
    "        nodes[i + 1] = node_i\n",
    "        nodes[i + 51] = node_j\n",
    "\n",
    "    dist = {}\n",
    "    pairs = {}\n",
    "    for i in range(1, 101):\n",
    "        for j in range(1, 101):\n",
    "            if i != j:\n",
    "                dist[i,j] = sqrt( (nodes[i][0] - nodes[j][0])**2 + (nodes[i][1] - nodes[j][1])**2 )\n",
    "            else:\n",
    "                dist[i,j] = float_info.max\n",
    "    for i in range(1, 101):\n",
    "        for j in range(1, 101):\n",
    "            if i not in pairs:\n",
    "                pairs[i] = j\n",
    "            if i != j:\n",
    "                if dist[i,j] < dist[i,pairs[i]]:\n",
    "                    pairs[i] = j\n",
    "\n",
    "    nodes[0] = (0,0)\n",
    "    for i in range(1,101):\n",
    "        dist[0,i] = sqrt( (nodes[0][0] - nodes[i][0])**2 + (nodes[0][1] - nodes[i][1])**2 )\n",
    "        dist[i,0] = dist[0,i]\n",
    "    y = [[0 for _ in range(101)] for _ in range(101)]\n",
    "    for i in range(101):\n",
    "        if i > 0:\n",
    "            y[i][pairs[i]] = 1\n",
    "                \n",
    "    instances[k] = {\"nodes\": nodes, \"dist\": dist, \"y\": y}\n",
    "data_list = []\n",
    "for instance_name in instances:\n",
    "    y = torch.tensor(instances[instance_name][\"y\"], dtype=torch.float)\n",
    "    x = torch.tensor([instances[instance_name][\"nodes\"][i] for i in range(0, 101)], dtype=torch.float)\n",
    "    pos = []\n",
    "    for i in range(101):\n",
    "        pos.append(instances[instance_name][\"nodes\"][i])\n",
    "    pos = torch.tensor(pos, dtype=torch.double)\n",
    "    data_list.append(Data(x=x, y=y, edge_index = knn_graph(x, 15), pos=pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1-NN Batching and dividing data into train-test-validation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "def add_edge_labels(graph):\n",
    "    transform = T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False)\n",
    "    return transform(graph)\n",
    "\n",
    "labeled_graphs = [add_edge_labels(graph) for graph in data_list]\n",
    "\n",
    "train_size = [g[0] for g in labeled_graphs]\n",
    "val_size = [g[1] for g in labeled_graphs]\n",
    "test_size = [g[2] for g in labeled_graphs]\n",
    "\n",
    "train_loader = DataLoader(train_size, batch_size=20, shuffle=True)\n",
    "val_loader = DataLoader(val_size, batch_size=20, shuffle=False)\n",
    "test_loader = DataLoader(test_size, batch_size=20, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Encoder-Decoder architecture definition</h2>\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "model = Net(data_list[0].num_features, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Encoder-Decoder train-test-validate routine</h2>\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        z = model.encode(batch.x, batch.edge_index)\n",
    "\n",
    "        # We perform a new round of negative sampling for every training epoch:\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=batch.edge_index, num_nodes=batch.num_nodes,\n",
    "            num_neg_samples=batch.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "        # Concat positive and negative edge indices.\n",
    "        edge_label_index = torch.cat(\n",
    "            [batch.edge_label_index, neg_edge_index],\n",
    "            dim=-1,\n",
    "        )\n",
    "        # Label for positive edges: 1, for negative edges: 0.\n",
    "        edge_label = torch.cat([\n",
    "            batch.edge_label,\n",
    "            batch.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "        ], dim=0)\n",
    "\n",
    "        # Note: The model is trained in a supervised manner using the given\n",
    "        # `edge_label_index` and `edge_label` targets.\n",
    "        out = model.decode(z, edge_label_index).view(-1)\n",
    "        loss = criterion(out, edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    all_out = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in loader:\n",
    "        z = model.encode(batch.x, batch.edge_index)\n",
    "        out = model.decode(z, batch.edge_label_index).view(-1).sigmoid()\n",
    "        all_out.append(out.cpu().numpy())\n",
    "        all_labels.append(batch.edge_label.cpu().numpy())\n",
    "\n",
    "    all_out = np.concatenate(all_out)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return roc_auc_score(all_labels, all_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 122.2200, Val: 0.8519, Test: 0.8531\n",
      "Epoch: 002, Loss: 1.4640, Val: 0.8638, Test: 0.8646\n",
      "Epoch: 003, Loss: 0.7274, Val: 0.8689, Test: 0.8694\n",
      "Epoch: 004, Loss: 0.5976, Val: 0.8711, Test: 0.8717\n",
      "Epoch: 005, Loss: 0.5664, Val: 0.8718, Test: 0.8725\n",
      "Epoch: 006, Loss: 0.5568, Val: 0.8731, Test: 0.8737\n",
      "Epoch: 007, Loss: 0.5530, Val: 0.8743, Test: 0.8747\n",
      "Epoch: 008, Loss: 0.5514, Val: 0.8757, Test: 0.8763\n",
      "Epoch: 009, Loss: 0.5502, Val: 0.8769, Test: 0.8774\n",
      "Epoch: 010, Loss: 0.5489, Val: 0.8782, Test: 0.8787\n",
      "Epoch: 011, Loss: 0.5481, Val: 0.8803, Test: 0.8808\n",
      "Epoch: 012, Loss: 0.5472, Val: 0.8815, Test: 0.8821\n",
      "Epoch: 013, Loss: 0.5464, Val: 0.8841, Test: 0.8847\n",
      "Epoch: 014, Loss: 0.5448, Val: 0.8862, Test: 0.8868\n",
      "Epoch: 015, Loss: 0.5435, Val: 0.8886, Test: 0.8890\n",
      "Epoch: 016, Loss: 0.5421, Val: 0.8917, Test: 0.8921\n",
      "Epoch: 017, Loss: 0.5410, Val: 0.8945, Test: 0.8950\n",
      "Epoch: 018, Loss: 0.5394, Val: 0.8979, Test: 0.8983\n",
      "Epoch: 019, Loss: 0.5358, Val: 0.9014, Test: 0.9020\n",
      "Epoch: 020, Loss: 0.5338, Val: 0.9051, Test: 0.9057\n",
      "Epoch: 021, Loss: 0.5310, Val: 0.9096, Test: 0.9100\n",
      "Epoch: 022, Loss: 0.5281, Val: 0.9136, Test: 0.9140\n",
      "Epoch: 023, Loss: 0.5245, Val: 0.9169, Test: 0.9173\n",
      "Epoch: 024, Loss: 0.5218, Val: 0.9216, Test: 0.9218\n",
      "Epoch: 025, Loss: 0.5199, Val: 0.9234, Test: 0.9239\n",
      "Epoch: 026, Loss: 0.5170, Val: 0.9275, Test: 0.9276\n",
      "Epoch: 027, Loss: 0.5146, Val: 0.9280, Test: 0.9284\n",
      "Epoch: 028, Loss: 0.5120, Val: 0.9302, Test: 0.9305\n",
      "Epoch: 029, Loss: 0.5118, Val: 0.9313, Test: 0.9317\n",
      "Epoch: 030, Loss: 0.5106, Val: 0.9322, Test: 0.9326\n",
      "Epoch: 031, Loss: 0.5097, Val: 0.9327, Test: 0.9332\n",
      "Epoch: 032, Loss: 0.5092, Val: 0.9331, Test: 0.9336\n",
      "Epoch: 033, Loss: 0.5089, Val: 0.9336, Test: 0.9341\n",
      "Epoch: 034, Loss: 0.5089, Val: 0.9321, Test: 0.9328\n",
      "Epoch: 035, Loss: 0.5095, Val: 0.9337, Test: 0.9344\n",
      "Epoch: 036, Loss: 0.5071, Val: 0.9343, Test: 0.9350\n",
      "Epoch: 037, Loss: 0.5074, Val: 0.9337, Test: 0.9344\n",
      "Epoch: 038, Loss: 0.5069, Val: 0.9335, Test: 0.9343\n",
      "Epoch: 039, Loss: 0.5067, Val: 0.9350, Test: 0.9357\n",
      "Epoch: 040, Loss: 0.5065, Val: 0.9350, Test: 0.9355\n",
      "Epoch: 041, Loss: 0.5075, Val: 0.9353, Test: 0.9360\n",
      "Epoch: 042, Loss: 0.5064, Val: 0.9355, Test: 0.9361\n",
      "Epoch: 043, Loss: 0.5061, Val: 0.9352, Test: 0.9359\n",
      "Epoch: 044, Loss: 0.5054, Val: 0.9356, Test: 0.9363\n",
      "Epoch: 045, Loss: 0.5063, Val: 0.9359, Test: 0.9363\n",
      "Epoch: 046, Loss: 0.5058, Val: 0.9362, Test: 0.9367\n",
      "Epoch: 047, Loss: 0.5051, Val: 0.9350, Test: 0.9357\n",
      "Epoch: 048, Loss: 0.5059, Val: 0.9341, Test: 0.9350\n",
      "Epoch: 049, Loss: 0.5052, Val: 0.9352, Test: 0.9360\n",
      "Epoch: 050, Loss: 0.5043, Val: 0.9366, Test: 0.9371\n",
      "Epoch: 051, Loss: 0.5042, Val: 0.9349, Test: 0.9357\n",
      "Epoch: 052, Loss: 0.5053, Val: 0.9367, Test: 0.9374\n",
      "Epoch: 053, Loss: 0.5052, Val: 0.9370, Test: 0.9376\n",
      "Epoch: 054, Loss: 0.5034, Val: 0.9374, Test: 0.9381\n",
      "Epoch: 055, Loss: 0.5033, Val: 0.9376, Test: 0.9381\n",
      "Epoch: 056, Loss: 0.5031, Val: 0.9375, Test: 0.9381\n",
      "Epoch: 057, Loss: 0.5034, Val: 0.9363, Test: 0.9371\n",
      "Epoch: 058, Loss: 0.5030, Val: 0.9378, Test: 0.9383\n",
      "Epoch: 059, Loss: 0.5033, Val: 0.9377, Test: 0.9383\n",
      "Epoch: 060, Loss: 0.5035, Val: 0.9379, Test: 0.9384\n",
      "Epoch: 061, Loss: 0.5029, Val: 0.9369, Test: 0.9377\n",
      "Epoch: 062, Loss: 0.5021, Val: 0.9383, Test: 0.9390\n",
      "Epoch: 063, Loss: 0.5017, Val: 0.9371, Test: 0.9377\n",
      "Epoch: 064, Loss: 0.5021, Val: 0.9386, Test: 0.9391\n",
      "Epoch: 065, Loss: 0.5031, Val: 0.9388, Test: 0.9394\n",
      "Epoch: 066, Loss: 0.5016, Val: 0.9389, Test: 0.9391\n",
      "Epoch: 067, Loss: 0.5007, Val: 0.9386, Test: 0.9390\n",
      "Epoch: 068, Loss: 0.5019, Val: 0.9379, Test: 0.9386\n",
      "Epoch: 069, Loss: 0.5000, Val: 0.9390, Test: 0.9392\n",
      "Epoch: 070, Loss: 0.5012, Val: 0.9397, Test: 0.9402\n",
      "Epoch: 071, Loss: 0.5029, Val: 0.9383, Test: 0.9388\n",
      "Epoch: 072, Loss: 0.5006, Val: 0.9405, Test: 0.9408\n",
      "Epoch: 073, Loss: 0.5008, Val: 0.9402, Test: 0.9406\n",
      "Epoch: 074, Loss: 0.5018, Val: 0.9391, Test: 0.9395\n",
      "Epoch: 075, Loss: 0.5004, Val: 0.9405, Test: 0.9407\n",
      "Epoch: 076, Loss: 0.5009, Val: 0.9399, Test: 0.9403\n",
      "Epoch: 077, Loss: 0.5005, Val: 0.9402, Test: 0.9404\n",
      "Epoch: 078, Loss: 0.5002, Val: 0.9407, Test: 0.9410\n",
      "Epoch: 079, Loss: 0.4995, Val: 0.9409, Test: 0.9413\n",
      "Epoch: 080, Loss: 0.5002, Val: 0.9405, Test: 0.9405\n",
      "Epoch: 081, Loss: 0.5011, Val: 0.9414, Test: 0.9417\n",
      "Epoch: 082, Loss: 0.5000, Val: 0.9411, Test: 0.9415\n",
      "Epoch: 083, Loss: 0.5014, Val: 0.9402, Test: 0.9404\n",
      "Epoch: 084, Loss: 0.4979, Val: 0.9414, Test: 0.9415\n",
      "Epoch: 085, Loss: 0.5026, Val: 0.9417, Test: 0.9420\n",
      "Epoch: 086, Loss: 0.4987, Val: 0.9422, Test: 0.9422\n",
      "Epoch: 087, Loss: 0.5015, Val: 0.9414, Test: 0.9416\n",
      "Epoch: 088, Loss: 0.5001, Val: 0.9428, Test: 0.9428\n",
      "Epoch: 089, Loss: 0.5008, Val: 0.9421, Test: 0.9422\n",
      "Epoch: 090, Loss: 0.4997, Val: 0.9421, Test: 0.9420\n",
      "Epoch: 091, Loss: 0.4993, Val: 0.9431, Test: 0.9432\n",
      "Epoch: 092, Loss: 0.4991, Val: 0.9377, Test: 0.9374\n",
      "Epoch: 093, Loss: 0.5023, Val: 0.9426, Test: 0.9425\n",
      "Epoch: 094, Loss: 0.4983, Val: 0.9434, Test: 0.9435\n",
      "Epoch: 095, Loss: 0.4980, Val: 0.9443, Test: 0.9442\n",
      "Epoch: 096, Loss: 0.4999, Val: 0.9433, Test: 0.9434\n",
      "Epoch: 097, Loss: 0.5011, Val: 0.9437, Test: 0.9436\n",
      "Epoch: 098, Loss: 0.4969, Val: 0.9427, Test: 0.9428\n",
      "Epoch: 099, Loss: 0.4967, Val: 0.9421, Test: 0.9419\n",
      "Epoch: 100, Loss: 0.4985, Val: 0.9385, Test: 0.9389\n",
      "Final Test: 0.9442\n"
     ]
    }
   ],
   "source": [
    "# Train/Test Loop\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train(train_loader)\n",
    "    val_auc = test(val_loader)\n",
    "    test_auc = test(test_loader)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1-NN additional statistics</h2>\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict = {}\n",
    "cnt = 0\n",
    "for graph in data_list:\n",
    "    z_raw = model.encode(graph.x, graph.edge_index)\n",
    "    final_edge_index = model.decode_all(z_raw)\n",
    "    fei = final_edge_index.tolist()\n",
    "    edges_pred = {k:[] for k in range(101)}\n",
    "    edges_pred_inv = {k:[] for k in range(101)}\n",
    "    for i in range(len(fei[0])):\n",
    "        edges_pred[fei[0][i]].append(fei[1][i])\n",
    "        edges_pred_inv[fei[1][i]].append(fei[0][i])\n",
    "    ts0 = graph.edge_index.tolist()\n",
    "    edges = {k:[] for k in range(101)}\n",
    "    edges_inv = {k:[] for k in range(101)}\n",
    "    for i in range(len(ts0[0])):\n",
    "        edges[ts0[0][i]].append(ts0[1][i])\n",
    "        edges_inv[ts0[1][i]].append(ts0[0][i])\n",
    "    originals = {}\n",
    "    predictions = {}\n",
    "    for i in range(101):\n",
    "        originals[i] = set(edges[i] + edges_inv[i])\n",
    "        predictions[i] = set(edges_pred[i] + edges_pred_inv[i])\n",
    "    graph_dict[cnt] = {\"real\": originals, \"preds\": predictions}\n",
    "    cnt += 1\n",
    "\n",
    "confusion_dict = {}\n",
    "true_positives = []\n",
    "false_positives = []\n",
    "true_negatives = []\n",
    "false_negatives = []\n",
    "for key in graph_dict:\n",
    "    real = graph_dict[key][\"real\"]\n",
    "    pred = graph_dict[key][\"preds\"]\n",
    "    node_matrix = {}\n",
    "    for i in range(101):\n",
    "        tp = len(pred[i].intersection(real[i]))\n",
    "        fp = len(pred[i] - real[i])\n",
    "        real_neg = set([j for j in range(101)]) - {i} - real[i]\n",
    "        pred_neg = set([j for j in range(101)]) - {i} - pred[i]\n",
    "        tn = len(pred_neg.intersection(real_neg))\n",
    "        fn = len(pred_neg - real_neg)\n",
    "        total = tp + fp + tn + fn\n",
    "        node_matrix[i] = {\"tp\": tp, \"fp\": fp, \"tn\": tn, \"fn\": fn, \"total\": total}\n",
    "        true_positives.append(tp/total)\n",
    "        false_positives.append(fp/total)\n",
    "        true_negatives.append(tn/total)\n",
    "        false_negatives.append(fn/total)\n",
    "    confusion_dict[key] = node_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positives mean=0.17, stdev=0.03\n",
      "false positives mean=0.28, stdev=0.10\n",
      "true negatives mean=0.54, stdev=0.10\n",
      "false negatives mean=0.00, stdev=0.00\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, stdev\n",
    "print(\"true positives mean={0:.2f}, stdev={1:.2f}\".format(mean(true_positives), stdev(true_positives)))\n",
    "print(\"false positives mean={0:.2f}, stdev={1:.2f}\".format(mean(false_positives), stdev(false_positives)))\n",
    "print(\"true negatives mean={0:.2f}, stdev={1:.2f}\".format(mean(true_negatives), stdev(true_negatives)))\n",
    "print(\"false negatives mean={0:.2f}, stdev={1:.2f}\".format(mean(false_negatives), stdev(false_negatives)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some notion of closeness. It over-links, but rarely avoids a close node. Also, a middle stage as this is not always a requirement. We want to encode the data in embeddings that can be used for the next stage. The quality will be measured then over the ability to learn closeness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1-NN data preparation for second stage</h2>\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "data_raw_dict = {\n",
    "    \"positives\": [],\n",
    "    \"negatives\": []\n",
    "}\n",
    "cnt = 0\n",
    "pos_cnt = 0\n",
    "neg_cnt = 0\n",
    "for graph in data_list:\n",
    "    neg_cnt = 0\n",
    "    ng_matrix = graph.y.tolist()\n",
    "    encoding_matrix = model.encode(graph.x, graph.edge_index).tolist()\n",
    "    for i in range(101):\n",
    "        for j in sample(range(101), 30):\n",
    "            if i == j:\n",
    "                # if pos_cnt < 80000:\n",
    "                #     data_raw_dict[\"positives\"].append(encoding_matrix[i]+encoding_matrix[i])\n",
    "                #     pos_cnt += 1\n",
    "                continue\n",
    "            else:\n",
    "                if ng_matrix[i][j] > 0.5:\n",
    "                    if pos_cnt < 80000:\n",
    "                        data_raw_dict[\"positives\"].append(encoding_matrix[i]+encoding_matrix[j]+[1])\n",
    "                        pos_cnt += 1\n",
    "                else:\n",
    "                    if pos_cnt < 80000 and neg_cnt < 1010:\n",
    "                        data_raw_dict[\"negatives\"].append(encoding_matrix[i]+encoding_matrix[j]+[0])\n",
    "                        neg_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tensor = data_raw_dict[\"positives\"][:5000] + data_raw_dict[\"negatives\"][:5000]\n",
    "main_tensor = torch.tensor(pre_tensor, dtype=torch.float32)\n",
    "\n",
    "main_tensor = main_tensor[torch.randperm(main_tensor.size()[0])]\n",
    "labels = main_tensor[:,-1:]\n",
    "embeddings = main_tensor[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1-NN Wide or Deep network</h2>\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Wide(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(128, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "class Deep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(128, 128)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(128, 128)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    " \n",
    "def model_train(model, X_train, y_train, X_val, y_val):\n",
    "    # loss function and optimizer\n",
    "    loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    " \n",
    "    n_epochs = 20   # number of epochs to run\n",
    "    batch_size = 10  # size of each batch\n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
    " \n",
    "    # Hold the best model\n",
    "    best_acc = - np.inf   # init to negative infinity\n",
    "    best_weights = None\n",
    " \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "                # take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "                # forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "                # print progress\n",
    "                acc = (y_pred.round() == y_batch).float().mean()\n",
    "                bar.set_postfix(\n",
    "                    loss=float(loss),\n",
    "                    acc=float(acc)\n",
    "                )\n",
    "        # evaluate accuracy at end of each epoch\n",
    "        model.eval()\n",
    "        y_pred = model(X_val)\n",
    "        acc = (y_pred.round() == y_val).float().mean()\n",
    "        acc = float(acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "    # restore model and return best accuracy\n",
    "    model.load_state_dict(best_weights)\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (wide): 0.93\n",
      "Accuracy (wide): 0.94\n",
      "Accuracy (wide): 0.92\n",
      "Accuracy (wide): 0.94\n",
      "Accuracy (wide): 0.93\n",
      "Accuracy (deep): 0.95\n",
      "Accuracy (deep): 0.94\n",
      "Accuracy (deep): 0.95\n",
      "Accuracy (deep): 0.95\n",
      "Accuracy (deep): 0.95\n",
      "Wide: 93.19% (+/- 0.53%)\n",
      "Deep: 94.86% (+/- 0.42%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "# train-test split: Hold out the test set for final model evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, train_size=0.7, shuffle=True)\n",
    " \n",
    "# define 5-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "cv_scores_wide = []\n",
    "for train, test in kfold.split(X_train, y_train):\n",
    "    # create model, train, and get accuracy\n",
    "    model = Wide()\n",
    "    acc = model_train(model, X_train[train], y_train[train], X_train[test], y_train[test])\n",
    "    print(\"Accuracy (wide): %.2f\" % acc)\n",
    "    cv_scores_wide.append(acc)\n",
    "cv_scores_deep = []\n",
    "for train, test in kfold.split(X_train, y_train):\n",
    "    # create model, train, and get accuracy\n",
    "    model = Deep()\n",
    "    acc = model_train(model, X_train[train], y_train[train], X_train[test], y_train[test])\n",
    "    print(\"Accuracy (deep): %.2f\" % acc)\n",
    "    cv_scores_deep.append(acc)\n",
    " \n",
    "# evaluate the model\n",
    "wide_acc = np.mean(cv_scores_wide)\n",
    "wide_std = np.std(cv_scores_wide)\n",
    "deep_acc = np.mean(cv_scores_deep)\n",
    "deep_std = np.std(cv_scores_deep)\n",
    "print(\"Wide: %.2f%% (+/- %.2f%%)\" % (wide_acc*100, wide_std*100))\n",
    "print(\"Deep: %.2f%% (+/- %.2f%%)\" % (deep_acc*100, deep_std*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrain a deep model\n",
      "Final model accuracy: 95.13%\n"
     ]
    }
   ],
   "source": [
    "# rebuild model with full set of training data\n",
    "if wide_acc > deep_acc:\n",
    "    print(\"Retrain a wide model\")\n",
    "    model = Wide()\n",
    "else:\n",
    "    print(\"Retrain a deep model\")\n",
    "    model = Deep()\n",
    "acc = model_train(model, X_train, y_train, X_test, y_test)\n",
    "print(f\"Final model accuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.1717339e-01 -5.7865076e-02 -1.4151484e-02  1.5557876e-02\n",
      " -4.2768332e-01 -4.4370964e-01  3.3552490e-02  4.1792460e-02\n",
      " -4.4831714e-01 -7.0630580e-02  2.1677202e-01  2.2268556e-01\n",
      " -6.3116723e-01  8.4077612e-02  5.1827902e-01  3.1405866e-01\n",
      " -2.0323183e-01  8.0681220e-02  1.3163482e-01 -1.4057782e-01\n",
      " -1.0440167e-02  9.3202174e-02  1.8332930e-01  2.2968042e-01\n",
      "  1.0498860e-01 -2.6336920e-01  5.9151962e-02  5.4599382e-02\n",
      " -5.2246571e-02 -2.9523142e-02  2.7236146e-01 -5.9649184e-02\n",
      " -2.9510462e-01 -2.3909754e-01 -2.0827483e-01 -4.0251181e-01\n",
      " -1.6330114e-01 -2.0440927e-01 -7.0687167e-02  2.9918218e-01\n",
      "  9.8987930e-03  9.5853359e-02 -2.3753516e-01 -1.8214691e-01\n",
      " -4.3035455e-02 -1.5556519e-02 -1.2626988e-01  1.3818063e-01\n",
      " -2.8056109e-01  5.3613499e-02 -2.5029457e-01  3.6895238e-02\n",
      " -1.3619283e-01  5.3167921e-01 -3.1269984e-03 -3.6275232e-01\n",
      "  2.5250453e-01  1.8157607e-01 -2.1313316e-01 -8.1577316e-02\n",
      "  1.5478041e-02  2.6419458e-01  5.4498559e-01  2.4573167e-01\n",
      "  4.3307829e-01 -5.6470633e-02 -1.1331800e-02  2.2720667e-02\n",
      " -4.2329457e-01 -4.5480761e-01  4.4342883e-02  4.5427322e-02\n",
      " -4.4606289e-01 -8.0577120e-02  2.1183130e-01  2.3732670e-01\n",
      " -6.3463920e-01  9.8108679e-02  5.1491594e-01  3.1170848e-01\n",
      " -2.1752605e-01  8.0698565e-02  1.3647492e-01 -1.4649908e-01\n",
      " -1.1975139e-02  8.7222464e-02  1.8109830e-01  2.2941652e-01\n",
      "  9.2107445e-02 -2.6699829e-01  5.0050810e-02  4.7848269e-02\n",
      " -3.7784070e-02 -1.9218609e-02  2.7938482e-01 -5.8992282e-02\n",
      " -2.8603178e-01 -2.3820066e-01 -1.9802438e-01 -4.0632907e-01\n",
      " -1.5602684e-01 -2.1411142e-01 -7.1299389e-02  3.2000360e-01\n",
      "  1.4883975e-02  9.6994124e-02 -2.4576062e-01 -1.7677087e-01\n",
      " -3.5438828e-02 -1.8663231e-02 -1.2395805e-01  1.4290737e-01\n",
      " -2.8109637e-01  5.1804379e-02 -2.5030202e-01  2.8354084e-02\n",
      " -1.3504303e-01  5.2837777e-01  5.8637280e-04 -3.6035749e-01\n",
      "  2.5716203e-01  1.8610978e-01 -2.0425469e-01 -7.5680345e-02\n",
      "  1.3318401e-02  2.5793216e-01  5.3815591e-01  2.5184375e-01] -> [0.8791382] (expected [1.])\n",
      "[-0.330864    0.06145173 -0.19024104 -0.0187828   0.02340415 -0.02353234\n",
      " -0.09699653  0.01063286  0.07676873  0.10265465 -0.20202741 -0.15060855\n",
      "  0.16400571 -0.16511227 -0.13620533  0.17994827  0.12047814 -0.13892125\n",
      " -0.04614743  0.10489418  0.10139035  0.10708459  0.02570695 -0.2025975\n",
      "  0.27189144 -0.02998438  0.10394385  0.05386116 -0.0839919  -0.16725492\n",
      " -0.08409136 -0.05255683  0.03403759  0.01767641 -0.01208991  0.09068033\n",
      "  0.07768191  0.12292603  0.10131624 -0.34487686 -0.05155137  0.00810724\n",
      " -0.01497776 -0.16132963 -0.01401632  0.15828408  0.09045064  0.03769962\n",
      "  0.14451867 -0.05035711  0.15338582  0.08894943  0.18451214 -0.09988489\n",
      "  0.1258642  -0.01964192 -0.12790039 -0.1538146  -0.08576083 -0.08980085\n",
      " -0.06369558  0.13308588  0.15275443 -0.06599046  0.32791537 -0.05607147\n",
      " -0.01604865 -0.00805374 -0.3239973  -0.42272744  0.09292679  0.04010247\n",
      " -0.43265417 -0.13727242  0.17752124  0.2289034  -0.580992    0.06468205\n",
      "  0.36844456  0.31403068 -0.14854711  0.19902556  0.08086179 -0.1680052\n",
      "  0.00577518  0.02677535  0.1272771   0.15678254  0.05438989 -0.19849831\n",
      "  0.09766823 -0.03170805 -0.04216717  0.01377597  0.23065558  0.00603591\n",
      " -0.25511116 -0.19574961 -0.10372823 -0.37275997 -0.10334317 -0.1962428\n",
      "  0.01648377  0.26799214 -0.0055501   0.06452463 -0.14870533 -0.14268668\n",
      "  0.05730914  0.03938692 -0.08882365  0.06507913 -0.26012477  0.08325417\n",
      " -0.21879144 -0.02418476 -0.19352373  0.45169723 -0.04488386 -0.27481258\n",
      "  0.14917243  0.18355168 -0.14959304 -0.11858585 -0.02265492  0.17981535\n",
      "  0.5236769   0.20937355] -> [9.802447e-09] (expected [0.])\n",
      "[ 0.22271447 -0.15952486  0.18382713 -0.0736852  -0.1735313  -0.2958808\n",
      "  0.1651783  -0.12247935 -0.1891824  -0.0376753  -0.01021766  0.17950147\n",
      " -0.16975714  0.05774587  0.1254158   0.14762676 -0.12215778  0.63846254\n",
      " -0.07816781 -0.16154514 -0.07402876 -0.26636425 -0.13094182  0.13025358\n",
      " -0.24119826 -0.06175733  0.1358243  -0.1056514  -0.02328973  0.08909103\n",
      " -0.04106475  0.0768939  -0.27535206 -0.2432819   0.17938295 -0.48485598\n",
      " -0.21559832 -0.19126189  0.14253952  0.26128456 -0.05623893  0.05568039\n",
      "  0.02272103  0.03955377  0.23339663 -0.10984882  0.06858939  0.02321001\n",
      " -0.0886617   0.37852514 -0.4213081  -0.00954623 -0.43551075  0.03039259\n",
      " -0.12275901  0.00415562 -0.09740531  0.0846632  -0.02359264 -0.10913993\n",
      "  0.04740994 -0.1211182   0.21601045 -0.07385829 -0.32684976  0.22578472\n",
      " -0.18895131 -0.007252   -0.09391057 -0.12486035  0.06219388 -0.28197473\n",
      "  0.01795186  0.4119055  -0.104416   -0.26223752  0.21181701 -0.01028562\n",
      " -0.0881796   0.13924631  0.12256099 -0.01716547 -0.02672764  0.29523718\n",
      "  0.06995692  0.08960645  0.01676747 -0.22883645  0.30053788  0.06775854\n",
      "  0.2258673   0.07662316 -0.17618199 -0.37895942 -0.03361698 -0.16629015\n",
      " -0.07676308 -0.48291153  0.01749101 -0.43210956 -0.13394362  0.07412936\n",
      "  0.18788399 -0.2770224  -0.09938193  0.07316856  0.10176948 -0.49174565\n",
      " -0.20396474 -0.07086708  0.00149342  0.16793291  0.11178198 -0.08262042\n",
      " -0.22674532  0.3133783   0.3294002  -0.1406103   0.1196633  -0.16833246\n",
      " -0.23056692 -0.27149156 -0.15944612  0.02952655  0.125426    0.14204529\n",
      "  0.28754896 -0.09822766] -> [5.1421516e-06] (expected [0.])\n",
      "[-0.0837351  -0.0448166  -0.02411401 -0.04934224 -0.17384487  0.0436961\n",
      " -0.16253164  0.00429083 -0.23038313  0.04281615  0.10458341 -0.14767751\n",
      " -0.16443859 -0.25273436  0.2536457   0.28187922  0.18969768 -0.07971613\n",
      " -0.00124664  0.0167587   0.12776399  0.10312244  0.17823721  0.26184237\n",
      "  0.33748722 -0.12687102  0.15838829  0.15304278 -0.34170038 -0.28474018\n",
      "  0.14931548 -0.0870682  -0.40944332 -0.18892667 -0.45725405  0.03313363\n",
      " -0.05818337  0.18113264 -0.11131335 -0.33085167  0.00981047  0.03956431\n",
      " -0.05151433 -0.0908279  -0.1923779   0.13452251 -0.22167386  0.07144933\n",
      " -0.078137    0.03095455 -0.01341315  0.17364275  0.12554172  0.33699083\n",
      "  0.09001357 -0.22338557 -0.01216271  0.06234008 -0.27405703 -0.10425153\n",
      "  0.01439177  0.30483744  0.48207515 -0.03447701  0.14754231 -0.1325877\n",
      " -0.01699865 -0.05028243 -0.28408775 -0.13301158 -0.1358625   0.08784046\n",
      " -0.34732074 -0.06451582  0.14791946  0.02272195 -0.34264398 -0.17731088\n",
      "  0.36250448  0.30614182  0.03638357 -0.02239242  0.0565416  -0.06525221\n",
      "  0.06708034  0.10107493  0.1651579   0.23970264  0.22873928 -0.21504852\n",
      "  0.07301868  0.10800594 -0.21460019 -0.1220111   0.21358922 -0.04224831\n",
      " -0.36934036 -0.09301305 -0.40087014 -0.03471315 -0.08040024  0.02077419\n",
      " -0.12134337 -0.07245152  0.0294169   0.04603503 -0.15192299 -0.05395331\n",
      " -0.0933306   0.14605504 -0.19364022  0.02699062 -0.18397355  0.04634377\n",
      " -0.03619428  0.0591231  -0.0431584   0.46648848  0.03742462 -0.2539232\n",
      "  0.12606579  0.12370135 -0.2251651  -0.11487918 -0.01822015  0.28772992\n",
      "  0.49116087  0.09274285] -> [0.41050267] (expected [1.])\n",
      "[-7.63157979e-02 -2.40801387e-02  6.28724545e-02 -3.44757959e-02\n",
      "  1.64169565e-01  1.27958730e-02  9.96308103e-02  1.40649468e-01\n",
      "  8.38868394e-02 -2.85503089e-01 -1.41207337e-01  9.28785130e-02\n",
      " -4.95303869e-02 -7.77546614e-02 -1.27960384e-01  2.22919405e-01\n",
      "  8.11097696e-02  9.67583209e-02 -4.66409475e-02 -2.14993894e-01\n",
      "  3.77927944e-02 -6.59181550e-02 -6.07011244e-02  4.76374179e-02\n",
      " -6.41053170e-02 -2.46628150e-02 -8.66867602e-03 -4.12586555e-02\n",
      "  3.04690301e-02  1.43760383e-01  7.06293434e-03  1.76219314e-01\n",
      "  5.16500324e-02  2.35103041e-01  1.01228997e-01  2.44096160e-01\n",
      "  1.96702570e-01  9.34800804e-02 -5.77658042e-03 -1.25255555e-01\n",
      "  7.33259320e-03 -7.32862055e-02 -3.66118737e-03  1.56098679e-01\n",
      "  1.33527920e-01  1.36991665e-01  1.06185719e-01 -1.03929698e-01\n",
      "  3.40567641e-02  2.78462470e-02  2.24395454e-01 -2.43084431e-01\n",
      " -5.09101376e-02 -1.23238772e-01  3.37802768e-02  1.80506930e-01\n",
      " -7.67727867e-02  3.44040580e-02  2.73749754e-02 -1.14939764e-01\n",
      " -1.28552318e-01 -7.54444301e-02 -2.40921378e-02 -1.61090046e-02\n",
      "  1.32202223e-01 -3.02148402e-01  2.04013363e-01 -3.61703224e-02\n",
      "  9.46377963e-03  1.68025225e-01 -1.47889435e-01 -1.23928487e-01\n",
      "  1.78889155e-01 -4.99179959e-03  3.28678973e-02 -6.48027211e-02\n",
      "  1.92209333e-01 -2.33848020e-03 -2.74872854e-02 -2.61941344e-01\n",
      " -1.05844392e-02  5.35906404e-02  8.15910175e-02 -1.90383457e-02\n",
      " -1.10120445e-01 -1.31365150e-01 -1.44217923e-01  1.37729719e-01\n",
      " -3.24740946e-01  9.45773721e-03 -1.66778952e-01  1.16029754e-02\n",
      "  2.19870359e-02  1.53416544e-01  4.92712855e-03  4.93074730e-02\n",
      " -4.22657281e-02  1.38083979e-01 -1.36388168e-01  2.16364086e-01\n",
      " -1.41709000e-02 -2.08452344e-04 -1.71694905e-01  2.16244325e-01\n",
      "  2.47190781e-02 -5.79462349e-02  1.28445730e-01  2.93491840e-01\n",
      "  3.04497294e-02 -9.92563367e-02 -1.09400377e-01 -7.20842406e-02\n",
      " -4.58106101e-02  6.93200529e-02 -8.91467780e-02 -1.01222351e-01\n",
      " -8.20192769e-02 -2.83215642e-01 -6.67031705e-02  2.84511328e-01\n",
      "  5.32905385e-02 -2.10873820e-02  1.52256384e-01  1.98680595e-01\n",
      "  2.67397948e-02 -2.40933120e-01 -3.86802971e-01 -1.57303333e-01] -> [0.00160045] (expected [0.])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Test out inference with 5 samples\n",
    "    for i in range(5):\n",
    "        y_pred = model(X_test[i:i+1])\n",
    "        print(f\"{X_test[i].numpy()} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one above is an example for visual validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>NgSets prototype, undirected</h1>\n",
    "Once validated the simple prototype, let's continue to the actual problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load the data</h2>\n",
    "Here an \"export\" folder with .txt instance files used on the CTWVRP project, and a .csv file with the neighborhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_files_list = [\"./export/\"+f for f in os.listdir(\"./export\") ]\n",
    "instance_dict = {}\n",
    "for dir_str in data_files_list:\n",
    "    with open(dir_str, 'r') as text_file:\n",
    "        cnt = 0\n",
    "        instance = \"\"\n",
    "        for line in text_file:\n",
    "            if cnt < 9:\n",
    "                if cnt == 0:\n",
    "                    instance = line.split()[0]\n",
    "                    instance_dict[instance] = []\n",
    "                cnt += 1\n",
    "                continue\n",
    "            split_line = line.split()\n",
    "            instance_dict[instance].append([int(i) for i in split_line])\n",
    "        text_file.close()\n",
    "\n",
    "ng_dict = {}\n",
    "cnt = -1\n",
    "with open(\"ng_outs.csv\", 'r') as text_file:\n",
    "    for line in text_file:\n",
    "        if cnt < 2:\n",
    "            cnt += 1\n",
    "            continue\n",
    "        raw_line = line.strip()\n",
    "        split_line_list = raw_line.split(sep=\";\")\n",
    "        instance = split_line_list[3]\n",
    "        if instance not in ng_dict:\n",
    "            ng_dict[instance] = [[0 for i in range(101)]]\n",
    "        ng_dict[instance].append([0] + [int(i) for i in split_line_list[5:-1]])\n",
    "        if len(split_line_list[5:-1]) != 100:\n",
    "            print(\"case found for instance \"+instance)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import knn_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_list = []\n",
    "# for instance_name in ng_dict:\n",
    "#     y = torch.tensor(ng_dict[instance_name], dtype=torch.double)\n",
    "#     x = torch.tensor(instance_dict[instance_name], dtype=torch.double)\n",
    "#     attr = [[i] for i in range(n_edges)]\n",
    "#     loc_dict = {(i[0],j[0]): sqrt((i[1]-j[1])**2 + (i[2]-j[2])**2) for i in instance_dict[instance_name] for j in instance_dict[instance_name]}\n",
    "#     cnt = -1\n",
    "#     for i in range(101):\n",
    "#         for j in range(101):\n",
    "#             if i != j:\n",
    "#                 cnt += 1\n",
    "#                 attr[cnt].append(loc_dict[i,j])\n",
    "#     attr = torch.tensor(attr, dtype=torch.double)\n",
    "#     pos = []\n",
    "#     for i in instance_dict[instance_name]:\n",
    "#         pos.append([i[1], i[2]])\n",
    "#     pos = torch.tensor(pos, dtype=torch.double)\n",
    "#     data_list.append(Data(x=x, y=y, edge_index=edge_index, pos=pos, edge_attr=attr))\n",
    "\n",
    "data_list = []\n",
    "for instance_name in ng_dict:\n",
    "    y = torch.tensor(ng_dict[instance_name], dtype=torch.float)\n",
    "    x = torch.tensor(instance_dict[instance_name], dtype=torch.float)\n",
    "    pos = []\n",
    "    for i in instance_dict[instance_name]:\n",
    "        pos.append([i[1], i[2]])\n",
    "    pos = torch.tensor(pos, dtype=torch.double)\n",
    "    data_list.append(Data(x=x, y=y, edge_index = knn_graph(pos, 15), pos=pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Prepare data as in prototype</h2>\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "def add_edge_labels(graph):\n",
    "    transform = T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False)\n",
    "    return transform(graph)\n",
    "\n",
    "labeled_graphs = [add_edge_labels(graph) for graph in data_list]\n",
    "\n",
    "train_size = [g[0] for g in labeled_graphs]\n",
    "val_size = [g[1] for g in labeled_graphs]\n",
    "test_size = [g[2] for g in labeled_graphs]\n",
    "\n",
    "train_loader = DataLoader(train_size, batch_size=150, shuffle=True)\n",
    "val_loader = DataLoader(val_size, batch_size=150, shuffle=False)\n",
    "test_loader = DataLoader(test_size, batch_size=150, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Encode-decode routine</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ed = Net(data_list[0].num_features, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model_ed.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(loader):\n",
    "    model_ed.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        z = model_ed.encode(batch.x, batch.edge_index)\n",
    "\n",
    "        # We perform a new round of negative sampling for every training epoch:\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=batch.edge_index, num_nodes=batch.num_nodes,\n",
    "            num_neg_samples=batch.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "        # Concat positive and negative edge indices.\n",
    "        edge_label_index = torch.cat(\n",
    "            [batch.edge_label_index, neg_edge_index],\n",
    "            dim=-1,\n",
    "        )\n",
    "        # Label for positive edges: 1, for negative edges: 0.\n",
    "        edge_label = torch.cat([\n",
    "            batch.edge_label,\n",
    "            batch.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "        ], dim=0)\n",
    "\n",
    "        # Note: The model is trained in a supervised manner using the given\n",
    "        # `edge_label_index` and `edge_label` targets.\n",
    "        out = model_ed.decode(z, edge_label_index).view(-1)\n",
    "        loss = criterion(out, edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model_ed.eval()\n",
    "    all_out = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in loader:\n",
    "        z = model_ed.encode(batch.x, batch.edge_index)\n",
    "        out = model_ed.decode(z, batch.edge_label_index).view(-1).sigmoid()\n",
    "        all_out.append(out.cpu().numpy())\n",
    "        all_labels.append(batch.edge_label.cpu().numpy())\n",
    "\n",
    "    all_out = np.concatenate(all_out)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return roc_auc_score(all_labels, all_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 3054.1865, Val: 0.5342, Test: 0.5341\n",
      "Epoch: 002, Loss: 19.2132, Val: 0.6063, Test: 0.6062\n",
      "Epoch: 003, Loss: 8.8700, Val: 0.6729, Test: 0.6726\n",
      "Epoch: 004, Loss: 5.2983, Val: 0.7367, Test: 0.7363\n",
      "Epoch: 005, Loss: 3.4569, Val: 0.7851, Test: 0.7852\n",
      "Final Test: 0.7852\n"
     ]
    }
   ],
   "source": [
    "# Train/Test Loop\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 6):\n",
    "    loss = train(train_loader)\n",
    "    val_auc = test(val_loader)\n",
    "    test_auc = test(test_loader)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Check on values</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict = {}\n",
    "cnt = 0\n",
    "for graph in data_list:\n",
    "    z_raw = model_ed.encode(graph.x, graph.edge_index)\n",
    "    final_edge_index = model_ed.decode_all(z_raw)\n",
    "    fei = final_edge_index.tolist()\n",
    "    edges_pred = {k:[] for k in range(101)}\n",
    "    edges_pred_inv = {k:[] for k in range(101)}\n",
    "    for i in range(len(fei[0])):\n",
    "        edges_pred[fei[0][i]].append(fei[1][i])\n",
    "        edges_pred_inv[fei[1][i]].append(fei[0][i])\n",
    "    ts0 = graph.edge_index.tolist()\n",
    "    edges = {k:[] for k in range(101)}\n",
    "    edges_inv = {k:[] for k in range(101)}\n",
    "    for i in range(len(ts0[0])):\n",
    "        edges[ts0[0][i]].append(ts0[1][i])\n",
    "        edges_inv[ts0[1][i]].append(ts0[0][i])\n",
    "    originals = {}\n",
    "    predictions = {}\n",
    "    for i in range(101):\n",
    "        originals[i] = set(edges[i] + edges_inv[i])\n",
    "        predictions[i] = set(edges_pred[i] + edges_pred_inv[i])\n",
    "    graph_dict[cnt] = {\"real\": originals, \"preds\": predictions}\n",
    "    cnt += 1\n",
    "\n",
    "confusion_dict = {}\n",
    "true_positives = []\n",
    "false_positives = []\n",
    "true_negatives = []\n",
    "false_negatives = []\n",
    "for key in graph_dict:\n",
    "    real = graph_dict[key][\"real\"]\n",
    "    pred = graph_dict[key][\"preds\"]\n",
    "    node_matrix = {}\n",
    "    for i in range(101):\n",
    "        tp = len(pred[i].intersection(real[i]))\n",
    "        fp = len(pred[i] - real[i])\n",
    "        real_neg = set([j for j in range(101)]) - {i} - real[i]\n",
    "        pred_neg = set([j for j in range(101)]) - {i} - pred[i]\n",
    "        tn = len(pred_neg.intersection(real_neg))\n",
    "        fn = len(pred_neg - real_neg)\n",
    "        total = tp + fp + tn + fn\n",
    "        node_matrix[i] = {\"tp\": tp, \"fp\": fp, \"tn\": tn, \"fn\": fn, \"total\": total}\n",
    "        true_positives.append(tp/total)\n",
    "        false_positives.append(fp/total)\n",
    "        true_negatives.append(tn/total)\n",
    "        false_negatives.append(fn/total)\n",
    "    confusion_dict[key] = node_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positives mean=0.25, stdev=0.04\n",
      "false positives mean=0.59, stdev=0.21\n",
      "true negatives mean=0.16, stdev=0.21\n",
      "false negatives mean=0.00, stdev=0.01\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, stdev\n",
    "print(\"true positives mean={0:.2f}, stdev={1:.2f}\".format(mean(true_positives), stdev(true_positives)))\n",
    "print(\"false positives mean={0:.2f}, stdev={1:.2f}\".format(mean(false_positives), stdev(false_positives)))\n",
    "print(\"true negatives mean={0:.2f}, stdev={1:.2f}\".format(mean(true_negatives), stdev(true_negatives)))\n",
    "print(\"false negatives mean={0:.2f}, stdev={1:.2f}\".format(mean(false_negatives), stdev(false_negatives)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>NgLearning</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "data_raw_dict  = {\n",
    "     \"positives\": [],\n",
    "     \"negatives\": []\n",
    "}\n",
    "\n",
    "cnt = 0\n",
    "pos_cnt = 0\n",
    "neg_cnt = 0\n",
    "indices = [i for i in range(len(data_list))]\n",
    "for idx in sample(indices, 200):\n",
    "    graph = data_list[idx]\n",
    "    neg_cnt = 0\n",
    "    ng_matrix = graph.y.tolist()\n",
    "    x_matrix = graph.x.tolist()\n",
    "    encoding_matrix = model_ed.encode(graph.x, graph.edge_index).tolist()\n",
    "    for i in range(101):\n",
    "        for j in range(101):\n",
    "            if i == j:\n",
    "                # if pos_cnt < 80000:\n",
    "                #     data_raw_dict[\"positives\"].append(encoding_matrix[i]+encoding_matrix[i])\n",
    "                #     pos_cnt += 1\n",
    "                continue\n",
    "            else:\n",
    "                if ng_matrix[i][j] > 0.5:\n",
    "                        data_raw_dict[\"positives\"].append(encoding_matrix[i]+x_matrix[i]+encoding_matrix[j]+x_matrix[j]+[1])\n",
    "                        pos_cnt += 1\n",
    "                else:\n",
    "                    data_raw_dict[\"negatives\"].append(encoding_matrix[i]+x_matrix[i]+encoding_matrix[j]+x_matrix[j]+[0])\n",
    "                    neg_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tensor = sample(data_raw_dict[\"positives\"], 10000) + sample(data_raw_dict[\"negatives\"], 10000)\n",
    "main_tensor = torch.tensor(pre_tensor, dtype=torch.float32)\n",
    "\n",
    "main_tensor = main_tensor[torch.randperm(main_tensor.size()[0])]\n",
    "labels = main_tensor[:,-1:]\n",
    "embeddings = main_tensor[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Learning stage</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Deep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(142, 142)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(142, 70)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(70, 70)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(70, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import copy\n",
    "\n",
    "def model_train(model, X_train, y_train, X_val, y_val):\n",
    "    # loss function and optimizer\n",
    "    loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    " \n",
    "    n_epochs = 20   # number of epochs to run\n",
    "    batch_size = 500  # size of each batch\n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
    " \n",
    "    # Hold the best model\n",
    "    best_acc = - np.inf   # init to negative infinity\n",
    "    best_weights = None\n",
    " \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "                # take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "                # forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "                # print progress\n",
    "                acc = (y_pred.round() == y_batch).float().mean()\n",
    "                bar.set_postfix(\n",
    "                    loss=float(loss),\n",
    "                    acc=float(acc)\n",
    "                )\n",
    "        # evaluate accuracy at end of each epoch\n",
    "        model.eval()\n",
    "        y_pred = model(X_val)\n",
    "        acc = (y_pred.round() == y_val).float().mean()\n",
    "        acc = float(acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "    # restore model and return best accuracy\n",
    "    model.load_state_dict(best_weights)\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (deep): 0.80\n",
      "Accuracy (deep): 0.79\n",
      "Accuracy (deep): 0.80\n",
      "Accuracy (deep): 0.80\n",
      "Accuracy (deep): 0.80\n",
      "Deep: 79.86% (+/- 0.41%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "# train-test split: Hold out the test set for final model evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, train_size=0.7, shuffle=True)\n",
    " \n",
    "# define 5-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "cv_scores_deep = []\n",
    "for train, test in kfold.split(X_train, y_train):\n",
    "    # create model, train, and get accuracy\n",
    "    model_ls = Deep()\n",
    "    acc = model_train(model_ls, X_train[train], y_train[train], X_train[test], y_train[test])\n",
    "    print(\"Accuracy (deep): %.2f\" % acc)\n",
    "    cv_scores_deep.append(acc)\n",
    " \n",
    "# evaluate the model\n",
    "deep_acc = np.mean(cv_scores_deep)\n",
    "deep_std = np.std(cv_scores_deep)\n",
    "print(\"Deep: %.2f%% (+/- %.2f%%)\" % (deep_acc*100, deep_std*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model accuracy: 79.43%\n"
     ]
    }
   ],
   "source": [
    "model_ls = Deep()\n",
    "acc = model_train(model_ls, X_train, y_train, X_test, y_test)\n",
    "print(f\"Final model accuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.34824884 -0.36458543  0.34209353 -0.40368187  0.44998032 -0.6455222\n",
      " -0.24943261 -0.16076028  0.81623566  0.32858828  0.1401279   0.02790643\n",
      " -0.64043134 -0.74666744 -0.0035337  -0.56267107  0.25050822 -0.22999151\n",
      " -0.17261726  0.04425763 -0.3583087  -0.9353499  -0.15694048  0.81424946\n",
      "  0.0312843  -0.3942736  -0.56929785 -0.21066405  0.14455457 -0.38482082\n",
      " -0.0708671  -0.03757123 -0.4867992  -0.9206139   0.33374938 -0.03163411\n",
      " -0.20573357 -1.358158   -0.11647518  0.32746667  0.5777776  -0.40665197\n",
      " -0.00517291  0.31417698  0.18779095 -0.8299682  -0.33597237  0.03479974\n",
      "  0.11010712 -0.17035238  0.02037082  0.73113555 -0.25327745 -0.30412978\n",
      " -0.03329861 -0.6088855  -0.9155575  -0.09249165 -0.36688438  0.00162618\n",
      " -0.3377703   0.556208    0.49177033 -0.24819893  0.19426002 -0.54047716\n",
      "  0.37577215 -0.4197988   0.5189862  -0.6187207  -0.19632323 -0.12735264\n",
      "  0.816805    0.4287169   0.16433594 -0.03567228 -0.619009   -0.6863785\n",
      "  0.10873306 -0.6471885   0.20190006 -0.21122718 -0.10554066 -0.00596702\n",
      " -0.2845598  -1.0143473  -0.16630629  0.86419266  0.2034442  -0.42420134\n",
      " -0.5142317  -0.18075918  0.05980136 -0.39325628 -0.03081454 -0.08215278\n",
      " -0.6515412  -1.0129685   0.4018265  -0.02829049 -0.2018546  -1.4432591\n",
      " -0.2544509   0.44704956  0.7627045  -0.4489553   0.02741461  0.30555114\n",
      "  0.17020941 -0.6767728  -0.34724483  0.15725508  0.12158114 -0.20254296\n",
      "  0.12783906  0.7191715  -0.34316835 -0.41501004 -0.17277412 -0.65141\n",
      " -0.93202096 -0.0086002  -0.45195657  0.12164146 -0.3848927   0.549278\n",
      "  0.4882732  -0.429573  ] -> [0.82793015] (expected [1.])\n",
      "[ 1.2162979   0.81219953  0.6632457   0.09240326  0.23413056  1.3817507\n",
      " -0.43230605  0.7622817   0.20545879 -1.5443293   0.37127125  1.0140673\n",
      " -0.02272385 -0.51784366 -0.07077367  0.36071825 -0.78633034 -0.3500301\n",
      "  1.068275    0.57162917  0.07283896  1.0571401   0.75936604 -0.2291407\n",
      "  0.814704   -1.1107526   0.58915555  0.99491256  0.7207367  -0.37277493\n",
      " -0.43219307 -1.188686    0.1482864   1.2998043   0.212704   -0.7602303\n",
      "  1.1370013   0.3174182   0.6344584   0.49857235 -0.96703273  0.5668205\n",
      "  0.8930001  -0.3700149   0.37316304  1.0894284   0.35687396 -1.2182144\n",
      " -0.05455705 -1.0428135  -0.5338799  -0.15293108 -0.542704    0.21488109\n",
      "  0.20420358  0.21431515 -0.4403456   0.16223386 -0.62673545 -0.91851926\n",
      " -0.51586366 -1.7867157  -1.510278    1.6215539   1.2162979   0.81219953\n",
      "  0.6632456   0.09240325  0.23413058  1.3817507  -0.43230605  0.7622817\n",
      "  0.20545879 -1.5443292   0.37127125  1.0140673  -0.02272384 -0.51784366\n",
      " -0.07077368  0.36071825 -0.78633034 -0.3500301   1.068275    0.57162917\n",
      "  0.07283896  1.0571401   0.75936604 -0.22914067  0.814704   -1.1107526\n",
      "  0.58915555  0.99491256  0.7207367  -0.37277496 -0.4321931  -1.188686\n",
      "  0.1482864   1.2998043   0.21270402 -0.7602303   1.1370013   0.3174182\n",
      "  0.63445836  0.49857232 -0.96703273  0.5668205   0.8930001  -0.3700149\n",
      "  0.37316304  1.0894284   0.35687393 -1.2182144  -0.05455705 -1.0428135\n",
      " -0.5338798  -0.15293108 -0.542704    0.21488112  0.20420356  0.21431515\n",
      " -0.4403456   0.16223386 -0.62673545 -0.9185194  -0.51586366 -1.7867157\n",
      " -1.510278    1.6215539 ] -> [0.8789789] (expected [1.])\n",
      "[ 5.8547872e-01  1.1318297e+00 -1.9831786e-01  7.6726824e-01\n",
      " -5.6297797e-01  1.0553844e-01 -1.9262306e-01  7.9116076e-02\n",
      " -6.1821640e-01 -7.1073693e-01  2.1122530e-01  2.8147906e-01\n",
      "  2.6032290e-01  4.1328979e-01 -4.5300624e-01  8.7886941e-01\n",
      " -4.4692449e-02  2.4200082e-01 -3.6139220e-01  4.1214147e-01\n",
      " -3.9582759e-01  1.0732375e+00  3.6617395e-01 -1.3906429e+00\n",
      " -1.3670986e+00  6.1032051e-01 -2.8165114e-01 -4.5414439e-01\n",
      "  2.7930722e-01  3.8300550e-01 -3.7510747e-01  3.5102955e-01\n",
      "  1.1108956e+00  1.0917157e+00 -1.0329398e+00  1.8919376e-01\n",
      " -4.7718531e-01  9.9602401e-01  6.3183981e-01 -1.1741198e+00\n",
      " -1.0103047e+00  1.1018641e+00 -6.8291229e-01 -2.4929364e-01\n",
      " -5.1574528e-01 -1.2963946e-01  1.7925268e-01 -5.1916909e-01\n",
      " -2.7829328e-01  4.0112036e-01 -1.0246062e+00 -9.1535771e-01\n",
      "  4.0557021e-01  8.8586533e-01  6.5687168e-01  7.8914624e-01\n",
      "  7.0639777e-01 -4.6749339e-03  1.1144272e+00 -8.4816462e-01\n",
      "  1.9142906e-01 -3.2407841e-01 -3.4599251e-01  1.0174131e+00\n",
      "  4.3005502e-01  1.1970426e+00 -2.6024327e-01  8.1014073e-01\n",
      " -5.1601374e-01  2.9163513e-02 -1.3542233e-01  1.9080396e-01\n",
      " -7.3029268e-01 -7.4942881e-01  2.1459061e-01  3.4114695e-01\n",
      "  2.6998645e-01  4.7408661e-01 -5.6322920e-01  9.3792570e-01\n",
      " -8.6306967e-02  3.8107428e-01 -3.5666335e-01  4.0400702e-01\n",
      " -3.0593142e-01  1.0932399e+00  3.0452231e-01 -1.3822572e+00\n",
      " -1.3519449e+00  6.3286245e-01 -2.7214116e-01 -4.5138302e-01\n",
      "  1.8323594e-01  5.3936309e-01 -3.4419727e-01  2.1999241e-01\n",
      "  1.1052001e+00  1.1947534e+00 -1.0218159e+00  2.3293149e-01\n",
      " -5.6488317e-01  1.1775179e+00  5.2235448e-01 -1.1816750e+00\n",
      " -1.0856771e+00  1.3002248e+00 -7.6019275e-01 -2.3810910e-01\n",
      " -4.9670985e-01 -1.6237646e-02  1.3678430e-01 -5.0184149e-01\n",
      " -3.6025938e-01  3.6069685e-01 -1.0473320e+00 -1.0506262e+00\n",
      "  4.0770108e-01  8.2017148e-01  5.5610180e-01  8.4969401e-01\n",
      "  8.0247116e-01  4.4602901e-04  1.1118397e+00 -8.2451457e-01\n",
      "  2.5269338e-01 -2.4999940e-01 -3.3469105e-01  8.7672472e-01] -> [0.6335486] (expected [0.])\n",
      "[-0.2900003   0.5184649  -0.20532274  0.7861287  -0.38457033  0.34073347\n",
      " -0.27667338  0.2889157  -0.7140033  -0.63330317  0.23567906  0.19595797\n",
      "  0.423802    0.7060254   0.036123    1.1756334  -0.35884976  0.21743552\n",
      " -0.0229692   0.7169448   0.13283263  1.1416091   0.49344015 -1.3610694\n",
      " -0.7078916   0.24464057  0.06956778 -0.2813023  -0.48318046  0.1803984\n",
      " -0.28702295  0.42158613  0.42416647  0.9543617  -0.74859846  0.03419836\n",
      " -0.03058008  0.31072816  0.19686848 -0.83904636 -0.24294677  0.8445854\n",
      " -0.43735936  0.04136432 -0.8932051   0.82988405 -0.04391998 -0.03887289\n",
      "  0.00650765 -0.25720152 -0.5389747  -0.770879    0.49172878  0.46910217\n",
      " -0.10245103  0.41081995  0.5382097   0.58095646  0.9405831  -0.25513172\n",
      " -0.17888753 -0.57493937 -0.16583396  0.41129228 -0.2710116   0.5173699\n",
      " -0.19399074  0.7259499  -0.40456137  0.42435718 -0.2543099   0.24907418\n",
      " -0.73035085 -0.6682074   0.2181415   0.20873134  0.42720148  0.65573794\n",
      "  0.03645328  1.2050833  -0.350753    0.23595227  0.07086651  0.74174714\n",
      "  0.1662466   1.150368    0.5230633  -1.3925886  -0.64773715  0.19888632\n",
      "  0.14285693 -0.24816582 -0.4813667   0.11307237 -0.31453943  0.42441127\n",
      "  0.38556153  0.96905434 -0.79439765 -0.08071978  0.01974325  0.2970225\n",
      "  0.23366597 -0.8151213  -0.21196046  0.84109694 -0.35848492  0.02946879\n",
      " -0.93367213  0.8814897  -0.03787649 -0.12702414  0.01660577 -0.22597489\n",
      " -0.5461934  -0.7248562   0.49810034  0.45412907 -0.11941985  0.4160133\n",
      "  0.54079694  0.5976644   0.8914027  -0.28088373 -0.19877683 -0.64163816\n",
      " -0.25137767  0.4066538 ] -> [0.5984512] (expected [1.])\n",
      "[ 9.29670513e-01 -1.32725209e-01  5.82583249e-02 -1.92243621e-01\n",
      " -2.42903113e-01 -5.16754150e-01 -4.00128663e-01 -2.82525301e-01\n",
      "  8.96855712e-01  3.15868676e-01 -6.35249093e-02  8.63972306e-02\n",
      " -7.02001452e-01 -8.14190388e-01 -3.18785608e-01 -1.42232955e-01\n",
      " -1.89200997e-01 -2.83209421e-02 -3.42895359e-01  4.90169004e-02\n",
      " -7.40715086e-01 -5.36799610e-01 -5.84819466e-02  7.09113836e-01\n",
      " -5.19737005e-01 -3.27388227e-01 -6.55465722e-01 -3.55430059e-02\n",
      "  4.67004478e-01 -4.21563715e-01 -3.01600009e-01 -3.12054336e-01\n",
      "  3.03014427e-01 -1.33872852e-01  6.15190744e-01 -3.97432864e-01\n",
      "  9.68365520e-02 -6.51055932e-01  2.66105294e-01  4.42248017e-01\n",
      "  9.35989022e-02 -3.53971720e-01  1.43854797e-01 -1.10816427e-01\n",
      " -8.40380415e-02 -1.31501675e+00 -2.29146823e-01 -4.12059307e-01\n",
      "  6.97103381e-01  2.28047460e-01  5.35341538e-02  3.84899259e-01\n",
      " -3.37170064e-01 -4.16932970e-01  1.99410170e-01 -4.88903403e-01\n",
      " -9.10300732e-01 -4.63871717e-01 -6.95788637e-02 -5.31469658e-02\n",
      " -4.81526375e-01  8.70010853e-01 -9.07959491e-02  1.24504030e-01\n",
      "  7.81756639e-01  1.37117937e-01 -2.06816196e-01 -7.45414972e-01\n",
      " -3.46874982e-01 -9.23773408e-01 -5.14127791e-01 -4.66719568e-02\n",
      "  5.10734618e-01  3.34541142e-01 -6.31228626e-01 -1.10701747e-01\n",
      " -8.84352624e-02 -4.77275550e-01 -1.29921108e-01  8.68159533e-02\n",
      " -1.14763252e-01  2.95465201e-01 -4.45201099e-01  5.00013605e-02\n",
      " -7.97961771e-01 -6.11565173e-01  1.48824155e-01  1.30210233e+00\n",
      " -6.46700084e-01 -2.80782640e-01 -5.83146065e-02  3.26694787e-01\n",
      "  1.85371768e-02 -5.19303679e-01 -4.29333925e-01 -8.73601735e-02\n",
      " -1.88552216e-03 -3.50046635e-01  5.39245546e-01 -4.76956964e-01\n",
      "  4.55913879e-02 -8.91137362e-01  2.61951610e-02  7.11116791e-01\n",
      "  6.35026634e-01 -5.76701224e-01  1.19249232e-01  7.77249783e-02\n",
      " -1.53369248e-01 -1.97411025e+00 -5.43202281e-01 -3.14243793e-01\n",
      "  9.85068142e-01 -8.74807984e-02  1.75997749e-01  6.55731559e-01\n",
      " -9.09703672e-02 -4.05595094e-01 -1.02381967e-01 -9.57517684e-01\n",
      " -1.33635199e+00 -6.55395687e-01 -3.53595108e-01  9.55953002e-02\n",
      " -8.40794444e-01  7.98455119e-01 -1.19908944e-01 -9.91486087e-02] -> [0.49280652] (expected [0.])\n"
     ]
    }
   ],
   "source": [
    "model_ls.eval()\n",
    "with torch.no_grad():\n",
    "    # Test out inference with 5 samples\n",
    "    for i in range(5):\n",
    "        y_pred = model_ls(X_test[i:i+1])\n",
    "        print(f\"{X_test[i].numpy()} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on instance 428"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_raw = model_ed.encode(data_list[428].x, data_list[428].edge_index)\n",
    "preds = {}\n",
    "for i in range(len(z_raw)):\n",
    "    for j in range(len(z_raw)):\n",
    "        if i != j:\n",
    "            node_i = z_raw[i].tolist()\n",
    "            node_j = z_raw[j].tolist()\n",
    "            target = data_list[428].y[i][j]\n",
    "            preds[i,j] = {\"pred\":model_ls(torch.tensor(node_i+node_j, dtype=torch.float)),\"target\":target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred': tensor([0.1292], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[77,63]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: tensor([21.5312], grad_fn=<AddBackward0>), 1: tensor([26.0008], grad_fn=<AddBackward0>), 2: tensor([40.3351], grad_fn=<AddBackward0>), 3: tensor([35.5938], grad_fn=<AddBackward0>), 4: tensor([38.9639], grad_fn=<AddBackward0>), 5: tensor([32.8199], grad_fn=<AddBackward0>), 6: tensor([26.7021], grad_fn=<AddBackward0>), 7: tensor([30.9510], grad_fn=<AddBackward0>), 8: tensor([21.5312], grad_fn=<AddBackward0>), 9: tensor([39.9153], grad_fn=<AddBackward0>), 10: tensor([35.4807], grad_fn=<AddBackward0>), 11: tensor([31.6647], grad_fn=<AddBackward0>), 12: tensor([27.4517], grad_fn=<AddBackward0>), 13: tensor([36.6048], grad_fn=<AddBackward0>), 14: tensor([38.6634], grad_fn=<AddBackward0>), 15: tensor([36.5882], grad_fn=<AddBackward0>), 16: tensor([22.8904], grad_fn=<AddBackward0>), 17: tensor([34.0399], grad_fn=<AddBackward0>), 18: tensor([37.7751], grad_fn=<AddBackward0>), 19: tensor([29.0070], grad_fn=<AddBackward0>), 20: tensor([35.7476], grad_fn=<AddBackward0>), 21: tensor([25.0341], grad_fn=<AddBackward0>), 22: tensor([39.9153], grad_fn=<AddBackward0>), 23: tensor([31.6704], grad_fn=<AddBackward0>), 24: tensor([28.3887], grad_fn=<AddBackward0>), 25: tensor([36.9242], grad_fn=<AddBackward0>), 26: tensor([38.8030], grad_fn=<AddBackward0>), 27: tensor([26.0023], grad_fn=<AddBackward0>), 28: tensor([24.3387], grad_fn=<AddBackward0>), 29: tensor([37.6493], grad_fn=<AddBackward0>), 30: tensor([27.4104], grad_fn=<AddBackward0>), 31: tensor([31.3095], grad_fn=<AddBackward0>), 32: tensor([34.4059], grad_fn=<AddBackward0>), 33: tensor([33.5196], grad_fn=<AddBackward0>), 34: tensor([38.0798], grad_fn=<AddBackward0>), 35: tensor([38.8030], grad_fn=<AddBackward0>), 36: tensor([34.1982], grad_fn=<AddBackward0>), 37: tensor([37.7751], grad_fn=<AddBackward0>), 38: tensor([38.3859], grad_fn=<AddBackward0>), 39: tensor([27.9884], grad_fn=<AddBackward0>), 40: tensor([27.0921], grad_fn=<AddBackward0>), 41: tensor([25.5340], grad_fn=<AddBackward0>), 42: tensor([25.1645], grad_fn=<AddBackward0>), 43: tensor([37.9278], grad_fn=<AddBackward0>), 44: tensor([30.5627], grad_fn=<AddBackward0>), 45: tensor([36.6860], grad_fn=<AddBackward0>), 46: tensor([26.3654], grad_fn=<AddBackward0>), 47: tensor([25.6849], grad_fn=<AddBackward0>), 48: tensor([32.1773], grad_fn=<AddBackward0>), 49: tensor([36.5954], grad_fn=<AddBackward0>), 50: tensor([34.9495], grad_fn=<AddBackward0>), 51: tensor([32.4365], grad_fn=<AddBackward0>), 52: tensor([38.8824], grad_fn=<AddBackward0>), 53: tensor([36.6560], grad_fn=<AddBackward0>), 54: tensor([23.2884], grad_fn=<AddBackward0>), 55: tensor([28.9480], grad_fn=<AddBackward0>), 56: tensor([28.5208], grad_fn=<AddBackward0>), 57: tensor([32.2859], grad_fn=<AddBackward0>), 58: tensor([26.0023], grad_fn=<AddBackward0>), 59: tensor([33.5196], grad_fn=<AddBackward0>), 60: tensor([28.0710], grad_fn=<AddBackward0>), 61: tensor([36.9242], grad_fn=<AddBackward0>), 62: tensor([37.9664], grad_fn=<AddBackward0>), 63: tensor([26.1887], grad_fn=<AddBackward0>), 64: tensor([37.2187], grad_fn=<AddBackward0>), 65: tensor([27.8951], grad_fn=<AddBackward0>), 66: tensor([30.4790], grad_fn=<AddBackward0>), 67: tensor([37.6237], grad_fn=<AddBackward0>), 68: tensor([25.1645], grad_fn=<AddBackward0>), 69: tensor([28.7310], grad_fn=<AddBackward0>), 70: tensor([37.3803], grad_fn=<AddBackward0>), 71: tensor([33.5803], grad_fn=<AddBackward0>), 72: tensor([26.7441], grad_fn=<AddBackward0>), 73: tensor([22.8904], grad_fn=<AddBackward0>), 74: tensor([28.2594], grad_fn=<AddBackward0>), 75: tensor([33.5803], grad_fn=<AddBackward0>), 76: tensor([40.3499], grad_fn=<AddBackward0>), 77: tensor([33.9441], grad_fn=<AddBackward0>), 78: tensor([38.4306], grad_fn=<AddBackward0>), 79: tensor([21.8017], grad_fn=<AddBackward0>), 80: tensor([36.5140], grad_fn=<AddBackward0>), 81: tensor([38.7078], grad_fn=<AddBackward0>), 82: tensor([40.3351], grad_fn=<AddBackward0>), 83: tensor([25.0341], grad_fn=<AddBackward0>), 84: tensor([29.8413], grad_fn=<AddBackward0>), 85: tensor([34.5230], grad_fn=<AddBackward0>), 86: tensor([37.6237], grad_fn=<AddBackward0>), 87: tensor([30.9644], grad_fn=<AddBackward0>), 88: tensor([28.9768], grad_fn=<AddBackward0>), 89: tensor([34.8096], grad_fn=<AddBackward0>), 90: tensor([31.2476], grad_fn=<AddBackward0>), 91: tensor([29.9553], grad_fn=<AddBackward0>), 92: tensor([37.6836], grad_fn=<AddBackward0>), 93: tensor([39.4978], grad_fn=<AddBackward0>), 94: tensor([23.8441], grad_fn=<AddBackward0>), 95: tensor([34.0516], grad_fn=<AddBackward0>), 96: tensor([27.8310], grad_fn=<AddBackward0>), 97: tensor([36.5954], grad_fn=<AddBackward0>), 98: tensor([23.7485], grad_fn=<AddBackward0>), 99: tensor([27.4104], grad_fn=<AddBackward0>), 100: tensor([34.8543], grad_fn=<AddBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "sums = {}\n",
    "for i in range(101):\n",
    "    sum_i = 0\n",
    "    for j in range(101):\n",
    "        if i != j:\n",
    "            sum_i += preds[i,j][\"pred\"]\n",
    "    sums[i] = sum_i\n",
    "print(sums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still space for improvement. I'd consider more data, undirected case for encoding and add tw into the structure of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Hyperbolic tangent Ng-Set distinction</h1>\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_files_list = [\"./export/\"+f for f in os.listdir(\"./export\") ]\n",
    "instance_dict = {}\n",
    "for dir_str in data_files_list:\n",
    "    with open(dir_str, 'r') as text_file:\n",
    "        cnt = 0\n",
    "        instance = \"\"\n",
    "        for line in text_file:\n",
    "            if cnt < 9:\n",
    "                if cnt == 0:\n",
    "                    instance = line.split()[0]\n",
    "                    instance_dict[instance] = []\n",
    "                cnt += 1\n",
    "                continue\n",
    "            split_line = line.split()\n",
    "            instance_dict[instance].append([int(i) for i in split_line])\n",
    "        text_file.close()\n",
    "\n",
    "ng_dict = {}\n",
    "cnt = -1\n",
    "with open(\"ng_outs.csv\", 'r') as text_file:\n",
    "    for line in text_file:\n",
    "        if cnt < 2:\n",
    "            cnt += 1\n",
    "            continue\n",
    "        raw_line = line.strip()\n",
    "        split_line_list = raw_line.split(sep=\";\")\n",
    "        instance = split_line_list[3]\n",
    "        if instance not in ng_dict:\n",
    "            ng_dict[instance] = [[0 for i in range(101)]]\n",
    "        ng_dict[instance].append([0] + [int(i) for i in split_line_list[5:-1]])\n",
    "        if len(split_line_list[5:-1]) != 100:\n",
    "            print(\"case found for instance \"+instance)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean, stdev, median\n",
    "\n",
    "statistic_dict = {}\n",
    "for instance_name, ng_matrix in ng_dict.items():\n",
    "    row_sum = []\n",
    "    for i in range(1,101):\n",
    "        row = ng_matrix[i][1:i] + ng_matrix[i][i+1:101]\n",
    "        row_sum.append(sum(row))\n",
    "    statistic_dict[instance_name] = {\"avg\": mean(row_sum), \"std\": stdev(row_sum), \"median\": median(row_sum)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_average = max(statistic_dict, key=lambda x: statistic_dict[x][\"avg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg': 5.49, 'std': 3.307460193153189, 'median': 5.0}"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistic_dict[max_average]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_median = max(statistic_dict, key=lambda x: statistic_dict[x][\"median\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg': 4.32, 'std': 2.4531159323965466, 'median': 5.0}"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistic_dict[max_median]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "2\n",
      "5\n",
      "3\n",
      "2\n",
      "4\n",
      "2\n",
      "5\n",
      "4\n",
      "5\n",
      "5\n",
      "9\n",
      "4\n",
      "6\n",
      "2\n",
      "7\n",
      "3\n",
      "2\n",
      "2\n",
      "5\n",
      "5\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "6\n",
      "2\n",
      "5\n",
      "5\n",
      "4\n",
      "6\n",
      "5\n",
      "7\n",
      "3\n",
      "6\n",
      "5\n",
      "6\n",
      "2\n",
      "5\n",
      "3\n",
      "6\n",
      "6\n",
      "4\n",
      "3\n",
      "2\n",
      "4\n",
      "5\n",
      "8\n",
      "7\n",
      "6\n",
      "6\n",
      "6\n",
      "4\n",
      "12\n",
      "13\n",
      "5\n",
      "5\n",
      "3\n",
      "7\n",
      "10\n",
      "8\n",
      "9\n",
      "10\n",
      "12\n",
      "10\n",
      "6\n",
      "11\n",
      "6\n",
      "11\n",
      "11\n",
      "6\n",
      "7\n",
      "6\n",
      "2\n",
      "7\n",
      "10\n",
      "12\n",
      "8\n",
      "11\n",
      "6\n",
      "9\n",
      "11\n",
      "13\n",
      "5\n",
      "11\n",
      "12\n",
      "6\n",
      "7\n",
      "7\n",
      "12\n",
      "12\n",
      "14\n",
      "12\n",
      "12\n",
      "6\n",
      "5\n",
      "13\n",
      "7\n",
      "12\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for line in ng_dict[max_average]:\n",
    "    print(sum(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9870"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ng_dict.keys()).index(max_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import knn_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 1: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 2: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 3: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 4: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 5: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 6: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 7: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 8: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 9: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 10: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 11: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 12: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 13: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 14: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 15: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 16: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 17: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 18: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 19: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 20: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 21: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 22: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 23: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 24: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 25: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 26: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 27: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 28: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 29: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 30: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 31: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 32: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 33: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 34: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 35: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 36: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 37: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 38: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 39: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 40: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 41: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 42: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 43: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 44: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 45: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 46: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 47: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 48: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 49: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 50: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 51: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 52: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 53: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 54: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 55: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 56: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 57: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 58: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 59: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 60: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 61: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 62: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 63: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 64: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 65: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 66: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 67: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 68: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 69: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 70: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 71: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 72: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 73: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 74: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 75: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 76: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 77: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 78: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 79: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 80: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 81: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 82: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 83: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 84: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 85: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 86: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 87: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 88: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 89: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 90: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 91: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 92: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 93: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 94: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 95: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 96: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 97: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 98: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 99: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}, 100: {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 0}}\n"
     ]
    }
   ],
   "source": [
    "tw_distances = {i:{j:0 for j in range(101)} for i in range(101)}\n",
    "print(tw_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "def ranges_intersect(a, b, c, d):\n",
    "    return a <= d and c <= b\n",
    "def remove_edge(edge_index, u, v):\n",
    "    # Step 1: Identify all the edges that do not match [u, v]\n",
    "    mask = ~((edge_index[0] == u) & (edge_index[1] == v))\n",
    "    \n",
    "    # Step 2: Filter the edge_index tensor using the mask\n",
    "    edge_index = edge_index[:, mask]\n",
    "    \n",
    "    return edge_index\n",
    "\n",
    "\n",
    "for instance_name in ng_dict:\n",
    "    instance = instance_dict[instance_name]\n",
    "    y = torch.tensor(ng_dict[instance_name], dtype=torch.float)\n",
    "    x = torch.tensor(instance, dtype=torch.float)\n",
    "    pos = []\n",
    "    tw_sets_dict = {}\n",
    "    for i in instance:\n",
    "        pos.append([i[1], i[2]])\n",
    "    # edges_to_prune = []\n",
    "    # for i in range(101):\n",
    "    #     for j in range(101):\n",
    "    #         if i != j:\n",
    "    #             if not ranges_intersect(instance[i][4], instance[i][5], instance[j][4], instance[j][5]):\n",
    "    #                 i_ea = instance[i][4]\n",
    "    #                 i_ld = instance[i][5]\n",
    "    #                 j_ea = instance[j][4]\n",
    "    #                 j_ld = instance[j][5]\n",
    "    #                 dist = [abs(i_ea - j_ld), abs(j_ea - i_ld)]\n",
    "    #                 tw_distances[i][j] = min(dist)\n",
    "    #                 if min(dist) > 90:\n",
    "    #                     edges_to_prune.append([i,j])\n",
    "    # prune smth\n",
    "    pos = torch.tensor(pos, dtype=torch.double)\n",
    "    edge_index = knn_graph(pos, 15)\n",
    "    # for e in edges_to_prune:\n",
    "    #     u, v = e\n",
    "    #     edge_index = remove_edge(edge_index, u, v)\n",
    "    data_list.append(Data(x=x, y=y, edge_index=edge_index, pos=pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19262\n"
     ]
    }
   ],
   "source": [
    "print(len(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "from random import shuffle\n",
    "\n",
    "def add_edge_labels(graph):\n",
    "    transform = T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False)\n",
    "    return transform(graph)\n",
    "\n",
    "labeled_graphs = [add_edge_labels(graph) for graph in data_list]\n",
    "shuffle(labeled_graphs)\n",
    "\n",
    "train_size = [g[0] for g in labeled_graphs]\n",
    "val_size = [g[1] for g in labeled_graphs]\n",
    "test_size = [g[2] for g in labeled_graphs]\n",
    "\n",
    "train_loader = DataLoader(train_size, batch_size=150, shuffle=True)\n",
    "val_loader = DataLoader(val_size, batch_size=150, shuffle=False)\n",
    "test_loader = DataLoader(test_size, batch_size=150, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Data(x=[101, 7], edge_index=[2, 1276], y=[101, 101], pos=[101, 2], edge_label=[638], edge_label_index=[2, 638]), Data(x=[101, 7], edge_index=[2, 1276], y=[101, 101], pos=[101, 2], edge_label=[74], edge_label_index=[2, 74]), Data(x=[101, 7], edge_index=[2, 1350], y=[101, 101], pos=[101, 2], edge_label=[148], edge_label_index=[2, 148]))\n",
      "Data(x=[101, 7], edge_index=[2, 1515], y=[101, 101], pos=[101, 2])\n"
     ]
    }
   ],
   "source": [
    "print(labeled_graphs[0])\n",
    "print(data_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ed = Net(data_list[0].num_features, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model_ed.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(loader, epoch):\n",
    "    model_ed.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    if epoch == 8:\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = 0.001\n",
    "\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        z = model_ed.encode(batch.x, batch.edge_index)\n",
    "\n",
    "        # We perform a new round of negative sampling for every training epoch:\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=batch.edge_index, num_nodes=batch.num_nodes,\n",
    "            num_neg_samples=batch.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "        # Concat positive and negative edge indices.\n",
    "        edge_label_index = torch.cat(\n",
    "            [batch.edge_label_index, neg_edge_index],\n",
    "            dim=-1,\n",
    "        )\n",
    "        # Label for positive edges: 1, for negative edges: 0.\n",
    "        edge_label = torch.cat([\n",
    "            batch.edge_label,\n",
    "            batch.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "        ], dim=0)\n",
    "\n",
    "        # Note: The model is trained in a supervised manner using the given\n",
    "        # `edge_label_index` and `edge_label` targets.\n",
    "        out = model_ed.decode(z, edge_label_index).view(-1)\n",
    "        loss = criterion(out, edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model_ed.eval()\n",
    "    all_out = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in loader:\n",
    "        z = model_ed.encode(batch.x, batch.edge_index)\n",
    "        out = model_ed.decode(z, batch.edge_label_index).view(-1).sigmoid()\n",
    "        all_out.append(out.cpu().numpy())\n",
    "        all_labels.append(batch.edge_label.cpu().numpy())\n",
    "\n",
    "    all_out = np.concatenate(all_out)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return roc_auc_score(all_labels, all_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 2.1153, Val: 0.7849, Test: 0.7848\n",
      "Epoch: 002, Loss: 2.0125, Val: 0.7877, Test: 0.7876\n",
      "Epoch: 003, Loss: 1.9439, Val: 0.7894, Test: 0.7892\n",
      "Epoch: 004, Loss: 1.7783, Val: 0.7905, Test: 0.7904\n",
      "Epoch: 005, Loss: 1.7022, Val: 0.7909, Test: 0.7908\n",
      "Epoch: 006, Loss: 1.6206, Val: 0.7913, Test: 0.7912\n",
      "Epoch: 007, Loss: 1.5781, Val: 0.7911, Test: 0.7911\n",
      "Final Test: 0.7912\n"
     ]
    }
   ],
   "source": [
    "# Train/Test Loop\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 24):\n",
    "    loss = train(train_loader, epoch)\n",
    "    val_auc = test(val_loader)\n",
    "    test_auc = test(test_loader)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict = {}\n",
    "cnt = 0\n",
    "for graph in data_list:\n",
    "    z_raw = model_ed.encode(graph.x, graph.edge_index)\n",
    "    final_edge_index = model_ed.decode_all(z_raw)\n",
    "    fei = final_edge_index.tolist()\n",
    "    edges_pred = {k:[] for k in range(101)}\n",
    "    edges_pred_inv = {k:[] for k in range(101)}\n",
    "    for i in range(len(fei[0])):\n",
    "        edges_pred[fei[0][i]].append(fei[1][i])\n",
    "        edges_pred_inv[fei[1][i]].append(fei[0][i])\n",
    "    ts0 = graph.edge_index.tolist()\n",
    "    edges = {k:[] for k in range(101)}\n",
    "    edges_inv = {k:[] for k in range(101)}\n",
    "    for i in range(len(ts0[0])):\n",
    "        edges[ts0[0][i]].append(ts0[1][i])\n",
    "        edges_inv[ts0[1][i]].append(ts0[0][i])\n",
    "    originals = {}\n",
    "    predictions = {}\n",
    "    for i in range(101):\n",
    "        originals[i] = set(edges[i] + edges_inv[i])\n",
    "        predictions[i] = set(edges_pred[i] + edges_pred_inv[i])\n",
    "    graph_dict[cnt] = {\"real\": originals, \"preds\": predictions}\n",
    "    cnt += 1\n",
    "\n",
    "confusion_dict = {}\n",
    "true_positives = []\n",
    "false_positives = []\n",
    "true_negatives = []\n",
    "false_negatives = []\n",
    "for key in graph_dict:\n",
    "    real = graph_dict[key][\"real\"]\n",
    "    pred = graph_dict[key][\"preds\"]\n",
    "    node_matrix = {}\n",
    "    for i in range(101):\n",
    "        tp = len(pred[i].intersection(real[i]))\n",
    "        fp = len(pred[i] - real[i])\n",
    "        real_neg = set([j for j in range(101)]) - {i} - real[i]\n",
    "        pred_neg = set([j for j in range(101)]) - {i} - pred[i]\n",
    "        tn = len(pred_neg.intersection(real_neg))\n",
    "        fn = len(pred_neg - real_neg)\n",
    "        total = tp + fp + tn + fn\n",
    "        node_matrix[i] = {\"tp\": tp, \"fp\": fp, \"tn\": tn, \"fn\": fn, \"total\": total}\n",
    "        true_positives.append(tp/total)\n",
    "        false_positives.append(fp/total)\n",
    "        true_negatives.append(tn/total)\n",
    "        false_negatives.append(fn/total)\n",
    "    confusion_dict[key] = node_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positives mean=0.19, stdev=0.03\n",
      "false positives mean=0.59, stdev=0.21\n",
      "true negatives mean=0.22, stdev=0.21\n",
      "false negatives mean=0.00, stdev=0.00\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, stdev\n",
    "print(\"true positives mean={0:.2f}, stdev={1:.2f}\".format(mean(true_positives), stdev(true_positives)))\n",
    "print(\"false positives mean={0:.2f}, stdev={1:.2f}\".format(mean(false_positives), stdev(false_positives)))\n",
    "print(\"true negatives mean={0:.2f}, stdev={1:.2f}\".format(mean(true_negatives), stdev(true_negatives)))\n",
    "print(\"false negatives mean={0:.2f}, stdev={1:.2f}\".format(mean(false_negatives), stdev(false_negatives)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('full_pairing_embedding_1.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "columns = ', '.join([f'value{i} DECIMAL(10)' for i in range(1, 144)])\n",
    "\n",
    "cur.execute(f'''\n",
    "    CREATE TABLE IF NOT EXISTS positives (\n",
    "        {columns}\n",
    "    )\n",
    "''')\n",
    "conn.commit()\n",
    "conn.close()\n",
    "conn = sqlite3.connect('full_pairing_embedding_1.db')\n",
    "cur = conn.cursor()\n",
    "cur.execute(f'''\n",
    "    CREATE TABLE IF NOT EXISTS negatives (\n",
    "        {columns}\n",
    "    )\n",
    "''')\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "indices = [i for i in range(len(data_list))]\n",
    "conn = sqlite3.connect('full_pairing_embedding_1.db')\n",
    "cur = conn.cursor()\n",
    "for idx in sample(indices, 2000):\n",
    "# for idx in indices:\n",
    "    graph = data_list[idx]\n",
    "    ng_matrix = graph.y.tolist()\n",
    "    encoding_matrix = model_ed.encode(graph.x, graph.edge_index).tolist()\n",
    "    \n",
    "    for i in range(1,101):\n",
    "        for j in range(1,101):\n",
    "            if i == j:\n",
    "                # if pos_cnt < 80000:\n",
    "                #     data_raw_dict[\"positives\"].append(encoding_matrix[i]+encoding_matrix[i])\n",
    "                #     pos_cnt += 1\n",
    "                continue\n",
    "            else:\n",
    "                \n",
    "                if ng_matrix[i][j] > 0.5:\n",
    "                    new_entry  = encoding_matrix[i]+graph.x[i].tolist()+encoding_matrix[j]+graph.x[j].tolist()+[1]\n",
    "                    new_entry = [\"%.5f\" % e for e in new_entry]\n",
    "                    new_entry = [float(e) for e in new_entry]\n",
    "                    placeholders = ', '.join(['?'] * 143)\n",
    "                    cur.execute(f'''\n",
    "                        INSERT INTO positives ({', '.join([f'value{i}' for i in range(1, 144)])})\n",
    "                        VALUES ({placeholders})\n",
    "                    ''', new_entry)\n",
    "                    conn.commit()\n",
    "                else:\n",
    "                    new_entry  = encoding_matrix[i]+graph.x[i].tolist()+encoding_matrix[j]+graph.x[j].tolist()+[0]\n",
    "                    new_entry = [\"%.5f\" % e for e in new_entry]\n",
    "                    new_entry = [float(e) for e in new_entry]\n",
    "                    placeholders = ', '.join(['?'] * 143)\n",
    "                    cur.execute(f'''\n",
    "                        INSERT INTO negatives ({', '.join([f'value{i}' for i in range(1, 144)])})\n",
    "                        VALUES ({placeholders})\n",
    "                    ''', new_entry)\n",
    "                    conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, is better a segmentation of tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('ER_embeddings.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "columns = ', '.join([f'value{i} DECIMAL(10)' for i in range(1, 72)])\n",
    "\n",
    "cur.execute(f'''\n",
    "    CREATE TABLE IF NOT EXISTS embeddings (\n",
    "        entry_id INT, \n",
    "        graph_id INT, \n",
    "        node_id INT, \n",
    "        {columns}\n",
    "    )\n",
    "''')\n",
    "conn.commit()\n",
    "conn.close()\n",
    "conn = sqlite3.connect('ER_embeddings.db')\n",
    "cur = conn.cursor()\n",
    "cur.execute(f'''\n",
    "    CREATE TABLE IF NOT EXISTS targets (\n",
    "        target_id INT, \n",
    "        graph_id INT, \n",
    "        node_i_id INT, \n",
    "        node_j_id INT, \n",
    "        target INT\n",
    "    )\n",
    "''')\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('ER_embeddings.db')\n",
    "indices = [i for i in range(len(data_list))]\n",
    "cur = conn.cursor()\n",
    "embeddings_idx = 0\n",
    "targets_idx = 0\n",
    "for graph_idx in sample(indices, 3):\n",
    "    graph = data_list[graph_idx]\n",
    "    ng_matrix = graph.y.tolist()\n",
    "    graph_x = graph.x\n",
    "    encoding_matrix = model_ed.encode(graph_x, graph.edge_index).tolist()\n",
    "    graph_x = graph_x.tolist()\n",
    "    for i in range(1,101):\n",
    "        encode_i = encoding_matrix[i]\n",
    "        node_i = graph_x[i]\n",
    "        new_entry = encode_i + node_i\n",
    "        new_entry = [\"%.5f\" % e for e in new_entry]\n",
    "        new_entry = [float(e) for e in new_entry]\n",
    "        new_entry = [embeddings_idx, graph_idx, i] + new_entry\n",
    "        placeholders = ', '.join(['?'] * 74)\n",
    "        cur.execute(f'''\n",
    "            INSERT INTO embeddings (entry_id, graph_id, node_id, {', '.join([f'value{i}' for i in range(1, 72)])})\n",
    "            VALUES ({placeholders})\n",
    "        ''', new_entry)\n",
    "        conn.commit()\n",
    "        embeddings_idx += 1\n",
    "        for j in range(1,101):\n",
    "            target = int(ng_matrix[i][j])\n",
    "            new_entry = [targets_idx, graph_idx, i, j, target]\n",
    "            placeholders = ', '.join(['?'] * 5)\n",
    "            cur.execute(f'''\n",
    "                INSERT INTO targets (target_id, graph_id, node_i_id, node_j_id, target)\n",
    "                VALUES ({placeholders})\n",
    "            ''', new_entry)\n",
    "            conn.commit()\n",
    "            targets_idx += 1\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66118, 0.94838, -0.876, -0.41615, 0.04691, -0.16675, 0.22066, -1.54137, 1.06522, -0.10525, -0.70218, 0.69687, 0.04271, -0.28538, 0.33736, 0.1252, -0.23725, -0.24863, -0.93389, 0.62139, 1.41741, 1.14515, 0.63426, 0.12067, -0.38322, 0.80235, 0.09743, -1.18841, -0.65156, 0.07186, -0.28567, 1.30586, 0.8459, -0.32406, -0.4773, -0.83307, -0.08354, 0.08339, 0.57718, -0.12751, 1.44274, 2.01152, -0.22892, 0.75242, -0.54804, -0.86146, -1.30952, -0.40795, -0.26426, 0.37206, -0.35543, -0.33895, -0.78025, 0.74631, -0.18311, -0.60011, -0.88556, -0.61174, 0.49392, 0.14463, 0.1734, -0.08591, -1.37122, 1.09354, 89, 38, 51, 5, 69, 129, 5, 0.5623, 0.83388, -0.58978, -0.3412, 0.04648, -0.18592, 0.09629, -1.17214, 0.82701, -0.15364, -0.55551, 0.47671, 0.08338, -0.18229, 0.22717, 0.22672, -0.13553, -0.25909, -0.73256, 0.3921, 1.21664, 0.98731, 0.51993, 0.03022, -0.3539, 0.58093, 0.00815, -0.88521, -0.53244, 0.04313, -0.19526, 1.03785, 0.62545, -0.39687, -0.45633, -0.64313, -0.02626, 0.10135, 0.42857, -0.12398, 1.20926, 1.60004, -0.20242, 0.68135, -0.37902, -0.70285, -0.94493, -0.28906, -0.07423, 0.40196, -0.24983, -0.13198, -0.64383, 0.50935, -0.21207, -0.45622, -0.6952, -0.31385, 0.43793, 0.12109, 0.11487, -0.08357, -1.07908, 0.79226, 79, 32, 43, 7, 47, 107, 5, 1]\n",
      "[-0.21749, -0.51044, 0.05778, -0.05605, 0.04599, 0.3858, 0.26905, -0.39329, -0.07354, 0.05849, 0.19767, 0.34986, -0.22746, 0.00407, 0.24991, -0.40861, 0.24252, -0.20605, -0.4828, 0.19644, -0.20497, -0.39787, -0.04221, 0.69822, 0.27966, 0.08548, -0.06371, -0.14738, 0.08337, 0.50873, -0.23865, -0.28704, 0.03947, 0.05438, 0.01483, -0.29969, 0.13924, 0.08044, 0.12614, 0.32546, 0.1976, 0.00699, 0.37793, -0.73679, 0.29332, 0.14954, -0.23652, 0.26969, 0.1392, -0.05668, 0.29579, -0.25583, -0.48136, 0.24884, -0.06732, 0.03793, 0.26145, -0.3688, -0.38796, -0.06804, 0.03641, 0.33167, 0.20035, 0.24808, 78, 83, 60, 3, 185, 474, 5, -0.21835, -0.50907, 0.05862, -0.05671, 0.04521, 0.3847, 0.26897, -0.39357, -0.07445, 0.05719, 0.19727, 0.34812, -0.22787, 0.00546, 0.24924, -0.40666, 0.24146, -0.20582, -0.48233, 0.19643, -0.20473, -0.39725, -0.04235, 0.69521, 0.2788, 0.08623, -0.06279, -0.14734, 0.08484, 0.50703, -0.2389, -0.28673, 0.03994, 0.05474, 0.01567, -0.2985, 0.13926, 0.08029, 0.12487, 0.32436, 0.19604, 0.0087, 0.37739, -0.73495, 0.29314, 0.1487, -0.23635, 0.26953, 0.14013, -0.05669, 0.29542, -0.25479, -0.47947, 0.24879, -0.06704, 0.03776, 0.26088, -0.36833, -0.38685, -0.06742, 0.03566, 0.33063, 0.2017, 0.24757, 79, 81, 59, 5, 321, 664, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('full_pairing_embedding_1.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('SELECT * FROM positives ORDER BY RANDOM() LIMIT 2')\n",
    "rows = cur.fetchall()\n",
    "\n",
    "for row in rows:\n",
    "    values = list(row)  # Convert the string back to a list of floats\n",
    "    print(values)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('full_pairing_embedding_1.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('SELECT * FROM positives ORDER BY RANDOM() LIMIT 20000')\n",
    "rows = cur.fetchall()\n",
    "\n",
    "pos_list = []\n",
    "\n",
    "for idx in range(len(rows)):\n",
    "    values_pos = list(rows[idx])  # Convert the string back to a list of floats\n",
    "    pos_list.append(values_pos)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "conn = sqlite3.connect('full_pairing_embedding_1.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('SELECT * FROM negatives ORDER BY RANDOM() LIMIT 20000')\n",
    "rows2 = cur.fetchall()\n",
    "\n",
    "neg_list = []\n",
    "\n",
    "for idx in range(len(rows2)):\n",
    "    values_neg = list(rows2[idx])  # Convert the string back to a list of floats\n",
    "    neg_list.append(values_neg)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.57898, -0.63049, 0.03278, 0.33593, 0.30389, 0.11502, -0.28721, 0.6724, 0.2678, 0.03665, 0.1683, 0.3719, -0.28926, 0.16718, -0.27839, -0.23663, 0.36102, 0.32207, 0.11786, 0.24861, -0.32926, -0.49168, -0.07949, 0.39353, 0.50661, 0.17755, 0.91301, -0.10856, 0.13453, -0.16025, 0.42263, -0.38474, -0.30971, 0.46857, 0.49044, -0.00892, -0.1835, -0.12363, -0.18695, -0.01489, -0.54967, -0.94709, -0.09228, -0.28446, 0.21309, 0.05564, -0.28252, 0.22293, -0.06702, -0.00716, -0.21626, -0.37727, 0.35125, -0.15354, 0.47556, 0.5335, -0.04678, 0.13895, 0.06938, -0.10934, 0.25334, 0.05662, -0.22331, 0.19743, 27, 41, 55, 6, 75, 200, 5, -0.64889, -0.73155, 0.04294, 0.38883, 0.30609, 0.17854, -0.22642, 0.7292, 0.2846, 0.0066, 0.09824, 0.46888, -0.26736, 0.24834, -0.28329, -0.21928, 0.42779, 0.38769, 0.13209, 0.35454, -0.41046, -0.57952, -0.0683, 0.42151, 0.6017, 0.19189, 1.03908, -0.07304, 0.11303, -0.15415, 0.40265, -0.42645, -0.2824, 0.55708, 0.57193, -0.05859, -0.21775, -0.09719, -0.20391, 0.05325, -0.67203, -1.05122, -0.00577, -0.38855, 0.21151, 0.05476, -0.23231, 0.31218, -0.05322, 0.00247, -0.25014, -0.49609, 0.45279, -0.15573, 0.55523, 0.59667, -0.02259, 0.13392, 0.0375, -0.14689, 0.28702, -0.07346, -0.23748, 0.24597, 21, 42, 54, 5, 49, 187, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "print(pos_list[30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another alternative is following that architecture directly in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14360bef2da64775a03ba8349e1aab12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=19000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indices = [i for i in range(len(data_list))]\n",
    "\n",
    "encodings_dict = {}\n",
    "targets_dict = {\"positives\": [], \"negatives\": []}\n",
    "\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import time\n",
    "\n",
    "max_count = 19000\n",
    "cnt = 0\n",
    "\n",
    "f = IntProgress(min=0, max=max_count) # instantiate the bar\n",
    "display(f) # display the bar\n",
    "for graph_idx in sample(indices, max_count):\n",
    "    graph = data_list[graph_idx]\n",
    "    ng_matrix = graph.y.tolist()\n",
    "    graph_x = graph.x\n",
    "    encoding_matrix = model_ed.encode(graph_x, graph.edge_index).tolist()\n",
    "    graph_x = graph_x.tolist()\n",
    "    for i in range(1,101):\n",
    "        encode_i = encoding_matrix[i]\n",
    "        node_i = graph_x[i]\n",
    "        new_entry = encode_i + node_i\n",
    "        new_entry = [\"%.5f\" % e for e in new_entry]\n",
    "        new_entry = [float(e) for e in new_entry]\n",
    "        encodings_dict[graph_idx, i] = new_entry\n",
    "        for j in range(1,101):\n",
    "            if i != j:\n",
    "                target = int(ng_matrix[i][j])\n",
    "                if target == 1:\n",
    "                    targets_dict[\"positives\"].append((graph_idx, i, j))\n",
    "                else:\n",
    "                    targets_dict[\"negatives\"].append((graph_idx, i, j))\n",
    "    f.value += 1\n",
    "    cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2457689\n"
     ]
    }
   ],
   "source": [
    "print(len(targets_dict[\"positives\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_candidates = sample(targets_dict[\"positives\"], 200000)\n",
    "neg_candidates = sample(targets_dict[\"negatives\"], 200000)\n",
    "pos_list = [encodings_dict[k[0], k[1]] + encodings_dict[k[0], k[2]] + [1] for k in pos_candidates]\n",
    "neg_list = [encodings_dict[k[0], k[1]] + encodings_dict[k[0], k[2]] + [0] for k in neg_candidates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4184,\n",
       " -0.14373,\n",
       " 0.06703,\n",
       " -0.44273,\n",
       " -0.47975,\n",
       " 0.80694,\n",
       " 0.37194,\n",
       " -0.27625,\n",
       " -0.04178,\n",
       " -1.25133,\n",
       " -0.21375,\n",
       " 0.44965,\n",
       " -0.00732,\n",
       " -0.56687,\n",
       " 0.36277,\n",
       " 0.2402,\n",
       " 0.0597,\n",
       " -0.90626,\n",
       " -0.03987,\n",
       " 0.12249,\n",
       " 0.06044,\n",
       " -0.53177,\n",
       " 0.63241,\n",
       " 1.00007,\n",
       " 1.13488,\n",
       " 0.34054,\n",
       " -0.5171,\n",
       " 0.16306,\n",
       " 0.57901,\n",
       " -1.26873,\n",
       " 0.53609,\n",
       " 0.88927,\n",
       " -1.07014,\n",
       " 0.33441,\n",
       " 0.13584,\n",
       " 0.06793,\n",
       " 0.066,\n",
       " 1.42043,\n",
       " -0.13282,\n",
       " -0.67707,\n",
       " -0.2292,\n",
       " -0.78642,\n",
       " -1.09499,\n",
       " -0.87302,\n",
       " -0.38874,\n",
       " -0.06499,\n",
       " -0.17514,\n",
       " 0.137,\n",
       " 0.16315,\n",
       " -0.2762,\n",
       " 0.17752,\n",
       " 0.11826,\n",
       " -0.59196,\n",
       " 0.84782,\n",
       " -0.17292,\n",
       " 0.46212,\n",
       " -0.01011,\n",
       " -0.3778,\n",
       " -0.08987,\n",
       " -0.58573,\n",
       " -0.25137,\n",
       " 0.50901,\n",
       " -0.70423,\n",
       " 0.17527,\n",
       " 80.0,\n",
       " 30.0,\n",
       " 56.0,\n",
       " 6.0,\n",
       " 159.0,\n",
       " 499.0,\n",
       " 5.0,\n",
       " 0.42659,\n",
       " -0.23607,\n",
       " 0.06116,\n",
       " -0.40866,\n",
       " -0.41269,\n",
       " 0.80919,\n",
       " 0.34471,\n",
       " -0.37751,\n",
       " 0.00146,\n",
       " -1.19498,\n",
       " -0.19214,\n",
       " 0.35083,\n",
       " -0.08892,\n",
       " -0.61121,\n",
       " 0.34402,\n",
       " 0.25551,\n",
       " -0.02442,\n",
       " -0.82551,\n",
       " -0.17668,\n",
       " 0.10556,\n",
       " 0.07599,\n",
       " -0.5217,\n",
       " 0.60992,\n",
       " 0.88821,\n",
       " 1.12171,\n",
       " 0.34667,\n",
       " -0.57406,\n",
       " 0.14202,\n",
       " 0.53752,\n",
       " -1.14703,\n",
       " 0.43002,\n",
       " 0.79935,\n",
       " -1.08151,\n",
       " 0.38063,\n",
       " 0.11364,\n",
       " -0.01123,\n",
       " 0.07287,\n",
       " 1.32465,\n",
       " -0.18553,\n",
       " -0.53224,\n",
       " -0.19377,\n",
       " -0.69228,\n",
       " -1.12718,\n",
       " -0.80865,\n",
       " -0.35725,\n",
       " -0.08024,\n",
       " -0.1703,\n",
       " 0.04112,\n",
       " 0.19558,\n",
       " -0.22392,\n",
       " 0.20648,\n",
       " 0.14638,\n",
       " -0.59831,\n",
       " 0.93988,\n",
       " -0.11294,\n",
       " 0.45223,\n",
       " -0.10382,\n",
       " -0.35538,\n",
       " -0.06748,\n",
       " -0.67218,\n",
       " -0.17345,\n",
       " 0.498,\n",
       " -0.66002,\n",
       " 0.17793,\n",
       " 10.0,\n",
       " 34.0,\n",
       " 58.0,\n",
       " 6.0,\n",
       " 288.0,\n",
       " 642.0,\n",
       " 5.0,\n",
       " 1]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tensor = pos_list + neg_list\n",
    "main_tensor = torch.tensor(pre_tensor, dtype=torch.float32)\n",
    "\n",
    "main_tensor = main_tensor[torch.randperm(main_tensor.size()[0])]\n",
    "labels = main_tensor[:,-1:]\n",
    "embeddings = main_tensor[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [-1.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels2 = labels.clone()\n",
    "labels2[labels2==0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Deep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(142, 142)\n",
    "        self.act1 = nn.ReLU()\n",
    "        # self.layer1_1 = nn.Linear(284, 142)\n",
    "        # self.act1_1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(142, 71)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(71, 71)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(71, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        # x = self.act1_1(self.layer1_1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import copy\n",
    "\n",
    "def model_train(model, X_train, y_train, X_val, y_val):\n",
    "    # loss function and optimizer\n",
    "    loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    " \n",
    "    n_epochs = 60   # number of epochs to run\n",
    "    batch_size = 500  # size of each batch\n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
    " \n",
    "    # Hold the best model\n",
    "    best_acc = - np.inf   # init to negative infinity\n",
    "    best_weights = None\n",
    " \n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch == 30:\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = 0.0001\n",
    "        model.train()\n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "                # take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "                # forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "                # print progress\n",
    "                acc = (y_pred.round() == y_batch).float().mean()\n",
    "                bar.set_postfix(\n",
    "                    loss=float(loss),\n",
    "                    acc=float(acc)\n",
    "                )\n",
    "        # evaluate accuracy at end of each epoch\n",
    "        model.eval()\n",
    "        y_pred = model(X_val)\n",
    "        acc = (y_pred.round() == y_val).float().mean()\n",
    "        acc = float(acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "    # restore model and return best accuracy\n",
    "    model.load_state_dict(best_weights)\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (deep): 0.95\n",
      "Accuracy (deep): 0.95\n",
      "Accuracy (deep): 0.95\n",
      "Accuracy (deep): 0.95\n",
      "Accuracy (deep): 0.95\n",
      "Deep: 95.19% (+/- 0.07%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "# train-test split: Hold out the test set for final model evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, train_size=0.7, shuffle=True)\n",
    " \n",
    "# define 5-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "cv_scores_deep = []\n",
    "for train, test in kfold.split(X_train, y_train):\n",
    "    # create model, train, and get accuracy\n",
    "    model_ls = Deep()\n",
    "    acc = model_train(model_ls, X_train[train], y_train[train], X_train[test], y_train[test])\n",
    "    print(\"Accuracy (deep): %.2f\" % acc)\n",
    "    cv_scores_deep.append(acc)\n",
    " \n",
    "# evaluate the model\n",
    "deep_acc = np.mean(cv_scores_deep)\n",
    "deep_std = np.std(cv_scores_deep)\n",
    "print(\"Deep: %.2f%% (+/- %.2f%%)\" % (deep_acc*100, deep_std*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model accuracy: 95.27%\n"
     ]
    }
   ],
   "source": [
    "model_ls = Deep()\n",
    "acc = model_train(model_ls, X_train, y_train, X_test, y_test)\n",
    "print(f\"Final model accuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.10190e-01  5.43470e-01 -1.82600e-01 -2.74680e-01  2.91740e-01\n",
      " -9.97800e-02  4.17140e-01  8.66900e-02  5.69400e-02  6.43070e-01\n",
      "  1.64420e-01 -2.55720e-01  3.10080e-01 -4.33800e-02  8.68300e-02\n",
      " -2.23030e-01  4.61050e-01  3.70760e-01  2.41250e-01 -7.05600e-02\n",
      " -3.24360e-01  3.37420e-01 -7.69910e-01 -6.22090e-01 -4.45230e-01\n",
      " -5.66360e-01  6.64520e-01  1.43640e-01 -6.93040e-01  6.89510e-01\n",
      " -8.53200e-02 -8.74920e-01  3.22580e-01 -6.71390e-01 -2.02150e-01\n",
      "  3.60720e-01 -1.10080e-01 -4.84550e-01 -2.75330e-01  1.07700e-02\n",
      "  1.88900e-02  4.54570e-01  1.20478e+00  4.09440e-01  3.68680e-01\n",
      "  1.85700e-01  1.10010e-01  2.87500e-01  1.54750e-01 -5.00210e-01\n",
      " -3.95490e-01  1.69870e-01  1.75220e-01 -3.72710e-01  3.72370e-01\n",
      " -2.15690e-01 -1.03600e-02  9.26660e-01 -4.96740e-01  4.87910e-01\n",
      "  4.44640e-01 -8.10500e-01 -5.47900e-02  7.02900e-02  6.00000e+00\n",
      "  4.90000e+01  4.60000e+01  5.00000e+00  5.28000e+02  5.88000e+02\n",
      "  5.00000e+00 -3.65200e-01  2.25570e-01 -2.40290e-01 -2.01410e-01\n",
      "  5.56300e-01  1.27770e-01  4.87210e-01 -3.07580e-01  9.94000e-02\n",
      "  8.55680e-01  3.19790e-01 -6.27380e-01  7.34000e-03 -2.33930e-01\n",
      "  2.42870e-01 -1.96900e-01  1.66030e-01  3.37860e-01  3.92000e-02\n",
      " -1.39240e-01 -1.01200e-01  5.66090e-01 -9.29510e-01 -8.79640e-01\n",
      " -4.61710e-01 -5.36040e-01  6.32440e-01  2.50310e-01 -7.91060e-01\n",
      "  1.08231e+00 -4.92390e-01 -9.64730e-01  3.21760e-01 -5.85110e-01\n",
      " -3.65570e-01  7.38200e-02  4.34600e-02 -8.27960e-01 -1.21750e-01\n",
      "  1.74060e-01  1.32130e-01  7.50660e-01  1.06424e+00  5.30280e-01\n",
      "  3.33530e-01  1.38360e-01 -1.14800e-02  1.19600e-01  3.20900e-01\n",
      " -3.66670e-01 -5.29850e-01  3.97190e-01  1.48180e-01 -1.51930e-01\n",
      "  9.48720e-01 -7.64100e-02 -3.62690e-01  9.24040e-01 -3.90590e-01\n",
      "  5.22120e-01  6.32080e-01 -7.11150e-01  5.84700e-02 -7.29100e-02\n",
      "  4.20000e+01  5.70000e+01  6.30000e+01  4.00000e+00  5.57000e+02\n",
      "  6.17000e+02  5.00000e+00] -> [0.989038] (expected [1.] and inverted [0.9735125])\n",
      "[-2.22100e-01  1.96880e-01 -2.27790e-01 -8.47300e-02  4.53420e-01\n",
      " -1.76270e-01  7.91500e-02 -4.77400e-02  1.71720e-01  6.97600e-01\n",
      "  2.97010e-01 -4.23040e-01  8.00900e-02 -8.14600e-02 -9.17100e-02\n",
      "  2.12400e-02  2.06050e-01  4.85570e-01 -4.57600e-02 -1.38280e-01\n",
      " -3.08240e-01  3.70880e-01 -5.69870e-01 -7.39200e-01 -4.90280e-01\n",
      " -4.42510e-01  3.54260e-01  7.93000e-03 -7.22800e-01  8.60130e-01\n",
      " -3.11270e-01 -1.02247e+00  2.82430e-01 -3.96920e-01 -9.50700e-02\n",
      "  1.06090e-01 -1.09080e-01 -8.02760e-01 -3.23040e-01  4.37320e-01\n",
      "  1.83020e-01  6.59560e-01  1.11440e+00  5.55230e-01  4.17290e-01\n",
      "  6.82000e-02  2.78600e-02 -1.53560e-01  2.23880e-01 -3.41550e-01\n",
      " -2.34780e-01  7.45700e-02  2.34490e-01 -2.09480e-01  2.80400e-01\n",
      " -1.56940e-01 -3.17160e-01  6.32290e-01 -3.80060e-01  2.29010e-01\n",
      "  5.60570e-01 -7.09520e-01  1.95240e-01  3.94200e-02  2.80000e+01\n",
      "  4.30000e+01  4.30000e+01  5.00000e+00  2.64000e+02  3.24000e+02\n",
      "  5.00000e+00  7.24300e-02  4.47680e-01 -2.82460e-01 -5.01320e-01\n",
      "  1.07270e-01 -1.28100e-02  2.34840e-01  1.16730e-01  1.52950e-01\n",
      "  4.15990e-01  4.34670e-01 -2.64880e-01  4.63150e-01 -4.82620e-01\n",
      "  2.60640e-01 -4.38800e-02  5.45260e-01  1.49270e-01  2.05360e-01\n",
      " -1.92610e-01 -5.09240e-01  9.93800e-02 -5.60290e-01 -3.72680e-01\n",
      " -2.28230e-01 -2.61310e-01  6.91220e-01  2.92400e-01 -9.55200e-01\n",
      "  4.71880e-01  1.00010e-01 -7.35520e-01 -9.53400e-02 -5.06630e-01\n",
      " -1.56770e-01  4.32060e-01 -1.14740e-01 -3.93070e-01 -4.68710e-01\n",
      " -1.70850e-01  2.53440e-01  4.99030e-01  1.27142e+00  2.90480e-01\n",
      "  5.60090e-01  4.59930e-01  2.42060e-01  2.60450e-01  1.08520e-01\n",
      " -9.71430e-01 -3.27130e-01  2.86000e-02  2.87680e-01  1.13310e-01\n",
      "  5.31600e-02  7.12400e-02 -9.25800e-02  9.45010e-01 -8.00390e-01\n",
      "  3.21150e-01  3.89960e-01 -1.02708e+00 -1.12090e-01  1.11720e-01\n",
      "  4.50000e+01  3.60000e+01  4.90000e+01  5.00000e+00  7.09000e+02\n",
      "  7.69000e+02  5.00000e+00] -> [0.97202456] (expected [0.] and inverted [0.])\n",
      "[ 3.75040e-01 -3.37620e-01  4.39560e-01 -2.23420e-01 -3.91280e-01\n",
      "  6.91130e-01  2.00410e-01 -3.45770e-01 -7.64400e-02 -6.74520e-01\n",
      " -2.67680e-01  5.30800e-01  3.16330e-01 -3.38860e-01  3.01180e-01\n",
      "  5.55100e-02 -1.76730e-01 -4.19050e-01  3.37920e-01 -9.67300e-02\n",
      "  4.56800e-02 -3.14880e-01  3.44740e-01  5.74600e-01  7.08580e-01\n",
      "  4.67130e-01 -2.85190e-01  1.76980e-01  5.13560e-01 -8.86250e-01\n",
      "  2.71520e-01  8.97410e-01 -7.91780e-01  3.77330e-01 -2.81640e-01\n",
      "  1.89960e-01  1.97800e-01  1.15080e+00 -3.03200e-02 -5.59760e-01\n",
      " -8.90700e-02 -6.85600e-01 -6.29250e-01 -5.44290e-01 -4.71020e-01\n",
      "  3.78590e-01 -2.28670e-01  1.08830e-01 -3.02700e-02 -3.01980e-01\n",
      " -6.83700e-02  1.69920e-01 -5.03930e-01  5.67060e-01 -9.76000e-03\n",
      "  5.17720e-01  2.43800e-01 -2.01380e-01 -1.59960e-01 -1.57940e-01\n",
      " -5.48960e-01  6.03360e-01 -6.97880e-01 -4.80800e-02  8.00000e+01\n",
      "  3.00000e+01  6.70000e+01  2.00000e+00  3.24000e+02  4.87000e+02\n",
      "  5.00000e+00  3.62960e-01 -2.63680e-01  4.52060e-01 -2.28660e-01\n",
      " -4.53810e-01  6.44750e-01  1.93500e-01 -2.89120e-01 -8.94600e-02\n",
      " -6.74570e-01 -3.57480e-01  5.84220e-01  3.98530e-01 -2.63660e-01\n",
      "  2.86070e-01  8.16000e-03 -1.50130e-01 -3.61630e-01  3.74390e-01\n",
      " -6.74300e-02  3.73900e-02 -3.37650e-01  3.33200e-01  6.32450e-01\n",
      "  7.01280e-01  4.53080e-01 -2.75260e-01  1.52940e-01  5.67160e-01\n",
      " -9.28130e-01  3.32730e-01  9.28630e-01 -7.72950e-01  3.60380e-01\n",
      " -2.92980e-01  2.89850e-01  1.92240e-01  1.21209e+00 -7.72100e-02\n",
      " -5.98110e-01 -1.26180e-01 -7.35320e-01 -6.33920e-01 -5.51630e-01\n",
      " -4.88760e-01  4.11130e-01 -1.94240e-01  1.95400e-01 -7.46000e-02\n",
      " -3.05990e-01 -2.66400e-02  1.54820e-01 -4.71860e-01  5.32370e-01\n",
      " -7.87400e-02  4.49060e-01  3.54300e-01 -2.07920e-01 -1.66460e-01\n",
      " -1.47520e-01 -5.93750e-01  6.13520e-01 -7.37390e-01 -2.57900e-02\n",
      "  9.00000e+01  3.20000e+01  6.20000e+01  6.00000e+00  3.30000e+02\n",
      "  5.18000e+02  5.00000e+00] -> [0.] (expected [1.] and inverted [0.8097354])\n",
      "[-1.5010e-02 -3.3392e-01  1.7964e-01  3.2906e-01  1.3595e-01  9.1760e-02\n",
      " -5.0872e-01 -3.4038e-01  1.2777e-01  3.6880e-01  6.0350e-02 -1.8129e-01\n",
      " -5.8750e-02  1.2123e-01 -4.0161e-01  8.1710e-02 -4.8048e-01  5.3777e-01\n",
      "  4.3910e-02 -2.5917e-01  1.0158e-01  3.0350e-02 -9.4240e-02 -3.0346e-01\n",
      " -2.4093e-01 -1.1600e-03 -2.8084e-01 -1.0719e-01 -9.4660e-02  3.5193e-01\n",
      " -1.7762e-01 -1.1061e-01  8.7840e-02  2.5040e-01 -1.4772e-01  1.7859e-01\n",
      "  3.7304e-01 -5.5325e-01 -1.6024e-01  3.6173e-01  1.0932e-01  2.9220e-01\n",
      "  1.5063e-01  1.8193e-01 -1.2726e-01 -4.4650e-02 -3.5722e-01 -6.1092e-01\n",
      " -2.4656e-01  4.8188e-01 -6.4060e-02 -1.0154e-01 -1.8253e-01  8.0200e-03\n",
      " -1.1402e-01  1.0575e-01 -9.7260e-02 -1.8494e-01  1.4659e-01 -5.2400e-03\n",
      "  9.1650e-02  3.2872e-01  1.9394e-01 -2.4405e-01  3.5000e+01  9.2000e+01\n",
      "  5.7000e+01  2.0000e+00  2.5600e+02  4.1900e+02  5.0000e+00 -4.4900e-03\n",
      " -3.6993e-01  1.9201e-01  3.3404e-01  1.4953e-01  1.0575e-01 -5.0864e-01\n",
      " -3.7890e-01  1.3939e-01  3.9035e-01  7.3160e-02 -2.0719e-01 -7.5140e-02\n",
      "  9.2880e-02 -3.9550e-01  7.3760e-02 -5.0906e-01  5.6022e-01  1.4020e-02\n",
      " -2.7619e-01  1.0605e-01  2.7880e-02 -1.1566e-01 -3.4650e-01 -2.4425e-01\n",
      "  9.8000e-03 -2.8240e-01 -9.9390e-02 -1.1590e-01  3.9112e-01 -2.0988e-01\n",
      " -1.2420e-01  7.8500e-02  2.6071e-01 -1.7392e-01  1.5832e-01  3.8606e-01\n",
      " -5.7317e-01 -1.7336e-01  3.9015e-01  1.2254e-01  3.1570e-01  1.4464e-01\n",
      "  1.9830e-01 -1.1922e-01 -3.0970e-02 -3.5385e-01 -6.3359e-01 -2.4606e-01\n",
      "  4.8909e-01 -7.3970e-02 -8.4720e-02 -1.9551e-01  4.5240e-02 -8.6470e-02\n",
      "  1.1407e-01 -1.1667e-01 -1.5284e-01  1.4273e-01 -1.7900e-02  1.0395e-01\n",
      "  3.2451e-01  1.9679e-01 -2.5232e-01  4.4000e+01  8.0000e+01  5.8000e+01\n",
      "  4.0000e+00  1.9800e+02  5.9000e+02  5.0000e+00] -> [1.7550916e-07] (expected [1.] and inverted [0.9655254])\n",
      "[-8.31790e-01  1.81760e+00 -5.95580e-01 -8.61700e-02 -3.75810e-01\n",
      " -6.86290e-01 -9.86800e-02  1.68676e+00 -4.14330e-01  2.01590e-01\n",
      " -1.15220e-01  5.42720e-01  7.47370e-01  1.27511e+00 -5.46340e-01\n",
      " -1.85710e-01  1.19983e+00 -2.49050e-01  1.67154e+00  2.30340e-01\n",
      " -1.82110e-01  3.97580e-01 -3.57220e-01  9.37340e-01 -7.81380e-01\n",
      " -1.13838e+00  8.45520e-01 -5.34000e-02 -3.62400e-02 -4.97830e-01\n",
      "  1.26142e+00 -1.33990e-01  1.21444e+00 -1.14809e+00  7.52800e-01\n",
      "  1.50603e+00  1.07610e-01 -4.26010e-01  4.76270e-01 -1.30460e+00\n",
      " -5.91660e-01 -3.61750e-01  1.41682e+00 -2.37340e-01 -3.32300e-02\n",
      " -6.41660e-01 -3.02690e-01  8.86260e-01 -5.46640e-01  9.31600e-02\n",
      " -3.68280e-01 -6.63530e-01  5.53700e-01 -2.15691e+00 -9.70710e-01\n",
      " -6.44160e-01  9.96070e-01 -3.34870e-01 -3.01000e-02  1.45865e+00\n",
      " -3.82200e-02 -2.52720e-01  3.37970e-01 -1.46090e-01  6.90000e+01\n",
      "  7.50000e+01  0.00000e+00  5.00000e+00  7.07000e+02  7.67000e+02\n",
      "  5.00000e+00 -8.27220e-01  1.81687e+00 -5.85040e-01 -9.36200e-02\n",
      " -3.86000e-01 -6.79700e-01 -8.75100e-02  1.68242e+00 -4.11770e-01\n",
      "  2.20920e-01 -1.05330e-01  5.33600e-01  7.55000e-01  1.25583e+00\n",
      " -5.38270e-01 -2.13650e-01  1.19767e+00 -2.35920e-01  1.67668e+00\n",
      "  2.08800e-01 -1.82590e-01  3.89700e-01 -3.99300e-01  9.14840e-01\n",
      " -7.93770e-01 -1.14413e+00  8.74850e-01 -3.43300e-02 -6.17100e-02\n",
      " -4.76520e-01  1.26288e+00 -1.34830e-01  1.21994e+00 -1.16535e+00\n",
      "  7.27770e-01  1.52510e+00  1.31530e-01 -4.30630e-01  4.67340e-01\n",
      " -1.32292e+00 -5.95060e-01 -3.61490e-01  1.42987e+00 -2.34770e-01\n",
      " -2.28400e-02 -6.25400e-01 -2.94050e-01  9.00040e-01 -5.68960e-01\n",
      "  9.14300e-02 -3.95300e-01 -6.52580e-01  5.45600e-01 -2.14732e+00\n",
      " -9.64980e-01 -6.47410e-01  1.01435e+00 -2.81280e-01 -4.67900e-02\n",
      "  1.47878e+00 -3.70700e-02 -2.64730e-01  3.36880e-01 -1.60760e-01\n",
      "  7.10000e+01  6.50000e+01  3.00000e+00  6.00000e+00  8.87000e+02\n",
      "  9.47000e+02  5.00000e+00] -> [0.48357898] (expected [0.] and inverted [0.])\n"
     ]
    }
   ],
   "source": [
    "model_ls.eval()\n",
    "with torch.no_grad():\n",
    "    # Test out inference with 5 samples\n",
    "    for i in range(5):\n",
    "        y_pred = model_ls(X_test[5+i:i+6])\n",
    "        list_row = X_test[i].tolist()\n",
    "        length = len(list_row)\n",
    "        ji_input = list_row[length//2:length] + list_row[:length//2]\n",
    "        ji_input = torch.tensor(ji_input, dtype=torch.float)\n",
    "        y_pred2 = model_ls(ji_input)\n",
    "        print(f\"{X_test[i].numpy()} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()} and inverted {y_pred2.numpy()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_raw = model_ed.encode(data_list[428].x, data_list[428].edge_index)\n",
    "preds = {}\n",
    "for i in range(len(z_raw)):\n",
    "    for j in range(len(z_raw)):\n",
    "        if i != j:\n",
    "            node_i = z_raw[i].tolist() + data_list[428].x[i].tolist()\n",
    "            node_j = z_raw[j].tolist() + data_list[428].x[j].tolist()\n",
    "            target = data_list[428].y[i][j]\n",
    "            preds[i,j] = {\"pred\":model_ls(torch.tensor(node_i+node_j, dtype=torch.float)),\"target\":target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'pred': tensor([0.0039], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "1 {'pred': tensor([1.4214e-27], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "2 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "3 {'pred': tensor([3.4687e-16], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "4 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "5 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "6 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "7 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "8 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "9 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "10 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "11 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "12 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "13 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "14 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "15 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "16 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "17 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "18 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "19 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "20 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "21 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "22 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "23 {'pred': tensor([0.0017], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "24 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "25 {'pred': tensor([1.0325e-11], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "26 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "27 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "28 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "29 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "30 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "31 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "32 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "33 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "34 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "35 {'pred': tensor([1.5100e-19], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "36 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "37 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "38 {'pred': tensor([7.7521e-15], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "39 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "40 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "41 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "42 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "43 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "44 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "45 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "46 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "47 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "48 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "49 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "50 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "51 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "52 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "53 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "54 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "55 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "56 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "57 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "58 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "59 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "60 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "61 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "62 {'pred': tensor([0.9595], grad_fn=<SigmoidBackward0>), 'target': tensor(1.)}\n",
      "63 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "64 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "65 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "66 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "67 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "68 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "69 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "70 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "71 {'pred': tensor([4.8841e-28], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "72 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "73 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "74 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "75 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "76 {'pred': tensor([0.7882], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "78 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "79 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "80 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "81 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "82 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "83 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "84 {'pred': tensor([2.9728e-06], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "85 {'pred': tensor([1.6529e-25], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "86 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "87 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "88 {'pred': tensor([0.0001], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "89 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "90 {'pred': tensor([8.3090e-37], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "91 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "92 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "93 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "94 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "95 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "96 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "97 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "98 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "99 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n",
      "100 {'pred': tensor([0.], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}\n"
     ]
    }
   ],
   "source": [
    "for i in range(101):\n",
    "    if i != 77:\n",
    "        print(i, preds[77,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred': tensor([0.7944], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[51,77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_raw = model_ed.encode(data_list[9870].x, data_list[9870].edge_index)\n",
    "preds = {}\n",
    "for i in range(1,101):\n",
    "    for j in range(1,101):\n",
    "        if i != j:\n",
    "            node_i = z_raw[i].tolist() + data_list[9870].x[i].tolist()\n",
    "            node_j = z_raw[j].tolist() + data_list[9870].x[j].tolist()\n",
    "            target = data_list[9870].y[i][j]\n",
    "            prediction = model_ls(torch.tensor(node_i+node_j, dtype=torch.float))\n",
    "            preds[i,j] = {\"pred\":prediction,\"target\":target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(z_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(53600.) tensor(1561.5818, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "loss_fn = torch.nn.BCELoss(reduction=\"sum\")\n",
    "x = []\n",
    "y = []\n",
    "for key, val in preds.items():\n",
    "    x.append((1 if val[\"pred\"] > 0.7 else 0))\n",
    "    y.append(val[\"target\"])\n",
    "    loss += loss_fn(val[\"pred\"], torch.tensor([val[\"target\"]], dtype=torch.float))\n",
    "print(loss_fn(torch.tensor(x, dtype=torch.float), torch.tensor(y, dtype=torch.float)), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(536.)\n"
     ]
    }
   ],
   "source": [
    "dif = 0\n",
    "for i in range(len(x)):\n",
    "    dif += abs(x[i] - y[i])\n",
    "print(dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1054)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.BCELoss()\n",
    "print(2+loss(torch.tensor([0.9], dtype=torch.float), torch.tensor([1], dtype=torch.float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_edges, x_edges_values, nodes, nodes_coor, nodes_timew, y_edges, nodes_demand = datatorch[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
