{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>First prototype: encode-decode-predict the immediate nearest neighbor</h1>\n",
    "In order to check if it's possible to learn a metric minimizer.\n",
    "Based on https://medium.com/the-modern-scientist/graph-neural-networks-series-part-3-node-embedding-36613cc967d5\n",
    "and https://machinelearningmastery.com/building-a-binary-classification-model-in-pytorch/ and suggestions of Florian Racoussier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1-NN data</h2>\n",
    "Create a simple dataset for testing the prototype. Each node is connected to at most 15 neighbors in order to provide structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from random import randint\n",
    "from sys import float_info\n",
    "from torch_geometric.nn import knn_graph\n",
    "\n",
    "instances = {}\n",
    "for k in range(0, 1000):\n",
    "    nodes = {}\n",
    "    for i in range(0, 50):\n",
    "        lat_i = randint(0, 100)\n",
    "        lon_i = randint(0, 100)\n",
    "        node_i = (lat_i, lon_i)\n",
    "        lat_j = randint(0, 100)\n",
    "        lon_j = randint(0, 100)\n",
    "        node_j = (lat_j, lon_j)\n",
    "        nodes[i + 1] = node_i\n",
    "        nodes[i + 51] = node_j\n",
    "\n",
    "    dist = {}\n",
    "    pairs = {}\n",
    "    for i in range(1, 101):\n",
    "        for j in range(1, 101):\n",
    "            if i != j:\n",
    "                dist[i,j] = sqrt( (nodes[i][0] - nodes[j][0])**2 + (nodes[i][1] - nodes[j][1])**2 )\n",
    "            else:\n",
    "                dist[i,j] = float_info.max\n",
    "    for i in range(1, 101):\n",
    "        for j in range(1, 101):\n",
    "            if i not in pairs:\n",
    "                pairs[i] = j\n",
    "            if i != j:\n",
    "                if dist[i,j] < dist[i,pairs[i]]:\n",
    "                    pairs[i] = j\n",
    "\n",
    "    nodes[0] = (0,0)\n",
    "    for i in range(1,101):\n",
    "        dist[0,i] = sqrt( (nodes[0][0] - nodes[i][0])**2 + (nodes[0][1] - nodes[i][1])**2 )\n",
    "        dist[i,0] = dist[0,i]\n",
    "    y = [[0 for _ in range(101)] for _ in range(101)]\n",
    "    for i in range(101):\n",
    "        if i > 0:\n",
    "            y[i][pairs[i]] = 1\n",
    "                \n",
    "    instances[k] = {\"nodes\": nodes, \"dist\": dist, \"y\": y}\n",
    "data_list = []\n",
    "for instance_name in instances:\n",
    "    y = torch.tensor(instances[instance_name][\"y\"], dtype=torch.float)\n",
    "    x = torch.tensor([instances[instance_name][\"nodes\"][i] for i in range(0, 101)], dtype=torch.float)\n",
    "    pos = []\n",
    "    for i in range(101):\n",
    "        pos.append(instances[instance_name][\"nodes\"][i])\n",
    "    pos = torch.tensor(pos, dtype=torch.double)\n",
    "    data_list.append(Data(x=x, y=y, edge_index = knn_graph(x, 15), pos=pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1-NN Batching and dividing data into train-test-validation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "def add_edge_labels(graph):\n",
    "    transform = T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False)\n",
    "    return transform(graph)\n",
    "\n",
    "labeled_graphs = [add_edge_labels(graph) for graph in data_list]\n",
    "\n",
    "train_size = [g[0] for g in labeled_graphs]\n",
    "val_size = [g[1] for g in labeled_graphs]\n",
    "test_size = [g[2] for g in labeled_graphs]\n",
    "\n",
    "train_loader = DataLoader(train_size, batch_size=20, shuffle=True)\n",
    "val_loader = DataLoader(val_size, batch_size=20, shuffle=False)\n",
    "test_loader = DataLoader(test_size, batch_size=20, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Encoder-Decoder architecture definition</h2>\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "model = Net(data_list[0].num_features, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Encoder-Decoder train-test-validate routine</h2>\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        z = model.encode(batch.x, batch.edge_index)\n",
    "\n",
    "        # We perform a new round of negative sampling for every training epoch:\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=batch.edge_index, num_nodes=batch.num_nodes,\n",
    "            num_neg_samples=batch.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "        # Concat positive and negative edge indices.\n",
    "        edge_label_index = torch.cat(\n",
    "            [batch.edge_label_index, neg_edge_index],\n",
    "            dim=-1,\n",
    "        )\n",
    "        # Label for positive edges: 1, for negative edges: 0.\n",
    "        edge_label = torch.cat([\n",
    "            batch.edge_label,\n",
    "            batch.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "        ], dim=0)\n",
    "\n",
    "        # Note: The model is trained in a supervised manner using the given\n",
    "        # `edge_label_index` and `edge_label` targets.\n",
    "        out = model.decode(z, edge_label_index).view(-1)\n",
    "        loss = criterion(out, edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    all_out = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in loader:\n",
    "        z = model.encode(batch.x, batch.edge_index)\n",
    "        out = model.decode(z, batch.edge_label_index).view(-1).sigmoid()\n",
    "        all_out.append(out.cpu().numpy())\n",
    "        all_labels.append(batch.edge_label.cpu().numpy())\n",
    "\n",
    "    all_out = np.concatenate(all_out)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return roc_auc_score(all_labels, all_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 122.2200, Val: 0.8519, Test: 0.8531\n",
      "Epoch: 002, Loss: 1.4640, Val: 0.8638, Test: 0.8646\n",
      "Epoch: 003, Loss: 0.7274, Val: 0.8689, Test: 0.8694\n",
      "Epoch: 004, Loss: 0.5976, Val: 0.8711, Test: 0.8717\n",
      "Epoch: 005, Loss: 0.5664, Val: 0.8718, Test: 0.8725\n",
      "Epoch: 006, Loss: 0.5568, Val: 0.8731, Test: 0.8737\n",
      "Epoch: 007, Loss: 0.5530, Val: 0.8743, Test: 0.8747\n",
      "Epoch: 008, Loss: 0.5514, Val: 0.8757, Test: 0.8763\n",
      "Epoch: 009, Loss: 0.5502, Val: 0.8769, Test: 0.8774\n",
      "Epoch: 010, Loss: 0.5489, Val: 0.8782, Test: 0.8787\n",
      "Epoch: 011, Loss: 0.5481, Val: 0.8803, Test: 0.8808\n",
      "Epoch: 012, Loss: 0.5472, Val: 0.8815, Test: 0.8821\n",
      "Epoch: 013, Loss: 0.5464, Val: 0.8841, Test: 0.8847\n",
      "Epoch: 014, Loss: 0.5448, Val: 0.8862, Test: 0.8868\n",
      "Epoch: 015, Loss: 0.5435, Val: 0.8886, Test: 0.8890\n",
      "Epoch: 016, Loss: 0.5421, Val: 0.8917, Test: 0.8921\n",
      "Epoch: 017, Loss: 0.5410, Val: 0.8945, Test: 0.8950\n",
      "Epoch: 018, Loss: 0.5394, Val: 0.8979, Test: 0.8983\n",
      "Epoch: 019, Loss: 0.5358, Val: 0.9014, Test: 0.9020\n",
      "Epoch: 020, Loss: 0.5338, Val: 0.9051, Test: 0.9057\n",
      "Epoch: 021, Loss: 0.5310, Val: 0.9096, Test: 0.9100\n",
      "Epoch: 022, Loss: 0.5281, Val: 0.9136, Test: 0.9140\n",
      "Epoch: 023, Loss: 0.5245, Val: 0.9169, Test: 0.9173\n",
      "Epoch: 024, Loss: 0.5218, Val: 0.9216, Test: 0.9218\n",
      "Epoch: 025, Loss: 0.5199, Val: 0.9234, Test: 0.9239\n",
      "Epoch: 026, Loss: 0.5170, Val: 0.9275, Test: 0.9276\n",
      "Epoch: 027, Loss: 0.5146, Val: 0.9280, Test: 0.9284\n",
      "Epoch: 028, Loss: 0.5120, Val: 0.9302, Test: 0.9305\n",
      "Epoch: 029, Loss: 0.5118, Val: 0.9313, Test: 0.9317\n",
      "Epoch: 030, Loss: 0.5106, Val: 0.9322, Test: 0.9326\n",
      "Epoch: 031, Loss: 0.5097, Val: 0.9327, Test: 0.9332\n",
      "Epoch: 032, Loss: 0.5092, Val: 0.9331, Test: 0.9336\n",
      "Epoch: 033, Loss: 0.5089, Val: 0.9336, Test: 0.9341\n",
      "Epoch: 034, Loss: 0.5089, Val: 0.9321, Test: 0.9328\n",
      "Epoch: 035, Loss: 0.5095, Val: 0.9337, Test: 0.9344\n",
      "Epoch: 036, Loss: 0.5071, Val: 0.9343, Test: 0.9350\n",
      "Epoch: 037, Loss: 0.5074, Val: 0.9337, Test: 0.9344\n",
      "Epoch: 038, Loss: 0.5069, Val: 0.9335, Test: 0.9343\n",
      "Epoch: 039, Loss: 0.5067, Val: 0.9350, Test: 0.9357\n",
      "Epoch: 040, Loss: 0.5065, Val: 0.9350, Test: 0.9355\n",
      "Epoch: 041, Loss: 0.5075, Val: 0.9353, Test: 0.9360\n",
      "Epoch: 042, Loss: 0.5064, Val: 0.9355, Test: 0.9361\n",
      "Epoch: 043, Loss: 0.5061, Val: 0.9352, Test: 0.9359\n",
      "Epoch: 044, Loss: 0.5054, Val: 0.9356, Test: 0.9363\n",
      "Epoch: 045, Loss: 0.5063, Val: 0.9359, Test: 0.9363\n",
      "Epoch: 046, Loss: 0.5058, Val: 0.9362, Test: 0.9367\n",
      "Epoch: 047, Loss: 0.5051, Val: 0.9350, Test: 0.9357\n",
      "Epoch: 048, Loss: 0.5059, Val: 0.9341, Test: 0.9350\n",
      "Epoch: 049, Loss: 0.5052, Val: 0.9352, Test: 0.9360\n",
      "Epoch: 050, Loss: 0.5043, Val: 0.9366, Test: 0.9371\n",
      "Epoch: 051, Loss: 0.5042, Val: 0.9349, Test: 0.9357\n",
      "Epoch: 052, Loss: 0.5053, Val: 0.9367, Test: 0.9374\n",
      "Epoch: 053, Loss: 0.5052, Val: 0.9370, Test: 0.9376\n",
      "Epoch: 054, Loss: 0.5034, Val: 0.9374, Test: 0.9381\n",
      "Epoch: 055, Loss: 0.5033, Val: 0.9376, Test: 0.9381\n",
      "Epoch: 056, Loss: 0.5031, Val: 0.9375, Test: 0.9381\n",
      "Epoch: 057, Loss: 0.5034, Val: 0.9363, Test: 0.9371\n",
      "Epoch: 058, Loss: 0.5030, Val: 0.9378, Test: 0.9383\n",
      "Epoch: 059, Loss: 0.5033, Val: 0.9377, Test: 0.9383\n",
      "Epoch: 060, Loss: 0.5035, Val: 0.9379, Test: 0.9384\n",
      "Epoch: 061, Loss: 0.5029, Val: 0.9369, Test: 0.9377\n",
      "Epoch: 062, Loss: 0.5021, Val: 0.9383, Test: 0.9390\n",
      "Epoch: 063, Loss: 0.5017, Val: 0.9371, Test: 0.9377\n",
      "Epoch: 064, Loss: 0.5021, Val: 0.9386, Test: 0.9391\n",
      "Epoch: 065, Loss: 0.5031, Val: 0.9388, Test: 0.9394\n",
      "Epoch: 066, Loss: 0.5016, Val: 0.9389, Test: 0.9391\n",
      "Epoch: 067, Loss: 0.5007, Val: 0.9386, Test: 0.9390\n",
      "Epoch: 068, Loss: 0.5019, Val: 0.9379, Test: 0.9386\n",
      "Epoch: 069, Loss: 0.5000, Val: 0.9390, Test: 0.9392\n",
      "Epoch: 070, Loss: 0.5012, Val: 0.9397, Test: 0.9402\n",
      "Epoch: 071, Loss: 0.5029, Val: 0.9383, Test: 0.9388\n",
      "Epoch: 072, Loss: 0.5006, Val: 0.9405, Test: 0.9408\n",
      "Epoch: 073, Loss: 0.5008, Val: 0.9402, Test: 0.9406\n",
      "Epoch: 074, Loss: 0.5018, Val: 0.9391, Test: 0.9395\n",
      "Epoch: 075, Loss: 0.5004, Val: 0.9405, Test: 0.9407\n",
      "Epoch: 076, Loss: 0.5009, Val: 0.9399, Test: 0.9403\n",
      "Epoch: 077, Loss: 0.5005, Val: 0.9402, Test: 0.9404\n",
      "Epoch: 078, Loss: 0.5002, Val: 0.9407, Test: 0.9410\n",
      "Epoch: 079, Loss: 0.4995, Val: 0.9409, Test: 0.9413\n",
      "Epoch: 080, Loss: 0.5002, Val: 0.9405, Test: 0.9405\n",
      "Epoch: 081, Loss: 0.5011, Val: 0.9414, Test: 0.9417\n",
      "Epoch: 082, Loss: 0.5000, Val: 0.9411, Test: 0.9415\n",
      "Epoch: 083, Loss: 0.5014, Val: 0.9402, Test: 0.9404\n",
      "Epoch: 084, Loss: 0.4979, Val: 0.9414, Test: 0.9415\n",
      "Epoch: 085, Loss: 0.5026, Val: 0.9417, Test: 0.9420\n",
      "Epoch: 086, Loss: 0.4987, Val: 0.9422, Test: 0.9422\n",
      "Epoch: 087, Loss: 0.5015, Val: 0.9414, Test: 0.9416\n",
      "Epoch: 088, Loss: 0.5001, Val: 0.9428, Test: 0.9428\n",
      "Epoch: 089, Loss: 0.5008, Val: 0.9421, Test: 0.9422\n",
      "Epoch: 090, Loss: 0.4997, Val: 0.9421, Test: 0.9420\n",
      "Epoch: 091, Loss: 0.4993, Val: 0.9431, Test: 0.9432\n",
      "Epoch: 092, Loss: 0.4991, Val: 0.9377, Test: 0.9374\n",
      "Epoch: 093, Loss: 0.5023, Val: 0.9426, Test: 0.9425\n",
      "Epoch: 094, Loss: 0.4983, Val: 0.9434, Test: 0.9435\n",
      "Epoch: 095, Loss: 0.4980, Val: 0.9443, Test: 0.9442\n",
      "Epoch: 096, Loss: 0.4999, Val: 0.9433, Test: 0.9434\n",
      "Epoch: 097, Loss: 0.5011, Val: 0.9437, Test: 0.9436\n",
      "Epoch: 098, Loss: 0.4969, Val: 0.9427, Test: 0.9428\n",
      "Epoch: 099, Loss: 0.4967, Val: 0.9421, Test: 0.9419\n",
      "Epoch: 100, Loss: 0.4985, Val: 0.9385, Test: 0.9389\n",
      "Final Test: 0.9442\n"
     ]
    }
   ],
   "source": [
    "# Train/Test Loop\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train(train_loader)\n",
    "    val_auc = test(val_loader)\n",
    "    test_auc = test(test_loader)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1-NN additional statistics</h2>\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict = {}\n",
    "cnt = 0\n",
    "for graph in data_list:\n",
    "    z_raw = model.encode(graph.x, graph.edge_index)\n",
    "    final_edge_index = model.decode_all(z_raw)\n",
    "    fei = final_edge_index.tolist()\n",
    "    edges_pred = {k:[] for k in range(101)}\n",
    "    edges_pred_inv = {k:[] for k in range(101)}\n",
    "    for i in range(len(fei[0])):\n",
    "        edges_pred[fei[0][i]].append(fei[1][i])\n",
    "        edges_pred_inv[fei[1][i]].append(fei[0][i])\n",
    "    ts0 = graph.edge_index.tolist()\n",
    "    edges = {k:[] for k in range(101)}\n",
    "    edges_inv = {k:[] for k in range(101)}\n",
    "    for i in range(len(ts0[0])):\n",
    "        edges[ts0[0][i]].append(ts0[1][i])\n",
    "        edges_inv[ts0[1][i]].append(ts0[0][i])\n",
    "    originals = {}\n",
    "    predictions = {}\n",
    "    for i in range(101):\n",
    "        originals[i] = set(edges[i] + edges_inv[i])\n",
    "        predictions[i] = set(edges_pred[i] + edges_pred_inv[i])\n",
    "    graph_dict[cnt] = {\"real\": originals, \"preds\": predictions}\n",
    "    cnt += 1\n",
    "\n",
    "confusion_dict = {}\n",
    "true_positives = []\n",
    "false_positives = []\n",
    "true_negatives = []\n",
    "false_negatives = []\n",
    "for key in graph_dict:\n",
    "    real = graph_dict[key][\"real\"]\n",
    "    pred = graph_dict[key][\"preds\"]\n",
    "    node_matrix = {}\n",
    "    for i in range(101):\n",
    "        tp = len(pred[i].intersection(real[i]))\n",
    "        fp = len(pred[i] - real[i])\n",
    "        real_neg = set([j for j in range(101)]) - {i} - real[i]\n",
    "        pred_neg = set([j for j in range(101)]) - {i} - pred[i]\n",
    "        tn = len(pred_neg.intersection(real_neg))\n",
    "        fn = len(pred_neg - real_neg)\n",
    "        total = tp + fp + tn + fn\n",
    "        node_matrix[i] = {\"tp\": tp, \"fp\": fp, \"tn\": tn, \"fn\": fn, \"total\": total}\n",
    "        true_positives.append(tp/total)\n",
    "        false_positives.append(fp/total)\n",
    "        true_negatives.append(tn/total)\n",
    "        false_negatives.append(fn/total)\n",
    "    confusion_dict[key] = node_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positives mean=0.17, stdev=0.03\n",
      "false positives mean=0.28, stdev=0.10\n",
      "true negatives mean=0.54, stdev=0.10\n",
      "false negatives mean=0.00, stdev=0.00\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, stdev\n",
    "print(\"true positives mean={0:.2f}, stdev={1:.2f}\".format(mean(true_positives), stdev(true_positives)))\n",
    "print(\"false positives mean={0:.2f}, stdev={1:.2f}\".format(mean(false_positives), stdev(false_positives)))\n",
    "print(\"true negatives mean={0:.2f}, stdev={1:.2f}\".format(mean(true_negatives), stdev(true_negatives)))\n",
    "print(\"false negatives mean={0:.2f}, stdev={1:.2f}\".format(mean(false_negatives), stdev(false_negatives)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some notion of closeness. It over-links, but rarely avoids a close node. Also, a middle stage as this is not always a requirement. We want to encode the data in embeddings that can be used for the next stage. The quality will be measured then over the ability to learn closeness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1-NN data preparation for second stage</h2>\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "data_raw_dict = {\n",
    "    \"positives\": [],\n",
    "    \"negatives\": []\n",
    "}\n",
    "cnt = 0\n",
    "pos_cnt = 0\n",
    "neg_cnt = 0\n",
    "for graph in data_list:\n",
    "    neg_cnt = 0\n",
    "    ng_matrix = graph.y.tolist()\n",
    "    encoding_matrix = model.encode(graph.x, graph.edge_index).tolist()\n",
    "    for i in range(101):\n",
    "        for j in sample(range(101), 30):\n",
    "            if i == j:\n",
    "                # if pos_cnt < 80000:\n",
    "                #     data_raw_dict[\"positives\"].append(encoding_matrix[i]+encoding_matrix[i])\n",
    "                #     pos_cnt += 1\n",
    "                continue\n",
    "            else:\n",
    "                if ng_matrix[i][j] > 0.5:\n",
    "                    if pos_cnt < 80000:\n",
    "                        data_raw_dict[\"positives\"].append(encoding_matrix[i]+encoding_matrix[j]+[1])\n",
    "                        pos_cnt += 1\n",
    "                else:\n",
    "                    if pos_cnt < 80000 and neg_cnt < 1010:\n",
    "                        data_raw_dict[\"negatives\"].append(encoding_matrix[i]+encoding_matrix[j]+[0])\n",
    "                        neg_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tensor = data_raw_dict[\"positives\"][:5000] + data_raw_dict[\"negatives\"][:5000]\n",
    "main_tensor = torch.tensor(pre_tensor, dtype=torch.float32)\n",
    "\n",
    "main_tensor = main_tensor[torch.randperm(main_tensor.size()[0])]\n",
    "labels = main_tensor[:,-1:]\n",
    "embeddings = main_tensor[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1-NN Wide or Deep network</h2>\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Wide(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(128, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "class Deep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(128, 128)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(128, 128)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    " \n",
    "def model_train(model, X_train, y_train, X_val, y_val):\n",
    "    # loss function and optimizer\n",
    "    loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    " \n",
    "    n_epochs = 20   # number of epochs to run\n",
    "    batch_size = 10  # size of each batch\n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
    " \n",
    "    # Hold the best model\n",
    "    best_acc = - np.inf   # init to negative infinity\n",
    "    best_weights = None\n",
    " \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "                # take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "                # forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "                # print progress\n",
    "                acc = (y_pred.round() == y_batch).float().mean()\n",
    "                bar.set_postfix(\n",
    "                    loss=float(loss),\n",
    "                    acc=float(acc)\n",
    "                )\n",
    "        # evaluate accuracy at end of each epoch\n",
    "        model.eval()\n",
    "        y_pred = model(X_val)\n",
    "        acc = (y_pred.round() == y_val).float().mean()\n",
    "        acc = float(acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "    # restore model and return best accuracy\n",
    "    model.load_state_dict(best_weights)\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (wide): 0.93\n",
      "Accuracy (wide): 0.94\n",
      "Accuracy (wide): 0.92\n",
      "Accuracy (wide): 0.94\n",
      "Accuracy (wide): 0.93\n",
      "Accuracy (deep): 0.95\n",
      "Accuracy (deep): 0.94\n",
      "Accuracy (deep): 0.95\n",
      "Accuracy (deep): 0.95\n",
      "Accuracy (deep): 0.95\n",
      "Wide: 93.19% (+/- 0.53%)\n",
      "Deep: 94.86% (+/- 0.42%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "# train-test split: Hold out the test set for final model evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, train_size=0.7, shuffle=True)\n",
    " \n",
    "# define 5-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "cv_scores_wide = []\n",
    "for train, test in kfold.split(X_train, y_train):\n",
    "    # create model, train, and get accuracy\n",
    "    model = Wide()\n",
    "    acc = model_train(model, X_train[train], y_train[train], X_train[test], y_train[test])\n",
    "    print(\"Accuracy (wide): %.2f\" % acc)\n",
    "    cv_scores_wide.append(acc)\n",
    "cv_scores_deep = []\n",
    "for train, test in kfold.split(X_train, y_train):\n",
    "    # create model, train, and get accuracy\n",
    "    model = Deep()\n",
    "    acc = model_train(model, X_train[train], y_train[train], X_train[test], y_train[test])\n",
    "    print(\"Accuracy (deep): %.2f\" % acc)\n",
    "    cv_scores_deep.append(acc)\n",
    " \n",
    "# evaluate the model\n",
    "wide_acc = np.mean(cv_scores_wide)\n",
    "wide_std = np.std(cv_scores_wide)\n",
    "deep_acc = np.mean(cv_scores_deep)\n",
    "deep_std = np.std(cv_scores_deep)\n",
    "print(\"Wide: %.2f%% (+/- %.2f%%)\" % (wide_acc*100, wide_std*100))\n",
    "print(\"Deep: %.2f%% (+/- %.2f%%)\" % (deep_acc*100, deep_std*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrain a deep model\n",
      "Final model accuracy: 95.13%\n"
     ]
    }
   ],
   "source": [
    "# rebuild model with full set of training data\n",
    "if wide_acc > deep_acc:\n",
    "    print(\"Retrain a wide model\")\n",
    "    model = Wide()\n",
    "else:\n",
    "    print(\"Retrain a deep model\")\n",
    "    model = Deep()\n",
    "acc = model_train(model, X_train, y_train, X_test, y_test)\n",
    "print(f\"Final model accuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.1717339e-01 -5.7865076e-02 -1.4151484e-02  1.5557876e-02\n",
      " -4.2768332e-01 -4.4370964e-01  3.3552490e-02  4.1792460e-02\n",
      " -4.4831714e-01 -7.0630580e-02  2.1677202e-01  2.2268556e-01\n",
      " -6.3116723e-01  8.4077612e-02  5.1827902e-01  3.1405866e-01\n",
      " -2.0323183e-01  8.0681220e-02  1.3163482e-01 -1.4057782e-01\n",
      " -1.0440167e-02  9.3202174e-02  1.8332930e-01  2.2968042e-01\n",
      "  1.0498860e-01 -2.6336920e-01  5.9151962e-02  5.4599382e-02\n",
      " -5.2246571e-02 -2.9523142e-02  2.7236146e-01 -5.9649184e-02\n",
      " -2.9510462e-01 -2.3909754e-01 -2.0827483e-01 -4.0251181e-01\n",
      " -1.6330114e-01 -2.0440927e-01 -7.0687167e-02  2.9918218e-01\n",
      "  9.8987930e-03  9.5853359e-02 -2.3753516e-01 -1.8214691e-01\n",
      " -4.3035455e-02 -1.5556519e-02 -1.2626988e-01  1.3818063e-01\n",
      " -2.8056109e-01  5.3613499e-02 -2.5029457e-01  3.6895238e-02\n",
      " -1.3619283e-01  5.3167921e-01 -3.1269984e-03 -3.6275232e-01\n",
      "  2.5250453e-01  1.8157607e-01 -2.1313316e-01 -8.1577316e-02\n",
      "  1.5478041e-02  2.6419458e-01  5.4498559e-01  2.4573167e-01\n",
      "  4.3307829e-01 -5.6470633e-02 -1.1331800e-02  2.2720667e-02\n",
      " -4.2329457e-01 -4.5480761e-01  4.4342883e-02  4.5427322e-02\n",
      " -4.4606289e-01 -8.0577120e-02  2.1183130e-01  2.3732670e-01\n",
      " -6.3463920e-01  9.8108679e-02  5.1491594e-01  3.1170848e-01\n",
      " -2.1752605e-01  8.0698565e-02  1.3647492e-01 -1.4649908e-01\n",
      " -1.1975139e-02  8.7222464e-02  1.8109830e-01  2.2941652e-01\n",
      "  9.2107445e-02 -2.6699829e-01  5.0050810e-02  4.7848269e-02\n",
      " -3.7784070e-02 -1.9218609e-02  2.7938482e-01 -5.8992282e-02\n",
      " -2.8603178e-01 -2.3820066e-01 -1.9802438e-01 -4.0632907e-01\n",
      " -1.5602684e-01 -2.1411142e-01 -7.1299389e-02  3.2000360e-01\n",
      "  1.4883975e-02  9.6994124e-02 -2.4576062e-01 -1.7677087e-01\n",
      " -3.5438828e-02 -1.8663231e-02 -1.2395805e-01  1.4290737e-01\n",
      " -2.8109637e-01  5.1804379e-02 -2.5030202e-01  2.8354084e-02\n",
      " -1.3504303e-01  5.2837777e-01  5.8637280e-04 -3.6035749e-01\n",
      "  2.5716203e-01  1.8610978e-01 -2.0425469e-01 -7.5680345e-02\n",
      "  1.3318401e-02  2.5793216e-01  5.3815591e-01  2.5184375e-01] -> [0.8791382] (expected [1.])\n",
      "[-0.330864    0.06145173 -0.19024104 -0.0187828   0.02340415 -0.02353234\n",
      " -0.09699653  0.01063286  0.07676873  0.10265465 -0.20202741 -0.15060855\n",
      "  0.16400571 -0.16511227 -0.13620533  0.17994827  0.12047814 -0.13892125\n",
      " -0.04614743  0.10489418  0.10139035  0.10708459  0.02570695 -0.2025975\n",
      "  0.27189144 -0.02998438  0.10394385  0.05386116 -0.0839919  -0.16725492\n",
      " -0.08409136 -0.05255683  0.03403759  0.01767641 -0.01208991  0.09068033\n",
      "  0.07768191  0.12292603  0.10131624 -0.34487686 -0.05155137  0.00810724\n",
      " -0.01497776 -0.16132963 -0.01401632  0.15828408  0.09045064  0.03769962\n",
      "  0.14451867 -0.05035711  0.15338582  0.08894943  0.18451214 -0.09988489\n",
      "  0.1258642  -0.01964192 -0.12790039 -0.1538146  -0.08576083 -0.08980085\n",
      " -0.06369558  0.13308588  0.15275443 -0.06599046  0.32791537 -0.05607147\n",
      " -0.01604865 -0.00805374 -0.3239973  -0.42272744  0.09292679  0.04010247\n",
      " -0.43265417 -0.13727242  0.17752124  0.2289034  -0.580992    0.06468205\n",
      "  0.36844456  0.31403068 -0.14854711  0.19902556  0.08086179 -0.1680052\n",
      "  0.00577518  0.02677535  0.1272771   0.15678254  0.05438989 -0.19849831\n",
      "  0.09766823 -0.03170805 -0.04216717  0.01377597  0.23065558  0.00603591\n",
      " -0.25511116 -0.19574961 -0.10372823 -0.37275997 -0.10334317 -0.1962428\n",
      "  0.01648377  0.26799214 -0.0055501   0.06452463 -0.14870533 -0.14268668\n",
      "  0.05730914  0.03938692 -0.08882365  0.06507913 -0.26012477  0.08325417\n",
      " -0.21879144 -0.02418476 -0.19352373  0.45169723 -0.04488386 -0.27481258\n",
      "  0.14917243  0.18355168 -0.14959304 -0.11858585 -0.02265492  0.17981535\n",
      "  0.5236769   0.20937355] -> [9.802447e-09] (expected [0.])\n",
      "[ 0.22271447 -0.15952486  0.18382713 -0.0736852  -0.1735313  -0.2958808\n",
      "  0.1651783  -0.12247935 -0.1891824  -0.0376753  -0.01021766  0.17950147\n",
      " -0.16975714  0.05774587  0.1254158   0.14762676 -0.12215778  0.63846254\n",
      " -0.07816781 -0.16154514 -0.07402876 -0.26636425 -0.13094182  0.13025358\n",
      " -0.24119826 -0.06175733  0.1358243  -0.1056514  -0.02328973  0.08909103\n",
      " -0.04106475  0.0768939  -0.27535206 -0.2432819   0.17938295 -0.48485598\n",
      " -0.21559832 -0.19126189  0.14253952  0.26128456 -0.05623893  0.05568039\n",
      "  0.02272103  0.03955377  0.23339663 -0.10984882  0.06858939  0.02321001\n",
      " -0.0886617   0.37852514 -0.4213081  -0.00954623 -0.43551075  0.03039259\n",
      " -0.12275901  0.00415562 -0.09740531  0.0846632  -0.02359264 -0.10913993\n",
      "  0.04740994 -0.1211182   0.21601045 -0.07385829 -0.32684976  0.22578472\n",
      " -0.18895131 -0.007252   -0.09391057 -0.12486035  0.06219388 -0.28197473\n",
      "  0.01795186  0.4119055  -0.104416   -0.26223752  0.21181701 -0.01028562\n",
      " -0.0881796   0.13924631  0.12256099 -0.01716547 -0.02672764  0.29523718\n",
      "  0.06995692  0.08960645  0.01676747 -0.22883645  0.30053788  0.06775854\n",
      "  0.2258673   0.07662316 -0.17618199 -0.37895942 -0.03361698 -0.16629015\n",
      " -0.07676308 -0.48291153  0.01749101 -0.43210956 -0.13394362  0.07412936\n",
      "  0.18788399 -0.2770224  -0.09938193  0.07316856  0.10176948 -0.49174565\n",
      " -0.20396474 -0.07086708  0.00149342  0.16793291  0.11178198 -0.08262042\n",
      " -0.22674532  0.3133783   0.3294002  -0.1406103   0.1196633  -0.16833246\n",
      " -0.23056692 -0.27149156 -0.15944612  0.02952655  0.125426    0.14204529\n",
      "  0.28754896 -0.09822766] -> [5.1421516e-06] (expected [0.])\n",
      "[-0.0837351  -0.0448166  -0.02411401 -0.04934224 -0.17384487  0.0436961\n",
      " -0.16253164  0.00429083 -0.23038313  0.04281615  0.10458341 -0.14767751\n",
      " -0.16443859 -0.25273436  0.2536457   0.28187922  0.18969768 -0.07971613\n",
      " -0.00124664  0.0167587   0.12776399  0.10312244  0.17823721  0.26184237\n",
      "  0.33748722 -0.12687102  0.15838829  0.15304278 -0.34170038 -0.28474018\n",
      "  0.14931548 -0.0870682  -0.40944332 -0.18892667 -0.45725405  0.03313363\n",
      " -0.05818337  0.18113264 -0.11131335 -0.33085167  0.00981047  0.03956431\n",
      " -0.05151433 -0.0908279  -0.1923779   0.13452251 -0.22167386  0.07144933\n",
      " -0.078137    0.03095455 -0.01341315  0.17364275  0.12554172  0.33699083\n",
      "  0.09001357 -0.22338557 -0.01216271  0.06234008 -0.27405703 -0.10425153\n",
      "  0.01439177  0.30483744  0.48207515 -0.03447701  0.14754231 -0.1325877\n",
      " -0.01699865 -0.05028243 -0.28408775 -0.13301158 -0.1358625   0.08784046\n",
      " -0.34732074 -0.06451582  0.14791946  0.02272195 -0.34264398 -0.17731088\n",
      "  0.36250448  0.30614182  0.03638357 -0.02239242  0.0565416  -0.06525221\n",
      "  0.06708034  0.10107493  0.1651579   0.23970264  0.22873928 -0.21504852\n",
      "  0.07301868  0.10800594 -0.21460019 -0.1220111   0.21358922 -0.04224831\n",
      " -0.36934036 -0.09301305 -0.40087014 -0.03471315 -0.08040024  0.02077419\n",
      " -0.12134337 -0.07245152  0.0294169   0.04603503 -0.15192299 -0.05395331\n",
      " -0.0933306   0.14605504 -0.19364022  0.02699062 -0.18397355  0.04634377\n",
      " -0.03619428  0.0591231  -0.0431584   0.46648848  0.03742462 -0.2539232\n",
      "  0.12606579  0.12370135 -0.2251651  -0.11487918 -0.01822015  0.28772992\n",
      "  0.49116087  0.09274285] -> [0.41050267] (expected [1.])\n",
      "[-7.63157979e-02 -2.40801387e-02  6.28724545e-02 -3.44757959e-02\n",
      "  1.64169565e-01  1.27958730e-02  9.96308103e-02  1.40649468e-01\n",
      "  8.38868394e-02 -2.85503089e-01 -1.41207337e-01  9.28785130e-02\n",
      " -4.95303869e-02 -7.77546614e-02 -1.27960384e-01  2.22919405e-01\n",
      "  8.11097696e-02  9.67583209e-02 -4.66409475e-02 -2.14993894e-01\n",
      "  3.77927944e-02 -6.59181550e-02 -6.07011244e-02  4.76374179e-02\n",
      " -6.41053170e-02 -2.46628150e-02 -8.66867602e-03 -4.12586555e-02\n",
      "  3.04690301e-02  1.43760383e-01  7.06293434e-03  1.76219314e-01\n",
      "  5.16500324e-02  2.35103041e-01  1.01228997e-01  2.44096160e-01\n",
      "  1.96702570e-01  9.34800804e-02 -5.77658042e-03 -1.25255555e-01\n",
      "  7.33259320e-03 -7.32862055e-02 -3.66118737e-03  1.56098679e-01\n",
      "  1.33527920e-01  1.36991665e-01  1.06185719e-01 -1.03929698e-01\n",
      "  3.40567641e-02  2.78462470e-02  2.24395454e-01 -2.43084431e-01\n",
      " -5.09101376e-02 -1.23238772e-01  3.37802768e-02  1.80506930e-01\n",
      " -7.67727867e-02  3.44040580e-02  2.73749754e-02 -1.14939764e-01\n",
      " -1.28552318e-01 -7.54444301e-02 -2.40921378e-02 -1.61090046e-02\n",
      "  1.32202223e-01 -3.02148402e-01  2.04013363e-01 -3.61703224e-02\n",
      "  9.46377963e-03  1.68025225e-01 -1.47889435e-01 -1.23928487e-01\n",
      "  1.78889155e-01 -4.99179959e-03  3.28678973e-02 -6.48027211e-02\n",
      "  1.92209333e-01 -2.33848020e-03 -2.74872854e-02 -2.61941344e-01\n",
      " -1.05844392e-02  5.35906404e-02  8.15910175e-02 -1.90383457e-02\n",
      " -1.10120445e-01 -1.31365150e-01 -1.44217923e-01  1.37729719e-01\n",
      " -3.24740946e-01  9.45773721e-03 -1.66778952e-01  1.16029754e-02\n",
      "  2.19870359e-02  1.53416544e-01  4.92712855e-03  4.93074730e-02\n",
      " -4.22657281e-02  1.38083979e-01 -1.36388168e-01  2.16364086e-01\n",
      " -1.41709000e-02 -2.08452344e-04 -1.71694905e-01  2.16244325e-01\n",
      "  2.47190781e-02 -5.79462349e-02  1.28445730e-01  2.93491840e-01\n",
      "  3.04497294e-02 -9.92563367e-02 -1.09400377e-01 -7.20842406e-02\n",
      " -4.58106101e-02  6.93200529e-02 -8.91467780e-02 -1.01222351e-01\n",
      " -8.20192769e-02 -2.83215642e-01 -6.67031705e-02  2.84511328e-01\n",
      "  5.32905385e-02 -2.10873820e-02  1.52256384e-01  1.98680595e-01\n",
      "  2.67397948e-02 -2.40933120e-01 -3.86802971e-01 -1.57303333e-01] -> [0.00160045] (expected [0.])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Test out inference with 5 samples\n",
    "    for i in range(5):\n",
    "        y_pred = model(X_test[i:i+1])\n",
    "        print(f\"{X_test[i].numpy()} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one above is an example for visual validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>NgSets prototype, undirected</h1>\n",
    "Once validated the simple prototype, let's continue to the actual problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load the data</h2>\n",
    "Here an \"export\" folder with .txt instance files used on the CTWVRP project, and a .csv file with the neighborhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_files_list = [\"./export/\"+f for f in os.listdir(\"./export\") ]\n",
    "instance_dict = {}\n",
    "for dir_str in data_files_list:\n",
    "    with open(dir_str, 'r') as text_file:\n",
    "        cnt = 0\n",
    "        instance = \"\"\n",
    "        for line in text_file:\n",
    "            if cnt < 9:\n",
    "                if cnt == 0:\n",
    "                    instance = line.split()[0]\n",
    "                    instance_dict[instance] = []\n",
    "                cnt += 1\n",
    "                continue\n",
    "            split_line = line.split()\n",
    "            instance_dict[instance].append([int(i) for i in split_line])\n",
    "        text_file.close()\n",
    "\n",
    "ng_dict = {}\n",
    "cnt = -1\n",
    "with open(\"ng_outs.csv\", 'r') as text_file:\n",
    "    for line in text_file:\n",
    "        if cnt < 2:\n",
    "            cnt += 1\n",
    "            continue\n",
    "        raw_line = line.strip()\n",
    "        split_line_list = raw_line.split(sep=\";\")\n",
    "        instance = split_line_list[3]\n",
    "        if instance not in ng_dict:\n",
    "            ng_dict[instance] = [[0 for i in range(101)]]\n",
    "        ng_dict[instance].append([0] + [int(i) for i in split_line_list[5:-1]])\n",
    "        if len(split_line_list[5:-1]) != 100:\n",
    "            print(\"case found for instance \"+instance)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import torch\n",
    "import torch_geometric as tg\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GraphConv, global_add_pool\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_list = []\n",
    "# for instance_name in ng_dict:\n",
    "#     y = torch.tensor(ng_dict[instance_name], dtype=torch.double)\n",
    "#     x = torch.tensor(instance_dict[instance_name], dtype=torch.double)\n",
    "#     attr = [[i] for i in range(n_edges)]\n",
    "#     loc_dict = {(i[0],j[0]): sqrt((i[1]-j[1])**2 + (i[2]-j[2])**2) for i in instance_dict[instance_name] for j in instance_dict[instance_name]}\n",
    "#     cnt = -1\n",
    "#     for i in range(101):\n",
    "#         for j in range(101):\n",
    "#             if i != j:\n",
    "#                 cnt += 1\n",
    "#                 attr[cnt].append(loc_dict[i,j])\n",
    "#     attr = torch.tensor(attr, dtype=torch.double)\n",
    "#     pos = []\n",
    "#     for i in instance_dict[instance_name]:\n",
    "#         pos.append([i[1], i[2]])\n",
    "#     pos = torch.tensor(pos, dtype=torch.double)\n",
    "#     data_list.append(Data(x=x, y=y, edge_index=edge_index, pos=pos, edge_attr=attr))\n",
    "\n",
    "data_list = []\n",
    "for instance_name in ng_dict:\n",
    "    y = torch.tensor(ng_dict[instance_name], dtype=torch.float)\n",
    "    x = torch.tensor(instance_dict[instance_name], dtype=torch.float)\n",
    "    pos = []\n",
    "    for i in instance_dict[instance_name]:\n",
    "        pos.append([i[1], i[2]])\n",
    "    pos = torch.tensor(pos, dtype=torch.double)\n",
    "    data_list.append(Data(x=x, y=y, edge_index = knn_graph(x, 15), pos=pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Prepare data as in prototype</h2>\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "def add_edge_labels(graph):\n",
    "    transform = T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False)\n",
    "    return transform(graph)\n",
    "\n",
    "labeled_graphs = [add_edge_labels(graph) for graph in data_list]\n",
    "\n",
    "train_size = [g[0] for g in labeled_graphs]\n",
    "val_size = [g[1] for g in labeled_graphs]\n",
    "test_size = [g[2] for g in labeled_graphs]\n",
    "\n",
    "train_loader = DataLoader(train_size, batch_size=500, shuffle=True)\n",
    "val_loader = DataLoader(val_size, batch_size=500, shuffle=False)\n",
    "test_loader = DataLoader(test_size, batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Encode-decode routine</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ed = Net(data_list[0].num_features, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model_ed.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(loader):\n",
    "    model_ed.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        z = model_ed.encode(batch.x, batch.edge_index)\n",
    "\n",
    "        # We perform a new round of negative sampling for every training epoch:\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=batch.edge_index, num_nodes=batch.num_nodes,\n",
    "            num_neg_samples=batch.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "        # Concat positive and negative edge indices.\n",
    "        edge_label_index = torch.cat(\n",
    "            [batch.edge_label_index, neg_edge_index],\n",
    "            dim=-1,\n",
    "        )\n",
    "        # Label for positive edges: 1, for negative edges: 0.\n",
    "        edge_label = torch.cat([\n",
    "            batch.edge_label,\n",
    "            batch.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "        ], dim=0)\n",
    "\n",
    "        # Note: The model is trained in a supervised manner using the given\n",
    "        # `edge_label_index` and `edge_label` targets.\n",
    "        out = model_ed.decode(z, edge_label_index).view(-1)\n",
    "        loss = criterion(out, edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model_ed.eval()\n",
    "    all_out = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in loader:\n",
    "        z = model_ed.encode(batch.x, batch.edge_index)\n",
    "        out = model_ed.decode(z, batch.edge_label_index).view(-1).sigmoid()\n",
    "        all_out.append(out.cpu().numpy())\n",
    "        all_labels.append(batch.edge_label.cpu().numpy())\n",
    "\n",
    "    all_out = np.concatenate(all_out)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return roc_auc_score(all_labels, all_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 9919.4426, Val: 0.5599, Test: 0.5596\n",
      "Epoch: 002, Loss: 68.1077, Val: 0.6494, Test: 0.6487\n",
      "Epoch: 003, Loss: 18.2747, Val: 0.6678, Test: 0.6669\n",
      "Epoch: 004, Loss: 11.6792, Val: 0.6743, Test: 0.6737\n",
      "Epoch: 005, Loss: 8.9833, Val: 0.6787, Test: 0.6783\n",
      "Epoch: 006, Loss: 7.3602, Val: 0.6872, Test: 0.6865\n",
      "Epoch: 007, Loss: 6.2161, Val: 0.6883, Test: 0.6877\n",
      "Epoch: 008, Loss: 5.3317, Val: 0.7015, Test: 0.7010\n",
      "Epoch: 009, Loss: 4.5943, Val: 0.7094, Test: 0.7091\n",
      "Epoch: 010, Loss: 3.9720, Val: 0.7196, Test: 0.7193\n",
      "Epoch: 011, Loss: 3.4475, Val: 0.7297, Test: 0.7295\n",
      "Epoch: 012, Loss: 2.9939, Val: 0.7362, Test: 0.7365\n",
      "Epoch: 013, Loss: 2.6055, Val: 0.7397, Test: 0.7401\n",
      "Epoch: 014, Loss: 2.2750, Val: 0.7417, Test: 0.7422\n",
      "Epoch: 015, Loss: 1.9978, Val: 0.7410, Test: 0.7415\n",
      "Epoch: 016, Loss: 1.7775, Val: 0.7392, Test: 0.7399\n",
      "Epoch: 017, Loss: 1.5995, Val: 0.7371, Test: 0.7377\n",
      "Epoch: 018, Loss: 1.4543, Val: 0.7332, Test: 0.7340\n",
      "Epoch: 019, Loss: 1.3433, Val: 0.7337, Test: 0.7346\n",
      "Epoch: 020, Loss: 1.2477, Val: 0.7315, Test: 0.7323\n",
      "Final Test: 0.7422\n"
     ]
    }
   ],
   "source": [
    "# Train/Test Loop\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 21):\n",
    "    loss = train(train_loader)\n",
    "    val_auc = test(val_loader)\n",
    "    test_auc = test(test_loader)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Check on values</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict = {}\n",
    "cnt = 0\n",
    "for graph in data_list:\n",
    "    z_raw = model_ed.encode(graph.x, graph.edge_index)\n",
    "    final_edge_index = model_ed.decode_all(z_raw)\n",
    "    fei = final_edge_index.tolist()\n",
    "    edges_pred = {k:[] for k in range(101)}\n",
    "    edges_pred_inv = {k:[] for k in range(101)}\n",
    "    for i in range(len(fei[0])):\n",
    "        edges_pred[fei[0][i]].append(fei[1][i])\n",
    "        edges_pred_inv[fei[1][i]].append(fei[0][i])\n",
    "    ts0 = graph.edge_index.tolist()\n",
    "    edges = {k:[] for k in range(101)}\n",
    "    edges_inv = {k:[] for k in range(101)}\n",
    "    for i in range(len(ts0[0])):\n",
    "        edges[ts0[0][i]].append(ts0[1][i])\n",
    "        edges_inv[ts0[1][i]].append(ts0[0][i])\n",
    "    originals = {}\n",
    "    predictions = {}\n",
    "    for i in range(101):\n",
    "        originals[i] = set(edges[i] + edges_inv[i])\n",
    "        predictions[i] = set(edges_pred[i] + edges_pred_inv[i])\n",
    "    graph_dict[cnt] = {\"real\": originals, \"preds\": predictions}\n",
    "    cnt += 1\n",
    "\n",
    "confusion_dict = {}\n",
    "true_positives = []\n",
    "false_positives = []\n",
    "true_negatives = []\n",
    "false_negatives = []\n",
    "for key in graph_dict:\n",
    "    real = graph_dict[key][\"real\"]\n",
    "    pred = graph_dict[key][\"preds\"]\n",
    "    node_matrix = {}\n",
    "    for i in range(101):\n",
    "        tp = len(pred[i].intersection(real[i]))\n",
    "        fp = len(pred[i] - real[i])\n",
    "        real_neg = set([j for j in range(101)]) - {i} - real[i]\n",
    "        pred_neg = set([j for j in range(101)]) - {i} - pred[i]\n",
    "        tn = len(pred_neg.intersection(real_neg))\n",
    "        fn = len(pred_neg - real_neg)\n",
    "        total = tp + fp + tn + fn\n",
    "        node_matrix[i] = {\"tp\": tp, \"fp\": fp, \"tn\": tn, \"fn\": fn, \"total\": total}\n",
    "        true_positives.append(tp/total)\n",
    "        false_positives.append(fp/total)\n",
    "        true_negatives.append(tn/total)\n",
    "        false_negatives.append(fn/total)\n",
    "    confusion_dict[key] = node_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positives mean=0.17, stdev=0.03\n",
      "false positives mean=0.65, stdev=0.18\n",
      "true negatives mean=0.18, stdev=0.18\n",
      "false negatives mean=0.00, stdev=0.00\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, stdev\n",
    "print(\"true positives mean={0:.2f}, stdev={1:.2f}\".format(mean(true_positives), stdev(true_positives)))\n",
    "print(\"false positives mean={0:.2f}, stdev={1:.2f}\".format(mean(false_positives), stdev(false_positives)))\n",
    "print(\"true negatives mean={0:.2f}, stdev={1:.2f}\".format(mean(true_negatives), stdev(true_negatives)))\n",
    "print(\"false negatives mean={0:.2f}, stdev={1:.2f}\".format(mean(false_negatives), stdev(false_negatives)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>NgLearning</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "data_raw_dict = {\n",
    "    \"positives\": [],\n",
    "    \"negatives\": []\n",
    "}\n",
    "cnt = 0\n",
    "pos_cnt = 0\n",
    "neg_cnt = 0\n",
    "for graph in data_list:\n",
    "    neg_cnt = 0\n",
    "    ng_matrix = graph.y.tolist()\n",
    "    encoding_matrix = model.encode(graph.x, graph.edge_index).tolist()\n",
    "    for i in range(101):\n",
    "        for j in sample(range(101), 30):\n",
    "            if i == j:\n",
    "                # if pos_cnt < 80000:\n",
    "                #     data_raw_dict[\"positives\"].append(encoding_matrix[i]+encoding_matrix[i])\n",
    "                #     pos_cnt += 1\n",
    "                continue\n",
    "            else:\n",
    "                if ng_matrix[i][j] > 0.5:\n",
    "                    if pos_cnt < 80000:\n",
    "                        data_raw_dict[\"positives\"].append(encoding_matrix[i]+encoding_matrix[j]+[1])\n",
    "                        pos_cnt += 1\n",
    "                else:\n",
    "                    if pos_cnt < 80000 and neg_cnt < 1010:\n",
    "                        data_raw_dict[\"negatives\"].append(encoding_matrix[i]+encoding_matrix[j]+[0])\n",
    "                        neg_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tensor = data_raw_dict[\"positives\"][:5000] + data_raw_dict[\"negatives\"][:5000]\n",
    "main_tensor = torch.tensor(pre_tensor, dtype=torch.float32)\n",
    "\n",
    "main_tensor = main_tensor[torch.randperm(main_tensor.size()[0])]\n",
    "labels = main_tensor[:,-1:]\n",
    "embeddings = main_tensor[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Learning stage</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(128, 128)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(128, 128)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, X_train, y_train, X_val, y_val):\n",
    "    # loss function and optimizer\n",
    "    loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    " \n",
    "    n_epochs = 20   # number of epochs to run\n",
    "    batch_size = 10  # size of each batch\n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
    " \n",
    "    # Hold the best model\n",
    "    best_acc = - np.inf   # init to negative infinity\n",
    "    best_weights = None\n",
    " \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "                # take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "                # forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "                # print progress\n",
    "                acc = (y_pred.round() == y_batch).float().mean()\n",
    "                bar.set_postfix(\n",
    "                    loss=float(loss),\n",
    "                    acc=float(acc)\n",
    "                )\n",
    "        # evaluate accuracy at end of each epoch\n",
    "        model.eval()\n",
    "        y_pred = model(X_val)\n",
    "        acc = (y_pred.round() == y_val).float().mean()\n",
    "        acc = float(acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "    # restore model and return best accuracy\n",
    "    model.load_state_dict(best_weights)\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (deep): 0.97\n",
      "Accuracy (deep): 0.97\n",
      "Accuracy (deep): 0.96\n",
      "Accuracy (deep): 0.97\n",
      "Accuracy (deep): 0.96\n",
      "Deep: 96.46% (+/- 0.45%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "# train-test split: Hold out the test set for final model evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, train_size=0.7, shuffle=True)\n",
    " \n",
    "# define 5-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "cv_scores_deep = []\n",
    "for train, test in kfold.split(X_train, y_train):\n",
    "    # create model, train, and get accuracy\n",
    "    model_ls = Deep()\n",
    "    acc = model_train(model_ls, X_train[train], y_train[train], X_train[test], y_train[test])\n",
    "    print(\"Accuracy (deep): %.2f\" % acc)\n",
    "    cv_scores_deep.append(acc)\n",
    " \n",
    "# evaluate the model\n",
    "deep_acc = np.mean(cv_scores_deep)\n",
    "deep_std = np.std(cv_scores_deep)\n",
    "print(\"Deep: %.2f%% (+/- %.2f%%)\" % (deep_acc*100, deep_std*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model accuracy: 97.37%\n"
     ]
    }
   ],
   "source": [
    "model_ls = Deep()\n",
    "acc = model_train(model_ls, X_train, y_train, X_test, y_test)\n",
    "print(f\"Final model accuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.46387836  0.1943435  -0.37253988  0.21790805  0.29583716  0.45350713\n",
      " -0.44931048  0.97286505  0.15320109  0.05452149 -0.63882375 -1.2315071\n",
      "  0.15864943  0.7793971   0.8734473   0.8504205   1.2025108  -0.08820149\n",
      "  1.1138158  -0.31058282 -0.39535397 -0.41618708 -0.4199708   0.3125999\n",
      " -0.00468891  0.03943263 -0.5514713   0.3135659  -0.6818434  -0.28593165\n",
      "  0.64737177 -1.148654   -0.2523658   0.36917987 -0.08944561  0.95397073\n",
      "  0.39610565  0.9281209  -0.06270041  0.11733012  0.49224165  0.11228694\n",
      "  0.39218405 -0.42325312  0.306798   -0.7165122  -0.7366651   1.0995601\n",
      " -1.1010973  -0.04442366  0.06648539 -0.4796762  -0.24985504  0.7891465\n",
      " -0.09640184  0.2128881  -0.41142714 -0.978595    0.54114723  0.07069811\n",
      " -0.79138887 -0.12155458  0.4828937   0.263994    0.3635391   0.1962971\n",
      " -0.36730343  0.17375365  0.2576986   0.45053726 -0.39176714  0.8915362\n",
      "  0.14990321  0.01941784 -0.6330897  -1.1591773   0.16111828  0.7265636\n",
      "  0.83141446  0.7369272   1.1015825  -0.05950847  1.0750304  -0.2848618\n",
      " -0.3753364  -0.40444878 -0.45335364  0.3125965  -0.01254072  0.12510857\n",
      " -0.52088815  0.27478144 -0.6132195  -0.30752292  0.65892494 -1.0576622\n",
      " -0.24252586  0.2606447  -0.05913464  0.8749399   0.38461792  0.87450874\n",
      " -0.01895694  0.06843917  0.5294821   0.05799014  0.38688442 -0.42039657\n",
      "  0.21470448 -0.7304095  -0.674762    1.0498719  -1.0432682  -0.03416038\n",
      "  0.07330845 -0.49167007 -0.20700575  0.77141225 -0.1411974   0.2141104\n",
      " -0.39246807 -1.0282834   0.5218976   0.0708883  -0.7818828  -0.09187057\n",
      "  0.48713946  0.20308559] -> [1.] (expected [1.])\n",
      "[ 8.8997287e-01 -3.6922622e-01  1.9007067e-01 -1.9577444e-02\n",
      "  1.0676481e+00  1.8709360e-01 -3.3088192e-01  5.3130072e-01\n",
      "  1.2290008e-01 -1.5956721e-01 -4.0258244e-02 -3.1821868e-01\n",
      " -7.8752381e-01  4.3234885e-01 -4.2600751e-02  2.5519574e-01\n",
      "  3.0242732e-01 -3.4321463e-01 -2.8705823e-01 -3.4201205e-01\n",
      " -5.8040626e-02 -7.8421675e-02  2.1379997e-01  3.0287585e-01\n",
      "  1.4371897e-01 -2.0125768e+00 -7.8185725e-01  6.1043894e-01\n",
      "  1.9064917e-01  3.8247752e-01  9.2042536e-02 -5.2867985e-01\n",
      "  6.3618235e-02  3.6699700e-01 -5.7230651e-01  8.9566344e-01\n",
      "  2.0631087e-01  4.9272832e-01 -2.4902508e-01  7.3036671e-01\n",
      " -7.4861932e-01 -4.2046723e-01  1.6288224e-01 -1.6553503e-01\n",
      "  7.6548636e-01 -4.1253424e-01  2.7031446e-01 -3.7563750e-01\n",
      " -9.0325242e-01 -5.5116379e-01 -9.8429769e-02  2.5973269e-01\n",
      " -1.9153580e-01 -4.1018036e-01  7.4063283e-01 -1.4397549e-03\n",
      "  5.4285073e-01  2.8189027e-01  3.2974702e-01 -5.2248460e-01\n",
      "  4.7391725e-01 -5.9122795e-01 -3.7740406e-01  3.1967914e-01\n",
      "  7.2402638e-01 -2.9861194e-01  1.9394571e-01 -1.8169081e-01\n",
      "  1.0480648e+00  2.1781129e-01 -3.1074211e-01  3.7786558e-01\n",
      "  1.2156093e-01 -1.6494766e-01 -4.7560088e-02 -1.7021045e-01\n",
      " -8.6147285e-01  3.4790167e-01 -5.4121025e-02  5.4219373e-02\n",
      "  1.6991960e-01 -4.3634009e-01 -2.9253912e-01 -2.5076523e-01\n",
      " -9.1114581e-02 -7.8193270e-02  4.5515187e-02  4.0368789e-01\n",
      "  1.8553339e-01 -1.8634626e+00 -7.0893490e-01  5.2343941e-01\n",
      "  3.5566747e-01  4.2283073e-01  1.3408391e-01 -3.4603095e-01\n",
      "  9.8806873e-02  2.3517482e-01 -4.8573223e-01  8.5349303e-01\n",
      "  3.1417096e-01  4.4923031e-01 -1.4712018e-01  6.2082958e-01\n",
      " -6.3824850e-01 -5.2333140e-01  1.6214460e-01 -1.4896208e-01\n",
      "  5.5775571e-01 -4.1801700e-01  3.7594464e-01 -4.0309766e-01\n",
      " -9.0662718e-01 -4.3718761e-01  1.7504381e-02  2.4581371e-01\n",
      " -1.7179020e-01 -3.9723703e-01  6.3317817e-01 -2.3922056e-02\n",
      "  5.2335483e-01  1.1181721e-01  3.0265486e-01 -4.5042092e-01\n",
      "  5.0469542e-01 -5.1198351e-01 -2.1646936e-01  1.6032723e-01] -> [0.9999031] (expected [1.])\n",
      "[-2.0036073e-01 -4.2276412e-01  3.4097263e-01 -4.9918392e-01\n",
      "  5.5367053e-02 -3.3217955e-01  1.1506031e-01 -6.5135640e-01\n",
      " -6.2419713e-01 -1.3019389e-01  2.5178584e-01  8.0729973e-01\n",
      " -3.4757307e-01 -1.7794285e-02 -8.3650482e-01 -1.1052160e+00\n",
      " -1.2681298e+00 -7.6217189e-02 -9.2919922e-01  3.0347872e-01\n",
      "  1.6653386e-01  5.4821551e-01 -4.4612363e-02  8.2641572e-02\n",
      " -2.2805955e-01 -3.5089400e-01 -9.9012636e-02 -2.2602211e-01\n",
      "  9.5983511e-01  1.5700202e-01 -3.3379421e-01  8.6352605e-01\n",
      "  1.0346331e-01 -8.2937098e-01 -2.3505986e-01 -1.3158986e-01\n",
      "  6.7528509e-02 -2.8611344e-01  2.9780212e-01  1.5112281e-02\n",
      " -5.1739669e-01 -5.9660584e-01 -5.5614132e-01 -3.6110035e-01\n",
      " -6.5692395e-01 -3.3153602e-01  1.6934606e-01 -1.3733962e+00\n",
      "  4.2696804e-01 -5.8446426e-02 -1.5859386e-01  4.5564741e-01\n",
      "  3.0138001e-01 -5.3532422e-05 -9.3103155e-02 -2.8091151e-01\n",
      "  6.6610807e-01 -1.6478625e-01  1.0372740e-01 -1.4929719e-01\n",
      "  5.7885951e-01  1.4892203e-01 -4.0744233e-01 -4.7703192e-01\n",
      " -2.0990576e-01 -4.1360185e-01  3.3435848e-01 -5.0648594e-01\n",
      "  7.6826230e-02 -3.2010061e-01  1.1327336e-01 -6.4272153e-01\n",
      " -6.0315108e-01 -1.3814612e-01  2.3633197e-01  7.9972184e-01\n",
      " -3.5905933e-01 -1.8391613e-02 -8.2087392e-01 -1.0997850e+00\n",
      " -1.2592318e+00 -8.2709745e-02 -9.1055459e-01  2.9828206e-01\n",
      "  1.5582740e-01  5.2826995e-01 -6.3615337e-02  1.0109623e-01\n",
      " -2.1993987e-01 -3.7338421e-01 -1.1066056e-01 -2.1736991e-01\n",
      "  9.6558511e-01  1.6347010e-01 -3.0115965e-01  8.5504097e-01\n",
      "  1.1214824e-01 -8.3847594e-01 -2.2515281e-01 -1.2027625e-01\n",
      "  8.2655065e-02 -2.7608454e-01  3.0757025e-01  1.3117909e-02\n",
      " -5.0245160e-01 -6.1197728e-01 -5.3369266e-01 -3.5429192e-01\n",
      " -6.6103733e-01 -3.4537023e-01  1.9738737e-01 -1.3492130e+00\n",
      "  3.9081761e-01 -6.3416332e-02 -1.4752321e-01  4.5019466e-01\n",
      "  2.9044610e-01 -1.6740693e-02 -9.3798071e-02 -2.7083263e-01\n",
      "  6.6869676e-01 -1.9047132e-01  1.0339868e-01 -1.5440178e-01\n",
      "  5.7043326e-01  1.4377782e-01 -3.9143956e-01 -4.8526639e-01] -> [0.03701789] (expected [1.])\n",
      "[ 0.5577651   0.23885083 -0.41018826  0.3950485   0.19665432  0.34854466\n",
      " -0.4868397   1.2359794   0.10312225  0.12028889 -0.6748158  -1.4692122\n",
      "  0.41673195  0.8219568   0.97869396  1.0826399   1.3866781  -0.10867871\n",
      "  1.2514056  -0.4450871  -0.45144176 -0.52750576 -0.4447088   0.22969459\n",
      " -0.02787946  0.12580465 -0.6428883   0.20711963 -0.8494898  -0.27226084\n",
      "  0.7177324  -1.2699504  -0.33992976  0.45544925 -0.01246607  1.0566791\n",
      "  0.46317953  1.0504532  -0.04723065  0.10335399  0.5377756   0.2735263\n",
      "  0.49534944 -0.36880073  0.29453242 -0.74684125 -0.91274756  1.3410244\n",
      " -1.2527622   0.08303609  0.12305391 -0.40944022 -0.28022555  0.8897938\n",
      " -0.22463317  0.23714341 -0.5107519  -1.1425457   0.5986153   0.08866221\n",
      " -0.9383515  -0.18168749  0.5963981   0.27216575  0.5734611   0.23940986\n",
      " -0.41318476  0.38273576  0.23538706  0.37613177 -0.5024712   1.2303358\n",
      "  0.12413894  0.12102243 -0.6830988  -1.469891    0.37311167  0.8350235\n",
      "  0.9929739   1.0929666   1.4067496  -0.1196178   1.2611681  -0.44023794\n",
      " -0.45968956 -0.52973646 -0.4450735   0.25104702 -0.01671967  0.07884717\n",
      " -0.6509918   0.2407097  -0.84709346 -0.2665275   0.72229135 -1.2942194\n",
      " -0.32680267  0.47681984 -0.02656654  1.0784359   0.4682827   1.0632206\n",
      " -0.0598876   0.11958125  0.5314013   0.26201403  0.49663985 -0.3755324\n",
      "  0.32656872 -0.7471734  -0.89573574  1.3456026  -1.2743965   0.05920898\n",
      "  0.11906864 -0.4252469  -0.2951201   0.8778481  -0.1939686   0.24190924\n",
      " -0.5063523  -1.1245781   0.6018837   0.08353853 -0.9274944  -0.18592949\n",
      "  0.5912283   0.28701153] -> [1.] (expected [1.])\n",
      "[-0.18529636 -0.2990043   0.26367855 -0.50085485  0.30243653 -0.09097912\n",
      " -0.0246391  -0.3459273  -0.32361928 -0.16145292  0.00827993  0.5406953\n",
      " -0.40478036  0.02151908 -0.52780867 -0.7953421  -0.88944477 -0.15145089\n",
      " -0.5943463   0.22076118  0.08811323  0.27509192 -0.1585065   0.26347855\n",
      " -0.15038218 -0.5970471  -0.17837572 -0.07611264  0.7114505   0.23088127\n",
      " -0.02825455  0.53365904  0.14313306 -0.64628506 -0.19232132  0.0894134\n",
      "  0.20965523 -0.11358111  0.2615826   0.05484861 -0.36154374 -0.6323009\n",
      " -0.295601   -0.24975374 -0.4837822  -0.36519167  0.31439692 -0.9414295\n",
      " -0.02909148 -0.15133704 -0.08188838  0.39719543  0.10849737 -0.16889201\n",
      "  0.04517372 -0.09177021  0.4930849  -0.24411696  0.11318144 -0.07450674\n",
      "  0.42497665  0.05012843 -0.21864091 -0.38146678  0.65607864 -0.49206448\n",
      "  0.19758117 -0.02339599 -0.14280243 -0.51662815 -0.19147071  0.38114414\n",
      " -0.93761396  0.21199045  0.2173039  -0.20363948  0.50776273  0.77326035\n",
      " -0.30186215  0.0203248  -0.16850904 -0.07279034 -0.33474654  0.07108879\n",
      "  0.03067648  0.56745523  0.09184478 -0.10734314 -0.25925732 -0.10350314\n",
      " -0.15346915 -0.29369247 -0.13990587 -0.11708982 -0.0995129  -0.06928143\n",
      " -0.49387717 -0.12913705 -0.44069868  0.34563595  0.0050983   0.24387893\n",
      "  0.22747827 -0.00347203 -0.53482795  0.16121194 -0.56859934 -0.5687021\n",
      " -0.26771644 -0.8189645  -1.1747519  -0.6587905   0.03949127  0.19616367\n",
      " -0.20966248  0.5307435   0.01720664  0.88570535 -0.19957869 -0.5889058\n",
      "  0.2856397  -0.40792876  0.25769356 -0.01924238 -0.21713439  0.1625121\n",
      " -0.14241812  0.07867517] -> [3.6026485e-08] (expected [0.])\n"
     ]
    }
   ],
   "source": [
    "model_ls.eval()\n",
    "with torch.no_grad():\n",
    "    # Test out inference with 5 samples\n",
    "    for i in range(5):\n",
    "        y_pred = model_ls(X_test[i:i+1])\n",
    "        print(f\"{X_test[i].numpy()} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on instance 428"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_raw = model_ed.encode(data_list[428].x, data_list[428].edge_index)\n",
    "preds = {}\n",
    "for i in range(len(z_raw)):\n",
    "    for j in range(len(z_raw)):\n",
    "        if i != j:\n",
    "            node_i = z_raw[i].tolist()\n",
    "            node_j = z_raw[j].tolist()\n",
    "            target = data_list[428].y[i][j]\n",
    "            preds[i,j] = {\"pred\":model_ls(torch.tensor(node_i+node_j, dtype=torch.float)),\"target\":target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred': tensor([0.0354], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[95,85]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still space for improvement. I'd consider more data, undirected case for encoding and add tw into the structure of the graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
