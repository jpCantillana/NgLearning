{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>First prototype: encode-decode-predict the immediate nearest neighbor</h1>\n",
    "In order to check if it's possible to learn a metric minimizer.\n",
    "Based on https://medium.com/the-modern-scientist/graph-neural-networks-series-part-3-node-embedding-36613cc967d5\n",
    "and https://machinelearningmastery.com/building-a-binary-classification-model-in-pytorch/ and suggestions of Florian Racoussier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1-NN data</h2>\n",
    "Create a simple dataset for testing the prototype. Each node is connected to at most 15 neighbors in order to provide structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from random import randint\n",
    "from sys import float_info\n",
    "from torch_geometric.nn import knn_graph\n",
    "\n",
    "instances = {}\n",
    "for k in range(0, 1000):\n",
    "    nodes = {}\n",
    "    for i in range(0, 50):\n",
    "        lat_i = randint(0, 100)\n",
    "        lon_i = randint(0, 100)\n",
    "        node_i = (lat_i, lon_i)\n",
    "        lat_j = randint(0, 100)\n",
    "        lon_j = randint(0, 100)\n",
    "        node_j = (lat_j, lon_j)\n",
    "        nodes[i + 1] = node_i\n",
    "        nodes[i + 51] = node_j\n",
    "\n",
    "    dist = {}\n",
    "    pairs = {}\n",
    "    for i in range(1, 101):\n",
    "        for j in range(1, 101):\n",
    "            if i != j:\n",
    "                dist[i,j] = sqrt( (nodes[i][0] - nodes[j][0])**2 + (nodes[i][1] - nodes[j][1])**2 )\n",
    "            else:\n",
    "                dist[i,j] = float_info.max\n",
    "    for i in range(1, 101):\n",
    "        for j in range(1, 101):\n",
    "            if i not in pairs:\n",
    "                pairs[i] = j\n",
    "            if i != j:\n",
    "                if dist[i,j] < dist[i,pairs[i]]:\n",
    "                    pairs[i] = j\n",
    "\n",
    "    nodes[0] = (0,0)\n",
    "    for i in range(1,101):\n",
    "        dist[0,i] = sqrt( (nodes[0][0] - nodes[i][0])**2 + (nodes[0][1] - nodes[i][1])**2 )\n",
    "        dist[i,0] = dist[0,i]\n",
    "    y = [[0 for _ in range(101)] for _ in range(101)]\n",
    "    for i in range(101):\n",
    "        if i > 0:\n",
    "            y[i][pairs[i]] = 1\n",
    "                \n",
    "    instances[k] = {\"nodes\": nodes, \"dist\": dist, \"y\": y}\n",
    "data_list = []\n",
    "for instance_name in instances:\n",
    "    y = torch.tensor(instances[instance_name][\"y\"], dtype=torch.float)\n",
    "    x = torch.tensor([instances[instance_name][\"nodes\"][i] for i in range(0, 101)], dtype=torch.float)\n",
    "    pos = []\n",
    "    for i in range(101):\n",
    "        pos.append(instances[instance_name][\"nodes\"][i])\n",
    "    pos = torch.tensor(pos, dtype=torch.double)\n",
    "    data_list.append(Data(x=x, y=y, edge_index = knn_graph(x, 15), pos=pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1-NN Batching and dividing data into train-test-validation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "def add_edge_labels(graph):\n",
    "    transform = T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False)\n",
    "    return transform(graph)\n",
    "\n",
    "labeled_graphs = [add_edge_labels(graph) for graph in data_list]\n",
    "\n",
    "train_size = [g[0] for g in labeled_graphs]\n",
    "val_size = [g[1] for g in labeled_graphs]\n",
    "test_size = [g[2] for g in labeled_graphs]\n",
    "\n",
    "train_loader = DataLoader(train_size, batch_size=20, shuffle=True)\n",
    "val_loader = DataLoader(val_size, batch_size=20, shuffle=False)\n",
    "test_loader = DataLoader(test_size, batch_size=20, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Encoder-Decoder architecture definition</h2>\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "model = Net(data_list[0].num_features, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Encoder-Decoder train-test-validate routine</h2>\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        z = model.encode(batch.x, batch.edge_index)\n",
    "\n",
    "        # We perform a new round of negative sampling for every training epoch:\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=batch.edge_index, num_nodes=batch.num_nodes,\n",
    "            num_neg_samples=batch.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "        # Concat positive and negative edge indices.\n",
    "        edge_label_index = torch.cat(\n",
    "            [batch.edge_label_index, neg_edge_index],\n",
    "            dim=-1,\n",
    "        )\n",
    "        # Label for positive edges: 1, for negative edges: 0.\n",
    "        edge_label = torch.cat([\n",
    "            batch.edge_label,\n",
    "            batch.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "        ], dim=0)\n",
    "\n",
    "        # Note: The model is trained in a supervised manner using the given\n",
    "        # `edge_label_index` and `edge_label` targets.\n",
    "        out = model.decode(z, edge_label_index).view(-1)\n",
    "        loss = criterion(out, edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    all_out = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in loader:\n",
    "        z = model.encode(batch.x, batch.edge_index)\n",
    "        out = model.decode(z, batch.edge_label_index).view(-1).sigmoid()\n",
    "        all_out.append(out.cpu().numpy())\n",
    "        all_labels.append(batch.edge_label.cpu().numpy())\n",
    "\n",
    "    all_out = np.concatenate(all_out)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return roc_auc_score(all_labels, all_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 122.2200, Val: 0.8519, Test: 0.8531\n",
      "Epoch: 002, Loss: 1.4640, Val: 0.8638, Test: 0.8646\n",
      "Epoch: 003, Loss: 0.7274, Val: 0.8689, Test: 0.8694\n",
      "Epoch: 004, Loss: 0.5976, Val: 0.8711, Test: 0.8717\n",
      "Epoch: 005, Loss: 0.5664, Val: 0.8718, Test: 0.8725\n",
      "Epoch: 006, Loss: 0.5568, Val: 0.8731, Test: 0.8737\n",
      "Epoch: 007, Loss: 0.5530, Val: 0.8743, Test: 0.8747\n",
      "Epoch: 008, Loss: 0.5514, Val: 0.8757, Test: 0.8763\n",
      "Epoch: 009, Loss: 0.5502, Val: 0.8769, Test: 0.8774\n",
      "Epoch: 010, Loss: 0.5489, Val: 0.8782, Test: 0.8787\n",
      "Epoch: 011, Loss: 0.5481, Val: 0.8803, Test: 0.8808\n",
      "Epoch: 012, Loss: 0.5472, Val: 0.8815, Test: 0.8821\n",
      "Epoch: 013, Loss: 0.5464, Val: 0.8841, Test: 0.8847\n",
      "Epoch: 014, Loss: 0.5448, Val: 0.8862, Test: 0.8868\n",
      "Epoch: 015, Loss: 0.5435, Val: 0.8886, Test: 0.8890\n",
      "Epoch: 016, Loss: 0.5421, Val: 0.8917, Test: 0.8921\n",
      "Epoch: 017, Loss: 0.5410, Val: 0.8945, Test: 0.8950\n",
      "Epoch: 018, Loss: 0.5394, Val: 0.8979, Test: 0.8983\n",
      "Epoch: 019, Loss: 0.5358, Val: 0.9014, Test: 0.9020\n",
      "Epoch: 020, Loss: 0.5338, Val: 0.9051, Test: 0.9057\n",
      "Epoch: 021, Loss: 0.5310, Val: 0.9096, Test: 0.9100\n",
      "Epoch: 022, Loss: 0.5281, Val: 0.9136, Test: 0.9140\n",
      "Epoch: 023, Loss: 0.5245, Val: 0.9169, Test: 0.9173\n",
      "Epoch: 024, Loss: 0.5218, Val: 0.9216, Test: 0.9218\n",
      "Epoch: 025, Loss: 0.5199, Val: 0.9234, Test: 0.9239\n",
      "Epoch: 026, Loss: 0.5170, Val: 0.9275, Test: 0.9276\n",
      "Epoch: 027, Loss: 0.5146, Val: 0.9280, Test: 0.9284\n",
      "Epoch: 028, Loss: 0.5120, Val: 0.9302, Test: 0.9305\n",
      "Epoch: 029, Loss: 0.5118, Val: 0.9313, Test: 0.9317\n",
      "Epoch: 030, Loss: 0.5106, Val: 0.9322, Test: 0.9326\n",
      "Epoch: 031, Loss: 0.5097, Val: 0.9327, Test: 0.9332\n",
      "Epoch: 032, Loss: 0.5092, Val: 0.9331, Test: 0.9336\n",
      "Epoch: 033, Loss: 0.5089, Val: 0.9336, Test: 0.9341\n",
      "Epoch: 034, Loss: 0.5089, Val: 0.9321, Test: 0.9328\n",
      "Epoch: 035, Loss: 0.5095, Val: 0.9337, Test: 0.9344\n",
      "Epoch: 036, Loss: 0.5071, Val: 0.9343, Test: 0.9350\n",
      "Epoch: 037, Loss: 0.5074, Val: 0.9337, Test: 0.9344\n",
      "Epoch: 038, Loss: 0.5069, Val: 0.9335, Test: 0.9343\n",
      "Epoch: 039, Loss: 0.5067, Val: 0.9350, Test: 0.9357\n",
      "Epoch: 040, Loss: 0.5065, Val: 0.9350, Test: 0.9355\n",
      "Epoch: 041, Loss: 0.5075, Val: 0.9353, Test: 0.9360\n",
      "Epoch: 042, Loss: 0.5064, Val: 0.9355, Test: 0.9361\n",
      "Epoch: 043, Loss: 0.5061, Val: 0.9352, Test: 0.9359\n",
      "Epoch: 044, Loss: 0.5054, Val: 0.9356, Test: 0.9363\n",
      "Epoch: 045, Loss: 0.5063, Val: 0.9359, Test: 0.9363\n",
      "Epoch: 046, Loss: 0.5058, Val: 0.9362, Test: 0.9367\n",
      "Epoch: 047, Loss: 0.5051, Val: 0.9350, Test: 0.9357\n",
      "Epoch: 048, Loss: 0.5059, Val: 0.9341, Test: 0.9350\n",
      "Epoch: 049, Loss: 0.5052, Val: 0.9352, Test: 0.9360\n",
      "Epoch: 050, Loss: 0.5043, Val: 0.9366, Test: 0.9371\n",
      "Epoch: 051, Loss: 0.5042, Val: 0.9349, Test: 0.9357\n",
      "Epoch: 052, Loss: 0.5053, Val: 0.9367, Test: 0.9374\n",
      "Epoch: 053, Loss: 0.5052, Val: 0.9370, Test: 0.9376\n",
      "Epoch: 054, Loss: 0.5034, Val: 0.9374, Test: 0.9381\n",
      "Epoch: 055, Loss: 0.5033, Val: 0.9376, Test: 0.9381\n",
      "Epoch: 056, Loss: 0.5031, Val: 0.9375, Test: 0.9381\n",
      "Epoch: 057, Loss: 0.5034, Val: 0.9363, Test: 0.9371\n",
      "Epoch: 058, Loss: 0.5030, Val: 0.9378, Test: 0.9383\n",
      "Epoch: 059, Loss: 0.5033, Val: 0.9377, Test: 0.9383\n",
      "Epoch: 060, Loss: 0.5035, Val: 0.9379, Test: 0.9384\n",
      "Epoch: 061, Loss: 0.5029, Val: 0.9369, Test: 0.9377\n",
      "Epoch: 062, Loss: 0.5021, Val: 0.9383, Test: 0.9390\n",
      "Epoch: 063, Loss: 0.5017, Val: 0.9371, Test: 0.9377\n",
      "Epoch: 064, Loss: 0.5021, Val: 0.9386, Test: 0.9391\n",
      "Epoch: 065, Loss: 0.5031, Val: 0.9388, Test: 0.9394\n",
      "Epoch: 066, Loss: 0.5016, Val: 0.9389, Test: 0.9391\n",
      "Epoch: 067, Loss: 0.5007, Val: 0.9386, Test: 0.9390\n",
      "Epoch: 068, Loss: 0.5019, Val: 0.9379, Test: 0.9386\n",
      "Epoch: 069, Loss: 0.5000, Val: 0.9390, Test: 0.9392\n",
      "Epoch: 070, Loss: 0.5012, Val: 0.9397, Test: 0.9402\n",
      "Epoch: 071, Loss: 0.5029, Val: 0.9383, Test: 0.9388\n",
      "Epoch: 072, Loss: 0.5006, Val: 0.9405, Test: 0.9408\n",
      "Epoch: 073, Loss: 0.5008, Val: 0.9402, Test: 0.9406\n",
      "Epoch: 074, Loss: 0.5018, Val: 0.9391, Test: 0.9395\n",
      "Epoch: 075, Loss: 0.5004, Val: 0.9405, Test: 0.9407\n",
      "Epoch: 076, Loss: 0.5009, Val: 0.9399, Test: 0.9403\n",
      "Epoch: 077, Loss: 0.5005, Val: 0.9402, Test: 0.9404\n",
      "Epoch: 078, Loss: 0.5002, Val: 0.9407, Test: 0.9410\n",
      "Epoch: 079, Loss: 0.4995, Val: 0.9409, Test: 0.9413\n",
      "Epoch: 080, Loss: 0.5002, Val: 0.9405, Test: 0.9405\n",
      "Epoch: 081, Loss: 0.5011, Val: 0.9414, Test: 0.9417\n",
      "Epoch: 082, Loss: 0.5000, Val: 0.9411, Test: 0.9415\n",
      "Epoch: 083, Loss: 0.5014, Val: 0.9402, Test: 0.9404\n",
      "Epoch: 084, Loss: 0.4979, Val: 0.9414, Test: 0.9415\n",
      "Epoch: 085, Loss: 0.5026, Val: 0.9417, Test: 0.9420\n",
      "Epoch: 086, Loss: 0.4987, Val: 0.9422, Test: 0.9422\n",
      "Epoch: 087, Loss: 0.5015, Val: 0.9414, Test: 0.9416\n",
      "Epoch: 088, Loss: 0.5001, Val: 0.9428, Test: 0.9428\n",
      "Epoch: 089, Loss: 0.5008, Val: 0.9421, Test: 0.9422\n",
      "Epoch: 090, Loss: 0.4997, Val: 0.9421, Test: 0.9420\n",
      "Epoch: 091, Loss: 0.4993, Val: 0.9431, Test: 0.9432\n",
      "Epoch: 092, Loss: 0.4991, Val: 0.9377, Test: 0.9374\n",
      "Epoch: 093, Loss: 0.5023, Val: 0.9426, Test: 0.9425\n",
      "Epoch: 094, Loss: 0.4983, Val: 0.9434, Test: 0.9435\n",
      "Epoch: 095, Loss: 0.4980, Val: 0.9443, Test: 0.9442\n",
      "Epoch: 096, Loss: 0.4999, Val: 0.9433, Test: 0.9434\n",
      "Epoch: 097, Loss: 0.5011, Val: 0.9437, Test: 0.9436\n",
      "Epoch: 098, Loss: 0.4969, Val: 0.9427, Test: 0.9428\n",
      "Epoch: 099, Loss: 0.4967, Val: 0.9421, Test: 0.9419\n",
      "Epoch: 100, Loss: 0.4985, Val: 0.9385, Test: 0.9389\n",
      "Final Test: 0.9442\n"
     ]
    }
   ],
   "source": [
    "# Train/Test Loop\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train(train_loader)\n",
    "    val_auc = test(val_loader)\n",
    "    test_auc = test(test_loader)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1-NN additional statistics</h2>\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict = {}\n",
    "cnt = 0\n",
    "for graph in data_list:\n",
    "    z_raw = model.encode(graph.x, graph.edge_index)\n",
    "    final_edge_index = model.decode_all(z_raw)\n",
    "    fei = final_edge_index.tolist()\n",
    "    edges_pred = {k:[] for k in range(101)}\n",
    "    edges_pred_inv = {k:[] for k in range(101)}\n",
    "    for i in range(len(fei[0])):\n",
    "        edges_pred[fei[0][i]].append(fei[1][i])\n",
    "        edges_pred_inv[fei[1][i]].append(fei[0][i])\n",
    "    ts0 = graph.edge_index.tolist()\n",
    "    edges = {k:[] for k in range(101)}\n",
    "    edges_inv = {k:[] for k in range(101)}\n",
    "    for i in range(len(ts0[0])):\n",
    "        edges[ts0[0][i]].append(ts0[1][i])\n",
    "        edges_inv[ts0[1][i]].append(ts0[0][i])\n",
    "    originals = {}\n",
    "    predictions = {}\n",
    "    for i in range(101):\n",
    "        originals[i] = set(edges[i] + edges_inv[i])\n",
    "        predictions[i] = set(edges_pred[i] + edges_pred_inv[i])\n",
    "    graph_dict[cnt] = {\"real\": originals, \"preds\": predictions}\n",
    "    cnt += 1\n",
    "\n",
    "confusion_dict = {}\n",
    "true_positives = []\n",
    "false_positives = []\n",
    "true_negatives = []\n",
    "false_negatives = []\n",
    "for key in graph_dict:\n",
    "    real = graph_dict[key][\"real\"]\n",
    "    pred = graph_dict[key][\"preds\"]\n",
    "    node_matrix = {}\n",
    "    for i in range(101):\n",
    "        tp = len(pred[i].intersection(real[i]))\n",
    "        fp = len(pred[i] - real[i])\n",
    "        real_neg = set([j for j in range(101)]) - {i} - real[i]\n",
    "        pred_neg = set([j for j in range(101)]) - {i} - pred[i]\n",
    "        tn = len(pred_neg.intersection(real_neg))\n",
    "        fn = len(pred_neg - real_neg)\n",
    "        total = tp + fp + tn + fn\n",
    "        node_matrix[i] = {\"tp\": tp, \"fp\": fp, \"tn\": tn, \"fn\": fn, \"total\": total}\n",
    "        true_positives.append(tp/total)\n",
    "        false_positives.append(fp/total)\n",
    "        true_negatives.append(tn/total)\n",
    "        false_negatives.append(fn/total)\n",
    "    confusion_dict[key] = node_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positives mean=0.17, stdev=0.03\n",
      "false positives mean=0.28, stdev=0.10\n",
      "true negatives mean=0.54, stdev=0.10\n",
      "false negatives mean=0.00, stdev=0.00\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, stdev\n",
    "print(\"true positives mean={0:.2f}, stdev={1:.2f}\".format(mean(true_positives), stdev(true_positives)))\n",
    "print(\"false positives mean={0:.2f}, stdev={1:.2f}\".format(mean(false_positives), stdev(false_positives)))\n",
    "print(\"true negatives mean={0:.2f}, stdev={1:.2f}\".format(mean(true_negatives), stdev(true_negatives)))\n",
    "print(\"false negatives mean={0:.2f}, stdev={1:.2f}\".format(mean(false_negatives), stdev(false_negatives)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some notion of closeness. It over-links, but rarely avoids a close node. Also, a middle stage as this is not always a requirement. We want to encode the data in embeddings that can be used for the next stage. The quality will be measured then over the ability to learn closeness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1-NN data preparation for second stage</h2>\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "data_raw_dict = {\n",
    "    \"positives\": [],\n",
    "    \"negatives\": []\n",
    "}\n",
    "cnt = 0\n",
    "pos_cnt = 0\n",
    "neg_cnt = 0\n",
    "for graph in data_list:\n",
    "    neg_cnt = 0\n",
    "    ng_matrix = graph.y.tolist()\n",
    "    encoding_matrix = model.encode(graph.x, graph.edge_index).tolist()\n",
    "    for i in range(101):\n",
    "        for j in sample(range(101), 30):\n",
    "            if i == j:\n",
    "                # if pos_cnt < 80000:\n",
    "                #     data_raw_dict[\"positives\"].append(encoding_matrix[i]+encoding_matrix[i])\n",
    "                #     pos_cnt += 1\n",
    "                continue\n",
    "            else:\n",
    "                if ng_matrix[i][j] > 0.5:\n",
    "                    if pos_cnt < 80000:\n",
    "                        data_raw_dict[\"positives\"].append(encoding_matrix[i]+encoding_matrix[j]+[1])\n",
    "                        pos_cnt += 1\n",
    "                else:\n",
    "                    if pos_cnt < 80000 and neg_cnt < 1010:\n",
    "                        data_raw_dict[\"negatives\"].append(encoding_matrix[i]+encoding_matrix[j]+[0])\n",
    "                        neg_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tensor = data_raw_dict[\"positives\"][:5000] + data_raw_dict[\"negatives\"][:5000]\n",
    "main_tensor = torch.tensor(pre_tensor, dtype=torch.float32)\n",
    "\n",
    "main_tensor = main_tensor[torch.randperm(main_tensor.size()[0])]\n",
    "labels = main_tensor[:,-1:]\n",
    "embeddings = main_tensor[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1-NN Wide or Deep network</h2>\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Wide(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(128, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "class Deep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(128, 128)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(128, 128)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    " \n",
    "def model_train(model, X_train, y_train, X_val, y_val):\n",
    "    # loss function and optimizer\n",
    "    loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    " \n",
    "    n_epochs = 20   # number of epochs to run\n",
    "    batch_size = 10  # size of each batch\n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
    " \n",
    "    # Hold the best model\n",
    "    best_acc = - np.inf   # init to negative infinity\n",
    "    best_weights = None\n",
    " \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "                # take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "                # forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "                # print progress\n",
    "                acc = (y_pred.round() == y_batch).float().mean()\n",
    "                bar.set_postfix(\n",
    "                    loss=float(loss),\n",
    "                    acc=float(acc)\n",
    "                )\n",
    "        # evaluate accuracy at end of each epoch\n",
    "        model.eval()\n",
    "        y_pred = model(X_val)\n",
    "        acc = (y_pred.round() == y_val).float().mean()\n",
    "        acc = float(acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "    # restore model and return best accuracy\n",
    "    model.load_state_dict(best_weights)\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (wide): 0.93\n",
      "Accuracy (wide): 0.94\n",
      "Accuracy (wide): 0.92\n",
      "Accuracy (wide): 0.94\n",
      "Accuracy (wide): 0.93\n",
      "Accuracy (deep): 0.95\n",
      "Accuracy (deep): 0.94\n",
      "Accuracy (deep): 0.95\n",
      "Accuracy (deep): 0.95\n",
      "Accuracy (deep): 0.95\n",
      "Wide: 93.19% (+/- 0.53%)\n",
      "Deep: 94.86% (+/- 0.42%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "# train-test split: Hold out the test set for final model evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, train_size=0.7, shuffle=True)\n",
    " \n",
    "# define 5-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "cv_scores_wide = []\n",
    "for train, test in kfold.split(X_train, y_train):\n",
    "    # create model, train, and get accuracy\n",
    "    model = Wide()\n",
    "    acc = model_train(model, X_train[train], y_train[train], X_train[test], y_train[test])\n",
    "    print(\"Accuracy (wide): %.2f\" % acc)\n",
    "    cv_scores_wide.append(acc)\n",
    "cv_scores_deep = []\n",
    "for train, test in kfold.split(X_train, y_train):\n",
    "    # create model, train, and get accuracy\n",
    "    model = Deep()\n",
    "    acc = model_train(model, X_train[train], y_train[train], X_train[test], y_train[test])\n",
    "    print(\"Accuracy (deep): %.2f\" % acc)\n",
    "    cv_scores_deep.append(acc)\n",
    " \n",
    "# evaluate the model\n",
    "wide_acc = np.mean(cv_scores_wide)\n",
    "wide_std = np.std(cv_scores_wide)\n",
    "deep_acc = np.mean(cv_scores_deep)\n",
    "deep_std = np.std(cv_scores_deep)\n",
    "print(\"Wide: %.2f%% (+/- %.2f%%)\" % (wide_acc*100, wide_std*100))\n",
    "print(\"Deep: %.2f%% (+/- %.2f%%)\" % (deep_acc*100, deep_std*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrain a deep model\n",
      "Final model accuracy: 95.13%\n"
     ]
    }
   ],
   "source": [
    "# rebuild model with full set of training data\n",
    "if wide_acc > deep_acc:\n",
    "    print(\"Retrain a wide model\")\n",
    "    model = Wide()\n",
    "else:\n",
    "    print(\"Retrain a deep model\")\n",
    "    model = Deep()\n",
    "acc = model_train(model, X_train, y_train, X_test, y_test)\n",
    "print(f\"Final model accuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.1717339e-01 -5.7865076e-02 -1.4151484e-02  1.5557876e-02\n",
      " -4.2768332e-01 -4.4370964e-01  3.3552490e-02  4.1792460e-02\n",
      " -4.4831714e-01 -7.0630580e-02  2.1677202e-01  2.2268556e-01\n",
      " -6.3116723e-01  8.4077612e-02  5.1827902e-01  3.1405866e-01\n",
      " -2.0323183e-01  8.0681220e-02  1.3163482e-01 -1.4057782e-01\n",
      " -1.0440167e-02  9.3202174e-02  1.8332930e-01  2.2968042e-01\n",
      "  1.0498860e-01 -2.6336920e-01  5.9151962e-02  5.4599382e-02\n",
      " -5.2246571e-02 -2.9523142e-02  2.7236146e-01 -5.9649184e-02\n",
      " -2.9510462e-01 -2.3909754e-01 -2.0827483e-01 -4.0251181e-01\n",
      " -1.6330114e-01 -2.0440927e-01 -7.0687167e-02  2.9918218e-01\n",
      "  9.8987930e-03  9.5853359e-02 -2.3753516e-01 -1.8214691e-01\n",
      " -4.3035455e-02 -1.5556519e-02 -1.2626988e-01  1.3818063e-01\n",
      " -2.8056109e-01  5.3613499e-02 -2.5029457e-01  3.6895238e-02\n",
      " -1.3619283e-01  5.3167921e-01 -3.1269984e-03 -3.6275232e-01\n",
      "  2.5250453e-01  1.8157607e-01 -2.1313316e-01 -8.1577316e-02\n",
      "  1.5478041e-02  2.6419458e-01  5.4498559e-01  2.4573167e-01\n",
      "  4.3307829e-01 -5.6470633e-02 -1.1331800e-02  2.2720667e-02\n",
      " -4.2329457e-01 -4.5480761e-01  4.4342883e-02  4.5427322e-02\n",
      " -4.4606289e-01 -8.0577120e-02  2.1183130e-01  2.3732670e-01\n",
      " -6.3463920e-01  9.8108679e-02  5.1491594e-01  3.1170848e-01\n",
      " -2.1752605e-01  8.0698565e-02  1.3647492e-01 -1.4649908e-01\n",
      " -1.1975139e-02  8.7222464e-02  1.8109830e-01  2.2941652e-01\n",
      "  9.2107445e-02 -2.6699829e-01  5.0050810e-02  4.7848269e-02\n",
      " -3.7784070e-02 -1.9218609e-02  2.7938482e-01 -5.8992282e-02\n",
      " -2.8603178e-01 -2.3820066e-01 -1.9802438e-01 -4.0632907e-01\n",
      " -1.5602684e-01 -2.1411142e-01 -7.1299389e-02  3.2000360e-01\n",
      "  1.4883975e-02  9.6994124e-02 -2.4576062e-01 -1.7677087e-01\n",
      " -3.5438828e-02 -1.8663231e-02 -1.2395805e-01  1.4290737e-01\n",
      " -2.8109637e-01  5.1804379e-02 -2.5030202e-01  2.8354084e-02\n",
      " -1.3504303e-01  5.2837777e-01  5.8637280e-04 -3.6035749e-01\n",
      "  2.5716203e-01  1.8610978e-01 -2.0425469e-01 -7.5680345e-02\n",
      "  1.3318401e-02  2.5793216e-01  5.3815591e-01  2.5184375e-01] -> [0.8791382] (expected [1.])\n",
      "[-0.330864    0.06145173 -0.19024104 -0.0187828   0.02340415 -0.02353234\n",
      " -0.09699653  0.01063286  0.07676873  0.10265465 -0.20202741 -0.15060855\n",
      "  0.16400571 -0.16511227 -0.13620533  0.17994827  0.12047814 -0.13892125\n",
      " -0.04614743  0.10489418  0.10139035  0.10708459  0.02570695 -0.2025975\n",
      "  0.27189144 -0.02998438  0.10394385  0.05386116 -0.0839919  -0.16725492\n",
      " -0.08409136 -0.05255683  0.03403759  0.01767641 -0.01208991  0.09068033\n",
      "  0.07768191  0.12292603  0.10131624 -0.34487686 -0.05155137  0.00810724\n",
      " -0.01497776 -0.16132963 -0.01401632  0.15828408  0.09045064  0.03769962\n",
      "  0.14451867 -0.05035711  0.15338582  0.08894943  0.18451214 -0.09988489\n",
      "  0.1258642  -0.01964192 -0.12790039 -0.1538146  -0.08576083 -0.08980085\n",
      " -0.06369558  0.13308588  0.15275443 -0.06599046  0.32791537 -0.05607147\n",
      " -0.01604865 -0.00805374 -0.3239973  -0.42272744  0.09292679  0.04010247\n",
      " -0.43265417 -0.13727242  0.17752124  0.2289034  -0.580992    0.06468205\n",
      "  0.36844456  0.31403068 -0.14854711  0.19902556  0.08086179 -0.1680052\n",
      "  0.00577518  0.02677535  0.1272771   0.15678254  0.05438989 -0.19849831\n",
      "  0.09766823 -0.03170805 -0.04216717  0.01377597  0.23065558  0.00603591\n",
      " -0.25511116 -0.19574961 -0.10372823 -0.37275997 -0.10334317 -0.1962428\n",
      "  0.01648377  0.26799214 -0.0055501   0.06452463 -0.14870533 -0.14268668\n",
      "  0.05730914  0.03938692 -0.08882365  0.06507913 -0.26012477  0.08325417\n",
      " -0.21879144 -0.02418476 -0.19352373  0.45169723 -0.04488386 -0.27481258\n",
      "  0.14917243  0.18355168 -0.14959304 -0.11858585 -0.02265492  0.17981535\n",
      "  0.5236769   0.20937355] -> [9.802447e-09] (expected [0.])\n",
      "[ 0.22271447 -0.15952486  0.18382713 -0.0736852  -0.1735313  -0.2958808\n",
      "  0.1651783  -0.12247935 -0.1891824  -0.0376753  -0.01021766  0.17950147\n",
      " -0.16975714  0.05774587  0.1254158   0.14762676 -0.12215778  0.63846254\n",
      " -0.07816781 -0.16154514 -0.07402876 -0.26636425 -0.13094182  0.13025358\n",
      " -0.24119826 -0.06175733  0.1358243  -0.1056514  -0.02328973  0.08909103\n",
      " -0.04106475  0.0768939  -0.27535206 -0.2432819   0.17938295 -0.48485598\n",
      " -0.21559832 -0.19126189  0.14253952  0.26128456 -0.05623893  0.05568039\n",
      "  0.02272103  0.03955377  0.23339663 -0.10984882  0.06858939  0.02321001\n",
      " -0.0886617   0.37852514 -0.4213081  -0.00954623 -0.43551075  0.03039259\n",
      " -0.12275901  0.00415562 -0.09740531  0.0846632  -0.02359264 -0.10913993\n",
      "  0.04740994 -0.1211182   0.21601045 -0.07385829 -0.32684976  0.22578472\n",
      " -0.18895131 -0.007252   -0.09391057 -0.12486035  0.06219388 -0.28197473\n",
      "  0.01795186  0.4119055  -0.104416   -0.26223752  0.21181701 -0.01028562\n",
      " -0.0881796   0.13924631  0.12256099 -0.01716547 -0.02672764  0.29523718\n",
      "  0.06995692  0.08960645  0.01676747 -0.22883645  0.30053788  0.06775854\n",
      "  0.2258673   0.07662316 -0.17618199 -0.37895942 -0.03361698 -0.16629015\n",
      " -0.07676308 -0.48291153  0.01749101 -0.43210956 -0.13394362  0.07412936\n",
      "  0.18788399 -0.2770224  -0.09938193  0.07316856  0.10176948 -0.49174565\n",
      " -0.20396474 -0.07086708  0.00149342  0.16793291  0.11178198 -0.08262042\n",
      " -0.22674532  0.3133783   0.3294002  -0.1406103   0.1196633  -0.16833246\n",
      " -0.23056692 -0.27149156 -0.15944612  0.02952655  0.125426    0.14204529\n",
      "  0.28754896 -0.09822766] -> [5.1421516e-06] (expected [0.])\n",
      "[-0.0837351  -0.0448166  -0.02411401 -0.04934224 -0.17384487  0.0436961\n",
      " -0.16253164  0.00429083 -0.23038313  0.04281615  0.10458341 -0.14767751\n",
      " -0.16443859 -0.25273436  0.2536457   0.28187922  0.18969768 -0.07971613\n",
      " -0.00124664  0.0167587   0.12776399  0.10312244  0.17823721  0.26184237\n",
      "  0.33748722 -0.12687102  0.15838829  0.15304278 -0.34170038 -0.28474018\n",
      "  0.14931548 -0.0870682  -0.40944332 -0.18892667 -0.45725405  0.03313363\n",
      " -0.05818337  0.18113264 -0.11131335 -0.33085167  0.00981047  0.03956431\n",
      " -0.05151433 -0.0908279  -0.1923779   0.13452251 -0.22167386  0.07144933\n",
      " -0.078137    0.03095455 -0.01341315  0.17364275  0.12554172  0.33699083\n",
      "  0.09001357 -0.22338557 -0.01216271  0.06234008 -0.27405703 -0.10425153\n",
      "  0.01439177  0.30483744  0.48207515 -0.03447701  0.14754231 -0.1325877\n",
      " -0.01699865 -0.05028243 -0.28408775 -0.13301158 -0.1358625   0.08784046\n",
      " -0.34732074 -0.06451582  0.14791946  0.02272195 -0.34264398 -0.17731088\n",
      "  0.36250448  0.30614182  0.03638357 -0.02239242  0.0565416  -0.06525221\n",
      "  0.06708034  0.10107493  0.1651579   0.23970264  0.22873928 -0.21504852\n",
      "  0.07301868  0.10800594 -0.21460019 -0.1220111   0.21358922 -0.04224831\n",
      " -0.36934036 -0.09301305 -0.40087014 -0.03471315 -0.08040024  0.02077419\n",
      " -0.12134337 -0.07245152  0.0294169   0.04603503 -0.15192299 -0.05395331\n",
      " -0.0933306   0.14605504 -0.19364022  0.02699062 -0.18397355  0.04634377\n",
      " -0.03619428  0.0591231  -0.0431584   0.46648848  0.03742462 -0.2539232\n",
      "  0.12606579  0.12370135 -0.2251651  -0.11487918 -0.01822015  0.28772992\n",
      "  0.49116087  0.09274285] -> [0.41050267] (expected [1.])\n",
      "[-7.63157979e-02 -2.40801387e-02  6.28724545e-02 -3.44757959e-02\n",
      "  1.64169565e-01  1.27958730e-02  9.96308103e-02  1.40649468e-01\n",
      "  8.38868394e-02 -2.85503089e-01 -1.41207337e-01  9.28785130e-02\n",
      " -4.95303869e-02 -7.77546614e-02 -1.27960384e-01  2.22919405e-01\n",
      "  8.11097696e-02  9.67583209e-02 -4.66409475e-02 -2.14993894e-01\n",
      "  3.77927944e-02 -6.59181550e-02 -6.07011244e-02  4.76374179e-02\n",
      " -6.41053170e-02 -2.46628150e-02 -8.66867602e-03 -4.12586555e-02\n",
      "  3.04690301e-02  1.43760383e-01  7.06293434e-03  1.76219314e-01\n",
      "  5.16500324e-02  2.35103041e-01  1.01228997e-01  2.44096160e-01\n",
      "  1.96702570e-01  9.34800804e-02 -5.77658042e-03 -1.25255555e-01\n",
      "  7.33259320e-03 -7.32862055e-02 -3.66118737e-03  1.56098679e-01\n",
      "  1.33527920e-01  1.36991665e-01  1.06185719e-01 -1.03929698e-01\n",
      "  3.40567641e-02  2.78462470e-02  2.24395454e-01 -2.43084431e-01\n",
      " -5.09101376e-02 -1.23238772e-01  3.37802768e-02  1.80506930e-01\n",
      " -7.67727867e-02  3.44040580e-02  2.73749754e-02 -1.14939764e-01\n",
      " -1.28552318e-01 -7.54444301e-02 -2.40921378e-02 -1.61090046e-02\n",
      "  1.32202223e-01 -3.02148402e-01  2.04013363e-01 -3.61703224e-02\n",
      "  9.46377963e-03  1.68025225e-01 -1.47889435e-01 -1.23928487e-01\n",
      "  1.78889155e-01 -4.99179959e-03  3.28678973e-02 -6.48027211e-02\n",
      "  1.92209333e-01 -2.33848020e-03 -2.74872854e-02 -2.61941344e-01\n",
      " -1.05844392e-02  5.35906404e-02  8.15910175e-02 -1.90383457e-02\n",
      " -1.10120445e-01 -1.31365150e-01 -1.44217923e-01  1.37729719e-01\n",
      " -3.24740946e-01  9.45773721e-03 -1.66778952e-01  1.16029754e-02\n",
      "  2.19870359e-02  1.53416544e-01  4.92712855e-03  4.93074730e-02\n",
      " -4.22657281e-02  1.38083979e-01 -1.36388168e-01  2.16364086e-01\n",
      " -1.41709000e-02 -2.08452344e-04 -1.71694905e-01  2.16244325e-01\n",
      "  2.47190781e-02 -5.79462349e-02  1.28445730e-01  2.93491840e-01\n",
      "  3.04497294e-02 -9.92563367e-02 -1.09400377e-01 -7.20842406e-02\n",
      " -4.58106101e-02  6.93200529e-02 -8.91467780e-02 -1.01222351e-01\n",
      " -8.20192769e-02 -2.83215642e-01 -6.67031705e-02  2.84511328e-01\n",
      "  5.32905385e-02 -2.10873820e-02  1.52256384e-01  1.98680595e-01\n",
      "  2.67397948e-02 -2.40933120e-01 -3.86802971e-01 -1.57303333e-01] -> [0.00160045] (expected [0.])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Test out inference with 5 samples\n",
    "    for i in range(5):\n",
    "        y_pred = model(X_test[i:i+1])\n",
    "        print(f\"{X_test[i].numpy()} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one above is an example for visual validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>NgSets prototype, undirected</h1>\n",
    "Once validated the simple prototype, let's continue to the actual problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load the data</h2>\n",
    "Here an \"export\" folder with .txt instance files used on the CTWVRP project, and a .csv file with the neighborhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_files_list = [\"./export/\"+f for f in os.listdir(\"./export\") ]\n",
    "instance_dict = {}\n",
    "for dir_str in data_files_list:\n",
    "    with open(dir_str, 'r') as text_file:\n",
    "        cnt = 0\n",
    "        instance = \"\"\n",
    "        for line in text_file:\n",
    "            if cnt < 9:\n",
    "                if cnt == 0:\n",
    "                    instance = line.split()[0]\n",
    "                    instance_dict[instance] = []\n",
    "                cnt += 1\n",
    "                continue\n",
    "            split_line = line.split()\n",
    "            instance_dict[instance].append([int(i) for i in split_line])\n",
    "        text_file.close()\n",
    "\n",
    "ng_dict = {}\n",
    "cnt = -1\n",
    "with open(\"ng_outs.csv\", 'r') as text_file:\n",
    "    for line in text_file:\n",
    "        if cnt < 2:\n",
    "            cnt += 1\n",
    "            continue\n",
    "        raw_line = line.strip()\n",
    "        split_line_list = raw_line.split(sep=\";\")\n",
    "        instance = split_line_list[3]\n",
    "        if instance not in ng_dict:\n",
    "            ng_dict[instance] = [[0 for i in range(101)]]\n",
    "        ng_dict[instance].append([0] + [int(i) for i in split_line_list[5:-1]])\n",
    "        if len(split_line_list[5:-1]) != 100:\n",
    "            print(\"case found for instance \"+instance)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import knn_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_list = []\n",
    "# for instance_name in ng_dict:\n",
    "#     y = torch.tensor(ng_dict[instance_name], dtype=torch.double)\n",
    "#     x = torch.tensor(instance_dict[instance_name], dtype=torch.double)\n",
    "#     attr = [[i] for i in range(n_edges)]\n",
    "#     loc_dict = {(i[0],j[0]): sqrt((i[1]-j[1])**2 + (i[2]-j[2])**2) for i in instance_dict[instance_name] for j in instance_dict[instance_name]}\n",
    "#     cnt = -1\n",
    "#     for i in range(101):\n",
    "#         for j in range(101):\n",
    "#             if i != j:\n",
    "#                 cnt += 1\n",
    "#                 attr[cnt].append(loc_dict[i,j])\n",
    "#     attr = torch.tensor(attr, dtype=torch.double)\n",
    "#     pos = []\n",
    "#     for i in instance_dict[instance_name]:\n",
    "#         pos.append([i[1], i[2]])\n",
    "#     pos = torch.tensor(pos, dtype=torch.double)\n",
    "#     data_list.append(Data(x=x, y=y, edge_index=edge_index, pos=pos, edge_attr=attr))\n",
    "\n",
    "data_list = []\n",
    "for instance_name in ng_dict:\n",
    "    y = torch.tensor(ng_dict[instance_name], dtype=torch.float)\n",
    "    x = torch.tensor(instance_dict[instance_name], dtype=torch.float)\n",
    "    pos = []\n",
    "    for i in instance_dict[instance_name]:\n",
    "        pos.append([i[1], i[2]])\n",
    "    pos = torch.tensor(pos, dtype=torch.double)\n",
    "    data_list.append(Data(x=x, y=y, edge_index = knn_graph(pos, 20), pos=pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Prepare data as in prototype</h2>\n",
    "Add text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "def add_edge_labels(graph):\n",
    "    transform = T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False)\n",
    "    return transform(graph)\n",
    "\n",
    "labeled_graphs = [add_edge_labels(graph) for graph in data_list]\n",
    "\n",
    "train_size = [g[0] for g in labeled_graphs]\n",
    "val_size = [g[1] for g in labeled_graphs]\n",
    "test_size = [g[2] for g in labeled_graphs]\n",
    "\n",
    "train_loader = DataLoader(train_size, batch_size=150, shuffle=True)\n",
    "val_loader = DataLoader(val_size, batch_size=150, shuffle=False)\n",
    "test_loader = DataLoader(test_size, batch_size=150, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Encode-decode routine</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ed = Net(data_list[0].num_features, 128, 64).to(device)\n",
    "optimizer = torch.optim.Adam(params=model_ed.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train(loader):\n",
    "    model_ed.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        z = model_ed.encode(batch.x, batch.edge_index)\n",
    "\n",
    "        # We perform a new round of negative sampling for every training epoch:\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=batch.edge_index, num_nodes=batch.num_nodes,\n",
    "            num_neg_samples=batch.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "        # Concat positive and negative edge indices.\n",
    "        edge_label_index = torch.cat(\n",
    "            [batch.edge_label_index, neg_edge_index],\n",
    "            dim=-1,\n",
    "        )\n",
    "        # Label for positive edges: 1, for negative edges: 0.\n",
    "        edge_label = torch.cat([\n",
    "            batch.edge_label,\n",
    "            batch.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "        ], dim=0)\n",
    "\n",
    "        # Note: The model is trained in a supervised manner using the given\n",
    "        # `edge_label_index` and `edge_label` targets.\n",
    "        out = model_ed.decode(z, edge_label_index).view(-1)\n",
    "        loss = criterion(out, edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model_ed.eval()\n",
    "    all_out = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in loader:\n",
    "        z = model_ed.encode(batch.x, batch.edge_index)\n",
    "        out = model_ed.decode(z, batch.edge_label_index).view(-1).sigmoid()\n",
    "        all_out.append(out.cpu().numpy())\n",
    "        all_labels.append(batch.edge_label.cpu().numpy())\n",
    "\n",
    "    all_out = np.concatenate(all_out)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return roc_auc_score(all_labels, all_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 3054.1865, Val: 0.5342, Test: 0.5341\n",
      "Epoch: 002, Loss: 19.2132, Val: 0.6063, Test: 0.6062\n",
      "Epoch: 003, Loss: 8.8700, Val: 0.6729, Test: 0.6726\n",
      "Epoch: 004, Loss: 5.2983, Val: 0.7367, Test: 0.7363\n",
      "Epoch: 005, Loss: 3.4569, Val: 0.7851, Test: 0.7852\n",
      "Final Test: 0.7852\n"
     ]
    }
   ],
   "source": [
    "# Train/Test Loop\n",
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 6):\n",
    "    loss = train(train_loader)\n",
    "    val_auc = test(val_loader)\n",
    "    test_auc = test(test_loader)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Check on values</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict = {}\n",
    "cnt = 0\n",
    "for graph in data_list:\n",
    "    z_raw = model_ed.encode(graph.x, graph.edge_index)\n",
    "    final_edge_index = model_ed.decode_all(z_raw)\n",
    "    fei = final_edge_index.tolist()\n",
    "    edges_pred = {k:[] for k in range(101)}\n",
    "    edges_pred_inv = {k:[] for k in range(101)}\n",
    "    for i in range(len(fei[0])):\n",
    "        edges_pred[fei[0][i]].append(fei[1][i])\n",
    "        edges_pred_inv[fei[1][i]].append(fei[0][i])\n",
    "    ts0 = graph.edge_index.tolist()\n",
    "    edges = {k:[] for k in range(101)}\n",
    "    edges_inv = {k:[] for k in range(101)}\n",
    "    for i in range(len(ts0[0])):\n",
    "        edges[ts0[0][i]].append(ts0[1][i])\n",
    "        edges_inv[ts0[1][i]].append(ts0[0][i])\n",
    "    originals = {}\n",
    "    predictions = {}\n",
    "    for i in range(101):\n",
    "        originals[i] = set(edges[i] + edges_inv[i])\n",
    "        predictions[i] = set(edges_pred[i] + edges_pred_inv[i])\n",
    "    graph_dict[cnt] = {\"real\": originals, \"preds\": predictions}\n",
    "    cnt += 1\n",
    "\n",
    "confusion_dict = {}\n",
    "true_positives = []\n",
    "false_positives = []\n",
    "true_negatives = []\n",
    "false_negatives = []\n",
    "for key in graph_dict:\n",
    "    real = graph_dict[key][\"real\"]\n",
    "    pred = graph_dict[key][\"preds\"]\n",
    "    node_matrix = {}\n",
    "    for i in range(101):\n",
    "        tp = len(pred[i].intersection(real[i]))\n",
    "        fp = len(pred[i] - real[i])\n",
    "        real_neg = set([j for j in range(101)]) - {i} - real[i]\n",
    "        pred_neg = set([j for j in range(101)]) - {i} - pred[i]\n",
    "        tn = len(pred_neg.intersection(real_neg))\n",
    "        fn = len(pred_neg - real_neg)\n",
    "        total = tp + fp + tn + fn\n",
    "        node_matrix[i] = {\"tp\": tp, \"fp\": fp, \"tn\": tn, \"fn\": fn, \"total\": total}\n",
    "        true_positives.append(tp/total)\n",
    "        false_positives.append(fp/total)\n",
    "        true_negatives.append(tn/total)\n",
    "        false_negatives.append(fn/total)\n",
    "    confusion_dict[key] = node_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positives mean=0.25, stdev=0.04\n",
      "false positives mean=0.59, stdev=0.21\n",
      "true negatives mean=0.16, stdev=0.21\n",
      "false negatives mean=0.00, stdev=0.01\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, stdev\n",
    "print(\"true positives mean={0:.2f}, stdev={1:.2f}\".format(mean(true_positives), stdev(true_positives)))\n",
    "print(\"false positives mean={0:.2f}, stdev={1:.2f}\".format(mean(false_positives), stdev(false_positives)))\n",
    "print(\"true negatives mean={0:.2f}, stdev={1:.2f}\".format(mean(true_negatives), stdev(true_negatives)))\n",
    "print(\"false negatives mean={0:.2f}, stdev={1:.2f}\".format(mean(false_negatives), stdev(false_negatives)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>NgLearning</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "data_raw_dict  = {\n",
    "     \"positives\": [],\n",
    "     \"negatives\": []\n",
    "}\n",
    "\n",
    "cnt = 0\n",
    "pos_cnt = 0\n",
    "neg_cnt = 0\n",
    "indices = [i for i in range(len(data_list))]\n",
    "for idx in sample(indices, 5000):\n",
    "    graph = data_list[idx]\n",
    "    neg_cnt = 0\n",
    "    ng_matrix = graph.y.tolist()\n",
    "    encoding_matrix = model_ed.encode(graph.x, graph.edge_index).tolist()\n",
    "    for i in range(101):\n",
    "        for j in range(101):\n",
    "            if i == j:\n",
    "                # if pos_cnt < 80000:\n",
    "                #     data_raw_dict[\"positives\"].append(encoding_matrix[i]+encoding_matrix[i])\n",
    "                #     pos_cnt += 1\n",
    "                continue\n",
    "            else:\n",
    "                if ng_matrix[i][j] > 0.5:\n",
    "                        data_raw_dict[\"positives\"].append(encoding_matrix[i]+encoding_matrix[j]+[1])\n",
    "                        pos_cnt += 1\n",
    "                else:\n",
    "                    data_raw_dict[\"negatives\"].append(encoding_matrix[i]+encoding_matrix[j]+[0])\n",
    "                    neg_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tensor = sample(data_raw_dict[\"positives\"], 20000) + sample(data_raw_dict[\"negatives\"], 20000)\n",
    "main_tensor = torch.tensor(pre_tensor, dtype=torch.float32)\n",
    "\n",
    "main_tensor = main_tensor[torch.randperm(main_tensor.size()[0])]\n",
    "labels = main_tensor[:,-1:]\n",
    "embeddings = main_tensor[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Learning stage</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Deep(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(128, 128)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(64, 64)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import copy\n",
    "\n",
    "def model_train(model, X_train, y_train, X_val, y_val):\n",
    "    # loss function and optimizer\n",
    "    loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    " \n",
    "    n_epochs = 10   # number of epochs to run\n",
    "    batch_size = 500  # size of each batch\n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
    " \n",
    "    # Hold the best model\n",
    "    best_acc = - np.inf   # init to negative infinity\n",
    "    best_weights = None\n",
    " \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "                # take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "                # forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "                # print progress\n",
    "                acc = (y_pred.round() == y_batch).float().mean()\n",
    "                bar.set_postfix(\n",
    "                    loss=float(loss),\n",
    "                    acc=float(acc)\n",
    "                )\n",
    "        # evaluate accuracy at end of each epoch\n",
    "        model.eval()\n",
    "        y_pred = model(X_val)\n",
    "        acc = (y_pred.round() == y_val).float().mean()\n",
    "        acc = float(acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "    # restore model and return best accuracy\n",
    "    model.load_state_dict(best_weights)\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (deep): 0.82\n",
      "Accuracy (deep): 0.82\n",
      "Accuracy (deep): 0.82\n",
      "Accuracy (deep): 0.82\n",
      "Accuracy (deep): 0.83\n",
      "Deep: 82.31% (+/- 0.37%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "# train-test split: Hold out the test set for final model evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, train_size=0.7, shuffle=True)\n",
    " \n",
    "# define 5-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "cv_scores_deep = []\n",
    "for train, test in kfold.split(X_train, y_train):\n",
    "    # create model, train, and get accuracy\n",
    "    model_ls = Deep()\n",
    "    acc = model_train(model_ls, X_train[train], y_train[train], X_train[test], y_train[test])\n",
    "    print(\"Accuracy (deep): %.2f\" % acc)\n",
    "    cv_scores_deep.append(acc)\n",
    " \n",
    "# evaluate the model\n",
    "deep_acc = np.mean(cv_scores_deep)\n",
    "deep_std = np.std(cv_scores_deep)\n",
    "print(\"Deep: %.2f%% (+/- %.2f%%)\" % (deep_acc*100, deep_std*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model accuracy: 82.84%\n"
     ]
    }
   ],
   "source": [
    "model_ls = Deep()\n",
    "acc = model_train(model_ls, X_train, y_train, X_test, y_test)\n",
    "print(f\"Final model accuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.34824884 -0.36458543  0.34209353 -0.40368187  0.44998032 -0.6455222\n",
      " -0.24943261 -0.16076028  0.81623566  0.32858828  0.1401279   0.02790643\n",
      " -0.64043134 -0.74666744 -0.0035337  -0.56267107  0.25050822 -0.22999151\n",
      " -0.17261726  0.04425763 -0.3583087  -0.9353499  -0.15694048  0.81424946\n",
      "  0.0312843  -0.3942736  -0.56929785 -0.21066405  0.14455457 -0.38482082\n",
      " -0.0708671  -0.03757123 -0.4867992  -0.9206139   0.33374938 -0.03163411\n",
      " -0.20573357 -1.358158   -0.11647518  0.32746667  0.5777776  -0.40665197\n",
      " -0.00517291  0.31417698  0.18779095 -0.8299682  -0.33597237  0.03479974\n",
      "  0.11010712 -0.17035238  0.02037082  0.73113555 -0.25327745 -0.30412978\n",
      " -0.03329861 -0.6088855  -0.9155575  -0.09249165 -0.36688438  0.00162618\n",
      " -0.3377703   0.556208    0.49177033 -0.24819893  0.19426002 -0.54047716\n",
      "  0.37577215 -0.4197988   0.5189862  -0.6187207  -0.19632323 -0.12735264\n",
      "  0.816805    0.4287169   0.16433594 -0.03567228 -0.619009   -0.6863785\n",
      "  0.10873306 -0.6471885   0.20190006 -0.21122718 -0.10554066 -0.00596702\n",
      " -0.2845598  -1.0143473  -0.16630629  0.86419266  0.2034442  -0.42420134\n",
      " -0.5142317  -0.18075918  0.05980136 -0.39325628 -0.03081454 -0.08215278\n",
      " -0.6515412  -1.0129685   0.4018265  -0.02829049 -0.2018546  -1.4432591\n",
      " -0.2544509   0.44704956  0.7627045  -0.4489553   0.02741461  0.30555114\n",
      "  0.17020941 -0.6767728  -0.34724483  0.15725508  0.12158114 -0.20254296\n",
      "  0.12783906  0.7191715  -0.34316835 -0.41501004 -0.17277412 -0.65141\n",
      " -0.93202096 -0.0086002  -0.45195657  0.12164146 -0.3848927   0.549278\n",
      "  0.4882732  -0.429573  ] -> [0.82793015] (expected [1.])\n",
      "[ 1.2162979   0.81219953  0.6632457   0.09240326  0.23413056  1.3817507\n",
      " -0.43230605  0.7622817   0.20545879 -1.5443293   0.37127125  1.0140673\n",
      " -0.02272385 -0.51784366 -0.07077367  0.36071825 -0.78633034 -0.3500301\n",
      "  1.068275    0.57162917  0.07283896  1.0571401   0.75936604 -0.2291407\n",
      "  0.814704   -1.1107526   0.58915555  0.99491256  0.7207367  -0.37277493\n",
      " -0.43219307 -1.188686    0.1482864   1.2998043   0.212704   -0.7602303\n",
      "  1.1370013   0.3174182   0.6344584   0.49857235 -0.96703273  0.5668205\n",
      "  0.8930001  -0.3700149   0.37316304  1.0894284   0.35687396 -1.2182144\n",
      " -0.05455705 -1.0428135  -0.5338799  -0.15293108 -0.542704    0.21488109\n",
      "  0.20420358  0.21431515 -0.4403456   0.16223386 -0.62673545 -0.91851926\n",
      " -0.51586366 -1.7867157  -1.510278    1.6215539   1.2162979   0.81219953\n",
      "  0.6632456   0.09240325  0.23413058  1.3817507  -0.43230605  0.7622817\n",
      "  0.20545879 -1.5443292   0.37127125  1.0140673  -0.02272384 -0.51784366\n",
      " -0.07077368  0.36071825 -0.78633034 -0.3500301   1.068275    0.57162917\n",
      "  0.07283896  1.0571401   0.75936604 -0.22914067  0.814704   -1.1107526\n",
      "  0.58915555  0.99491256  0.7207367  -0.37277496 -0.4321931  -1.188686\n",
      "  0.1482864   1.2998043   0.21270402 -0.7602303   1.1370013   0.3174182\n",
      "  0.63445836  0.49857232 -0.96703273  0.5668205   0.8930001  -0.3700149\n",
      "  0.37316304  1.0894284   0.35687393 -1.2182144  -0.05455705 -1.0428135\n",
      " -0.5338798  -0.15293108 -0.542704    0.21488112  0.20420356  0.21431515\n",
      " -0.4403456   0.16223386 -0.62673545 -0.9185194  -0.51586366 -1.7867157\n",
      " -1.510278    1.6215539 ] -> [0.8789789] (expected [1.])\n",
      "[ 5.8547872e-01  1.1318297e+00 -1.9831786e-01  7.6726824e-01\n",
      " -5.6297797e-01  1.0553844e-01 -1.9262306e-01  7.9116076e-02\n",
      " -6.1821640e-01 -7.1073693e-01  2.1122530e-01  2.8147906e-01\n",
      "  2.6032290e-01  4.1328979e-01 -4.5300624e-01  8.7886941e-01\n",
      " -4.4692449e-02  2.4200082e-01 -3.6139220e-01  4.1214147e-01\n",
      " -3.9582759e-01  1.0732375e+00  3.6617395e-01 -1.3906429e+00\n",
      " -1.3670986e+00  6.1032051e-01 -2.8165114e-01 -4.5414439e-01\n",
      "  2.7930722e-01  3.8300550e-01 -3.7510747e-01  3.5102955e-01\n",
      "  1.1108956e+00  1.0917157e+00 -1.0329398e+00  1.8919376e-01\n",
      " -4.7718531e-01  9.9602401e-01  6.3183981e-01 -1.1741198e+00\n",
      " -1.0103047e+00  1.1018641e+00 -6.8291229e-01 -2.4929364e-01\n",
      " -5.1574528e-01 -1.2963946e-01  1.7925268e-01 -5.1916909e-01\n",
      " -2.7829328e-01  4.0112036e-01 -1.0246062e+00 -9.1535771e-01\n",
      "  4.0557021e-01  8.8586533e-01  6.5687168e-01  7.8914624e-01\n",
      "  7.0639777e-01 -4.6749339e-03  1.1144272e+00 -8.4816462e-01\n",
      "  1.9142906e-01 -3.2407841e-01 -3.4599251e-01  1.0174131e+00\n",
      "  4.3005502e-01  1.1970426e+00 -2.6024327e-01  8.1014073e-01\n",
      " -5.1601374e-01  2.9163513e-02 -1.3542233e-01  1.9080396e-01\n",
      " -7.3029268e-01 -7.4942881e-01  2.1459061e-01  3.4114695e-01\n",
      "  2.6998645e-01  4.7408661e-01 -5.6322920e-01  9.3792570e-01\n",
      " -8.6306967e-02  3.8107428e-01 -3.5666335e-01  4.0400702e-01\n",
      " -3.0593142e-01  1.0932399e+00  3.0452231e-01 -1.3822572e+00\n",
      " -1.3519449e+00  6.3286245e-01 -2.7214116e-01 -4.5138302e-01\n",
      "  1.8323594e-01  5.3936309e-01 -3.4419727e-01  2.1999241e-01\n",
      "  1.1052001e+00  1.1947534e+00 -1.0218159e+00  2.3293149e-01\n",
      " -5.6488317e-01  1.1775179e+00  5.2235448e-01 -1.1816750e+00\n",
      " -1.0856771e+00  1.3002248e+00 -7.6019275e-01 -2.3810910e-01\n",
      " -4.9670985e-01 -1.6237646e-02  1.3678430e-01 -5.0184149e-01\n",
      " -3.6025938e-01  3.6069685e-01 -1.0473320e+00 -1.0506262e+00\n",
      "  4.0770108e-01  8.2017148e-01  5.5610180e-01  8.4969401e-01\n",
      "  8.0247116e-01  4.4602901e-04  1.1118397e+00 -8.2451457e-01\n",
      "  2.5269338e-01 -2.4999940e-01 -3.3469105e-01  8.7672472e-01] -> [0.6335486] (expected [0.])\n",
      "[-0.2900003   0.5184649  -0.20532274  0.7861287  -0.38457033  0.34073347\n",
      " -0.27667338  0.2889157  -0.7140033  -0.63330317  0.23567906  0.19595797\n",
      "  0.423802    0.7060254   0.036123    1.1756334  -0.35884976  0.21743552\n",
      " -0.0229692   0.7169448   0.13283263  1.1416091   0.49344015 -1.3610694\n",
      " -0.7078916   0.24464057  0.06956778 -0.2813023  -0.48318046  0.1803984\n",
      " -0.28702295  0.42158613  0.42416647  0.9543617  -0.74859846  0.03419836\n",
      " -0.03058008  0.31072816  0.19686848 -0.83904636 -0.24294677  0.8445854\n",
      " -0.43735936  0.04136432 -0.8932051   0.82988405 -0.04391998 -0.03887289\n",
      "  0.00650765 -0.25720152 -0.5389747  -0.770879    0.49172878  0.46910217\n",
      " -0.10245103  0.41081995  0.5382097   0.58095646  0.9405831  -0.25513172\n",
      " -0.17888753 -0.57493937 -0.16583396  0.41129228 -0.2710116   0.5173699\n",
      " -0.19399074  0.7259499  -0.40456137  0.42435718 -0.2543099   0.24907418\n",
      " -0.73035085 -0.6682074   0.2181415   0.20873134  0.42720148  0.65573794\n",
      "  0.03645328  1.2050833  -0.350753    0.23595227  0.07086651  0.74174714\n",
      "  0.1662466   1.150368    0.5230633  -1.3925886  -0.64773715  0.19888632\n",
      "  0.14285693 -0.24816582 -0.4813667   0.11307237 -0.31453943  0.42441127\n",
      "  0.38556153  0.96905434 -0.79439765 -0.08071978  0.01974325  0.2970225\n",
      "  0.23366597 -0.8151213  -0.21196046  0.84109694 -0.35848492  0.02946879\n",
      " -0.93367213  0.8814897  -0.03787649 -0.12702414  0.01660577 -0.22597489\n",
      " -0.5461934  -0.7248562   0.49810034  0.45412907 -0.11941985  0.4160133\n",
      "  0.54079694  0.5976644   0.8914027  -0.28088373 -0.19877683 -0.64163816\n",
      " -0.25137767  0.4066538 ] -> [0.5984512] (expected [1.])\n",
      "[ 9.29670513e-01 -1.32725209e-01  5.82583249e-02 -1.92243621e-01\n",
      " -2.42903113e-01 -5.16754150e-01 -4.00128663e-01 -2.82525301e-01\n",
      "  8.96855712e-01  3.15868676e-01 -6.35249093e-02  8.63972306e-02\n",
      " -7.02001452e-01 -8.14190388e-01 -3.18785608e-01 -1.42232955e-01\n",
      " -1.89200997e-01 -2.83209421e-02 -3.42895359e-01  4.90169004e-02\n",
      " -7.40715086e-01 -5.36799610e-01 -5.84819466e-02  7.09113836e-01\n",
      " -5.19737005e-01 -3.27388227e-01 -6.55465722e-01 -3.55430059e-02\n",
      "  4.67004478e-01 -4.21563715e-01 -3.01600009e-01 -3.12054336e-01\n",
      "  3.03014427e-01 -1.33872852e-01  6.15190744e-01 -3.97432864e-01\n",
      "  9.68365520e-02 -6.51055932e-01  2.66105294e-01  4.42248017e-01\n",
      "  9.35989022e-02 -3.53971720e-01  1.43854797e-01 -1.10816427e-01\n",
      " -8.40380415e-02 -1.31501675e+00 -2.29146823e-01 -4.12059307e-01\n",
      "  6.97103381e-01  2.28047460e-01  5.35341538e-02  3.84899259e-01\n",
      " -3.37170064e-01 -4.16932970e-01  1.99410170e-01 -4.88903403e-01\n",
      " -9.10300732e-01 -4.63871717e-01 -6.95788637e-02 -5.31469658e-02\n",
      " -4.81526375e-01  8.70010853e-01 -9.07959491e-02  1.24504030e-01\n",
      "  7.81756639e-01  1.37117937e-01 -2.06816196e-01 -7.45414972e-01\n",
      " -3.46874982e-01 -9.23773408e-01 -5.14127791e-01 -4.66719568e-02\n",
      "  5.10734618e-01  3.34541142e-01 -6.31228626e-01 -1.10701747e-01\n",
      " -8.84352624e-02 -4.77275550e-01 -1.29921108e-01  8.68159533e-02\n",
      " -1.14763252e-01  2.95465201e-01 -4.45201099e-01  5.00013605e-02\n",
      " -7.97961771e-01 -6.11565173e-01  1.48824155e-01  1.30210233e+00\n",
      " -6.46700084e-01 -2.80782640e-01 -5.83146065e-02  3.26694787e-01\n",
      "  1.85371768e-02 -5.19303679e-01 -4.29333925e-01 -8.73601735e-02\n",
      " -1.88552216e-03 -3.50046635e-01  5.39245546e-01 -4.76956964e-01\n",
      "  4.55913879e-02 -8.91137362e-01  2.61951610e-02  7.11116791e-01\n",
      "  6.35026634e-01 -5.76701224e-01  1.19249232e-01  7.77249783e-02\n",
      " -1.53369248e-01 -1.97411025e+00 -5.43202281e-01 -3.14243793e-01\n",
      "  9.85068142e-01 -8.74807984e-02  1.75997749e-01  6.55731559e-01\n",
      " -9.09703672e-02 -4.05595094e-01 -1.02381967e-01 -9.57517684e-01\n",
      " -1.33635199e+00 -6.55395687e-01 -3.53595108e-01  9.55953002e-02\n",
      " -8.40794444e-01  7.98455119e-01 -1.19908944e-01 -9.91486087e-02] -> [0.49280652] (expected [0.])\n"
     ]
    }
   ],
   "source": [
    "model_ls.eval()\n",
    "with torch.no_grad():\n",
    "    # Test out inference with 5 samples\n",
    "    for i in range(5):\n",
    "        y_pred = model_ls(X_test[i:i+1])\n",
    "        print(f\"{X_test[i].numpy()} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on instance 428"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_raw = model_ed.encode(data_list[428].x, data_list[428].edge_index)\n",
    "preds = {}\n",
    "for i in range(len(z_raw)):\n",
    "    for j in range(len(z_raw)):\n",
    "        if i != j:\n",
    "            node_i = z_raw[i].tolist()\n",
    "            node_j = z_raw[j].tolist()\n",
    "            target = data_list[428].y[i][j]\n",
    "            preds[i,j] = {\"pred\":model_ls(torch.tensor(node_i+node_j, dtype=torch.float)),\"target\":target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred': tensor([0.1005], grad_fn=<SigmoidBackward0>), 'target': tensor(0.)}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[6,92]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: tensor([21.5312], grad_fn=<AddBackward0>), 1: tensor([26.0008], grad_fn=<AddBackward0>), 2: tensor([40.3351], grad_fn=<AddBackward0>), 3: tensor([35.5938], grad_fn=<AddBackward0>), 4: tensor([38.9639], grad_fn=<AddBackward0>), 5: tensor([32.8199], grad_fn=<AddBackward0>), 6: tensor([26.7021], grad_fn=<AddBackward0>), 7: tensor([30.9510], grad_fn=<AddBackward0>), 8: tensor([21.5312], grad_fn=<AddBackward0>), 9: tensor([39.9153], grad_fn=<AddBackward0>), 10: tensor([35.4807], grad_fn=<AddBackward0>), 11: tensor([31.6647], grad_fn=<AddBackward0>), 12: tensor([27.4517], grad_fn=<AddBackward0>), 13: tensor([36.6048], grad_fn=<AddBackward0>), 14: tensor([38.6634], grad_fn=<AddBackward0>), 15: tensor([36.5882], grad_fn=<AddBackward0>), 16: tensor([22.8904], grad_fn=<AddBackward0>), 17: tensor([34.0399], grad_fn=<AddBackward0>), 18: tensor([37.7751], grad_fn=<AddBackward0>), 19: tensor([29.0070], grad_fn=<AddBackward0>), 20: tensor([35.7476], grad_fn=<AddBackward0>), 21: tensor([25.0341], grad_fn=<AddBackward0>), 22: tensor([39.9153], grad_fn=<AddBackward0>), 23: tensor([31.6704], grad_fn=<AddBackward0>), 24: tensor([28.3887], grad_fn=<AddBackward0>), 25: tensor([36.9242], grad_fn=<AddBackward0>), 26: tensor([38.8030], grad_fn=<AddBackward0>), 27: tensor([26.0023], grad_fn=<AddBackward0>), 28: tensor([24.3387], grad_fn=<AddBackward0>), 29: tensor([37.6493], grad_fn=<AddBackward0>), 30: tensor([27.4104], grad_fn=<AddBackward0>), 31: tensor([31.3095], grad_fn=<AddBackward0>), 32: tensor([34.4059], grad_fn=<AddBackward0>), 33: tensor([33.5196], grad_fn=<AddBackward0>), 34: tensor([38.0798], grad_fn=<AddBackward0>), 35: tensor([38.8030], grad_fn=<AddBackward0>), 36: tensor([34.1982], grad_fn=<AddBackward0>), 37: tensor([37.7751], grad_fn=<AddBackward0>), 38: tensor([38.3859], grad_fn=<AddBackward0>), 39: tensor([27.9884], grad_fn=<AddBackward0>), 40: tensor([27.0921], grad_fn=<AddBackward0>), 41: tensor([25.5340], grad_fn=<AddBackward0>), 42: tensor([25.1645], grad_fn=<AddBackward0>), 43: tensor([37.9278], grad_fn=<AddBackward0>), 44: tensor([30.5627], grad_fn=<AddBackward0>), 45: tensor([36.6860], grad_fn=<AddBackward0>), 46: tensor([26.3654], grad_fn=<AddBackward0>), 47: tensor([25.6849], grad_fn=<AddBackward0>), 48: tensor([32.1773], grad_fn=<AddBackward0>), 49: tensor([36.5954], grad_fn=<AddBackward0>), 50: tensor([34.9495], grad_fn=<AddBackward0>), 51: tensor([32.4365], grad_fn=<AddBackward0>), 52: tensor([38.8824], grad_fn=<AddBackward0>), 53: tensor([36.6560], grad_fn=<AddBackward0>), 54: tensor([23.2884], grad_fn=<AddBackward0>), 55: tensor([28.9480], grad_fn=<AddBackward0>), 56: tensor([28.5208], grad_fn=<AddBackward0>), 57: tensor([32.2859], grad_fn=<AddBackward0>), 58: tensor([26.0023], grad_fn=<AddBackward0>), 59: tensor([33.5196], grad_fn=<AddBackward0>), 60: tensor([28.0710], grad_fn=<AddBackward0>), 61: tensor([36.9242], grad_fn=<AddBackward0>), 62: tensor([37.9664], grad_fn=<AddBackward0>), 63: tensor([26.1887], grad_fn=<AddBackward0>), 64: tensor([37.2187], grad_fn=<AddBackward0>), 65: tensor([27.8951], grad_fn=<AddBackward0>), 66: tensor([30.4790], grad_fn=<AddBackward0>), 67: tensor([37.6237], grad_fn=<AddBackward0>), 68: tensor([25.1645], grad_fn=<AddBackward0>), 69: tensor([28.7310], grad_fn=<AddBackward0>), 70: tensor([37.3803], grad_fn=<AddBackward0>), 71: tensor([33.5803], grad_fn=<AddBackward0>), 72: tensor([26.7441], grad_fn=<AddBackward0>), 73: tensor([22.8904], grad_fn=<AddBackward0>), 74: tensor([28.2594], grad_fn=<AddBackward0>), 75: tensor([33.5803], grad_fn=<AddBackward0>), 76: tensor([40.3499], grad_fn=<AddBackward0>), 77: tensor([33.9441], grad_fn=<AddBackward0>), 78: tensor([38.4306], grad_fn=<AddBackward0>), 79: tensor([21.8017], grad_fn=<AddBackward0>), 80: tensor([36.5140], grad_fn=<AddBackward0>), 81: tensor([38.7078], grad_fn=<AddBackward0>), 82: tensor([40.3351], grad_fn=<AddBackward0>), 83: tensor([25.0341], grad_fn=<AddBackward0>), 84: tensor([29.8413], grad_fn=<AddBackward0>), 85: tensor([34.5230], grad_fn=<AddBackward0>), 86: tensor([37.6237], grad_fn=<AddBackward0>), 87: tensor([30.9644], grad_fn=<AddBackward0>), 88: tensor([28.9768], grad_fn=<AddBackward0>), 89: tensor([34.8096], grad_fn=<AddBackward0>), 90: tensor([31.2476], grad_fn=<AddBackward0>), 91: tensor([29.9553], grad_fn=<AddBackward0>), 92: tensor([37.6836], grad_fn=<AddBackward0>), 93: tensor([39.4978], grad_fn=<AddBackward0>), 94: tensor([23.8441], grad_fn=<AddBackward0>), 95: tensor([34.0516], grad_fn=<AddBackward0>), 96: tensor([27.8310], grad_fn=<AddBackward0>), 97: tensor([36.5954], grad_fn=<AddBackward0>), 98: tensor([23.7485], grad_fn=<AddBackward0>), 99: tensor([27.4104], grad_fn=<AddBackward0>), 100: tensor([34.8543], grad_fn=<AddBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "sums = {}\n",
    "for i in range(101):\n",
    "    sum_i = 0\n",
    "    for j in range(101):\n",
    "        if i != j:\n",
    "            sum_i += preds[i,j][\"pred\"]\n",
    "    sums[i] = sum_i\n",
    "print(sums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still space for improvement. I'd consider more data, undirected case for encoding and add tw into the structure of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
